12/17 01:17:09 AM Namespace(auto_aug=False, batch_size=96, ckpt_dir='./checkpoints/', classes=10, cutout=False, cutout_length=16, data_root='./dataset/', dataset='cifar10', device=device(type='cuda'), epochs=600, exp_name='spos_c10_train_supernet', layers=20, learning_rate=0.025, momentum=0.9, num_choices=4, print_freq=100, resize=False, seed=0, val_interval=5, weight_decay=0.0003)
Files already downloaded and verified
Files already downloaded and verified
12/17 01:17:11 AM SinglePath_OneShot(
  (stem): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (choice_block): ModuleList(
    (0): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
          (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(32, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (1): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (2): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (3): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (4): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=64, bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=64, bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (11): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
    )
    (5): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (11): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (6): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (11): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (7): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=80, bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (6): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (11): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (8): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(160, 160, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(160, 160, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
        (cb_proj): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
        )
      )
    )
    (9): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (10): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (11): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (12): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (13): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (14): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (15): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)
          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (6): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (11): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (16): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (11): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(320, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (17): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (11): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (18): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (11): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
    (19): ModuleList(
      (0): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (1): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (2): Choice_Block(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=320, bias=False)
          (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): ReLU(inplace=True)
        )
      )
      (3): Choice_Block_x(
        (cb_main): Sequential(
          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (6): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (7): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
          (11): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (12): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (13): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (14): ReLU(inplace=True)
        )
      )
    )
  )
  (last_conv): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (global_pooling): AdaptiveAvgPool2d(output_size=1)
  (classifier): Linear(in_features=1024, out_features=10, bias=False)
)


12/17 01:17:17 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 001/521, train_loss: 2.307(2.307), train_acc: 3.125(3.125)
12/17 01:17:24 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 101/521, train_loss: 2.234(2.365), train_acc: 15.625(11.489)
12/17 01:17:31 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 201/521, train_loss: 2.313(2.318), train_acc: 12.500(12.246)
12/17 01:17:37 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 301/521, train_loss: 2.271(2.290), train_acc: 12.500(13.140)
12/17 01:17:44 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 401/521, train_loss: 2.167(2.272), train_acc: 10.417(13.856)
12/17 01:17:50 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 501/521, train_loss: 2.218(2.256), train_acc: 17.708(14.471)
12/17 01:17:51 AM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 521/521, train_loss: 2.185(2.253), train_acc: 23.750(14.570)
12/17 01:17:51 AM [Supernet Training] epoch: 001, train_loss: 2.253, train_acc: 14.570
12/17 01:17:53 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:17:53 AM [Supernet Validation] epoch: 001, val_loss: 2.397, val_acc: 15.190, best_acc: 15.190


12/17 01:17:53 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 001/521, train_loss: 2.147(2.147), train_acc: 19.792(19.792)
12/17 01:18:00 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 101/521, train_loss: 2.136(2.144), train_acc: 19.792(19.028)
12/17 01:18:06 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 201/521, train_loss: 2.057(2.135), train_acc: 16.667(19.123)
12/17 01:18:13 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 301/521, train_loss: 2.016(2.116), train_acc: 29.167(19.982)
12/17 01:18:19 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 401/521, train_loss: 2.184(2.098), train_acc: 17.708(20.820)
12/17 01:18:25 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 501/521, train_loss: 1.959(2.078), train_acc: 29.167(21.565)
12/17 01:18:27 AM [Supernet Training] lr: 0.02496 epoch: 002/600, step: 521/521, train_loss: 1.963(2.075), train_acc: 22.500(21.618)
12/17 01:18:27 AM [Supernet Training] epoch: 002, train_loss: 2.075, train_acc: 21.618
12/17 01:18:28 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:18:28 AM [Supernet Validation] epoch: 002, val_loss: 2.211, val_acc: 22.200, best_acc: 22.200


12/17 01:18:29 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 001/521, train_loss: 2.267(2.267), train_acc: 19.792(19.792)
12/17 01:18:35 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 101/521, train_loss: 1.955(1.972), train_acc: 26.042(25.278)
12/17 01:18:41 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 201/521, train_loss: 1.903(1.946), train_acc: 27.083(26.150)
12/17 01:18:48 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 301/521, train_loss: 1.883(1.933), train_acc: 35.417(26.883)
12/17 01:18:54 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 401/521, train_loss: 1.911(1.924), train_acc: 23.958(27.073)
12/17 01:19:00 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 501/521, train_loss: 1.887(1.918), train_acc: 30.208(27.347)
12/17 01:19:02 AM [Supernet Training] lr: 0.02492 epoch: 003/600, step: 521/521, train_loss: 1.801(1.917), train_acc: 23.750(27.392)
12/17 01:19:02 AM [Supernet Training] epoch: 003, train_loss: 1.917, train_acc: 27.392
12/17 01:19:03 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:19:03 AM [Supernet Validation] epoch: 003, val_loss: 1.957, val_acc: 26.720, best_acc: 26.720


12/17 01:19:04 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 001/521, train_loss: 1.793(1.793), train_acc: 36.458(36.458)
12/17 01:19:10 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 101/521, train_loss: 1.842(1.865), train_acc: 32.292(29.187)
12/17 01:19:16 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 201/521, train_loss: 1.755(1.861), train_acc: 30.208(29.607)
12/17 01:19:23 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 301/521, train_loss: 1.934(1.858), train_acc: 30.208(29.596)
12/17 01:19:30 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 401/521, train_loss: 1.828(1.851), train_acc: 29.167(30.021)
12/17 01:19:36 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 501/521, train_loss: 1.946(1.847), train_acc: 23.958(30.229)
12/17 01:19:37 AM [Supernet Training] lr: 0.02488 epoch: 004/600, step: 521/521, train_loss: 1.736(1.846), train_acc: 32.500(30.252)
12/17 01:19:37 AM [Supernet Training] epoch: 004, train_loss: 1.846, train_acc: 30.252
12/17 01:19:39 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:19:39 AM [Supernet Validation] epoch: 004, val_loss: 1.901, val_acc: 29.450, best_acc: 29.450


12/17 01:19:40 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 001/521, train_loss: 1.927(1.927), train_acc: 28.125(28.125)
12/17 01:19:46 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 101/521, train_loss: 1.606(1.816), train_acc: 36.458(31.178)
12/17 01:19:52 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 201/521, train_loss: 1.836(1.818), train_acc: 30.208(31.105)
12/17 01:19:59 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 301/521, train_loss: 1.864(1.808), train_acc: 31.250(31.568)
12/17 01:20:06 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 401/521, train_loss: 1.630(1.801), train_acc: 38.542(32.024)
12/17 01:20:12 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 501/521, train_loss: 1.610(1.795), train_acc: 40.625(32.123)
12/17 01:20:13 AM [Supernet Training] lr: 0.02483 epoch: 005/600, step: 521/521, train_loss: 1.738(1.792), train_acc: 31.250(32.270)
12/17 01:20:13 AM [Supernet Training] epoch: 005, train_loss: 1.792, train_acc: 32.270
12/17 01:20:15 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:20:15 AM [Supernet Validation] epoch: 005, val_loss: 1.835, val_acc: 32.720, best_acc: 32.720


12/17 01:20:15 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 001/521, train_loss: 1.877(1.877), train_acc: 36.458(36.458)
12/17 01:20:22 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 101/521, train_loss: 1.819(1.763), train_acc: 35.417(32.972)
12/17 01:20:28 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 201/521, train_loss: 1.842(1.749), train_acc: 33.333(33.872)
12/17 01:20:34 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 301/521, train_loss: 1.789(1.742), train_acc: 36.458(34.368)
12/17 01:20:41 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 401/521, train_loss: 1.666(1.728), train_acc: 40.625(34.848)
12/17 01:20:47 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 501/521, train_loss: 1.834(1.724), train_acc: 27.083(34.997)
12/17 01:20:48 AM [Supernet Training] lr: 0.02479 epoch: 006/600, step: 521/521, train_loss: 1.725(1.722), train_acc: 27.500(35.062)
12/17 01:20:48 AM [Supernet Training] epoch: 006, train_loss: 1.722, train_acc: 35.062
12/17 01:20:50 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:20:50 AM [Supernet Validation] epoch: 006, val_loss: 1.747, val_acc: 34.220, best_acc: 34.220


12/17 01:20:50 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 001/521, train_loss: 1.471(1.471), train_acc: 45.833(45.833)
12/17 01:20:57 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 101/521, train_loss: 1.653(1.689), train_acc: 42.708(37.098)
12/17 01:21:03 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 201/521, train_loss: 1.455(1.686), train_acc: 41.667(36.666)
12/17 01:21:09 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 301/521, train_loss: 1.716(1.679), train_acc: 41.667(36.995)
12/17 01:21:15 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 401/521, train_loss: 1.638(1.675), train_acc: 37.500(37.058)
12/17 01:21:22 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 501/521, train_loss: 1.513(1.672), train_acc: 37.500(37.086)
12/17 01:21:23 AM [Supernet Training] lr: 0.02475 epoch: 007/600, step: 521/521, train_loss: 1.601(1.672), train_acc: 40.000(37.104)
12/17 01:21:23 AM [Supernet Training] epoch: 007, train_loss: 1.672, train_acc: 37.104
12/17 01:21:25 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:21:25 AM [Supernet Validation] epoch: 007, val_loss: 1.719, val_acc: 35.910, best_acc: 35.910


12/17 01:21:25 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 001/521, train_loss: 1.527(1.527), train_acc: 45.833(45.833)
12/17 01:21:31 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 101/521, train_loss: 1.683(1.647), train_acc: 40.625(38.655)
12/17 01:21:38 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 201/521, train_loss: 1.708(1.635), train_acc: 32.292(38.676)
12/17 01:21:44 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 301/521, train_loss: 1.656(1.633), train_acc: 39.583(38.895)
12/17 01:21:50 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 401/521, train_loss: 1.723(1.627), train_acc: 30.208(39.144)
12/17 01:21:56 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 501/521, train_loss: 1.469(1.625), train_acc: 54.167(39.282)
12/17 01:21:58 AM [Supernet Training] lr: 0.02471 epoch: 008/600, step: 521/521, train_loss: 1.580(1.625), train_acc: 42.500(39.302)
12/17 01:21:58 AM [Supernet Training] epoch: 008, train_loss: 1.625, train_acc: 39.302
12/17 01:21:59 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:21:59 AM [Supernet Validation] epoch: 008, val_loss: 1.682, val_acc: 37.940, best_acc: 37.940


12/17 01:22:00 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 001/521, train_loss: 1.667(1.667), train_acc: 34.375(34.375)
12/17 01:22:06 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 101/521, train_loss: 1.865(1.615), train_acc: 28.125(39.026)
12/17 01:22:13 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 201/521, train_loss: 1.553(1.605), train_acc: 41.667(39.397)
12/17 01:22:19 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 301/521, train_loss: 1.527(1.600), train_acc: 45.833(39.566)
12/17 01:22:25 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 401/521, train_loss: 1.303(1.594), train_acc: 52.083(39.939)
12/17 01:22:32 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 501/521, train_loss: 1.761(1.594), train_acc: 43.750(40.062)
12/17 01:22:33 AM [Supernet Training] lr: 0.02467 epoch: 009/600, step: 521/521, train_loss: 1.433(1.592), train_acc: 52.500(40.174)
12/17 01:22:33 AM [Supernet Training] epoch: 009, train_loss: 1.592, train_acc: 40.174
12/17 01:22:35 AM [Supernet Validation] epoch: 009, val_loss: 1.680, val_acc: 37.720, best_acc: 37.940


12/17 01:22:35 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 001/521, train_loss: 1.517(1.517), train_acc: 40.625(40.625)
12/17 01:22:41 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 101/521, train_loss: 1.470(1.584), train_acc: 46.875(40.274)
12/17 01:22:48 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 201/521, train_loss: 1.658(1.572), train_acc: 34.375(41.055)
12/17 01:22:54 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 301/521, train_loss: 1.457(1.573), train_acc: 44.792(41.127)
12/17 01:23:00 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 401/521, train_loss: 1.388(1.565), train_acc: 58.333(41.620)
12/17 01:23:06 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 501/521, train_loss: 1.668(1.563), train_acc: 39.583(41.710)
12/17 01:23:08 AM [Supernet Training] lr: 0.02463 epoch: 010/600, step: 521/521, train_loss: 1.461(1.562), train_acc: 52.500(41.776)
12/17 01:23:08 AM [Supernet Training] epoch: 010, train_loss: 1.562, train_acc: 41.776
12/17 01:23:09 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:23:09 AM [Supernet Validation] epoch: 010, val_loss: 1.607, val_acc: 40.790, best_acc: 40.790


12/17 01:23:10 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 001/521, train_loss: 1.713(1.713), train_acc: 34.375(34.375)
12/17 01:23:16 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 101/521, train_loss: 1.521(1.534), train_acc: 40.625(42.966)
12/17 01:23:22 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 201/521, train_loss: 1.445(1.530), train_acc: 46.875(43.102)
12/17 01:23:29 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 301/521, train_loss: 1.643(1.530), train_acc: 38.542(43.200)
12/17 01:23:35 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 401/521, train_loss: 1.484(1.533), train_acc: 40.625(42.890)
12/17 01:23:41 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 501/521, train_loss: 1.392(1.530), train_acc: 46.875(42.983)
12/17 01:23:42 AM [Supernet Training] lr: 0.02458 epoch: 011/600, step: 521/521, train_loss: 1.477(1.530), train_acc: 46.250(42.968)
12/17 01:23:42 AM [Supernet Training] epoch: 011, train_loss: 1.530, train_acc: 42.968
12/17 01:23:44 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:23:44 AM [Supernet Validation] epoch: 011, val_loss: 1.557, val_acc: 42.040, best_acc: 42.040


12/17 01:23:44 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 001/521, train_loss: 1.378(1.378), train_acc: 45.833(45.833)
12/17 01:23:51 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 101/521, train_loss: 1.564(1.497), train_acc: 33.333(43.740)
12/17 01:23:57 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 201/521, train_loss: 1.476(1.501), train_acc: 40.625(43.968)
12/17 01:24:03 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 301/521, train_loss: 1.302(1.502), train_acc: 50.000(44.196)
12/17 01:24:09 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 401/521, train_loss: 1.483(1.497), train_acc: 45.833(44.410)
12/17 01:24:16 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 501/521, train_loss: 1.325(1.499), train_acc: 52.083(44.449)
12/17 01:24:17 AM [Supernet Training] lr: 0.02454 epoch: 012/600, step: 521/521, train_loss: 1.461(1.498), train_acc: 43.750(44.482)
12/17 01:24:17 AM [Supernet Training] epoch: 012, train_loss: 1.498, train_acc: 44.482
12/17 01:24:19 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:24:19 AM [Supernet Validation] epoch: 012, val_loss: 1.566, val_acc: 42.380, best_acc: 42.380


12/17 01:24:19 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 001/521, train_loss: 1.599(1.599), train_acc: 45.833(45.833)
12/17 01:24:25 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 101/521, train_loss: 1.537(1.474), train_acc: 46.875(45.080)
12/17 01:24:31 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 201/521, train_loss: 1.453(1.475), train_acc: 45.833(45.077)
12/17 01:24:38 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 301/521, train_loss: 1.500(1.474), train_acc: 42.708(45.349)
12/17 01:24:44 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 401/521, train_loss: 1.423(1.473), train_acc: 52.083(45.459)
12/17 01:24:50 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 501/521, train_loss: 1.296(1.473), train_acc: 48.958(45.553)
12/17 01:24:52 AM [Supernet Training] lr: 0.02450 epoch: 013/600, step: 521/521, train_loss: 1.620(1.475), train_acc: 37.500(45.444)
12/17 01:24:52 AM [Supernet Training] epoch: 013, train_loss: 1.475, train_acc: 45.444
12/17 01:24:53 AM [Supernet Validation] epoch: 013, val_loss: 1.563, val_acc: 42.260, best_acc: 42.380


12/17 01:24:54 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 001/521, train_loss: 1.560(1.560), train_acc: 39.583(39.583)
12/17 01:25:00 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 101/521, train_loss: 1.290(1.443), train_acc: 53.125(47.030)
12/17 01:25:06 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 201/521, train_loss: 1.372(1.444), train_acc: 47.917(46.720)
12/17 01:25:12 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 301/521, train_loss: 1.298(1.442), train_acc: 52.083(46.844)
12/17 01:25:19 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 401/521, train_loss: 1.398(1.443), train_acc: 45.833(46.732)
12/17 01:25:25 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 501/521, train_loss: 1.378(1.440), train_acc: 48.958(46.852)
12/17 01:25:26 AM [Supernet Training] lr: 0.02446 epoch: 014/600, step: 521/521, train_loss: 1.216(1.441), train_acc: 60.000(46.810)
12/17 01:25:26 AM [Supernet Training] epoch: 014, train_loss: 1.441, train_acc: 46.810
12/17 01:25:28 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:25:28 AM [Supernet Validation] epoch: 014, val_loss: 1.517, val_acc: 44.420, best_acc: 44.420


12/17 01:25:28 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 001/521, train_loss: 1.247(1.247), train_acc: 56.250(56.250)
12/17 01:25:35 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 101/521, train_loss: 1.563(1.413), train_acc: 41.667(47.349)
12/17 01:25:41 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 201/521, train_loss: 1.388(1.418), train_acc: 50.000(47.284)
12/17 01:25:47 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 301/521, train_loss: 1.440(1.413), train_acc: 44.792(47.567)
12/17 01:25:53 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 401/521, train_loss: 1.490(1.416), train_acc: 44.792(47.382)
12/17 01:26:00 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 501/521, train_loss: 1.177(1.416), train_acc: 62.500(47.517)
12/17 01:26:01 AM [Supernet Training] lr: 0.02442 epoch: 015/600, step: 521/521, train_loss: 1.629(1.418), train_acc: 41.250(47.448)
12/17 01:26:01 AM [Supernet Training] epoch: 015, train_loss: 1.418, train_acc: 47.448
12/17 01:26:03 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:26:03 AM [Supernet Validation] epoch: 015, val_loss: 1.478, val_acc: 45.910, best_acc: 45.910


12/17 01:26:03 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 001/521, train_loss: 1.593(1.593), train_acc: 41.667(41.667)
12/17 01:26:09 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 101/521, train_loss: 1.379(1.393), train_acc: 52.083(48.566)
12/17 01:26:16 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 201/521, train_loss: 1.365(1.392), train_acc: 43.750(48.746)
12/17 01:26:22 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 301/521, train_loss: 1.543(1.393), train_acc: 46.875(48.543)
12/17 01:26:28 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 401/521, train_loss: 1.369(1.396), train_acc: 41.667(48.374)
12/17 01:26:35 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 501/521, train_loss: 1.314(1.394), train_acc: 48.958(48.453)
12/17 01:26:36 AM [Supernet Training] lr: 0.02438 epoch: 016/600, step: 521/521, train_loss: 1.375(1.394), train_acc: 43.750(48.460)
12/17 01:26:36 AM [Supernet Training] epoch: 016, train_loss: 1.394, train_acc: 48.460
12/17 01:26:38 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:26:38 AM [Supernet Validation] epoch: 016, val_loss: 1.444, val_acc: 47.060, best_acc: 47.060


12/17 01:26:38 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 001/521, train_loss: 1.234(1.234), train_acc: 57.292(57.292)
12/17 01:26:44 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 101/521, train_loss: 1.320(1.378), train_acc: 53.125(49.309)
12/17 01:26:51 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 201/521, train_loss: 1.410(1.375), train_acc: 44.792(49.420)
12/17 01:26:57 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 301/521, train_loss: 1.277(1.374), train_acc: 50.000(49.484)
12/17 01:27:04 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 401/521, train_loss: 1.286(1.372), train_acc: 59.375(49.553)
12/17 01:27:10 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 501/521, train_loss: 1.548(1.373), train_acc: 39.583(49.557)
12/17 01:27:11 AM [Supernet Training] lr: 0.02433 epoch: 017/600, step: 521/521, train_loss: 1.054(1.370), train_acc: 61.250(49.632)
12/17 01:27:11 AM [Supernet Training] epoch: 017, train_loss: 1.370, train_acc: 49.632
12/17 01:27:13 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:27:13 AM [Supernet Validation] epoch: 017, val_loss: 1.424, val_acc: 48.010, best_acc: 48.010


12/17 01:27:13 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 001/521, train_loss: 1.306(1.306), train_acc: 53.125(53.125)
12/17 01:27:19 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 101/521, train_loss: 1.603(1.364), train_acc: 41.667(50.186)
12/17 01:27:26 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 201/521, train_loss: 1.452(1.363), train_acc: 45.833(49.984)
12/17 01:27:32 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 301/521, train_loss: 1.174(1.353), train_acc: 57.292(50.194)
12/17 01:27:38 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 401/521, train_loss: 1.254(1.350), train_acc: 58.333(50.379)
12/17 01:27:45 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 501/521, train_loss: 1.215(1.344), train_acc: 54.167(50.497)
12/17 01:27:46 AM [Supernet Training] lr: 0.02429 epoch: 018/600, step: 521/521, train_loss: 1.414(1.343), train_acc: 52.500(50.560)
12/17 01:27:46 AM [Supernet Training] epoch: 018, train_loss: 1.343, train_acc: 50.560
12/17 01:27:47 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:27:47 AM [Supernet Validation] epoch: 018, val_loss: 1.404, val_acc: 49.010, best_acc: 49.010


12/17 01:27:48 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 001/521, train_loss: 1.225(1.225), train_acc: 60.417(60.417)
12/17 01:27:54 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 101/521, train_loss: 1.227(1.315), train_acc: 50.000(51.877)
12/17 01:28:00 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 201/521, train_loss: 1.154(1.304), train_acc: 52.083(52.228)
12/17 01:28:07 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 301/521, train_loss: 1.427(1.312), train_acc: 41.667(52.177)
12/17 01:28:13 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 401/521, train_loss: 1.313(1.310), train_acc: 48.958(52.203)
12/17 01:28:19 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 501/521, train_loss: 1.254(1.315), train_acc: 58.333(52.008)
12/17 01:28:20 AM [Supernet Training] lr: 0.02425 epoch: 019/600, step: 521/521, train_loss: 1.459(1.315), train_acc: 47.500(51.992)
12/17 01:28:20 AM [Supernet Training] epoch: 019, train_loss: 1.315, train_acc: 51.992
12/17 01:28:22 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:28:22 AM [Supernet Validation] epoch: 019, val_loss: 1.333, val_acc: 51.930, best_acc: 51.930


12/17 01:28:22 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 001/521, train_loss: 1.063(1.063), train_acc: 62.500(62.500)
12/17 01:28:29 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 101/521, train_loss: 1.365(1.309), train_acc: 43.750(52.011)
12/17 01:28:35 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 201/521, train_loss: 1.265(1.303), train_acc: 56.250(52.446)
12/17 01:28:41 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 301/521, train_loss: 1.399(1.296), train_acc: 52.083(52.471)
12/17 01:28:48 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 401/521, train_loss: 1.320(1.295), train_acc: 51.042(52.486)
12/17 01:28:54 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 501/521, train_loss: 1.334(1.292), train_acc: 56.250(52.690)
12/17 01:28:55 AM [Supernet Training] lr: 0.02421 epoch: 020/600, step: 521/521, train_loss: 1.492(1.293), train_acc: 42.500(52.690)
12/17 01:28:55 AM [Supernet Training] epoch: 020, train_loss: 1.293, train_acc: 52.690
12/17 01:28:57 AM [Supernet Validation] epoch: 020, val_loss: 1.339, val_acc: 51.280, best_acc: 51.930


12/17 01:28:57 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 001/521, train_loss: 1.379(1.379), train_acc: 51.042(51.042)
12/17 01:29:03 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 101/521, train_loss: 1.235(1.271), train_acc: 56.250(53.826)
12/17 01:29:10 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 201/521, train_loss: 1.141(1.274), train_acc: 53.125(53.312)
12/17 01:29:16 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 301/521, train_loss: 1.271(1.275), train_acc: 60.417(53.499)
12/17 01:29:22 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 401/521, train_loss: 1.211(1.270), train_acc: 55.208(53.673)
12/17 01:29:29 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 501/521, train_loss: 1.382(1.272), train_acc: 48.958(53.595)
12/17 01:29:30 AM [Supernet Training] lr: 0.02417 epoch: 021/600, step: 521/521, train_loss: 1.300(1.270), train_acc: 58.750(53.722)
12/17 01:29:30 AM [Supernet Training] epoch: 021, train_loss: 1.270, train_acc: 53.722
12/17 01:29:31 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:29:31 AM [Supernet Validation] epoch: 021, val_loss: 1.302, val_acc: 52.490, best_acc: 52.490


12/17 01:29:32 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 001/521, train_loss: 1.080(1.080), train_acc: 61.458(61.458)
12/17 01:29:38 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 101/521, train_loss: 1.359(1.262), train_acc: 46.875(54.074)
12/17 01:29:45 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 201/521, train_loss: 1.382(1.256), train_acc: 53.125(54.395)
12/17 01:29:51 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 301/521, train_loss: 1.059(1.254), train_acc: 64.583(54.506)
12/17 01:29:57 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 401/521, train_loss: 1.229(1.251), train_acc: 54.167(54.699)
12/17 01:30:03 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 501/521, train_loss: 1.571(1.246), train_acc: 43.750(54.863)
12/17 01:30:05 AM [Supernet Training] lr: 0.02413 epoch: 022/600, step: 521/521, train_loss: 1.496(1.247), train_acc: 47.500(54.866)
12/17 01:30:05 AM [Supernet Training] epoch: 022, train_loss: 1.247, train_acc: 54.866
12/17 01:30:17 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:30:17 AM [Supernet Validation] epoch: 022, val_loss: 1.301, val_acc: 52.970, best_acc: 52.970


12/17 01:30:17 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 001/521, train_loss: 1.338(1.338), train_acc: 46.875(46.875)
12/17 01:30:23 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 101/521, train_loss: 1.300(1.250), train_acc: 53.125(54.724)
12/17 01:30:30 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 201/521, train_loss: 1.276(1.239), train_acc: 54.167(55.017)
12/17 01:30:36 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 301/521, train_loss: 1.267(1.233), train_acc: 57.292(55.236)
12/17 01:30:42 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 401/521, train_loss: 1.228(1.231), train_acc: 55.208(55.291)
12/17 01:30:49 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 501/521, train_loss: 1.202(1.222), train_acc: 55.208(55.695)
12/17 01:30:50 AM [Supernet Training] lr: 0.02408 epoch: 023/600, step: 521/521, train_loss: 1.280(1.221), train_acc: 53.750(55.728)
12/17 01:30:50 AM [Supernet Training] epoch: 023, train_loss: 1.221, train_acc: 55.728
12/17 01:30:52 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:30:52 AM [Supernet Validation] epoch: 023, val_loss: 1.298, val_acc: 54.070, best_acc: 54.070


12/17 01:30:52 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 001/521, train_loss: 1.113(1.113), train_acc: 60.417(60.417)
12/17 01:30:58 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 101/521, train_loss: 1.297(1.205), train_acc: 51.042(55.538)
12/17 01:31:05 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 201/521, train_loss: 1.155(1.202), train_acc: 58.333(56.136)
12/17 01:31:11 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 301/521, train_loss: 1.179(1.198), train_acc: 60.417(56.350)
12/17 01:31:17 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 401/521, train_loss: 1.311(1.201), train_acc: 54.167(56.427)
12/17 01:31:24 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 501/521, train_loss: 1.158(1.201), train_acc: 60.417(56.420)
12/17 01:31:25 AM [Supernet Training] lr: 0.02404 epoch: 024/600, step: 521/521, train_loss: 1.453(1.200), train_acc: 48.750(56.456)
12/17 01:31:25 AM [Supernet Training] epoch: 024, train_loss: 1.200, train_acc: 56.456
12/17 01:31:26 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:31:26 AM [Supernet Validation] epoch: 024, val_loss: 1.244, val_acc: 54.850, best_acc: 54.850


12/17 01:31:27 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 001/521, train_loss: 0.908(0.908), train_acc: 66.667(66.667)
12/17 01:31:33 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 101/521, train_loss: 1.174(1.183), train_acc: 54.167(57.467)
12/17 01:31:39 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 201/521, train_loss: 1.238(1.169), train_acc: 53.125(57.981)
12/17 01:31:46 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 301/521, train_loss: 1.244(1.170), train_acc: 62.500(57.780)
12/17 01:31:52 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 401/521, train_loss: 1.138(1.170), train_acc: 60.417(57.746)
12/17 01:31:58 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 501/521, train_loss: 1.420(1.174), train_acc: 44.792(57.626)
12/17 01:31:59 AM [Supernet Training] lr: 0.02400 epoch: 025/600, step: 521/521, train_loss: 1.006(1.173), train_acc: 66.250(57.636)
12/17 01:32:00 AM [Supernet Training] epoch: 025, train_loss: 1.173, train_acc: 57.636
12/17 01:32:01 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:32:01 AM [Supernet Validation] epoch: 025, val_loss: 1.219, val_acc: 56.600, best_acc: 56.600


12/17 01:32:02 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 001/521, train_loss: 1.080(1.080), train_acc: 59.375(59.375)
12/17 01:32:08 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 101/521, train_loss: 1.101(1.166), train_acc: 67.708(58.333)
12/17 01:32:14 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 201/521, train_loss: 1.139(1.156), train_acc: 57.292(58.494)
12/17 01:32:21 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 301/521, train_loss: 1.057(1.151), train_acc: 60.417(58.461)
12/17 01:32:27 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 401/521, train_loss: 1.167(1.151), train_acc: 52.083(58.442)
12/17 01:32:33 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 501/521, train_loss: 1.041(1.148), train_acc: 65.625(58.599)
12/17 01:32:35 AM [Supernet Training] lr: 0.02396 epoch: 026/600, step: 521/521, train_loss: 1.235(1.147), train_acc: 51.250(58.632)
12/17 01:32:35 AM [Supernet Training] epoch: 026, train_loss: 1.147, train_acc: 58.632
12/17 01:32:36 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:32:36 AM [Supernet Validation] epoch: 026, val_loss: 1.231, val_acc: 56.630, best_acc: 56.630


12/17 01:32:37 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 001/521, train_loss: 1.310(1.310), train_acc: 53.125(53.125)
12/17 01:32:43 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 101/521, train_loss: 1.262(1.127), train_acc: 53.125(59.076)
12/17 01:32:49 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 201/521, train_loss: 1.232(1.128), train_acc: 58.333(59.038)
12/17 01:32:56 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 301/521, train_loss: 1.212(1.126), train_acc: 55.208(59.109)
12/17 01:33:02 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 401/521, train_loss: 1.213(1.128), train_acc: 55.208(58.954)
12/17 01:33:09 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 501/521, train_loss: 1.160(1.126), train_acc: 55.208(59.074)
12/17 01:33:10 AM [Supernet Training] lr: 0.02392 epoch: 027/600, step: 521/521, train_loss: 1.114(1.124), train_acc: 62.500(59.172)
12/17 01:33:10 AM [Supernet Training] epoch: 027, train_loss: 1.124, train_acc: 59.172
12/17 01:33:12 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:33:12 AM [Supernet Validation] epoch: 027, val_loss: 1.147, val_acc: 58.240, best_acc: 58.240


12/17 01:33:12 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 001/521, train_loss: 1.052(1.052), train_acc: 61.458(61.458)
12/17 01:33:19 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 101/521, train_loss: 1.160(1.110), train_acc: 53.125(60.014)
12/17 01:33:25 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 201/521, train_loss: 1.292(1.113), train_acc: 53.125(60.033)
12/17 01:33:31 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 301/521, train_loss: 1.071(1.110), train_acc: 60.417(60.109)
12/17 01:33:38 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 401/521, train_loss: 0.945(1.112), train_acc: 64.583(59.941)
12/17 01:33:44 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 501/521, train_loss: 1.214(1.106), train_acc: 58.333(60.263)
12/17 01:33:45 AM [Supernet Training] lr: 0.02388 epoch: 028/600, step: 521/521, train_loss: 0.963(1.105), train_acc: 70.000(60.324)
12/17 01:33:45 AM [Supernet Training] epoch: 028, train_loss: 1.105, train_acc: 60.324
12/17 01:33:47 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:33:47 AM [Supernet Validation] epoch: 028, val_loss: 1.125, val_acc: 60.230, best_acc: 60.230


12/17 01:33:47 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 001/521, train_loss: 0.901(0.901), train_acc: 69.792(69.792)
12/17 01:33:54 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 101/521, train_loss: 1.025(1.070), train_acc: 65.625(61.345)
12/17 01:34:00 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 201/521, train_loss: 1.234(1.077), train_acc: 56.250(61.220)
12/17 01:34:07 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 301/521, train_loss: 1.116(1.082), train_acc: 59.375(61.026)
12/17 01:34:13 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 401/521, train_loss: 1.031(1.081), train_acc: 64.583(61.095)
12/17 01:34:19 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 501/521, train_loss: 1.134(1.082), train_acc: 58.333(61.117)
12/17 01:34:21 AM [Supernet Training] lr: 0.02383 epoch: 029/600, step: 521/521, train_loss: 0.961(1.084), train_acc: 70.000(61.114)
12/17 01:34:21 AM [Supernet Training] epoch: 029, train_loss: 1.084, train_acc: 61.114
12/17 01:34:22 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:34:22 AM [Supernet Validation] epoch: 029, val_loss: 1.115, val_acc: 60.580, best_acc: 60.580


12/17 01:34:23 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 001/521, train_loss: 0.977(0.977), train_acc: 67.708(67.708)
12/17 01:34:29 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 101/521, train_loss: 0.884(1.079), train_acc: 70.833(60.881)
12/17 01:34:35 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 201/521, train_loss: 1.216(1.077), train_acc: 58.333(61.111)
12/17 01:34:42 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 301/521, train_loss: 0.979(1.074), train_acc: 67.708(61.185)
12/17 01:34:48 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 401/521, train_loss: 1.128(1.068), train_acc: 61.458(61.578)
12/17 01:34:55 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 501/521, train_loss: 1.032(1.067), train_acc: 65.625(61.677)
12/17 01:34:56 AM [Supernet Training] lr: 0.02379 epoch: 030/600, step: 521/521, train_loss: 1.067(1.066), train_acc: 58.750(61.656)
12/17 01:34:56 AM [Supernet Training] epoch: 030, train_loss: 1.066, train_acc: 61.656
12/17 01:34:58 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:34:58 AM [Supernet Validation] epoch: 030, val_loss: 1.101, val_acc: 60.820, best_acc: 60.820


12/17 01:34:58 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 001/521, train_loss: 1.122(1.122), train_acc: 56.250(56.250)
12/17 01:35:04 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 101/521, train_loss: 1.071(1.061), train_acc: 66.667(62.387)
12/17 01:35:11 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 201/521, train_loss: 1.054(1.056), train_acc: 68.750(62.412)
12/17 01:35:17 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 301/521, train_loss: 0.922(1.053), train_acc: 64.583(62.455)
12/17 01:35:23 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 401/521, train_loss: 1.044(1.049), train_acc: 62.500(62.505)
12/17 01:35:30 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 501/521, train_loss: 1.237(1.045), train_acc: 53.125(62.577)
12/17 01:35:31 AM [Supernet Training] lr: 0.02375 epoch: 031/600, step: 521/521, train_loss: 1.115(1.047), train_acc: 62.500(62.520)
12/17 01:35:31 AM [Supernet Training] epoch: 031, train_loss: 1.047, train_acc: 62.520
12/17 01:35:33 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:35:33 AM [Supernet Validation] epoch: 031, val_loss: 1.069, val_acc: 61.990, best_acc: 61.990


12/17 01:35:33 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 001/521, train_loss: 1.152(1.152), train_acc: 60.417(60.417)
12/17 01:35:39 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 101/521, train_loss: 0.984(1.020), train_acc: 65.625(63.645)
12/17 01:35:46 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 201/521, train_loss: 0.976(1.025), train_acc: 59.375(63.422)
12/17 01:35:52 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 301/521, train_loss: 1.032(1.026), train_acc: 62.500(63.434)
12/17 01:35:59 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 401/521, train_loss: 0.942(1.025), train_acc: 65.625(63.451)
12/17 01:36:05 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 501/521, train_loss: 1.055(1.023), train_acc: 57.292(63.463)
12/17 01:36:06 AM [Supernet Training] lr: 0.02371 epoch: 032/600, step: 521/521, train_loss: 1.051(1.024), train_acc: 57.500(63.426)
12/17 01:36:06 AM [Supernet Training] epoch: 032, train_loss: 1.024, train_acc: 63.426
12/17 01:36:08 AM [Supernet Validation] epoch: 032, val_loss: 1.055, val_acc: 61.720, best_acc: 61.990


12/17 01:36:08 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 001/521, train_loss: 0.965(0.965), train_acc: 64.583(64.583)
12/17 01:36:15 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 101/521, train_loss: 0.955(1.006), train_acc: 65.625(63.944)
12/17 01:36:21 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 201/521, train_loss: 0.984(1.017), train_acc: 59.375(63.853)
12/17 01:36:27 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 301/521, train_loss: 0.974(1.015), train_acc: 66.667(63.583)
12/17 01:36:34 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 401/521, train_loss: 0.930(1.010), train_acc: 59.375(63.599)
12/17 01:36:40 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 501/521, train_loss: 0.973(1.010), train_acc: 65.625(63.735)
12/17 01:36:42 AM [Supernet Training] lr: 0.02367 epoch: 033/600, step: 521/521, train_loss: 0.922(1.010), train_acc: 67.500(63.758)
12/17 01:36:42 AM [Supernet Training] epoch: 033, train_loss: 1.010, train_acc: 63.758
12/17 01:36:43 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:36:43 AM [Supernet Validation] epoch: 033, val_loss: 1.038, val_acc: 63.090, best_acc: 63.090


12/17 01:36:44 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 001/521, train_loss: 0.899(0.899), train_acc: 65.625(65.625)
12/17 01:36:50 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 101/521, train_loss: 1.202(0.981), train_acc: 57.292(65.130)
12/17 01:36:57 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 201/521, train_loss: 0.877(0.981), train_acc: 69.792(65.138)
12/17 01:37:03 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 301/521, train_loss: 0.911(0.982), train_acc: 69.792(64.957)
12/17 01:37:09 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 401/521, train_loss: 0.961(0.992), train_acc: 65.625(64.633)
12/17 01:37:16 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 501/521, train_loss: 1.092(0.993), train_acc: 56.250(64.486)
12/17 01:37:17 AM [Supernet Training] lr: 0.02363 epoch: 034/600, step: 521/521, train_loss: 0.995(0.992), train_acc: 67.500(64.528)
12/17 01:37:17 AM [Supernet Training] epoch: 034, train_loss: 0.992, train_acc: 64.528
12/17 01:37:19 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:37:19 AM [Supernet Validation] epoch: 034, val_loss: 1.023, val_acc: 63.900, best_acc: 63.900


12/17 01:37:19 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 001/521, train_loss: 0.940(0.940), train_acc: 66.667(66.667)
12/17 01:37:25 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 101/521, train_loss: 1.231(0.987), train_acc: 58.333(64.470)
12/17 01:37:32 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 201/521, train_loss: 1.004(0.985), train_acc: 59.375(64.646)
12/17 01:37:38 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 301/521, train_loss: 1.019(0.976), train_acc: 61.458(64.936)
12/17 01:37:44 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 401/521, train_loss: 0.984(0.973), train_acc: 66.667(65.126)
12/17 01:37:51 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 501/521, train_loss: 1.156(0.977), train_acc: 53.125(65.041)
12/17 01:37:52 AM [Supernet Training] lr: 0.02358 epoch: 035/600, step: 521/521, train_loss: 1.136(0.977), train_acc: 62.500(64.990)
12/17 01:37:52 AM [Supernet Training] epoch: 035, train_loss: 0.977, train_acc: 64.990
12/17 01:37:54 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:37:54 AM [Supernet Validation] epoch: 035, val_loss: 1.012, val_acc: 64.330, best_acc: 64.330


12/17 01:37:54 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 001/521, train_loss: 0.987(0.987), train_acc: 62.500(62.500)
12/17 01:38:00 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 101/521, train_loss: 1.145(0.961), train_acc: 62.500(65.470)
12/17 01:38:07 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 201/521, train_loss: 1.058(0.964), train_acc: 59.375(65.568)
12/17 01:38:13 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 301/521, train_loss: 0.891(0.959), train_acc: 67.708(65.712)
12/17 01:38:20 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 401/521, train_loss: 1.107(0.959), train_acc: 62.500(65.485)
12/17 01:38:26 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 501/521, train_loss: 1.067(0.960), train_acc: 64.583(65.546)
12/17 01:38:27 AM [Supernet Training] lr: 0.02354 epoch: 036/600, step: 521/521, train_loss: 0.728(0.959), train_acc: 77.500(65.604)
12/17 01:38:27 AM [Supernet Training] epoch: 036, train_loss: 0.959, train_acc: 65.604
12/17 01:38:29 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:38:29 AM [Supernet Validation] epoch: 036, val_loss: 0.984, val_acc: 64.890, best_acc: 64.890


12/17 01:38:29 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 001/521, train_loss: 1.047(1.047), train_acc: 60.417(60.417)
12/17 01:38:36 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 101/521, train_loss: 0.967(0.960), train_acc: 63.542(65.780)
12/17 01:38:42 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 201/521, train_loss: 0.944(0.953), train_acc: 66.667(65.801)
12/17 01:38:48 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 301/521, train_loss: 0.941(0.946), train_acc: 65.625(66.224)
12/17 01:38:54 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 401/521, train_loss: 0.877(0.947), train_acc: 66.667(66.108)
12/17 01:39:01 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 501/521, train_loss: 1.290(0.946), train_acc: 63.542(66.172)
12/17 01:39:02 AM [Supernet Training] lr: 0.02350 epoch: 037/600, step: 521/521, train_loss: 0.838(0.946), train_acc: 67.500(66.178)
12/17 01:39:02 AM [Supernet Training] epoch: 037, train_loss: 0.946, train_acc: 66.178
12/17 01:39:04 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:39:04 AM [Supernet Validation] epoch: 037, val_loss: 0.988, val_acc: 64.960, best_acc: 64.960


12/17 01:39:04 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 001/521, train_loss: 0.789(0.789), train_acc: 67.708(67.708)
12/17 01:39:10 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 101/521, train_loss: 0.913(0.933), train_acc: 64.583(66.749)
12/17 01:39:16 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 201/521, train_loss: 0.781(0.925), train_acc: 76.042(67.019)
12/17 01:39:23 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 301/521, train_loss: 0.841(0.929), train_acc: 68.750(66.937)
12/17 01:39:29 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 401/521, train_loss: 0.838(0.931), train_acc: 67.708(66.854)
12/17 01:39:35 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 501/521, train_loss: 0.962(0.931), train_acc: 64.583(66.800)
12/17 01:39:37 AM [Supernet Training] lr: 0.02346 epoch: 038/600, step: 521/521, train_loss: 0.867(0.931), train_acc: 73.750(66.838)
12/17 01:39:37 AM [Supernet Training] epoch: 038, train_loss: 0.931, train_acc: 66.838
12/17 01:39:38 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:39:38 AM [Supernet Validation] epoch: 038, val_loss: 0.962, val_acc: 65.950, best_acc: 65.950


12/17 01:39:39 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 001/521, train_loss: 0.796(0.796), train_acc: 77.083(77.083)
12/17 01:39:45 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 101/521, train_loss: 0.923(0.923), train_acc: 63.542(67.502)
12/17 01:39:51 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 201/521, train_loss: 0.958(0.916), train_acc: 72.917(67.340)
12/17 01:39:58 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 301/521, train_loss: 1.042(0.913), train_acc: 63.542(67.362)
12/17 01:40:04 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 401/521, train_loss: 0.896(0.911), train_acc: 65.625(67.503)
12/17 01:40:10 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 501/521, train_loss: 1.077(0.915), train_acc: 61.458(67.236)
12/17 01:40:11 AM [Supernet Training] lr: 0.02342 epoch: 039/600, step: 521/521, train_loss: 0.840(0.913), train_acc: 73.750(67.294)
12/17 01:40:11 AM [Supernet Training] epoch: 039, train_loss: 0.913, train_acc: 67.294
12/17 01:40:13 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:40:13 AM [Supernet Validation] epoch: 039, val_loss: 0.956, val_acc: 66.190, best_acc: 66.190


12/17 01:40:13 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 001/521, train_loss: 0.783(0.783), train_acc: 68.750(68.750)
12/17 01:40:20 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 101/521, train_loss: 0.965(0.904), train_acc: 65.625(67.389)
12/17 01:40:26 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 201/521, train_loss: 0.917(0.899), train_acc: 68.750(67.983)
12/17 01:40:32 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 301/521, train_loss: 0.942(0.908), train_acc: 65.625(67.629)
12/17 01:40:39 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 401/521, train_loss: 0.912(0.909), train_acc: 65.625(67.485)
12/17 01:40:45 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 501/521, train_loss: 0.667(0.906), train_acc: 77.083(67.673)
12/17 01:40:46 AM [Supernet Training] lr: 0.02338 epoch: 040/600, step: 521/521, train_loss: 0.951(0.906), train_acc: 62.500(67.694)
12/17 01:40:46 AM [Supernet Training] epoch: 040, train_loss: 0.906, train_acc: 67.694
12/17 01:40:48 AM [Supernet Validation] epoch: 040, val_loss: 0.972, val_acc: 66.090, best_acc: 66.190


12/17 01:40:48 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 001/521, train_loss: 0.888(0.888), train_acc: 63.542(63.542)
12/17 01:40:54 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 101/521, train_loss: 0.802(0.882), train_acc: 70.833(68.286)
12/17 01:41:01 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 201/521, train_loss: 0.817(0.896), train_acc: 73.958(68.066)
12/17 01:41:07 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 301/521, train_loss: 0.777(0.892), train_acc: 67.708(68.203)
12/17 01:41:13 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 401/521, train_loss: 1.004(0.894), train_acc: 61.458(68.215)
12/17 01:41:19 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 501/521, train_loss: 1.022(0.891), train_acc: 62.500(68.226)
12/17 01:41:21 AM [Supernet Training] lr: 0.02333 epoch: 041/600, step: 521/521, train_loss: 0.922(0.891), train_acc: 66.250(68.228)
12/17 01:41:21 AM [Supernet Training] epoch: 041, train_loss: 0.891, train_acc: 68.228
12/17 01:41:22 AM [Supernet Validation] epoch: 041, val_loss: 0.965, val_acc: 66.070, best_acc: 66.190


12/17 01:41:22 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 001/521, train_loss: 1.099(1.099), train_acc: 61.458(61.458)
12/17 01:41:29 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 101/521, train_loss: 0.958(0.880), train_acc: 66.667(68.626)
12/17 01:41:35 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 201/521, train_loss: 1.030(0.884), train_acc: 64.583(68.361)
12/17 01:41:41 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 301/521, train_loss: 0.642(0.886), train_acc: 81.250(68.418)
12/17 01:41:48 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 401/521, train_loss: 0.910(0.884), train_acc: 62.500(68.555)
12/17 01:41:54 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 501/521, train_loss: 1.034(0.884), train_acc: 63.542(68.442)
12/17 01:41:55 AM [Supernet Training] lr: 0.02329 epoch: 042/600, step: 521/521, train_loss: 0.847(0.883), train_acc: 70.000(68.506)
12/17 01:41:55 AM [Supernet Training] epoch: 042, train_loss: 0.883, train_acc: 68.506
12/17 01:41:57 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:41:57 AM [Supernet Validation] epoch: 042, val_loss: 0.910, val_acc: 67.310, best_acc: 67.310


12/17 01:41:57 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 001/521, train_loss: 0.794(0.794), train_acc: 66.667(66.667)
12/17 01:42:04 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 101/521, train_loss: 0.967(0.886), train_acc: 62.500(68.637)
12/17 01:42:10 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 201/521, train_loss: 0.968(0.870), train_acc: 67.708(68.869)
12/17 01:42:16 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 301/521, train_loss: 1.072(0.876), train_acc: 66.667(68.657)
12/17 01:42:23 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 401/521, train_loss: 1.054(0.875), train_acc: 66.667(68.831)
12/17 01:42:29 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 501/521, train_loss: 0.784(0.870), train_acc: 76.042(69.029)
12/17 01:42:30 AM [Supernet Training] lr: 0.02325 epoch: 043/600, step: 521/521, train_loss: 0.888(0.870), train_acc: 68.750(69.020)
12/17 01:42:30 AM [Supernet Training] epoch: 043, train_loss: 0.870, train_acc: 69.020
12/17 01:42:32 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:42:32 AM [Supernet Validation] epoch: 043, val_loss: 0.909, val_acc: 68.090, best_acc: 68.090


12/17 01:42:32 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 001/521, train_loss: 0.749(0.749), train_acc: 72.917(72.917)
12/17 01:42:39 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 101/521, train_loss: 1.028(0.887), train_acc: 60.417(68.698)
12/17 01:42:45 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 201/521, train_loss: 1.146(0.859), train_acc: 63.542(69.393)
12/17 01:42:52 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 301/521, train_loss: 0.883(0.860), train_acc: 69.792(69.421)
12/17 01:42:58 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 401/521, train_loss: 1.038(0.854), train_acc: 65.625(69.703)
12/17 01:43:04 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 501/521, train_loss: 0.686(0.856), train_acc: 73.958(69.592)
12/17 01:43:06 AM [Supernet Training] lr: 0.02321 epoch: 044/600, step: 521/521, train_loss: 0.700(0.854), train_acc: 72.500(69.678)
12/17 01:43:06 AM [Supernet Training] epoch: 044, train_loss: 0.854, train_acc: 69.678
12/17 01:43:07 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:43:07 AM [Supernet Validation] epoch: 044, val_loss: 0.912, val_acc: 68.140, best_acc: 68.140


12/17 01:43:08 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 001/521, train_loss: 0.908(0.908), train_acc: 71.875(71.875)
12/17 01:43:14 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 101/521, train_loss: 0.833(0.865), train_acc: 69.792(69.163)
12/17 01:43:21 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 201/521, train_loss: 0.969(0.844), train_acc: 67.708(70.103)
12/17 01:43:27 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 301/521, train_loss: 0.859(0.846), train_acc: 70.833(70.024)
12/17 01:43:33 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 401/521, train_loss: 0.913(0.843), train_acc: 69.792(70.101)
12/17 01:43:40 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 501/521, train_loss: 0.926(0.843), train_acc: 62.500(70.172)
12/17 01:43:41 AM [Supernet Training] lr: 0.02317 epoch: 045/600, step: 521/521, train_loss: 0.703(0.841), train_acc: 78.750(70.266)
12/17 01:43:41 AM [Supernet Training] epoch: 045, train_loss: 0.841, train_acc: 70.266
12/17 01:43:43 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:43:43 AM [Supernet Validation] epoch: 045, val_loss: 0.877, val_acc: 69.010, best_acc: 69.010


12/17 01:43:43 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 001/521, train_loss: 0.831(0.831), train_acc: 69.792(69.792)
12/17 01:43:49 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 101/521, train_loss: 0.676(0.826), train_acc: 77.083(70.441)
12/17 01:43:56 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 201/521, train_loss: 0.835(0.818), train_acc: 67.708(70.652)
12/17 01:44:02 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 301/521, train_loss: 0.917(0.830), train_acc: 63.542(70.515)
12/17 01:44:09 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 401/521, train_loss: 0.939(0.834), train_acc: 61.458(70.329)
12/17 01:44:15 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 501/521, train_loss: 1.045(0.834), train_acc: 65.625(70.353)
12/17 01:44:16 AM [Supernet Training] lr: 0.02313 epoch: 046/600, step: 521/521, train_loss: 0.904(0.832), train_acc: 66.250(70.388)
12/17 01:44:16 AM [Supernet Training] epoch: 046, train_loss: 0.832, train_acc: 70.388
12/17 01:44:18 AM [Supernet Validation] epoch: 046, val_loss: 0.884, val_acc: 68.820, best_acc: 69.010


12/17 01:44:18 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 001/521, train_loss: 0.695(0.695), train_acc: 76.042(76.042)
12/17 01:44:24 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 101/521, train_loss: 0.819(0.823), train_acc: 67.708(70.936)
12/17 01:44:31 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 201/521, train_loss: 0.719(0.820), train_acc: 70.833(71.103)
12/17 01:44:37 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 301/521, train_loss: 0.783(0.823), train_acc: 72.917(70.809)
12/17 01:44:44 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 401/521, train_loss: 0.838(0.824), train_acc: 71.875(70.880)
12/17 01:44:50 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 501/521, train_loss: 0.762(0.826), train_acc: 69.792(70.682)
12/17 01:44:51 AM [Supernet Training] lr: 0.02308 epoch: 047/600, step: 521/521, train_loss: 0.804(0.827), train_acc: 66.250(70.592)
12/17 01:44:51 AM [Supernet Training] epoch: 047, train_loss: 0.827, train_acc: 70.592
12/17 01:44:53 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:44:53 AM [Supernet Validation] epoch: 047, val_loss: 0.863, val_acc: 69.870, best_acc: 69.870


12/17 01:44:53 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 001/521, train_loss: 0.689(0.689), train_acc: 76.042(76.042)
12/17 01:45:00 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 101/521, train_loss: 0.762(0.819), train_acc: 72.917(70.689)
12/17 01:45:06 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 201/521, train_loss: 0.750(0.812), train_acc: 72.917(71.004)
12/17 01:45:13 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 301/521, train_loss: 0.828(0.813), train_acc: 67.708(70.979)
12/17 01:45:19 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 401/521, train_loss: 0.719(0.810), train_acc: 72.917(71.127)
12/17 01:45:25 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 501/521, train_loss: 0.724(0.810), train_acc: 76.042(71.097)
12/17 01:45:27 AM [Supernet Training] lr: 0.02304 epoch: 048/600, step: 521/521, train_loss: 0.690(0.809), train_acc: 71.250(71.130)
12/17 01:45:27 AM [Supernet Training] epoch: 048, train_loss: 0.809, train_acc: 71.130
12/17 01:45:28 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:45:28 AM [Supernet Validation] epoch: 048, val_loss: 0.850, val_acc: 70.320, best_acc: 70.320


12/17 01:45:29 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 001/521, train_loss: 0.802(0.802), train_acc: 68.750(68.750)
12/17 01:45:35 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 101/521, train_loss: 0.789(0.814), train_acc: 68.750(70.833)
12/17 01:45:41 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 201/521, train_loss: 0.767(0.801), train_acc: 73.958(71.543)
12/17 01:45:48 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 301/521, train_loss: 0.621(0.796), train_acc: 79.167(71.671)
12/17 01:45:54 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 401/521, train_loss: 0.787(0.798), train_acc: 71.875(71.509)
12/17 01:46:01 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 501/521, train_loss: 0.891(0.796), train_acc: 70.833(71.640)
12/17 01:46:02 AM [Supernet Training] lr: 0.02300 epoch: 049/600, step: 521/521, train_loss: 0.782(0.795), train_acc: 73.750(71.672)
12/17 01:46:02 AM [Supernet Training] epoch: 049, train_loss: 0.795, train_acc: 71.672
12/17 01:46:04 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:46:04 AM [Supernet Validation] epoch: 049, val_loss: 0.839, val_acc: 70.970, best_acc: 70.970


12/17 01:46:04 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 001/521, train_loss: 0.703(0.703), train_acc: 67.708(67.708)
12/17 01:46:10 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 101/521, train_loss: 0.962(0.803), train_acc: 70.833(71.555)
12/17 01:46:17 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 201/521, train_loss: 0.741(0.797), train_acc: 70.833(71.911)
12/17 01:46:23 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 301/521, train_loss: 0.859(0.794), train_acc: 71.875(71.920)
12/17 01:46:29 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 401/521, train_loss: 0.839(0.794), train_acc: 69.792(71.987)
12/17 01:46:36 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 501/521, train_loss: 0.661(0.789), train_acc: 73.958(72.176)
12/17 01:46:37 AM [Supernet Training] lr: 0.02296 epoch: 050/600, step: 521/521, train_loss: 0.755(0.789), train_acc: 75.000(72.164)
12/17 01:46:37 AM [Supernet Training] epoch: 050, train_loss: 0.789, train_acc: 72.164
12/17 01:46:39 AM [Supernet Validation] epoch: 050, val_loss: 0.852, val_acc: 70.650, best_acc: 70.970


12/17 01:46:39 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 001/521, train_loss: 0.812(0.812), train_acc: 70.833(70.833)
12/17 01:46:45 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 101/521, train_loss: 0.743(0.795), train_acc: 72.917(72.040)
12/17 01:46:52 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 201/521, train_loss: 1.011(0.781), train_acc: 59.375(72.269)
12/17 01:46:58 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 301/521, train_loss: 0.784(0.783), train_acc: 72.917(72.131)
12/17 01:47:04 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 401/521, train_loss: 0.533(0.783), train_acc: 82.292(72.028)
12/17 01:47:11 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 501/521, train_loss: 0.604(0.783), train_acc: 80.208(72.100)
12/17 01:47:12 AM [Supernet Training] lr: 0.02292 epoch: 051/600, step: 521/521, train_loss: 0.614(0.783), train_acc: 81.250(72.066)
12/17 01:47:12 AM [Supernet Training] epoch: 051, train_loss: 0.783, train_acc: 72.066
12/17 01:47:14 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:47:14 AM [Supernet Validation] epoch: 051, val_loss: 0.820, val_acc: 71.450, best_acc: 71.450


12/17 01:47:14 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 001/521, train_loss: 0.633(0.633), train_acc: 80.208(80.208)
12/17 01:47:20 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 101/521, train_loss: 0.594(0.766), train_acc: 77.083(73.113)
12/17 01:47:27 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 201/521, train_loss: 0.858(0.760), train_acc: 65.625(73.228)
12/17 01:47:33 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 301/521, train_loss: 0.840(0.765), train_acc: 67.708(72.927)
12/17 01:47:40 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 401/521, train_loss: 0.858(0.767), train_acc: 70.833(72.854)
12/17 01:47:46 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 501/521, train_loss: 0.838(0.767), train_acc: 70.833(72.904)
12/17 01:47:47 AM [Supernet Training] lr: 0.02288 epoch: 052/600, step: 521/521, train_loss: 0.775(0.767), train_acc: 70.000(72.900)
12/17 01:47:47 AM [Supernet Training] epoch: 052, train_loss: 0.767, train_acc: 72.900
12/17 01:47:49 AM [Supernet Validation] epoch: 052, val_loss: 0.826, val_acc: 71.350, best_acc: 71.450


12/17 01:47:49 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 001/521, train_loss: 0.710(0.710), train_acc: 72.917(72.917)
12/17 01:47:55 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 101/521, train_loss: 0.868(0.766), train_acc: 72.917(73.154)
12/17 01:48:02 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 201/521, train_loss: 0.841(0.772), train_acc: 69.792(72.792)
12/17 01:48:08 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 301/521, train_loss: 0.728(0.769), train_acc: 76.042(72.896)
12/17 01:48:15 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 401/521, train_loss: 1.131(0.767), train_acc: 57.292(73.041)
12/17 01:48:21 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 501/521, train_loss: 0.837(0.765), train_acc: 65.625(72.992)
12/17 01:48:22 AM [Supernet Training] lr: 0.02283 epoch: 053/600, step: 521/521, train_loss: 0.903(0.764), train_acc: 63.750(73.018)
12/17 01:48:23 AM [Supernet Training] epoch: 053, train_loss: 0.764, train_acc: 73.018
12/17 01:48:24 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:48:24 AM [Supernet Validation] epoch: 053, val_loss: 0.808, val_acc: 71.770, best_acc: 71.770


12/17 01:48:25 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 001/521, train_loss: 0.772(0.772), train_acc: 63.542(63.542)
12/17 01:48:31 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 101/521, train_loss: 0.842(0.755), train_acc: 72.917(73.009)
12/17 01:48:37 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 201/521, train_loss: 0.651(0.750), train_acc: 76.042(73.228)
12/17 01:48:44 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 301/521, train_loss: 0.844(0.752), train_acc: 68.750(73.152)
12/17 01:48:50 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 401/521, train_loss: 0.703(0.754), train_acc: 73.958(73.099)
12/17 01:48:56 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 501/521, train_loss: 1.002(0.752), train_acc: 65.625(73.212)
12/17 01:48:58 AM [Supernet Training] lr: 0.02279 epoch: 054/600, step: 521/521, train_loss: 0.692(0.752), train_acc: 80.000(73.170)
12/17 01:48:58 AM [Supernet Training] epoch: 054, train_loss: 0.752, train_acc: 73.170
12/17 01:48:59 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:48:59 AM [Supernet Validation] epoch: 054, val_loss: 0.805, val_acc: 71.790, best_acc: 71.790


12/17 01:49:00 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 001/521, train_loss: 0.639(0.639), train_acc: 79.167(79.167)
12/17 01:49:06 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 101/521, train_loss: 0.707(0.758), train_acc: 73.958(73.123)
12/17 01:49:13 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 201/521, train_loss: 0.854(0.745), train_acc: 71.875(73.528)
12/17 01:49:19 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 301/521, train_loss: 0.759(0.750), train_acc: 69.792(73.349)
12/17 01:49:25 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 401/521, train_loss: 0.710(0.748), train_acc: 75.000(73.374)
12/17 01:49:32 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 501/521, train_loss: 0.891(0.744), train_acc: 70.833(73.524)
12/17 01:49:33 AM [Supernet Training] lr: 0.02275 epoch: 055/600, step: 521/521, train_loss: 0.677(0.744), train_acc: 70.000(73.538)
12/17 01:49:33 AM [Supernet Training] epoch: 055, train_loss: 0.744, train_acc: 73.538
12/17 01:49:35 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:49:35 AM [Supernet Validation] epoch: 055, val_loss: 0.799, val_acc: 72.550, best_acc: 72.550


12/17 01:49:35 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 001/521, train_loss: 0.783(0.783), train_acc: 77.083(77.083)
12/17 01:49:42 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 101/521, train_loss: 0.841(0.734), train_acc: 67.708(74.804)
12/17 01:49:48 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 201/521, train_loss: 0.774(0.739), train_acc: 79.167(74.306)
12/17 01:49:54 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 301/521, train_loss: 0.852(0.740), train_acc: 67.708(74.197)
12/17 01:50:01 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 401/521, train_loss: 0.801(0.736), train_acc: 73.958(74.265)
12/17 01:50:07 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 501/521, train_loss: 0.923(0.738), train_acc: 70.833(74.093)
12/17 01:50:08 AM [Supernet Training] lr: 0.02271 epoch: 056/600, step: 521/521, train_loss: 0.904(0.738), train_acc: 77.500(74.066)
12/17 01:50:08 AM [Supernet Training] epoch: 056, train_loss: 0.738, train_acc: 74.066
12/17 01:50:10 AM [Supernet Validation] epoch: 056, val_loss: 0.799, val_acc: 72.360, best_acc: 72.550


12/17 01:50:10 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 001/521, train_loss: 0.599(0.599), train_acc: 77.083(77.083)
12/17 01:50:17 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 101/521, train_loss: 0.392(0.729), train_acc: 85.417(74.577)
12/17 01:50:23 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 201/521, train_loss: 0.601(0.724), train_acc: 81.250(74.788)
12/17 01:50:29 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 301/521, train_loss: 0.581(0.729), train_acc: 82.292(74.495)
12/17 01:50:36 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 401/521, train_loss: 0.760(0.726), train_acc: 72.917(74.436)
12/17 01:50:42 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 501/521, train_loss: 0.674(0.725), train_acc: 78.125(74.478)
12/17 01:50:43 AM [Supernet Training] lr: 0.02267 epoch: 057/600, step: 521/521, train_loss: 0.686(0.724), train_acc: 73.750(74.494)
12/17 01:50:43 AM [Supernet Training] epoch: 057, train_loss: 0.724, train_acc: 74.494
12/17 01:50:45 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:50:45 AM [Supernet Validation] epoch: 057, val_loss: 0.785, val_acc: 72.890, best_acc: 72.890


12/17 01:50:45 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 001/521, train_loss: 0.805(0.805), train_acc: 64.583(64.583)
12/17 01:50:52 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 101/521, train_loss: 0.727(0.740), train_acc: 73.958(73.917)
12/17 01:50:58 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 201/521, train_loss: 0.528(0.729), train_acc: 83.333(74.399)
12/17 01:51:05 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 301/521, train_loss: 0.884(0.728), train_acc: 71.875(74.187)
12/17 01:51:11 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 401/521, train_loss: 0.728(0.727), train_acc: 69.792(74.247)
12/17 01:51:17 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 501/521, train_loss: 0.855(0.722), train_acc: 67.708(74.470)
12/17 01:51:19 AM [Supernet Training] lr: 0.02263 epoch: 058/600, step: 521/521, train_loss: 0.749(0.720), train_acc: 75.000(74.554)
12/17 01:51:19 AM [Supernet Training] epoch: 058, train_loss: 0.720, train_acc: 74.554
12/17 01:51:21 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:51:21 AM [Supernet Validation] epoch: 058, val_loss: 0.769, val_acc: 73.050, best_acc: 73.050


12/17 01:51:21 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 001/521, train_loss: 0.634(0.634), train_acc: 77.083(77.083)
12/17 01:51:27 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 101/521, train_loss: 0.540(0.719), train_acc: 82.292(74.670)
12/17 01:51:34 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 201/521, train_loss: 0.748(0.712), train_acc: 72.917(74.746)
12/17 01:51:40 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 301/521, train_loss: 0.680(0.707), train_acc: 76.042(74.983)
12/17 01:51:46 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 401/521, train_loss: 0.730(0.713), train_acc: 77.083(74.800)
12/17 01:51:53 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 501/521, train_loss: 0.795(0.714), train_acc: 73.958(74.821)
12/17 01:51:54 AM [Supernet Training] lr: 0.02258 epoch: 059/600, step: 521/521, train_loss: 0.521(0.713), train_acc: 83.750(74.868)
12/17 01:51:54 AM [Supernet Training] epoch: 059, train_loss: 0.713, train_acc: 74.868
12/17 01:51:56 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:51:56 AM [Supernet Validation] epoch: 059, val_loss: 0.768, val_acc: 73.470, best_acc: 73.470


12/17 01:51:56 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 001/521, train_loss: 0.783(0.783), train_acc: 70.833(70.833)
12/17 01:52:03 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 101/521, train_loss: 0.578(0.683), train_acc: 76.042(75.784)
12/17 01:52:09 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 201/521, train_loss: 0.624(0.691), train_acc: 76.042(75.363)
12/17 01:52:15 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 301/521, train_loss: 0.603(0.698), train_acc: 78.125(75.166)
12/17 01:52:22 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 401/521, train_loss: 0.632(0.695), train_acc: 73.958(75.348)
12/17 01:52:28 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 501/521, train_loss: 0.690(0.700), train_acc: 76.042(75.214)
12/17 01:52:29 AM [Supernet Training] lr: 0.02254 epoch: 060/600, step: 521/521, train_loss: 0.644(0.701), train_acc: 77.500(75.188)
12/17 01:52:29 AM [Supernet Training] epoch: 060, train_loss: 0.701, train_acc: 75.188
12/17 01:52:31 AM [Supernet Validation] epoch: 060, val_loss: 0.772, val_acc: 73.140, best_acc: 73.470


12/17 01:52:31 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 001/521, train_loss: 0.713(0.713), train_acc: 75.000(75.000)
12/17 01:52:38 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 101/521, train_loss: 0.745(0.691), train_acc: 73.958(75.361)
12/17 01:52:44 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 201/521, train_loss: 0.714(0.695), train_acc: 72.917(75.130)
12/17 01:52:50 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 301/521, train_loss: 0.643(0.698), train_acc: 78.125(75.152)
12/17 01:52:57 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 401/521, train_loss: 0.716(0.698), train_acc: 76.042(75.249)
12/17 01:53:03 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 501/521, train_loss: 0.932(0.697), train_acc: 73.958(75.418)
12/17 01:53:04 AM [Supernet Training] lr: 0.02250 epoch: 061/600, step: 521/521, train_loss: 0.577(0.698), train_acc: 77.500(75.358)
12/17 01:53:04 AM [Supernet Training] epoch: 061, train_loss: 0.698, train_acc: 75.358
12/17 01:53:06 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:53:06 AM [Supernet Validation] epoch: 061, val_loss: 0.747, val_acc: 73.630, best_acc: 73.630


12/17 01:53:06 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 001/521, train_loss: 0.651(0.651), train_acc: 78.125(78.125)
12/17 01:53:13 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 101/521, train_loss: 0.891(0.691), train_acc: 72.917(75.670)
12/17 01:53:19 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 201/521, train_loss: 0.703(0.685), train_acc: 76.042(75.943)
12/17 01:53:25 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 301/521, train_loss: 0.712(0.685), train_acc: 75.000(75.882)
12/17 01:53:32 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 401/521, train_loss: 0.570(0.687), train_acc: 77.083(75.823)
12/17 01:53:38 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 501/521, train_loss: 0.652(0.689), train_acc: 78.125(75.736)
12/17 01:53:39 AM [Supernet Training] lr: 0.02246 epoch: 062/600, step: 521/521, train_loss: 1.058(0.691), train_acc: 60.000(75.688)
12/17 01:53:39 AM [Supernet Training] epoch: 062, train_loss: 0.691, train_acc: 75.688
12/17 01:53:41 AM [Supernet Validation] epoch: 062, val_loss: 0.753, val_acc: 73.530, best_acc: 73.630


12/17 01:53:41 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 001/521, train_loss: 0.732(0.732), train_acc: 73.958(73.958)
12/17 01:53:48 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 101/521, train_loss: 0.714(0.674), train_acc: 73.958(76.186)
12/17 01:53:54 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 201/521, train_loss: 0.562(0.678), train_acc: 78.125(75.897)
12/17 01:54:00 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 301/521, train_loss: 0.670(0.678), train_acc: 76.042(75.886)
12/17 01:54:07 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 401/521, train_loss: 0.848(0.681), train_acc: 70.833(76.023)
12/17 01:54:13 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 501/521, train_loss: 0.627(0.682), train_acc: 78.125(75.971)
12/17 01:54:14 AM [Supernet Training] lr: 0.02242 epoch: 063/600, step: 521/521, train_loss: 0.929(0.682), train_acc: 70.000(75.968)
12/17 01:54:14 AM [Supernet Training] epoch: 063, train_loss: 0.682, train_acc: 75.968
12/17 01:54:16 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:54:16 AM [Supernet Validation] epoch: 063, val_loss: 0.737, val_acc: 74.070, best_acc: 74.070


12/17 01:54:16 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 001/521, train_loss: 0.721(0.721), train_acc: 72.917(72.917)
12/17 01:54:23 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 101/521, train_loss: 0.663(0.673), train_acc: 75.000(76.104)
12/17 01:54:30 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 201/521, train_loss: 0.677(0.668), train_acc: 77.083(76.534)
12/17 01:54:36 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 301/521, train_loss: 0.734(0.671), train_acc: 75.000(76.568)
12/17 01:54:43 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 401/521, train_loss: 0.606(0.673), train_acc: 77.083(76.398)
12/17 01:54:50 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 501/521, train_loss: 0.621(0.674), train_acc: 78.125(76.314)
12/17 01:54:51 AM [Supernet Training] lr: 0.02238 epoch: 064/600, step: 521/521, train_loss: 0.623(0.673), train_acc: 80.000(76.328)
12/17 01:54:51 AM [Supernet Training] epoch: 064, train_loss: 0.673, train_acc: 76.328
12/17 01:54:53 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:54:53 AM [Supernet Validation] epoch: 064, val_loss: 0.743, val_acc: 74.340, best_acc: 74.340


12/17 01:54:53 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 001/521, train_loss: 0.589(0.589), train_acc: 84.375(84.375)
12/17 01:55:00 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 101/521, train_loss: 0.758(0.674), train_acc: 75.000(76.351)
12/17 01:55:06 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 201/521, train_loss: 0.554(0.667), train_acc: 79.167(76.534)
12/17 01:55:13 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 301/521, train_loss: 0.618(0.669), train_acc: 79.167(76.346)
12/17 01:55:19 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 401/521, train_loss: 0.822(0.668), train_acc: 75.000(76.387)
12/17 01:55:26 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 501/521, train_loss: 0.789(0.664), train_acc: 70.833(76.474)
12/17 01:55:27 AM [Supernet Training] lr: 0.02233 epoch: 065/600, step: 521/521, train_loss: 0.696(0.663), train_acc: 78.750(76.530)
12/17 01:55:27 AM [Supernet Training] epoch: 065, train_loss: 0.663, train_acc: 76.530
12/17 01:55:28 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:55:28 AM [Supernet Validation] epoch: 065, val_loss: 0.729, val_acc: 74.520, best_acc: 74.520


12/17 01:55:29 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 001/521, train_loss: 0.562(0.562), train_acc: 80.208(80.208)
12/17 01:55:35 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 101/521, train_loss: 0.475(0.674), train_acc: 81.250(76.155)
12/17 01:55:41 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 201/521, train_loss: 0.798(0.674), train_acc: 70.833(76.192)
12/17 01:55:48 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 301/521, train_loss: 0.747(0.667), train_acc: 71.875(76.235)
12/17 01:55:54 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 401/521, train_loss: 0.755(0.668), train_acc: 76.042(76.330)
12/17 01:56:00 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 501/521, train_loss: 0.462(0.665), train_acc: 84.375(76.439)
12/17 01:56:02 AM [Supernet Training] lr: 0.02229 epoch: 066/600, step: 521/521, train_loss: 0.709(0.663), train_acc: 77.500(76.532)
12/17 01:56:02 AM [Supernet Training] epoch: 066, train_loss: 0.663, train_acc: 76.532
12/17 01:56:03 AM [Supernet Validation] epoch: 066, val_loss: 0.753, val_acc: 74.110, best_acc: 74.520


12/17 01:56:04 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 001/521, train_loss: 0.724(0.724), train_acc: 77.083(77.083)
12/17 01:56:10 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 101/521, train_loss: 0.591(0.652), train_acc: 79.167(76.774)
12/17 01:56:16 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 201/521, train_loss: 0.755(0.660), train_acc: 75.000(76.684)
12/17 01:56:23 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 301/521, train_loss: 0.681(0.661), train_acc: 75.000(76.685)
12/17 01:56:29 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 401/521, train_loss: 0.622(0.657), train_acc: 79.167(76.746)
12/17 01:56:35 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 501/521, train_loss: 0.524(0.657), train_acc: 80.208(76.705)
12/17 01:56:37 AM [Supernet Training] lr: 0.02225 epoch: 067/600, step: 521/521, train_loss: 0.461(0.654), train_acc: 85.000(76.808)
12/17 01:56:37 AM [Supernet Training] epoch: 067, train_loss: 0.654, train_acc: 76.808
12/17 01:56:38 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:56:38 AM [Supernet Validation] epoch: 067, val_loss: 0.728, val_acc: 74.850, best_acc: 74.850


12/17 01:56:38 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 001/521, train_loss: 0.582(0.582), train_acc: 77.083(77.083)
12/17 01:56:45 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 101/521, train_loss: 0.518(0.649), train_acc: 80.208(77.424)
12/17 01:56:51 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 201/521, train_loss: 0.667(0.640), train_acc: 76.042(77.425)
12/17 01:56:57 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 301/521, train_loss: 0.756(0.642), train_acc: 79.167(77.267)
12/17 01:57:04 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 401/521, train_loss: 0.583(0.643), train_acc: 80.208(77.244)
12/17 01:57:10 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 501/521, train_loss: 0.720(0.644), train_acc: 76.042(77.250)
12/17 01:57:12 AM [Supernet Training] lr: 0.02221 epoch: 068/600, step: 521/521, train_loss: 0.714(0.645), train_acc: 75.000(77.196)
12/17 01:57:12 AM [Supernet Training] epoch: 068, train_loss: 0.645, train_acc: 77.196
12/17 01:57:13 AM [Supernet Validation] epoch: 068, val_loss: 0.724, val_acc: 74.530, best_acc: 74.850


12/17 01:57:14 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 001/521, train_loss: 0.586(0.586), train_acc: 81.250(81.250)
12/17 01:57:20 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 101/521, train_loss: 0.697(0.642), train_acc: 76.042(77.547)
12/17 01:57:26 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 201/521, train_loss: 0.652(0.632), train_acc: 75.000(77.773)
12/17 01:57:33 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 301/521, train_loss: 0.945(0.630), train_acc: 67.708(77.803)
12/17 01:57:39 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 401/521, train_loss: 0.565(0.636), train_acc: 81.250(77.665)
12/17 01:57:45 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 501/521, train_loss: 0.819(0.635), train_acc: 71.875(77.765)
12/17 01:57:47 AM [Supernet Training] lr: 0.02217 epoch: 069/600, step: 521/521, train_loss: 0.480(0.636), train_acc: 83.750(77.738)
12/17 01:57:47 AM [Supernet Training] epoch: 069, train_loss: 0.636, train_acc: 77.738
12/17 01:57:48 AM [Supernet Validation] epoch: 069, val_loss: 0.716, val_acc: 74.850, best_acc: 74.850


12/17 01:57:49 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 001/521, train_loss: 0.726(0.726), train_acc: 73.958(73.958)
12/17 01:57:55 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 101/521, train_loss: 0.802(0.645), train_acc: 69.792(77.290)
12/17 01:58:01 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 201/521, train_loss: 0.669(0.639), train_acc: 78.125(77.498)
12/17 01:58:08 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 301/521, train_loss: 0.605(0.638), train_acc: 78.125(77.585)
12/17 01:58:14 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 401/521, train_loss: 0.801(0.637), train_acc: 70.833(77.616)
12/17 01:58:21 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 501/521, train_loss: 0.541(0.636), train_acc: 76.042(77.622)
12/17 01:58:22 AM [Supernet Training] lr: 0.02213 epoch: 070/600, step: 521/521, train_loss: 0.691(0.636), train_acc: 71.250(77.620)
12/17 01:58:22 AM [Supernet Training] epoch: 070, train_loss: 0.636, train_acc: 77.620
12/17 01:58:24 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:58:24 AM [Supernet Validation] epoch: 070, val_loss: 0.712, val_acc: 75.410, best_acc: 75.410


12/17 01:58:24 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 001/521, train_loss: 0.659(0.659), train_acc: 73.958(73.958)
12/17 01:58:30 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 101/521, train_loss: 0.761(0.635), train_acc: 72.917(77.640)
12/17 01:58:37 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 201/521, train_loss: 0.681(0.630), train_acc: 76.042(77.814)
12/17 01:58:43 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 301/521, train_loss: 0.729(0.624), train_acc: 73.958(77.980)
12/17 01:58:49 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 401/521, train_loss: 0.530(0.624), train_acc: 78.125(77.990)
12/17 01:58:56 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 501/521, train_loss: 0.713(0.624), train_acc: 78.125(78.042)
12/17 01:58:57 AM [Supernet Training] lr: 0.02208 epoch: 071/600, step: 521/521, train_loss: 0.662(0.623), train_acc: 72.500(78.058)
12/17 01:58:57 AM [Supernet Training] epoch: 071, train_loss: 0.623, train_acc: 78.058
12/17 01:59:01 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 01:59:01 AM [Supernet Validation] epoch: 071, val_loss: 0.694, val_acc: 75.920, best_acc: 75.920


12/17 01:59:01 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 001/521, train_loss: 0.504(0.504), train_acc: 80.208(80.208)
12/17 01:59:08 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 101/521, train_loss: 0.797(0.617), train_acc: 71.875(78.218)
12/17 01:59:14 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 201/521, train_loss: 0.517(0.627), train_acc: 79.167(77.778)
12/17 01:59:20 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 301/521, train_loss: 0.394(0.622), train_acc: 85.417(78.070)
12/17 01:59:26 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 401/521, train_loss: 0.644(0.618), train_acc: 77.083(78.309)
12/17 01:59:33 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 501/521, train_loss: 0.676(0.618), train_acc: 77.083(78.271)
12/17 01:59:34 AM [Supernet Training] lr: 0.02204 epoch: 072/600, step: 521/521, train_loss: 0.777(0.618), train_acc: 76.250(78.248)
12/17 01:59:34 AM [Supernet Training] epoch: 072, train_loss: 0.618, train_acc: 78.248
12/17 01:59:36 AM [Supernet Validation] epoch: 072, val_loss: 0.689, val_acc: 75.890, best_acc: 75.920


12/17 01:59:36 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 001/521, train_loss: 0.564(0.564), train_acc: 78.125(78.125)
12/17 01:59:42 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 101/521, train_loss: 0.594(0.613), train_acc: 80.208(78.589)
12/17 01:59:49 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 201/521, train_loss: 0.639(0.616), train_acc: 75.000(78.400)
12/17 01:59:55 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 301/521, train_loss: 0.668(0.611), train_acc: 79.167(78.592)
12/17 02:00:01 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 401/521, train_loss: 0.435(0.613), train_acc: 84.375(78.504)
12/17 02:00:08 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 501/521, train_loss: 0.762(0.614), train_acc: 72.917(78.424)
12/17 02:00:10 AM [Supernet Training] lr: 0.02200 epoch: 073/600, step: 521/521, train_loss: 0.634(0.614), train_acc: 81.250(78.392)
12/17 02:00:10 AM [Supernet Training] epoch: 073, train_loss: 0.614, train_acc: 78.392
12/17 02:00:11 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:00:11 AM [Supernet Validation] epoch: 073, val_loss: 0.694, val_acc: 75.980, best_acc: 75.980


12/17 02:00:12 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 001/521, train_loss: 0.612(0.612), train_acc: 82.292(82.292)
12/17 02:00:18 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 101/521, train_loss: 0.680(0.606), train_acc: 72.917(78.837)
12/17 02:00:25 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 201/521, train_loss: 0.477(0.609), train_acc: 81.250(78.788)
12/17 02:00:31 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 301/521, train_loss: 0.496(0.605), train_acc: 84.375(78.949)
12/17 02:00:37 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 401/521, train_loss: 0.720(0.608), train_acc: 75.000(78.803)
12/17 02:00:44 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 501/521, train_loss: 0.555(0.604), train_acc: 81.250(78.896)
12/17 02:00:45 AM [Supernet Training] lr: 0.02196 epoch: 074/600, step: 521/521, train_loss: 0.699(0.604), train_acc: 77.500(78.900)
12/17 02:00:45 AM [Supernet Training] epoch: 074, train_loss: 0.604, train_acc: 78.900
12/17 02:00:46 AM [Supernet Validation] epoch: 074, val_loss: 0.697, val_acc: 75.950, best_acc: 75.980


12/17 02:00:47 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 001/521, train_loss: 0.574(0.574), train_acc: 82.292(82.292)
12/17 02:00:53 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 101/521, train_loss: 0.502(0.592), train_acc: 83.333(78.816)
12/17 02:01:00 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 201/521, train_loss: 0.504(0.597), train_acc: 83.333(78.804)
12/17 02:01:06 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 301/521, train_loss: 0.893(0.604), train_acc: 66.667(78.644)
12/17 02:01:12 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 401/521, train_loss: 0.915(0.604), train_acc: 69.792(78.728)
12/17 02:01:19 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 501/521, train_loss: 0.634(0.605), train_acc: 77.083(78.722)
12/17 02:01:20 AM [Supernet Training] lr: 0.02192 epoch: 075/600, step: 521/521, train_loss: 0.736(0.605), train_acc: 72.500(78.734)
12/17 02:01:20 AM [Supernet Training] epoch: 075, train_loss: 0.605, train_acc: 78.734
12/17 02:01:22 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:01:22 AM [Supernet Validation] epoch: 075, val_loss: 0.675, val_acc: 77.020, best_acc: 77.020


12/17 02:01:22 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 001/521, train_loss: 0.752(0.752), train_acc: 77.083(77.083)
12/17 02:01:28 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 101/521, train_loss: 0.605(0.594), train_acc: 79.167(78.950)
12/17 02:01:35 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 201/521, train_loss: 0.569(0.586), train_acc: 73.958(79.353)
12/17 02:01:41 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 301/521, train_loss: 0.683(0.599), train_acc: 76.042(79.032)
12/17 02:01:48 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 401/521, train_loss: 0.564(0.595), train_acc: 86.458(79.086)
12/17 02:01:54 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 501/521, train_loss: 0.640(0.592), train_acc: 73.958(79.183)
12/17 02:01:55 AM [Supernet Training] lr: 0.02188 epoch: 076/600, step: 521/521, train_loss: 0.589(0.593), train_acc: 78.750(79.172)
12/17 02:01:55 AM [Supernet Training] epoch: 076, train_loss: 0.593, train_acc: 79.172
12/17 02:01:57 AM [Supernet Validation] epoch: 076, val_loss: 0.690, val_acc: 76.500, best_acc: 77.020


12/17 02:01:57 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 001/521, train_loss: 0.690(0.690), train_acc: 72.917(72.917)
12/17 02:02:03 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 101/521, train_loss: 0.637(0.604), train_acc: 80.208(78.476)
12/17 02:02:10 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 201/521, train_loss: 0.566(0.595), train_acc: 80.208(78.923)
12/17 02:02:16 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 301/521, train_loss: 0.569(0.594), train_acc: 84.375(79.049)
12/17 02:02:23 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 401/521, train_loss: 0.599(0.591), train_acc: 80.208(79.112)
12/17 02:02:29 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 501/521, train_loss: 0.562(0.588), train_acc: 78.125(79.244)
12/17 02:02:30 AM [Supernet Training] lr: 0.02183 epoch: 077/600, step: 521/521, train_loss: 0.616(0.589), train_acc: 82.500(79.246)
12/17 02:02:30 AM [Supernet Training] epoch: 077, train_loss: 0.589, train_acc: 79.246
12/17 02:02:32 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:02:32 AM [Supernet Validation] epoch: 077, val_loss: 0.652, val_acc: 77.540, best_acc: 77.540


12/17 02:02:32 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 001/521, train_loss: 0.576(0.576), train_acc: 79.167(79.167)
12/17 02:02:39 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 101/521, train_loss: 0.319(0.568), train_acc: 90.625(79.837)
12/17 02:02:45 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 201/521, train_loss: 0.537(0.579), train_acc: 84.375(79.576)
12/17 02:02:51 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 301/521, train_loss: 0.581(0.587), train_acc: 77.083(79.329)
12/17 02:02:57 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 401/521, train_loss: 0.613(0.587), train_acc: 80.208(79.349)
12/17 02:03:04 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 501/521, train_loss: 0.542(0.585), train_acc: 83.333(79.510)
12/17 02:03:05 AM [Supernet Training] lr: 0.02179 epoch: 078/600, step: 521/521, train_loss: 0.575(0.584), train_acc: 78.750(79.524)
12/17 02:03:05 AM [Supernet Training] epoch: 078, train_loss: 0.584, train_acc: 79.524
12/17 02:03:06 AM [Supernet Validation] epoch: 078, val_loss: 0.672, val_acc: 77.000, best_acc: 77.540


12/17 02:03:07 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 001/521, train_loss: 0.594(0.594), train_acc: 73.958(73.958)
12/17 02:03:13 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 101/521, train_loss: 0.530(0.579), train_acc: 80.208(79.239)
12/17 02:03:19 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 201/521, train_loss: 0.497(0.569), train_acc: 84.375(79.721)
12/17 02:03:26 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 301/521, train_loss: 0.729(0.568), train_acc: 72.917(79.855)
12/17 02:03:32 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 401/521, train_loss: 0.538(0.570), train_acc: 81.250(79.788)
12/17 02:03:38 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 501/521, train_loss: 0.541(0.576), train_acc: 82.292(79.603)
12/17 02:03:39 AM [Supernet Training] lr: 0.02175 epoch: 079/600, step: 521/521, train_loss: 0.758(0.577), train_acc: 71.250(79.558)
12/17 02:03:39 AM [Supernet Training] epoch: 079, train_loss: 0.577, train_acc: 79.558
12/17 02:03:41 AM [Supernet Validation] epoch: 079, val_loss: 0.660, val_acc: 77.360, best_acc: 77.540


12/17 02:03:41 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 001/521, train_loss: 0.695(0.695), train_acc: 78.125(78.125)
12/17 02:03:47 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 101/521, train_loss: 0.408(0.549), train_acc: 84.375(80.549)
12/17 02:03:54 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 201/521, train_loss: 0.489(0.563), train_acc: 86.458(80.110)
12/17 02:04:00 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 301/521, train_loss: 0.601(0.569), train_acc: 75.000(79.966)
12/17 02:04:07 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 401/521, train_loss: 0.650(0.569), train_acc: 76.042(80.037)
12/17 02:04:14 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 501/521, train_loss: 0.524(0.568), train_acc: 82.292(80.007)
12/17 02:04:15 AM [Supernet Training] lr: 0.02171 epoch: 080/600, step: 521/521, train_loss: 0.723(0.568), train_acc: 77.500(80.040)
12/17 02:04:15 AM [Supernet Training] epoch: 080, train_loss: 0.568, train_acc: 80.040
12/17 02:04:17 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:04:17 AM [Supernet Validation] epoch: 080, val_loss: 0.639, val_acc: 77.870, best_acc: 77.870


12/17 02:04:17 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 001/521, train_loss: 0.477(0.477), train_acc: 85.417(85.417)
12/17 02:04:24 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 101/521, train_loss: 0.473(0.555), train_acc: 85.417(80.693)
12/17 02:04:30 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 201/521, train_loss: 0.497(0.565), train_acc: 83.333(80.436)
12/17 02:04:36 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 301/521, train_loss: 0.599(0.565), train_acc: 78.125(80.312)
12/17 02:04:42 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 401/521, train_loss: 0.443(0.567), train_acc: 85.417(80.263)
12/17 02:04:49 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 501/521, train_loss: 0.516(0.568), train_acc: 80.208(80.177)
12/17 02:04:50 AM [Supernet Training] lr: 0.02167 epoch: 081/600, step: 521/521, train_loss: 0.759(0.569), train_acc: 68.750(80.118)
12/17 02:04:50 AM [Supernet Training] epoch: 081, train_loss: 0.569, train_acc: 80.118
12/17 02:04:52 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:04:52 AM [Supernet Validation] epoch: 081, val_loss: 0.647, val_acc: 78.020, best_acc: 78.020


12/17 02:04:52 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 001/521, train_loss: 0.512(0.512), train_acc: 82.292(82.292)
12/17 02:04:59 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 101/521, train_loss: 0.517(0.559), train_acc: 82.292(80.528)
12/17 02:05:05 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 201/521, train_loss: 0.491(0.556), train_acc: 82.292(80.592)
12/17 02:05:11 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 301/521, train_loss: 0.616(0.562), train_acc: 82.292(80.447)
12/17 02:05:18 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 401/521, train_loss: 0.652(0.564), train_acc: 79.167(80.310)
12/17 02:05:24 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 501/521, train_loss: 0.674(0.565), train_acc: 76.042(80.194)
12/17 02:05:25 AM [Supernet Training] lr: 0.02163 epoch: 082/600, step: 521/521, train_loss: 0.529(0.565), train_acc: 81.250(80.198)
12/17 02:05:25 AM [Supernet Training] epoch: 082, train_loss: 0.565, train_acc: 80.198
12/17 02:05:27 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:05:27 AM [Supernet Validation] epoch: 082, val_loss: 0.639, val_acc: 78.170, best_acc: 78.170


12/17 02:05:27 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 001/521, train_loss: 0.479(0.479), train_acc: 81.250(81.250)
12/17 02:05:34 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 101/521, train_loss: 0.455(0.560), train_acc: 83.333(79.961)
12/17 02:05:40 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 201/521, train_loss: 0.639(0.554), train_acc: 77.083(80.333)
12/17 02:05:47 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 301/521, train_loss: 0.529(0.554), train_acc: 81.250(80.534)
12/17 02:05:53 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 401/521, train_loss: 0.619(0.560), train_acc: 76.042(80.437)
12/17 02:05:59 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 501/521, train_loss: 0.491(0.559), train_acc: 81.250(80.439)
12/17 02:06:01 AM [Supernet Training] lr: 0.02158 epoch: 083/600, step: 521/521, train_loss: 0.461(0.559), train_acc: 83.750(80.410)
12/17 02:06:01 AM [Supernet Training] epoch: 083, train_loss: 0.559, train_acc: 80.410
12/17 02:06:02 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:06:02 AM [Supernet Validation] epoch: 083, val_loss: 0.628, val_acc: 78.530, best_acc: 78.530


12/17 02:06:03 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 001/521, train_loss: 0.486(0.486), train_acc: 86.458(86.458)
12/17 02:06:09 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 101/521, train_loss: 0.617(0.560), train_acc: 77.083(80.621)
12/17 02:06:15 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 201/521, train_loss: 0.511(0.552), train_acc: 81.250(80.789)
12/17 02:06:22 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 301/521, train_loss: 0.572(0.553), train_acc: 81.250(80.714)
12/17 02:06:28 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 401/521, train_loss: 0.557(0.550), train_acc: 80.208(80.759)
12/17 02:06:35 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 501/521, train_loss: 0.470(0.552), train_acc: 82.292(80.668)
12/17 02:06:36 AM [Supernet Training] lr: 0.02154 epoch: 084/600, step: 521/521, train_loss: 0.476(0.551), train_acc: 77.500(80.704)
12/17 02:06:36 AM [Supernet Training] epoch: 084, train_loss: 0.551, train_acc: 80.704
12/17 02:06:38 AM [Supernet Validation] epoch: 084, val_loss: 0.642, val_acc: 77.930, best_acc: 78.530


12/17 02:06:38 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 001/521, train_loss: 0.486(0.486), train_acc: 82.292(82.292)
12/17 02:06:44 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 101/521, train_loss: 0.523(0.546), train_acc: 82.292(80.827)
12/17 02:06:51 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 201/521, train_loss: 0.492(0.546), train_acc: 82.292(80.659)
12/17 02:06:57 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 301/521, train_loss: 0.607(0.547), train_acc: 76.042(80.669)
12/17 02:07:03 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 401/521, train_loss: 0.596(0.549), train_acc: 82.292(80.686)
12/17 02:07:10 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 501/521, train_loss: 0.454(0.548), train_acc: 84.375(80.755)
12/17 02:07:11 AM [Supernet Training] lr: 0.02150 epoch: 085/600, step: 521/521, train_loss: 0.700(0.547), train_acc: 73.750(80.794)
12/17 02:07:11 AM [Supernet Training] epoch: 085, train_loss: 0.547, train_acc: 80.794
12/17 02:07:12 AM [Supernet Validation] epoch: 085, val_loss: 0.624, val_acc: 78.140, best_acc: 78.530


12/17 02:07:13 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 001/521, train_loss: 0.446(0.446), train_acc: 82.292(82.292)
12/17 02:07:19 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 101/521, train_loss: 0.599(0.548), train_acc: 83.333(80.456)
12/17 02:07:26 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 201/521, train_loss: 0.619(0.542), train_acc: 80.208(80.830)
12/17 02:07:32 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 301/521, train_loss: 0.537(0.537), train_acc: 78.125(81.091)
12/17 02:07:38 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 401/521, train_loss: 0.659(0.541), train_acc: 79.167(80.847)
12/17 02:07:45 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 501/521, train_loss: 0.549(0.543), train_acc: 79.167(80.832)
12/17 02:07:46 AM [Supernet Training] lr: 0.02146 epoch: 086/600, step: 521/521, train_loss: 0.713(0.543), train_acc: 73.750(80.780)
12/17 02:07:46 AM [Supernet Training] epoch: 086, train_loss: 0.543, train_acc: 80.780
12/17 02:07:47 AM [Supernet Validation] epoch: 086, val_loss: 0.636, val_acc: 78.390, best_acc: 78.530


12/17 02:07:48 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 001/521, train_loss: 0.614(0.614), train_acc: 82.292(82.292)
12/17 02:07:54 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 101/521, train_loss: 0.635(0.544), train_acc: 80.208(80.992)
12/17 02:08:00 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 201/521, train_loss: 0.579(0.540), train_acc: 77.083(80.924)
12/17 02:08:07 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 301/521, train_loss: 0.481(0.541), train_acc: 85.417(81.129)
12/17 02:08:13 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 401/521, train_loss: 0.319(0.541), train_acc: 91.667(81.047)
12/17 02:08:19 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 501/521, train_loss: 0.574(0.538), train_acc: 82.292(81.115)
12/17 02:08:21 AM [Supernet Training] lr: 0.02142 epoch: 087/600, step: 521/521, train_loss: 0.483(0.538), train_acc: 82.500(81.128)
12/17 02:08:21 AM [Supernet Training] epoch: 087, train_loss: 0.538, train_acc: 81.128
12/17 02:08:22 AM [Supernet Validation] epoch: 087, val_loss: 0.649, val_acc: 77.910, best_acc: 78.530


12/17 02:08:22 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 001/521, train_loss: 0.553(0.553), train_acc: 85.417(85.417)
12/17 02:08:29 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 101/521, train_loss: 0.435(0.527), train_acc: 84.375(81.415)
12/17 02:08:35 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 201/521, train_loss: 0.454(0.532), train_acc: 85.417(81.260)
12/17 02:08:41 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 301/521, train_loss: 0.472(0.533), train_acc: 80.208(81.330)
12/17 02:08:48 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 401/521, train_loss: 0.497(0.535), train_acc: 79.167(81.206)
12/17 02:08:54 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 501/521, train_loss: 0.602(0.537), train_acc: 76.042(81.121)
12/17 02:08:55 AM [Supernet Training] lr: 0.02138 epoch: 088/600, step: 521/521, train_loss: 0.380(0.536), train_acc: 87.500(81.174)
12/17 02:08:55 AM [Supernet Training] epoch: 088, train_loss: 0.536, train_acc: 81.174
12/17 02:08:57 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:08:57 AM [Supernet Validation] epoch: 088, val_loss: 0.602, val_acc: 79.030, best_acc: 79.030


12/17 02:08:57 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 001/521, train_loss: 0.685(0.685), train_acc: 71.875(71.875)
12/17 02:09:04 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 101/521, train_loss: 0.668(0.519), train_acc: 76.042(81.673)
12/17 02:09:10 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 201/521, train_loss: 0.579(0.532), train_acc: 77.083(81.359)
12/17 02:09:16 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 301/521, train_loss: 0.437(0.538), train_acc: 81.250(81.087)
12/17 02:09:23 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 401/521, train_loss: 0.543(0.530), train_acc: 81.250(81.429)
12/17 02:09:29 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 501/521, train_loss: 0.537(0.529), train_acc: 80.208(81.369)
12/17 02:09:30 AM [Supernet Training] lr: 0.02133 epoch: 089/600, step: 521/521, train_loss: 0.599(0.529), train_acc: 75.000(81.366)
12/17 02:09:30 AM [Supernet Training] epoch: 089, train_loss: 0.529, train_acc: 81.366
12/17 02:09:32 AM [Supernet Validation] epoch: 089, val_loss: 0.626, val_acc: 78.400, best_acc: 79.030


12/17 02:09:32 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 001/521, train_loss: 0.501(0.501), train_acc: 79.167(79.167)
12/17 02:09:38 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 101/521, train_loss: 0.637(0.538), train_acc: 79.167(81.250)
12/17 02:09:45 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 201/521, train_loss: 0.480(0.525), train_acc: 83.333(81.504)
12/17 02:09:51 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 301/521, train_loss: 0.661(0.529), train_acc: 72.917(81.274)
12/17 02:09:57 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 401/521, train_loss: 0.703(0.530), train_acc: 76.042(81.297)
12/17 02:10:04 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 501/521, train_loss: 0.496(0.527), train_acc: 84.375(81.450)
12/17 02:10:05 AM [Supernet Training] lr: 0.02129 epoch: 090/600, step: 521/521, train_loss: 0.480(0.525), train_acc: 83.750(81.444)
12/17 02:10:05 AM [Supernet Training] epoch: 090, train_loss: 0.525, train_acc: 81.444
12/17 02:10:06 AM [Supernet Validation] epoch: 090, val_loss: 0.619, val_acc: 78.810, best_acc: 79.030


12/17 02:10:07 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 001/521, train_loss: 0.501(0.501), train_acc: 85.417(85.417)
12/17 02:10:13 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 101/521, train_loss: 0.529(0.526), train_acc: 78.125(81.518)
12/17 02:10:19 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 201/521, train_loss: 0.504(0.518), train_acc: 79.167(81.939)
12/17 02:10:26 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 301/521, train_loss: 0.388(0.514), train_acc: 85.417(82.216)
12/17 02:10:32 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 401/521, train_loss: 0.499(0.518), train_acc: 86.458(82.058)
12/17 02:10:39 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 501/521, train_loss: 0.450(0.519), train_acc: 81.250(81.922)
12/17 02:10:40 AM [Supernet Training] lr: 0.02125 epoch: 091/600, step: 521/521, train_loss: 0.475(0.520), train_acc: 81.250(81.890)
12/17 02:10:40 AM [Supernet Training] epoch: 091, train_loss: 0.520, train_acc: 81.890
12/17 02:10:42 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:10:42 AM [Supernet Validation] epoch: 091, val_loss: 0.602, val_acc: 79.610, best_acc: 79.610


12/17 02:10:42 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 001/521, train_loss: 0.389(0.389), train_acc: 85.417(85.417)
12/17 02:10:48 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 101/521, train_loss: 0.496(0.507), train_acc: 81.250(82.147)
12/17 02:10:55 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 201/521, train_loss: 0.415(0.504), train_acc: 84.375(82.437)
12/17 02:11:01 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 301/521, train_loss: 0.503(0.512), train_acc: 80.208(82.015)
12/17 02:11:08 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 401/521, train_loss: 0.641(0.516), train_acc: 81.250(81.827)
12/17 02:11:14 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 501/521, train_loss: 0.281(0.513), train_acc: 89.583(81.872)
12/17 02:11:15 AM [Supernet Training] lr: 0.02121 epoch: 092/600, step: 521/521, train_loss: 0.665(0.514), train_acc: 81.250(81.872)
12/17 02:11:15 AM [Supernet Training] epoch: 092, train_loss: 0.514, train_acc: 81.872
12/17 02:11:17 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:11:17 AM [Supernet Validation] epoch: 092, val_loss: 0.599, val_acc: 79.800, best_acc: 79.800


12/17 02:11:17 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 001/521, train_loss: 0.364(0.364), train_acc: 87.500(87.500)
12/17 02:11:24 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 101/521, train_loss: 0.361(0.506), train_acc: 86.458(82.106)
12/17 02:11:30 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 201/521, train_loss: 0.489(0.501), train_acc: 87.500(82.406)
12/17 02:11:36 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 301/521, train_loss: 0.493(0.508), train_acc: 82.292(82.191)
12/17 02:11:43 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 401/521, train_loss: 0.510(0.512), train_acc: 81.250(82.066)
12/17 02:11:49 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 501/521, train_loss: 0.555(0.514), train_acc: 81.250(82.065)
12/17 02:11:50 AM [Supernet Training] lr: 0.02117 epoch: 093/600, step: 521/521, train_loss: 0.624(0.514), train_acc: 83.750(82.068)
12/17 02:11:50 AM [Supernet Training] epoch: 093, train_loss: 0.514, train_acc: 82.068
12/17 02:11:52 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:11:52 AM [Supernet Validation] epoch: 093, val_loss: 0.586, val_acc: 80.110, best_acc: 80.110


12/17 02:11:52 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 001/521, train_loss: 0.317(0.317), train_acc: 89.583(89.583)
12/17 02:11:59 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 101/521, train_loss: 0.426(0.495), train_acc: 85.417(82.828)
12/17 02:12:05 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 201/521, train_loss: 0.398(0.506), train_acc: 85.417(82.509)
12/17 02:12:11 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 301/521, train_loss: 0.436(0.506), train_acc: 80.208(82.378)
12/17 02:12:18 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 401/521, train_loss: 0.527(0.506), train_acc: 78.125(82.242)
12/17 02:12:24 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 501/521, train_loss: 0.544(0.508), train_acc: 82.292(82.146)
12/17 02:12:25 AM [Supernet Training] lr: 0.02113 epoch: 094/600, step: 521/521, train_loss: 0.418(0.508), train_acc: 87.500(82.128)
12/17 02:12:25 AM [Supernet Training] epoch: 094, train_loss: 0.508, train_acc: 82.128
12/17 02:12:27 AM [Supernet Validation] epoch: 094, val_loss: 0.600, val_acc: 79.100, best_acc: 80.110


12/17 02:12:27 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 001/521, train_loss: 0.465(0.465), train_acc: 83.333(83.333)
12/17 02:12:34 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 101/521, train_loss: 0.400(0.496), train_acc: 82.292(82.477)
12/17 02:12:40 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 201/521, train_loss: 0.421(0.499), train_acc: 82.292(82.390)
12/17 02:12:47 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 301/521, train_loss: 0.508(0.496), train_acc: 83.333(82.524)
12/17 02:12:53 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 401/521, train_loss: 0.594(0.497), train_acc: 80.208(82.453)
12/17 02:13:00 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 501/521, train_loss: 0.505(0.501), train_acc: 81.250(82.285)
12/17 02:13:01 AM [Supernet Training] lr: 0.02108 epoch: 095/600, step: 521/521, train_loss: 0.434(0.501), train_acc: 83.750(82.298)
12/17 02:13:01 AM [Supernet Training] epoch: 095, train_loss: 0.501, train_acc: 82.298
12/17 02:13:02 AM [Supernet Validation] epoch: 095, val_loss: 0.612, val_acc: 79.270, best_acc: 80.110


12/17 02:13:03 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 001/521, train_loss: 0.574(0.574), train_acc: 79.167(79.167)
12/17 02:13:09 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 101/521, train_loss: 0.463(0.496), train_acc: 83.333(82.570)
12/17 02:13:15 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 201/521, train_loss: 0.491(0.493), train_acc: 85.417(82.691)
12/17 02:13:22 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 301/521, train_loss: 0.430(0.495), train_acc: 83.333(82.679)
12/17 02:13:28 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 401/521, train_loss: 0.500(0.497), train_acc: 78.125(82.588)
12/17 02:13:35 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 501/521, train_loss: 0.540(0.500), train_acc: 81.250(82.454)
12/17 02:13:36 AM [Supernet Training] lr: 0.02104 epoch: 096/600, step: 521/521, train_loss: 0.499(0.500), train_acc: 81.250(82.442)
12/17 02:13:36 AM [Supernet Training] epoch: 096, train_loss: 0.500, train_acc: 82.442
12/17 02:13:37 AM [Supernet Validation] epoch: 096, val_loss: 0.583, val_acc: 79.890, best_acc: 80.110


12/17 02:13:38 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 001/521, train_loss: 0.535(0.535), train_acc: 79.167(79.167)
12/17 02:13:44 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 101/521, train_loss: 0.506(0.512), train_acc: 86.458(82.075)
12/17 02:13:51 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 201/521, train_loss: 0.415(0.497), train_acc: 87.500(82.530)
12/17 02:13:57 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 301/521, train_loss: 0.517(0.493), train_acc: 83.333(82.735)
12/17 02:14:03 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 401/521, train_loss: 0.411(0.491), train_acc: 84.375(82.829)
12/17 02:14:10 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 501/521, train_loss: 0.338(0.494), train_acc: 87.500(82.730)
12/17 02:14:11 AM [Supernet Training] lr: 0.02100 epoch: 097/600, step: 521/521, train_loss: 0.477(0.494), train_acc: 78.750(82.700)
12/17 02:14:11 AM [Supernet Training] epoch: 097, train_loss: 0.494, train_acc: 82.700
12/17 02:14:13 AM [Supernet Validation] epoch: 097, val_loss: 0.586, val_acc: 79.960, best_acc: 80.110


12/17 02:14:13 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 001/521, train_loss: 0.446(0.446), train_acc: 81.250(81.250)
12/17 02:14:19 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 101/521, train_loss: 0.479(0.489), train_acc: 83.333(82.580)
12/17 02:14:26 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 201/521, train_loss: 0.342(0.490), train_acc: 90.625(82.634)
12/17 02:14:32 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 301/521, train_loss: 0.593(0.495), train_acc: 79.167(82.510)
12/17 02:14:39 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 401/521, train_loss: 0.400(0.494), train_acc: 88.542(82.606)
12/17 02:14:45 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 501/521, train_loss: 0.368(0.492), train_acc: 88.542(82.568)
12/17 02:14:46 AM [Supernet Training] lr: 0.02096 epoch: 098/600, step: 521/521, train_loss: 0.519(0.492), train_acc: 80.000(82.580)
12/17 02:14:46 AM [Supernet Training] epoch: 098, train_loss: 0.492, train_acc: 82.580
12/17 02:14:48 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:14:48 AM [Supernet Validation] epoch: 098, val_loss: 0.576, val_acc: 80.510, best_acc: 80.510


12/17 02:14:48 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 001/521, train_loss: 0.412(0.412), train_acc: 84.375(84.375)
12/17 02:14:55 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 101/521, train_loss: 0.631(0.488), train_acc: 76.042(82.684)
12/17 02:15:01 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 201/521, train_loss: 0.483(0.484), train_acc: 84.375(82.976)
12/17 02:15:08 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 301/521, train_loss: 0.396(0.485), train_acc: 86.458(83.015)
12/17 02:15:14 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 401/521, train_loss: 0.678(0.485), train_acc: 75.000(83.074)
12/17 02:15:20 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 501/521, train_loss: 0.471(0.484), train_acc: 85.417(83.142)
12/17 02:15:22 AM [Supernet Training] lr: 0.02092 epoch: 099/600, step: 521/521, train_loss: 0.441(0.484), train_acc: 85.000(83.130)
12/17 02:15:22 AM [Supernet Training] epoch: 099, train_loss: 0.484, train_acc: 83.130
12/17 02:15:23 AM [Supernet Validation] epoch: 099, val_loss: 0.587, val_acc: 79.940, best_acc: 80.510


12/17 02:15:24 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 001/521, train_loss: 0.549(0.549), train_acc: 83.333(83.333)
12/17 02:15:30 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 101/521, train_loss: 0.278(0.496), train_acc: 88.542(82.818)
12/17 02:15:36 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 201/521, train_loss: 0.484(0.489), train_acc: 81.250(82.971)
12/17 02:15:43 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 301/521, train_loss: 0.557(0.492), train_acc: 82.292(82.800)
12/17 02:15:49 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 401/521, train_loss: 0.284(0.488), train_acc: 91.667(82.970)
12/17 02:15:55 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 501/521, train_loss: 0.528(0.486), train_acc: 82.292(82.924)
12/17 02:15:56 AM [Supernet Training] lr: 0.02088 epoch: 100/600, step: 521/521, train_loss: 0.659(0.487), train_acc: 72.500(82.886)
12/17 02:15:56 AM [Supernet Training] epoch: 100, train_loss: 0.487, train_acc: 82.886
12/17 02:15:58 AM [Supernet Validation] epoch: 100, val_loss: 0.582, val_acc: 80.200, best_acc: 80.510


12/17 02:15:58 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 001/521, train_loss: 0.423(0.423), train_acc: 85.417(85.417)
12/17 02:16:05 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 101/521, train_loss: 0.598(0.481), train_acc: 78.125(83.117)
12/17 02:16:11 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 201/521, train_loss: 0.618(0.484), train_acc: 78.125(83.043)
12/17 02:16:17 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 301/521, train_loss: 0.588(0.481), train_acc: 81.250(83.115)
12/17 02:16:24 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 401/521, train_loss: 0.435(0.480), train_acc: 83.333(83.136)
12/17 02:16:30 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 501/521, train_loss: 0.623(0.479), train_acc: 75.000(83.132)
12/17 02:16:31 AM [Supernet Training] lr: 0.02083 epoch: 101/600, step: 521/521, train_loss: 0.498(0.480), train_acc: 81.250(83.110)
12/17 02:16:31 AM [Supernet Training] epoch: 101, train_loss: 0.480, train_acc: 83.110
12/17 02:16:33 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:16:33 AM [Supernet Validation] epoch: 101, val_loss: 0.570, val_acc: 80.690, best_acc: 80.690


12/17 02:16:33 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 001/521, train_loss: 0.458(0.458), train_acc: 85.417(85.417)
12/17 02:16:40 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 101/521, train_loss: 0.513(0.473), train_acc: 85.417(83.447)
12/17 02:16:46 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 201/521, train_loss: 0.537(0.476), train_acc: 82.292(83.380)
12/17 02:16:52 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 301/521, train_loss: 0.383(0.474), train_acc: 83.333(83.461)
12/17 02:16:58 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 401/521, train_loss: 0.518(0.478), train_acc: 78.125(83.289)
12/17 02:17:05 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 501/521, train_loss: 0.525(0.476), train_acc: 79.167(83.319)
12/17 02:17:06 AM [Supernet Training] lr: 0.02079 epoch: 102/600, step: 521/521, train_loss: 0.436(0.476), train_acc: 78.750(83.302)
12/17 02:17:06 AM [Supernet Training] epoch: 102, train_loss: 0.476, train_acc: 83.302
12/17 02:17:08 AM [Supernet Validation] epoch: 102, val_loss: 0.578, val_acc: 80.280, best_acc: 80.690


12/17 02:17:08 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 001/521, train_loss: 0.487(0.487), train_acc: 82.292(82.292)
12/17 02:17:14 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 101/521, train_loss: 0.564(0.463), train_acc: 80.208(83.694)
12/17 02:17:20 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 201/521, train_loss: 0.501(0.471), train_acc: 77.083(83.421)
12/17 02:17:27 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 301/521, train_loss: 0.525(0.476), train_acc: 83.333(83.354)
12/17 02:17:33 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 401/521, train_loss: 0.404(0.475), train_acc: 84.375(83.287)
12/17 02:17:39 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 501/521, train_loss: 0.377(0.473), train_acc: 86.458(83.302)
12/17 02:17:41 AM [Supernet Training] lr: 0.02075 epoch: 103/600, step: 521/521, train_loss: 0.407(0.473), train_acc: 86.250(83.328)
12/17 02:17:41 AM [Supernet Training] epoch: 103, train_loss: 0.473, train_acc: 83.328
12/17 02:17:42 AM [Supernet Validation] epoch: 103, val_loss: 0.582, val_acc: 80.570, best_acc: 80.690


12/17 02:17:43 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 001/521, train_loss: 0.355(0.355), train_acc: 85.417(85.417)
12/17 02:17:49 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 101/521, train_loss: 0.448(0.477), train_acc: 85.417(83.323)
12/17 02:17:56 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 201/521, train_loss: 0.521(0.466), train_acc: 84.375(83.758)
12/17 02:18:02 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 301/521, train_loss: 0.513(0.470), train_acc: 83.333(83.524)
12/17 02:18:08 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 401/521, train_loss: 0.516(0.471), train_acc: 82.292(83.505)
12/17 02:18:15 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 501/521, train_loss: 0.414(0.468), train_acc: 86.458(83.668)
12/17 02:18:16 AM [Supernet Training] lr: 0.02071 epoch: 104/600, step: 521/521, train_loss: 0.473(0.468), train_acc: 81.250(83.654)
12/17 02:18:16 AM [Supernet Training] epoch: 104, train_loss: 0.468, train_acc: 83.654
12/17 02:18:18 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:18:18 AM [Supernet Validation] epoch: 104, val_loss: 0.573, val_acc: 80.950, best_acc: 80.950


12/17 02:18:18 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 001/521, train_loss: 0.476(0.476), train_acc: 83.333(83.333)
12/17 02:18:24 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 101/521, train_loss: 0.479(0.483), train_acc: 82.292(83.148)
12/17 02:18:31 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 201/521, train_loss: 0.376(0.473), train_acc: 85.417(83.525)
12/17 02:18:37 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 301/521, train_loss: 0.473(0.470), train_acc: 82.292(83.569)
12/17 02:18:43 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 401/521, train_loss: 0.415(0.470), train_acc: 85.417(83.500)
12/17 02:18:50 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 501/521, train_loss: 0.577(0.468), train_acc: 80.208(83.651)
12/17 02:18:51 AM [Supernet Training] lr: 0.02067 epoch: 105/600, step: 521/521, train_loss: 0.265(0.467), train_acc: 91.250(83.704)
12/17 02:18:51 AM [Supernet Training] epoch: 105, train_loss: 0.467, train_acc: 83.704
12/17 02:18:52 AM [Supernet Validation] epoch: 105, val_loss: 0.568, val_acc: 80.660, best_acc: 80.950


12/17 02:18:53 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 001/521, train_loss: 0.433(0.433), train_acc: 82.292(82.292)
12/17 02:18:59 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 101/521, train_loss: 0.460(0.451), train_acc: 83.333(83.952)
12/17 02:19:06 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 201/521, train_loss: 0.338(0.456), train_acc: 88.542(83.852)
12/17 02:19:12 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 301/521, train_loss: 0.319(0.452), train_acc: 88.542(83.963)
12/17 02:19:18 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 401/521, train_loss: 0.451(0.456), train_acc: 84.375(83.871)
12/17 02:19:25 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 501/521, train_loss: 0.295(0.458), train_acc: 91.667(83.774)
12/17 02:19:26 AM [Supernet Training] lr: 0.02063 epoch: 106/600, step: 521/521, train_loss: 0.480(0.459), train_acc: 86.250(83.732)
12/17 02:19:26 AM [Supernet Training] epoch: 106, train_loss: 0.459, train_acc: 83.732
12/17 02:19:28 AM [Supernet Validation] epoch: 106, val_loss: 0.574, val_acc: 80.770, best_acc: 80.950


12/17 02:19:28 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 001/521, train_loss: 0.414(0.414), train_acc: 87.500(87.500)
12/17 02:19:34 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 101/521, train_loss: 0.387(0.447), train_acc: 84.375(84.210)
12/17 02:19:41 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 201/521, train_loss: 0.524(0.448), train_acc: 83.333(84.168)
12/17 02:19:47 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 301/521, train_loss: 0.311(0.452), train_acc: 88.542(83.987)
12/17 02:19:53 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 401/521, train_loss: 0.356(0.458), train_acc: 85.417(83.819)
12/17 02:20:00 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 501/521, train_loss: 0.474(0.457), train_acc: 79.167(83.891)
12/17 02:20:01 AM [Supernet Training] lr: 0.02058 epoch: 107/600, step: 521/521, train_loss: 0.532(0.458), train_acc: 77.500(83.868)
12/17 02:20:01 AM [Supernet Training] epoch: 107, train_loss: 0.458, train_acc: 83.868
12/17 02:20:03 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:20:03 AM [Supernet Validation] epoch: 107, val_loss: 0.546, val_acc: 80.970, best_acc: 80.970


12/17 02:20:03 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 001/521, train_loss: 0.420(0.420), train_acc: 84.375(84.375)
12/17 02:20:10 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 101/521, train_loss: 0.591(0.455), train_acc: 75.000(84.045)
12/17 02:20:16 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 201/521, train_loss: 0.525(0.462), train_acc: 81.250(83.800)
12/17 02:20:22 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 301/521, train_loss: 0.455(0.459), train_acc: 86.458(83.891)
12/17 02:20:29 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 401/521, train_loss: 0.490(0.459), train_acc: 83.333(83.998)
12/17 02:20:36 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 501/521, train_loss: 0.454(0.458), train_acc: 87.500(83.974)
12/17 02:20:37 AM [Supernet Training] lr: 0.02054 epoch: 108/600, step: 521/521, train_loss: 0.409(0.459), train_acc: 90.000(83.922)
12/17 02:20:37 AM [Supernet Training] epoch: 108, train_loss: 0.459, train_acc: 83.922
12/17 02:20:39 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:20:39 AM [Supernet Validation] epoch: 108, val_loss: 0.560, val_acc: 81.160, best_acc: 81.160


12/17 02:20:39 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 001/521, train_loss: 0.556(0.556), train_acc: 82.292(82.292)
12/17 02:20:45 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 101/521, train_loss: 0.533(0.458), train_acc: 79.167(83.828)
12/17 02:20:52 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 201/521, train_loss: 0.355(0.450), train_acc: 87.500(84.121)
12/17 02:20:58 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 301/521, train_loss: 0.349(0.448), train_acc: 83.333(84.102)
12/17 02:21:05 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 401/521, train_loss: 0.377(0.450), train_acc: 87.500(84.131)
12/17 02:21:11 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 501/521, train_loss: 0.327(0.450), train_acc: 86.458(84.236)
12/17 02:21:12 AM [Supernet Training] lr: 0.02050 epoch: 109/600, step: 521/521, train_loss: 0.517(0.449), train_acc: 76.250(84.246)
12/17 02:21:12 AM [Supernet Training] epoch: 109, train_loss: 0.449, train_acc: 84.246
12/17 02:21:14 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:21:14 AM [Supernet Validation] epoch: 109, val_loss: 0.559, val_acc: 81.250, best_acc: 81.250


12/17 02:21:14 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 001/521, train_loss: 0.391(0.391), train_acc: 85.417(85.417)
12/17 02:21:20 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 101/521, train_loss: 0.594(0.443), train_acc: 81.250(84.323)
12/17 02:21:27 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 201/521, train_loss: 0.397(0.447), train_acc: 88.542(84.380)
12/17 02:21:33 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 301/521, train_loss: 0.476(0.447), train_acc: 86.458(84.333)
12/17 02:21:39 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 401/521, train_loss: 0.442(0.446), train_acc: 85.417(84.362)
12/17 02:21:46 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 501/521, train_loss: 0.588(0.447), train_acc: 81.250(84.383)
12/17 02:21:47 AM [Supernet Training] lr: 0.02046 epoch: 110/600, step: 521/521, train_loss: 0.361(0.446), train_acc: 83.750(84.414)
12/17 02:21:47 AM [Supernet Training] epoch: 110, train_loss: 0.446, train_acc: 84.414
12/17 02:21:48 AM [Supernet Validation] epoch: 110, val_loss: 0.555, val_acc: 81.220, best_acc: 81.250


12/17 02:21:49 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 88.542(88.542)
12/17 02:21:55 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 101/521, train_loss: 0.402(0.443), train_acc: 84.375(84.035)
12/17 02:22:02 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 201/521, train_loss: 0.483(0.441), train_acc: 82.292(84.380)
12/17 02:22:08 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 301/521, train_loss: 0.487(0.445), train_acc: 83.333(84.288)
12/17 02:22:14 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 401/521, train_loss: 0.482(0.448), train_acc: 81.250(84.284)
12/17 02:22:21 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 501/521, train_loss: 0.749(0.449), train_acc: 76.042(84.238)
12/17 02:22:22 AM [Supernet Training] lr: 0.02042 epoch: 111/600, step: 521/521, train_loss: 0.269(0.450), train_acc: 86.250(84.208)
12/17 02:22:22 AM [Supernet Training] epoch: 111, train_loss: 0.450, train_acc: 84.208
12/17 02:22:24 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:22:24 AM [Supernet Validation] epoch: 111, val_loss: 0.545, val_acc: 81.330, best_acc: 81.330


12/17 02:22:24 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 001/521, train_loss: 0.361(0.361), train_acc: 86.458(86.458)
12/17 02:22:30 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 101/521, train_loss: 0.509(0.437), train_acc: 84.375(84.427)
12/17 02:22:37 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 201/521, train_loss: 0.419(0.438), train_acc: 85.417(84.453)
12/17 02:22:43 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 301/521, train_loss: 0.444(0.432), train_acc: 83.333(84.662)
12/17 02:22:49 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 401/521, train_loss: 0.475(0.436), train_acc: 82.292(84.627)
12/17 02:22:56 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 501/521, train_loss: 0.558(0.440), train_acc: 82.292(84.531)
12/17 02:22:57 AM [Supernet Training] lr: 0.02038 epoch: 112/600, step: 521/521, train_loss: 0.416(0.440), train_acc: 82.500(84.566)
12/17 02:22:57 AM [Supernet Training] epoch: 112, train_loss: 0.440, train_acc: 84.566
12/17 02:22:59 AM [Supernet Validation] epoch: 112, val_loss: 0.550, val_acc: 81.110, best_acc: 81.330


12/17 02:22:59 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 001/521, train_loss: 0.391(0.391), train_acc: 88.542(88.542)
12/17 02:23:05 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 101/521, train_loss: 0.556(0.435), train_acc: 80.208(84.262)
12/17 02:23:12 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 201/521, train_loss: 0.172(0.436), train_acc: 95.833(84.442)
12/17 02:23:18 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 301/521, train_loss: 0.380(0.440), train_acc: 81.250(84.534)
12/17 02:23:25 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 401/521, train_loss: 0.513(0.436), train_acc: 81.250(84.565)
12/17 02:23:31 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 501/521, train_loss: 0.250(0.440), train_acc: 93.750(84.498)
12/17 02:23:32 AM [Supernet Training] lr: 0.02033 epoch: 113/600, step: 521/521, train_loss: 0.525(0.441), train_acc: 76.250(84.462)
12/17 02:23:32 AM [Supernet Training] epoch: 113, train_loss: 0.441, train_acc: 84.462
12/17 02:23:34 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:23:34 AM [Supernet Validation] epoch: 113, val_loss: 0.539, val_acc: 82.020, best_acc: 82.020


12/17 02:23:34 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 001/521, train_loss: 0.415(0.415), train_acc: 82.292(82.292)
12/17 02:23:41 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 101/521, train_loss: 0.475(0.414), train_acc: 82.292(85.520)
12/17 02:23:47 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 201/521, train_loss: 0.375(0.419), train_acc: 84.375(85.308)
12/17 02:23:54 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 301/521, train_loss: 0.519(0.423), train_acc: 81.250(85.254)
12/17 02:24:00 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 401/521, train_loss: 0.466(0.430), train_acc: 84.375(84.962)
12/17 02:24:06 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 501/521, train_loss: 0.355(0.433), train_acc: 86.458(84.801)
12/17 02:24:08 AM [Supernet Training] lr: 0.02029 epoch: 114/600, step: 521/521, train_loss: 0.515(0.434), train_acc: 76.250(84.754)
12/17 02:24:08 AM [Supernet Training] epoch: 114, train_loss: 0.434, train_acc: 84.754
12/17 02:24:09 AM [Supernet Validation] epoch: 114, val_loss: 0.565, val_acc: 80.670, best_acc: 82.020


12/17 02:24:10 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 001/521, train_loss: 0.650(0.650), train_acc: 75.000(75.000)
12/17 02:24:16 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 101/521, train_loss: 0.513(0.436), train_acc: 82.292(84.571)
12/17 02:24:22 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 201/521, train_loss: 0.404(0.429), train_acc: 85.417(84.992)
12/17 02:24:29 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 301/521, train_loss: 0.401(0.432), train_acc: 88.542(84.839)
12/17 02:24:35 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 401/521, train_loss: 0.369(0.430), train_acc: 83.333(84.918)
12/17 02:24:42 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 501/521, train_loss: 0.423(0.430), train_acc: 87.500(84.884)
12/17 02:24:43 AM [Supernet Training] lr: 0.02025 epoch: 115/600, step: 521/521, train_loss: 0.558(0.432), train_acc: 76.250(84.820)
12/17 02:24:43 AM [Supernet Training] epoch: 115, train_loss: 0.432, train_acc: 84.820
12/17 02:24:45 AM [Supernet Validation] epoch: 115, val_loss: 0.539, val_acc: 81.870, best_acc: 82.020


12/17 02:24:45 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 001/521, train_loss: 0.389(0.389), train_acc: 85.417(85.417)
12/17 02:24:52 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 101/521, train_loss: 0.413(0.422), train_acc: 83.333(84.942)
12/17 02:24:58 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 201/521, train_loss: 0.412(0.429), train_acc: 84.375(84.831)
12/17 02:25:04 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 301/521, train_loss: 0.467(0.428), train_acc: 86.458(84.821)
12/17 02:25:11 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 401/521, train_loss: 0.410(0.428), train_acc: 86.458(84.772)
12/17 02:25:17 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 501/521, train_loss: 0.414(0.429), train_acc: 81.250(84.747)
12/17 02:25:18 AM [Supernet Training] lr: 0.02021 epoch: 116/600, step: 521/521, train_loss: 0.280(0.430), train_acc: 90.000(84.718)
12/17 02:25:18 AM [Supernet Training] epoch: 116, train_loss: 0.430, train_acc: 84.718
12/17 02:25:20 AM [Supernet Validation] epoch: 116, val_loss: 0.546, val_acc: 81.350, best_acc: 82.020


12/17 02:25:20 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 001/521, train_loss: 0.647(0.647), train_acc: 80.208(80.208)
12/17 02:25:26 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 101/521, train_loss: 0.362(0.418), train_acc: 83.333(85.437)
12/17 02:25:33 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 201/521, train_loss: 0.336(0.429), train_acc: 86.458(84.924)
12/17 02:25:39 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 301/521, train_loss: 0.460(0.429), train_acc: 84.375(84.880)
12/17 02:25:46 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 401/521, train_loss: 0.366(0.429), train_acc: 83.333(84.980)
12/17 02:25:52 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 501/521, train_loss: 0.357(0.428), train_acc: 86.458(84.995)
12/17 02:25:53 AM [Supernet Training] lr: 0.02017 epoch: 117/600, step: 521/521, train_loss: 0.562(0.427), train_acc: 80.000(85.026)
12/17 02:25:53 AM [Supernet Training] epoch: 117, train_loss: 0.427, train_acc: 85.026
12/17 02:25:55 AM [Supernet Validation] epoch: 117, val_loss: 0.553, val_acc: 81.350, best_acc: 82.020


12/17 02:25:55 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 001/521, train_loss: 0.468(0.468), train_acc: 87.500(87.500)
12/17 02:26:01 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 101/521, train_loss: 0.412(0.404), train_acc: 85.417(85.747)
12/17 02:26:08 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 201/521, train_loss: 0.458(0.415), train_acc: 81.250(85.256)
12/17 02:26:14 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 301/521, train_loss: 0.451(0.425), train_acc: 85.417(85.033)
12/17 02:26:21 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 401/521, train_loss: 0.370(0.424), train_acc: 85.417(85.105)
12/17 02:26:27 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 501/521, train_loss: 0.371(0.424), train_acc: 85.417(85.136)
12/17 02:26:28 AM [Supernet Training] lr: 0.02013 epoch: 118/600, step: 521/521, train_loss: 0.435(0.424), train_acc: 88.750(85.132)
12/17 02:26:28 AM [Supernet Training] epoch: 118, train_loss: 0.424, train_acc: 85.132
12/17 02:26:30 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:26:30 AM [Supernet Validation] epoch: 118, val_loss: 0.540, val_acc: 82.050, best_acc: 82.050


12/17 02:26:31 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 001/521, train_loss: 0.464(0.464), train_acc: 81.250(81.250)
12/17 02:26:37 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 101/521, train_loss: 0.364(0.407), train_acc: 87.500(85.417)
12/17 02:26:43 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 201/521, train_loss: 0.510(0.424), train_acc: 84.375(84.924)
12/17 02:26:50 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 301/521, train_loss: 0.482(0.419), train_acc: 81.250(85.226)
12/17 02:26:56 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 401/521, train_loss: 0.362(0.419), train_acc: 88.542(85.224)
12/17 02:27:02 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 501/521, train_loss: 0.271(0.420), train_acc: 91.667(85.144)
12/17 02:27:03 AM [Supernet Training] lr: 0.02008 epoch: 119/600, step: 521/521, train_loss: 0.445(0.420), train_acc: 83.750(85.126)
12/17 02:27:04 AM [Supernet Training] epoch: 119, train_loss: 0.420, train_acc: 85.126
12/17 02:27:05 AM [Supernet Validation] epoch: 119, val_loss: 0.562, val_acc: 81.250, best_acc: 82.050


12/17 02:27:05 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 001/521, train_loss: 0.379(0.379), train_acc: 89.583(89.583)
12/17 02:27:12 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 101/521, train_loss: 0.478(0.411), train_acc: 80.208(85.747)
12/17 02:27:18 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 201/521, train_loss: 0.375(0.418), train_acc: 88.542(85.505)
12/17 02:27:25 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 301/521, train_loss: 0.324(0.417), train_acc: 90.625(85.420)
12/17 02:27:31 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 401/521, train_loss: 0.446(0.419), train_acc: 85.417(85.274)
12/17 02:27:37 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 501/521, train_loss: 0.365(0.421), train_acc: 87.500(85.219)
12/17 02:27:39 AM [Supernet Training] lr: 0.02004 epoch: 120/600, step: 521/521, train_loss: 0.416(0.422), train_acc: 85.000(85.188)
12/17 02:27:39 AM [Supernet Training] epoch: 120, train_loss: 0.422, train_acc: 85.188
12/17 02:27:40 AM [Supernet Validation] epoch: 120, val_loss: 0.550, val_acc: 81.830, best_acc: 82.050


12/17 02:27:41 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 001/521, train_loss: 0.372(0.372), train_acc: 84.375(84.375)
12/17 02:27:47 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 101/521, train_loss: 0.518(0.407), train_acc: 79.167(85.561)
12/17 02:27:53 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 201/521, train_loss: 0.497(0.403), train_acc: 82.292(85.888)
12/17 02:27:59 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 301/521, train_loss: 0.369(0.408), train_acc: 86.458(85.694)
12/17 02:28:06 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 401/521, train_loss: 0.334(0.409), train_acc: 90.625(85.653)
12/17 02:28:12 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 501/521, train_loss: 0.433(0.411), train_acc: 85.417(85.543)
12/17 02:28:13 AM [Supernet Training] lr: 0.02000 epoch: 121/600, step: 521/521, train_loss: 0.467(0.411), train_acc: 86.250(85.516)
12/17 02:28:13 AM [Supernet Training] epoch: 121, train_loss: 0.411, train_acc: 85.516
12/17 02:28:15 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:28:15 AM [Supernet Validation] epoch: 121, val_loss: 0.531, val_acc: 82.260, best_acc: 82.260


12/17 02:28:15 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 001/521, train_loss: 0.498(0.498), train_acc: 85.417(85.417)
12/17 02:28:21 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 101/521, train_loss: 0.396(0.424), train_acc: 90.625(85.303)
12/17 02:28:28 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 201/521, train_loss: 0.607(0.414), train_acc: 75.000(85.474)
12/17 02:28:34 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 301/521, train_loss: 0.370(0.413), train_acc: 82.292(85.583)
12/17 02:28:40 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 401/521, train_loss: 0.244(0.417), train_acc: 93.750(85.373)
12/17 02:28:47 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 501/521, train_loss: 0.260(0.417), train_acc: 92.708(85.363)
12/17 02:28:48 AM [Supernet Training] lr: 0.01996 epoch: 122/600, step: 521/521, train_loss: 0.700(0.417), train_acc: 78.750(85.330)
12/17 02:28:48 AM [Supernet Training] epoch: 122, train_loss: 0.417, train_acc: 85.330
12/17 02:28:50 AM [Supernet Validation] epoch: 122, val_loss: 0.554, val_acc: 81.410, best_acc: 82.260


12/17 02:28:50 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 001/521, train_loss: 0.480(0.480), train_acc: 84.375(84.375)
12/17 02:28:57 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 101/521, train_loss: 0.327(0.408), train_acc: 90.625(85.303)
12/17 02:29:03 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 201/521, train_loss: 0.534(0.404), train_acc: 81.250(85.640)
12/17 02:29:09 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 301/521, train_loss: 0.405(0.405), train_acc: 84.375(85.687)
12/17 02:29:16 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 401/521, train_loss: 0.266(0.408), train_acc: 88.542(85.671)
12/17 02:29:22 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 501/521, train_loss: 0.450(0.410), train_acc: 84.375(85.560)
12/17 02:29:24 AM [Supernet Training] lr: 0.01992 epoch: 123/600, step: 521/521, train_loss: 0.515(0.410), train_acc: 86.250(85.560)
12/17 02:29:24 AM [Supernet Training] epoch: 123, train_loss: 0.410, train_acc: 85.560
12/17 02:29:25 AM [Supernet Validation] epoch: 123, val_loss: 0.534, val_acc: 81.820, best_acc: 82.260


12/17 02:29:26 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 001/521, train_loss: 0.332(0.332), train_acc: 85.417(85.417)
12/17 02:29:32 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 101/521, train_loss: 0.537(0.420), train_acc: 83.333(85.231)
12/17 02:29:39 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 201/521, train_loss: 0.400(0.413), train_acc: 89.583(85.484)
12/17 02:29:45 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 301/521, train_loss: 0.334(0.409), train_acc: 89.583(85.725)
12/17 02:29:52 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 401/521, train_loss: 0.392(0.409), train_acc: 86.458(85.661)
12/17 02:29:58 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 501/521, train_loss: 0.251(0.411), train_acc: 92.708(85.579)
12/17 02:29:59 AM [Supernet Training] lr: 0.01988 epoch: 124/600, step: 521/521, train_loss: 0.204(0.410), train_acc: 97.500(85.614)
12/17 02:29:59 AM [Supernet Training] epoch: 124, train_loss: 0.410, train_acc: 85.614
12/17 02:30:01 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:30:01 AM [Supernet Validation] epoch: 124, val_loss: 0.525, val_acc: 82.530, best_acc: 82.530


12/17 02:30:02 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 001/521, train_loss: 0.452(0.452), train_acc: 81.250(81.250)
12/17 02:30:08 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 101/521, train_loss: 0.394(0.389), train_acc: 87.500(86.200)
12/17 02:30:14 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 201/521, train_loss: 0.379(0.399), train_acc: 84.375(85.836)
12/17 02:30:21 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 301/521, train_loss: 0.386(0.404), train_acc: 87.500(85.714)
12/17 02:30:27 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 401/521, train_loss: 0.530(0.402), train_acc: 79.167(85.832)
12/17 02:30:33 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 501/521, train_loss: 0.508(0.401), train_acc: 86.458(85.841)
12/17 02:30:34 AM [Supernet Training] lr: 0.01983 epoch: 125/600, step: 521/521, train_loss: 0.482(0.402), train_acc: 83.750(85.826)
12/17 02:30:35 AM [Supernet Training] epoch: 125, train_loss: 0.402, train_acc: 85.826
12/17 02:30:36 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:30:36 AM [Supernet Validation] epoch: 125, val_loss: 0.528, val_acc: 82.610, best_acc: 82.610


12/17 02:30:37 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 001/521, train_loss: 0.444(0.444), train_acc: 85.417(85.417)
12/17 02:30:43 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 101/521, train_loss: 0.196(0.393), train_acc: 92.708(86.345)
12/17 02:30:49 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 201/521, train_loss: 0.434(0.394), train_acc: 80.208(86.267)
12/17 02:30:56 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 301/521, train_loss: 0.414(0.396), train_acc: 82.292(86.088)
12/17 02:31:02 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 401/521, train_loss: 0.333(0.398), train_acc: 90.625(85.957)
12/17 02:31:09 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 501/521, train_loss: 0.429(0.400), train_acc: 85.417(85.934)
12/17 02:31:10 AM [Supernet Training] lr: 0.01979 epoch: 126/600, step: 521/521, train_loss: 0.445(0.401), train_acc: 81.250(85.886)
12/17 02:31:10 AM [Supernet Training] epoch: 126, train_loss: 0.401, train_acc: 85.886
12/17 02:31:12 AM [Supernet Validation] epoch: 126, val_loss: 0.527, val_acc: 82.250, best_acc: 82.610


12/17 02:31:12 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 001/521, train_loss: 0.396(0.396), train_acc: 84.375(84.375)
12/17 02:31:19 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 101/521, train_loss: 0.374(0.387), train_acc: 85.417(86.190)
12/17 02:31:25 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 201/521, train_loss: 0.271(0.392), train_acc: 91.667(86.121)
12/17 02:31:32 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 301/521, train_loss: 0.472(0.394), train_acc: 81.250(86.067)
12/17 02:31:38 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 401/521, train_loss: 0.430(0.396), train_acc: 85.417(85.947)
12/17 02:31:44 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 501/521, train_loss: 0.367(0.400), train_acc: 85.417(85.832)
12/17 02:31:46 AM [Supernet Training] lr: 0.01975 epoch: 127/600, step: 521/521, train_loss: 0.464(0.400), train_acc: 78.750(85.798)
12/17 02:31:46 AM [Supernet Training] epoch: 127, train_loss: 0.400, train_acc: 85.798
12/17 02:31:47 AM [Supernet Validation] epoch: 127, val_loss: 0.528, val_acc: 82.190, best_acc: 82.610


12/17 02:31:48 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 001/521, train_loss: 0.424(0.424), train_acc: 83.333(83.333)
12/17 02:31:54 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 101/521, train_loss: 0.459(0.391), train_acc: 82.292(86.097)
12/17 02:32:00 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 201/521, train_loss: 0.545(0.396), train_acc: 82.292(85.925)
12/17 02:32:07 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 301/521, train_loss: 0.401(0.397), train_acc: 84.375(85.974)
12/17 02:32:13 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 401/521, train_loss: 0.499(0.395), train_acc: 80.208(86.022)
12/17 02:32:19 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 501/521, train_loss: 0.426(0.396), train_acc: 84.375(85.970)
12/17 02:32:21 AM [Supernet Training] lr: 0.01971 epoch: 128/600, step: 521/521, train_loss: 0.617(0.396), train_acc: 77.500(85.936)
12/17 02:32:21 AM [Supernet Training] epoch: 128, train_loss: 0.396, train_acc: 85.936
12/17 02:32:22 AM [Supernet Validation] epoch: 128, val_loss: 0.528, val_acc: 82.210, best_acc: 82.610


12/17 02:32:23 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 001/521, train_loss: 0.298(0.298), train_acc: 89.583(89.583)
12/17 02:32:29 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 101/521, train_loss: 0.354(0.399), train_acc: 88.542(85.716)
12/17 02:32:36 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 201/521, train_loss: 0.366(0.395), train_acc: 89.583(85.842)
12/17 02:32:42 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 301/521, train_loss: 0.378(0.398), train_acc: 90.625(85.804)
12/17 02:32:48 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 401/521, train_loss: 0.443(0.397), train_acc: 83.333(85.897)
12/17 02:32:55 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 501/521, train_loss: 0.338(0.395), train_acc: 86.458(85.951)
12/17 02:32:56 AM [Supernet Training] lr: 0.01967 epoch: 129/600, step: 521/521, train_loss: 0.408(0.396), train_acc: 85.000(85.964)
12/17 02:32:56 AM [Supernet Training] epoch: 129, train_loss: 0.396, train_acc: 85.964
12/17 02:32:58 AM [Supernet Validation] epoch: 129, val_loss: 0.525, val_acc: 82.600, best_acc: 82.610


12/17 02:32:58 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 001/521, train_loss: 0.282(0.282), train_acc: 93.750(93.750)
12/17 02:33:04 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 101/521, train_loss: 0.371(0.392), train_acc: 86.458(86.396)
12/17 02:33:11 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 201/521, train_loss: 0.340(0.396), train_acc: 84.375(86.147)
12/17 02:33:17 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 301/521, train_loss: 0.253(0.389), train_acc: 90.625(86.292)
12/17 02:33:23 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 401/521, train_loss: 0.338(0.390), train_acc: 88.542(86.336)
12/17 02:33:30 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 501/521, train_loss: 0.384(0.391), train_acc: 88.542(86.304)
12/17 02:33:31 AM [Supernet Training] lr: 0.01963 epoch: 130/600, step: 521/521, train_loss: 0.318(0.392), train_acc: 88.750(86.264)
12/17 02:33:31 AM [Supernet Training] epoch: 130, train_loss: 0.392, train_acc: 86.264
12/17 02:33:32 AM [Supernet Validation] epoch: 130, val_loss: 0.523, val_acc: 82.590, best_acc: 82.610


12/17 02:33:33 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 001/521, train_loss: 0.604(0.604), train_acc: 76.042(76.042)
12/17 02:33:39 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 101/521, train_loss: 0.507(0.388), train_acc: 84.375(86.046)
12/17 02:33:45 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 201/521, train_loss: 0.346(0.388), train_acc: 85.417(86.178)
12/17 02:33:52 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 301/521, train_loss: 0.538(0.389), train_acc: 80.208(86.181)
12/17 02:33:58 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 401/521, train_loss: 0.391(0.395), train_acc: 84.375(85.967)
12/17 02:34:04 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 501/521, train_loss: 0.281(0.392), train_acc: 90.625(86.122)
12/17 02:34:06 AM [Supernet Training] lr: 0.01958 epoch: 131/600, step: 521/521, train_loss: 0.499(0.393), train_acc: 83.750(86.142)
12/17 02:34:06 AM [Supernet Training] epoch: 131, train_loss: 0.393, train_acc: 86.142
12/17 02:34:07 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:34:07 AM [Supernet Validation] epoch: 131, val_loss: 0.527, val_acc: 82.730, best_acc: 82.730


12/17 02:34:07 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 001/521, train_loss: 0.327(0.327), train_acc: 89.583(89.583)
12/17 02:34:14 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 101/521, train_loss: 0.337(0.386), train_acc: 87.500(86.613)
12/17 02:34:20 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 201/521, train_loss: 0.439(0.385), train_acc: 83.333(86.546)
12/17 02:34:26 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 301/521, train_loss: 0.370(0.388), train_acc: 89.583(86.420)
12/17 02:34:33 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 401/521, train_loss: 0.377(0.387), train_acc: 88.542(86.354)
12/17 02:34:39 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 501/521, train_loss: 0.542(0.387), train_acc: 81.250(86.371)
12/17 02:34:40 AM [Supernet Training] lr: 0.01954 epoch: 132/600, step: 521/521, train_loss: 0.335(0.386), train_acc: 91.250(86.406)
12/17 02:34:40 AM [Supernet Training] epoch: 132, train_loss: 0.386, train_acc: 86.406
12/17 02:34:42 AM [Supernet Validation] epoch: 132, val_loss: 0.523, val_acc: 82.690, best_acc: 82.730


12/17 02:34:42 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 001/521, train_loss: 0.304(0.304), train_acc: 86.458(86.458)
12/17 02:34:48 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 101/521, train_loss: 0.266(0.383), train_acc: 91.667(86.726)
12/17 02:34:55 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 201/521, train_loss: 0.310(0.381), train_acc: 89.583(86.712)
12/17 02:35:01 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 301/521, train_loss: 0.405(0.385), train_acc: 83.333(86.586)
12/17 02:35:07 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 401/521, train_loss: 0.261(0.388), train_acc: 91.667(86.510)
12/17 02:35:14 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 501/521, train_loss: 0.468(0.387), train_acc: 83.333(86.494)
12/17 02:35:15 AM [Supernet Training] lr: 0.01950 epoch: 133/600, step: 521/521, train_loss: 0.442(0.387), train_acc: 83.750(86.506)
12/17 02:35:15 AM [Supernet Training] epoch: 133, train_loss: 0.387, train_acc: 86.506
12/17 02:35:17 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:35:17 AM [Supernet Validation] epoch: 133, val_loss: 0.507, val_acc: 83.120, best_acc: 83.120


12/17 02:35:17 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 001/521, train_loss: 0.310(0.310), train_acc: 90.625(90.625)
12/17 02:35:24 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 101/521, train_loss: 0.349(0.366), train_acc: 89.583(87.252)
12/17 02:35:30 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 201/521, train_loss: 0.552(0.381), train_acc: 75.000(86.624)
12/17 02:35:36 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 301/521, train_loss: 0.343(0.377), train_acc: 85.417(86.666)
12/17 02:35:43 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 401/521, train_loss: 0.391(0.377), train_acc: 84.375(86.627)
12/17 02:35:49 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 501/521, train_loss: 0.496(0.379), train_acc: 80.208(86.539)
12/17 02:35:50 AM [Supernet Training] lr: 0.01946 epoch: 134/600, step: 521/521, train_loss: 0.557(0.380), train_acc: 77.500(86.496)
12/17 02:35:50 AM [Supernet Training] epoch: 134, train_loss: 0.380, train_acc: 86.496
12/17 02:35:52 AM [Supernet Validation] epoch: 134, val_loss: 0.523, val_acc: 82.390, best_acc: 83.120


12/17 02:35:52 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 001/521, train_loss: 0.309(0.309), train_acc: 88.542(88.542)
12/17 02:35:58 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 101/521, train_loss: 0.389(0.378), train_acc: 88.542(86.654)
12/17 02:36:05 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 201/521, train_loss: 0.362(0.377), train_acc: 86.458(86.676)
12/17 02:36:11 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 301/521, train_loss: 0.346(0.376), train_acc: 88.542(86.763)
12/17 02:36:18 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 401/521, train_loss: 0.482(0.376), train_acc: 82.292(86.703)
12/17 02:36:24 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 501/521, train_loss: 0.442(0.379), train_acc: 85.417(86.685)
12/17 02:36:25 AM [Supernet Training] lr: 0.01942 epoch: 135/600, step: 521/521, train_loss: 0.572(0.379), train_acc: 82.500(86.676)
12/17 02:36:25 AM [Supernet Training] epoch: 135, train_loss: 0.379, train_acc: 86.676
12/17 02:36:27 AM [Supernet Validation] epoch: 135, val_loss: 0.516, val_acc: 82.720, best_acc: 83.120


12/17 02:36:27 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 001/521, train_loss: 0.246(0.246), train_acc: 90.625(90.625)
12/17 02:36:33 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 101/521, train_loss: 0.215(0.373), train_acc: 91.667(86.417)
12/17 02:36:40 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 201/521, train_loss: 0.341(0.371), train_acc: 86.458(86.541)
12/17 02:36:46 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 301/521, train_loss: 0.497(0.378), train_acc: 78.125(86.465)
12/17 02:36:52 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 401/521, train_loss: 0.539(0.380), train_acc: 81.250(86.484)
12/17 02:36:58 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 501/521, train_loss: 0.480(0.381), train_acc: 85.417(86.483)
12/17 02:37:00 AM [Supernet Training] lr: 0.01938 epoch: 136/600, step: 521/521, train_loss: 0.489(0.380), train_acc: 86.250(86.520)
12/17 02:37:00 AM [Supernet Training] epoch: 136, train_loss: 0.380, train_acc: 86.520
12/17 02:37:01 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:37:01 AM [Supernet Validation] epoch: 136, val_loss: 0.507, val_acc: 83.280, best_acc: 83.280


12/17 02:37:02 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 001/521, train_loss: 0.481(0.481), train_acc: 81.250(81.250)
12/17 02:37:08 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 101/521, train_loss: 0.383(0.365), train_acc: 88.542(87.067)
12/17 02:37:15 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 201/521, train_loss: 0.457(0.363), train_acc: 85.417(87.153)
12/17 02:37:21 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 301/521, train_loss: 0.431(0.372), train_acc: 86.458(86.929)
12/17 02:37:27 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 401/521, train_loss: 0.341(0.371), train_acc: 85.417(86.939)
12/17 02:37:33 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 501/521, train_loss: 0.587(0.375), train_acc: 77.083(86.795)
12/17 02:37:35 AM [Supernet Training] lr: 0.01933 epoch: 137/600, step: 521/521, train_loss: 0.296(0.374), train_acc: 91.250(86.830)
12/17 02:37:35 AM [Supernet Training] epoch: 137, train_loss: 0.374, train_acc: 86.830
12/17 02:37:36 AM [Supernet Validation] epoch: 137, val_loss: 0.510, val_acc: 83.020, best_acc: 83.280


12/17 02:37:37 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 001/521, train_loss: 0.575(0.575), train_acc: 80.208(80.208)
12/17 02:37:43 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 101/521, train_loss: 0.292(0.363), train_acc: 90.625(87.067)
12/17 02:37:49 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 201/521, train_loss: 0.484(0.367), train_acc: 85.417(86.930)
12/17 02:37:55 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 301/521, train_loss: 0.275(0.367), train_acc: 88.542(86.881)
12/17 02:38:02 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 401/521, train_loss: 0.511(0.371), train_acc: 81.250(86.760)
12/17 02:38:08 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 501/521, train_loss: 0.378(0.369), train_acc: 84.375(86.885)
12/17 02:38:09 AM [Supernet Training] lr: 0.01929 epoch: 138/600, step: 521/521, train_loss: 0.335(0.369), train_acc: 87.500(86.880)
12/17 02:38:09 AM [Supernet Training] epoch: 138, train_loss: 0.369, train_acc: 86.880
12/17 02:38:11 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:38:11 AM [Supernet Validation] epoch: 138, val_loss: 0.497, val_acc: 83.500, best_acc: 83.500


12/17 02:38:11 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 001/521, train_loss: 0.352(0.352), train_acc: 87.500(87.500)
12/17 02:38:17 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 101/521, train_loss: 0.449(0.374), train_acc: 84.375(86.881)
12/17 02:38:24 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 201/521, train_loss: 0.404(0.375), train_acc: 84.375(86.780)
12/17 02:38:30 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 301/521, train_loss: 0.383(0.371), train_acc: 86.458(86.818)
12/17 02:38:36 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 401/521, train_loss: 0.387(0.371), train_acc: 86.458(86.869)
12/17 02:38:43 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 501/521, train_loss: 0.449(0.372), train_acc: 78.125(86.833)
12/17 02:38:44 AM [Supernet Training] lr: 0.01925 epoch: 139/600, step: 521/521, train_loss: 0.120(0.372), train_acc: 96.250(86.862)
12/17 02:38:44 AM [Supernet Training] epoch: 139, train_loss: 0.372, train_acc: 86.862
12/17 02:38:46 AM [Supernet Validation] epoch: 139, val_loss: 0.516, val_acc: 82.720, best_acc: 83.500


12/17 02:38:46 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 001/521, train_loss: 0.431(0.431), train_acc: 83.333(83.333)
12/17 02:38:53 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 101/521, train_loss: 0.226(0.357), train_acc: 93.750(87.335)
12/17 02:39:00 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 201/521, train_loss: 0.341(0.352), train_acc: 85.417(87.448)
12/17 02:39:06 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 301/521, train_loss: 0.417(0.358), train_acc: 85.417(87.261)
12/17 02:39:12 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 401/521, train_loss: 0.310(0.362), train_acc: 84.375(87.084)
12/17 02:39:19 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 501/521, train_loss: 0.352(0.362), train_acc: 89.583(87.086)
12/17 02:39:20 AM [Supernet Training] lr: 0.01921 epoch: 140/600, step: 521/521, train_loss: 0.375(0.363), train_acc: 86.250(87.086)
12/17 02:39:20 AM [Supernet Training] epoch: 140, train_loss: 0.363, train_acc: 87.086
12/17 02:39:22 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:39:22 AM [Supernet Validation] epoch: 140, val_loss: 0.504, val_acc: 83.520, best_acc: 83.520


12/17 02:39:22 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 001/521, train_loss: 0.246(0.246), train_acc: 92.708(92.708)
12/17 02:39:28 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 101/521, train_loss: 0.294(0.366), train_acc: 87.500(87.232)
12/17 02:39:35 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 201/521, train_loss: 0.439(0.370), train_acc: 88.542(87.122)
12/17 02:39:41 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 301/521, train_loss: 0.257(0.366), train_acc: 89.583(87.057)
12/17 02:39:47 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 401/521, train_loss: 0.317(0.364), train_acc: 90.625(87.173)
12/17 02:39:54 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 501/521, train_loss: 0.223(0.364), train_acc: 92.708(87.161)
12/17 02:39:55 AM [Supernet Training] lr: 0.01917 epoch: 141/600, step: 521/521, train_loss: 0.420(0.364), train_acc: 81.250(87.198)
12/17 02:39:55 AM [Supernet Training] epoch: 141, train_loss: 0.364, train_acc: 87.198
12/17 02:39:57 AM [Supernet Validation] epoch: 141, val_loss: 0.508, val_acc: 83.250, best_acc: 83.520


12/17 02:39:57 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 001/521, train_loss: 0.390(0.390), train_acc: 86.458(86.458)
12/17 02:40:03 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 101/521, train_loss: 0.394(0.350), train_acc: 88.542(88.016)
12/17 02:40:10 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 201/521, train_loss: 0.432(0.363), train_acc: 81.250(87.557)
12/17 02:40:16 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 301/521, train_loss: 0.372(0.363), train_acc: 84.375(87.400)
12/17 02:40:23 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 401/521, train_loss: 0.267(0.362), train_acc: 88.542(87.347)
12/17 02:40:29 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 501/521, train_loss: 0.239(0.364), train_acc: 91.667(87.205)
12/17 02:40:31 AM [Supernet Training] lr: 0.01913 epoch: 142/600, step: 521/521, train_loss: 0.312(0.365), train_acc: 87.500(87.146)
12/17 02:40:31 AM [Supernet Training] epoch: 142, train_loss: 0.365, train_acc: 87.146
12/17 02:40:32 AM [Supernet Validation] epoch: 142, val_loss: 0.496, val_acc: 83.320, best_acc: 83.520


12/17 02:40:32 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 001/521, train_loss: 0.454(0.454), train_acc: 82.292(82.292)
12/17 02:40:39 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 101/521, train_loss: 0.473(0.353), train_acc: 83.333(87.252)
12/17 02:40:45 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 201/521, train_loss: 0.438(0.362), train_acc: 84.375(87.023)
12/17 02:40:51 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 301/521, train_loss: 0.445(0.361), train_acc: 83.333(87.175)
12/17 02:40:58 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 401/521, train_loss: 0.489(0.363), train_acc: 85.417(87.126)
12/17 02:41:04 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 501/521, train_loss: 0.371(0.362), train_acc: 83.333(87.194)
12/17 02:41:05 AM [Supernet Training] lr: 0.01908 epoch: 143/600, step: 521/521, train_loss: 0.284(0.362), train_acc: 86.250(87.182)
12/17 02:41:05 AM [Supernet Training] epoch: 143, train_loss: 0.362, train_acc: 87.182
12/17 02:41:07 AM [Supernet Validation] epoch: 143, val_loss: 0.514, val_acc: 83.030, best_acc: 83.520


12/17 02:41:07 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 001/521, train_loss: 0.322(0.322), train_acc: 85.417(85.417)
12/17 02:41:14 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 101/521, train_loss: 0.362(0.370), train_acc: 85.417(86.943)
12/17 02:41:20 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 201/521, train_loss: 0.326(0.364), train_acc: 89.583(87.158)
12/17 02:41:26 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 301/521, train_loss: 0.430(0.359), train_acc: 86.458(87.272)
12/17 02:41:33 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 401/521, train_loss: 0.438(0.364), train_acc: 87.500(87.095)
12/17 02:41:39 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 501/521, train_loss: 0.417(0.361), train_acc: 88.542(87.250)
12/17 02:41:40 AM [Supernet Training] lr: 0.01904 epoch: 144/600, step: 521/521, train_loss: 0.264(0.361), train_acc: 90.000(87.240)
12/17 02:41:40 AM [Supernet Training] epoch: 144, train_loss: 0.361, train_acc: 87.240
12/17 02:41:42 AM [Supernet Validation] epoch: 144, val_loss: 0.520, val_acc: 83.080, best_acc: 83.520


12/17 02:41:42 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 001/521, train_loss: 0.202(0.202), train_acc: 93.750(93.750)
12/17 02:41:48 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 101/521, train_loss: 0.290(0.353), train_acc: 90.625(87.717)
12/17 02:41:55 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 201/521, train_loss: 0.373(0.352), train_acc: 85.417(87.604)
12/17 02:42:01 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 301/521, train_loss: 0.530(0.351), train_acc: 80.208(87.576)
12/17 02:42:08 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 401/521, train_loss: 0.241(0.351), train_acc: 91.667(87.635)
12/17 02:42:14 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 501/521, train_loss: 0.553(0.355), train_acc: 83.333(87.517)
12/17 02:42:16 AM [Supernet Training] lr: 0.01900 epoch: 145/600, step: 521/521, train_loss: 0.235(0.355), train_acc: 91.250(87.502)
12/17 02:42:16 AM [Supernet Training] epoch: 145, train_loss: 0.355, train_acc: 87.502
12/17 02:42:17 AM [Supernet Validation] epoch: 145, val_loss: 0.499, val_acc: 83.320, best_acc: 83.520


12/17 02:42:18 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 001/521, train_loss: 0.235(0.235), train_acc: 91.667(91.667)
12/17 02:42:24 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 101/521, train_loss: 0.396(0.345), train_acc: 86.458(87.624)
12/17 02:42:30 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 201/521, train_loss: 0.453(0.356), train_acc: 83.333(87.365)
12/17 02:42:37 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 301/521, train_loss: 0.345(0.356), train_acc: 88.542(87.403)
12/17 02:42:43 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 401/521, train_loss: 0.408(0.355), train_acc: 87.500(87.427)
12/17 02:42:49 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 501/521, train_loss: 0.371(0.356), train_acc: 86.458(87.377)
12/17 02:42:51 AM [Supernet Training] lr: 0.01896 epoch: 146/600, step: 521/521, train_loss: 0.389(0.357), train_acc: 86.250(87.360)
12/17 02:42:51 AM [Supernet Training] epoch: 146, train_loss: 0.357, train_acc: 87.360
12/17 02:42:52 AM [Supernet Validation] epoch: 146, val_loss: 0.505, val_acc: 83.480, best_acc: 83.520


12/17 02:42:52 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 001/521, train_loss: 0.371(0.371), train_acc: 84.375(84.375)
12/17 02:42:59 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 101/521, train_loss: 0.291(0.344), train_acc: 88.542(87.706)
12/17 02:43:05 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 201/521, train_loss: 0.421(0.350), train_acc: 86.458(87.402)
12/17 02:43:12 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 301/521, train_loss: 0.290(0.348), train_acc: 90.625(87.462)
12/17 02:43:18 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 401/521, train_loss: 0.347(0.349), train_acc: 89.583(87.469)
12/17 02:43:25 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 501/521, train_loss: 0.347(0.352), train_acc: 85.417(87.421)
12/17 02:43:26 AM [Supernet Training] lr: 0.01892 epoch: 147/600, step: 521/521, train_loss: 0.290(0.352), train_acc: 88.750(87.404)
12/17 02:43:26 AM [Supernet Training] epoch: 147, train_loss: 0.352, train_acc: 87.404
12/17 02:43:28 AM [Supernet Validation] epoch: 147, val_loss: 0.510, val_acc: 83.220, best_acc: 83.520


12/17 02:43:28 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 001/521, train_loss: 0.446(0.446), train_acc: 86.458(86.458)
12/17 02:43:35 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 101/521, train_loss: 0.411(0.358), train_acc: 83.333(87.407)
12/17 02:43:41 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 201/521, train_loss: 0.353(0.346), train_acc: 86.458(87.878)
12/17 02:43:47 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 301/521, train_loss: 0.305(0.350), train_acc: 86.458(87.749)
12/17 02:43:54 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 401/521, train_loss: 0.477(0.351), train_acc: 82.292(87.669)
12/17 02:44:00 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 501/521, train_loss: 0.203(0.351), train_acc: 94.792(87.695)
12/17 02:44:02 AM [Supernet Training] lr: 0.01888 epoch: 148/600, step: 521/521, train_loss: 0.279(0.351), train_acc: 91.250(87.676)
12/17 02:44:02 AM [Supernet Training] epoch: 148, train_loss: 0.351, train_acc: 87.676
12/17 02:44:03 AM [Supernet Validation] epoch: 148, val_loss: 0.507, val_acc: 83.300, best_acc: 83.520


12/17 02:44:04 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 001/521, train_loss: 0.302(0.302), train_acc: 90.625(90.625)
12/17 02:44:10 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 101/521, train_loss: 0.307(0.329), train_acc: 88.542(88.480)
12/17 02:44:16 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 201/521, train_loss: 0.233(0.339), train_acc: 90.625(87.977)
12/17 02:44:23 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 301/521, train_loss: 0.441(0.348), train_acc: 82.292(87.628)
12/17 02:44:29 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 401/521, train_loss: 0.330(0.348), train_acc: 88.542(87.770)
12/17 02:44:35 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 501/521, train_loss: 0.245(0.351), train_acc: 89.583(87.648)
12/17 02:44:37 AM [Supernet Training] lr: 0.01883 epoch: 149/600, step: 521/521, train_loss: 0.242(0.350), train_acc: 90.000(87.688)
12/17 02:44:37 AM [Supernet Training] epoch: 149, train_loss: 0.350, train_acc: 87.688
12/17 02:44:38 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:44:38 AM [Supernet Validation] epoch: 149, val_loss: 0.490, val_acc: 83.630, best_acc: 83.630


12/17 02:44:39 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 001/521, train_loss: 0.398(0.398), train_acc: 87.500(87.500)
12/17 02:44:45 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 101/521, train_loss: 0.215(0.338), train_acc: 92.708(88.243)
12/17 02:44:51 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 201/521, train_loss: 0.424(0.336), train_acc: 88.542(88.257)
12/17 02:44:57 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 301/521, train_loss: 0.297(0.339), train_acc: 92.708(87.984)
12/17 02:45:04 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 401/521, train_loss: 0.396(0.347), train_acc: 83.333(87.734)
12/17 02:45:10 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 501/521, train_loss: 0.452(0.348), train_acc: 83.333(87.687)
12/17 02:45:11 AM [Supernet Training] lr: 0.01879 epoch: 150/600, step: 521/521, train_loss: 0.428(0.347), train_acc: 87.500(87.718)
12/17 02:45:11 AM [Supernet Training] epoch: 150, train_loss: 0.347, train_acc: 87.718
12/17 02:45:13 AM [Supernet Validation] epoch: 150, val_loss: 0.505, val_acc: 83.500, best_acc: 83.630


12/17 02:45:13 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 001/521, train_loss: 0.320(0.320), train_acc: 88.542(88.542)
12/17 02:45:19 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 101/521, train_loss: 0.338(0.347), train_acc: 89.583(87.861)
12/17 02:45:26 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 201/521, train_loss: 0.368(0.351), train_acc: 83.333(87.619)
12/17 02:45:32 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 301/521, train_loss: 0.377(0.349), train_acc: 85.417(87.663)
12/17 02:45:38 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 401/521, train_loss: 0.396(0.349), train_acc: 87.500(87.617)
12/17 02:45:45 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 501/521, train_loss: 0.289(0.348), train_acc: 88.542(87.662)
12/17 02:45:46 AM [Supernet Training] lr: 0.01875 epoch: 151/600, step: 521/521, train_loss: 0.438(0.348), train_acc: 83.750(87.622)
12/17 02:45:46 AM [Supernet Training] epoch: 151, train_loss: 0.348, train_acc: 87.622
12/17 02:45:48 AM [Supernet Validation] epoch: 151, val_loss: 0.507, val_acc: 83.540, best_acc: 83.630


12/17 02:45:48 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 001/521, train_loss: 0.396(0.396), train_acc: 86.458(86.458)
12/17 02:45:54 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 101/521, train_loss: 0.395(0.341), train_acc: 83.333(88.274)
12/17 02:46:01 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 201/521, train_loss: 0.583(0.341), train_acc: 79.167(88.096)
12/17 02:46:08 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 301/521, train_loss: 0.208(0.340), train_acc: 92.708(88.085)
12/17 02:46:14 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 401/521, train_loss: 0.380(0.338), train_acc: 84.375(88.066)
12/17 02:46:21 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 501/521, train_loss: 0.326(0.340), train_acc: 88.542(88.028)
12/17 02:46:22 AM [Supernet Training] lr: 0.01871 epoch: 152/600, step: 521/521, train_loss: 0.250(0.342), train_acc: 90.000(87.944)
12/17 02:46:22 AM [Supernet Training] epoch: 152, train_loss: 0.342, train_acc: 87.944
12/17 02:46:23 AM [Supernet Validation] epoch: 152, val_loss: 0.510, val_acc: 83.400, best_acc: 83.630


12/17 02:46:24 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 001/521, train_loss: 0.341(0.341), train_acc: 89.583(89.583)
12/17 02:46:30 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 101/521, train_loss: 0.388(0.340), train_acc: 88.542(88.108)
12/17 02:46:37 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 201/521, train_loss: 0.530(0.346), train_acc: 79.167(87.759)
12/17 02:46:43 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 301/521, train_loss: 0.390(0.344), train_acc: 87.500(87.884)
12/17 02:46:49 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 401/521, train_loss: 0.374(0.345), train_acc: 86.458(87.807)
12/17 02:46:56 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 501/521, train_loss: 0.332(0.344), train_acc: 86.458(87.812)
12/17 02:46:57 AM [Supernet Training] lr: 0.01867 epoch: 153/600, step: 521/521, train_loss: 0.330(0.345), train_acc: 88.750(87.792)
12/17 02:46:57 AM [Supernet Training] epoch: 153, train_loss: 0.345, train_acc: 87.792
12/17 02:46:58 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:46:58 AM [Supernet Validation] epoch: 153, val_loss: 0.487, val_acc: 83.930, best_acc: 83.930


12/17 02:46:59 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 001/521, train_loss: 0.328(0.328), train_acc: 88.542(88.542)
12/17 02:47:05 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 101/521, train_loss: 0.319(0.337), train_acc: 89.583(88.150)
12/17 02:47:12 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 201/521, train_loss: 0.425(0.331), train_acc: 86.458(88.417)
12/17 02:47:18 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 301/521, train_loss: 0.533(0.330), train_acc: 80.208(88.358)
12/17 02:47:24 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 401/521, train_loss: 0.295(0.334), train_acc: 90.625(88.147)
12/17 02:47:31 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 501/521, train_loss: 0.523(0.337), train_acc: 83.333(88.063)
12/17 02:47:32 AM [Supernet Training] lr: 0.01862 epoch: 154/600, step: 521/521, train_loss: 0.380(0.338), train_acc: 86.250(88.010)
12/17 02:47:32 AM [Supernet Training] epoch: 154, train_loss: 0.338, train_acc: 88.010
12/17 02:47:34 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:47:34 AM [Supernet Validation] epoch: 154, val_loss: 0.489, val_acc: 83.950, best_acc: 83.950


12/17 02:47:34 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 001/521, train_loss: 0.263(0.263), train_acc: 90.625(90.625)
12/17 02:47:41 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 101/521, train_loss: 0.261(0.326), train_acc: 90.625(88.274)
12/17 02:47:47 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 201/521, train_loss: 0.322(0.332), train_acc: 88.542(87.951)
12/17 02:47:53 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 301/521, train_loss: 0.258(0.331), train_acc: 91.667(88.068)
12/17 02:48:00 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 401/521, train_loss: 0.262(0.336), train_acc: 88.542(87.970)
12/17 02:48:06 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 501/521, train_loss: 0.275(0.337), train_acc: 88.542(87.932)
12/17 02:48:07 AM [Supernet Training] lr: 0.01858 epoch: 155/600, step: 521/521, train_loss: 0.303(0.337), train_acc: 87.500(87.908)
12/17 02:48:07 AM [Supernet Training] epoch: 155, train_loss: 0.337, train_acc: 87.908
12/17 02:48:09 AM [Supernet Validation] epoch: 155, val_loss: 0.511, val_acc: 83.650, best_acc: 83.950


12/17 02:48:09 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 001/521, train_loss: 0.351(0.351), train_acc: 86.458(86.458)
12/17 02:48:16 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 101/521, train_loss: 0.330(0.331), train_acc: 83.333(88.212)
12/17 02:48:22 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 201/521, train_loss: 0.187(0.331), train_acc: 94.792(88.241)
12/17 02:48:29 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 301/521, train_loss: 0.388(0.337), train_acc: 85.417(88.074)
12/17 02:48:35 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 401/521, train_loss: 0.348(0.336), train_acc: 87.500(88.142)
12/17 02:48:41 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 501/521, train_loss: 0.239(0.337), train_acc: 91.667(88.053)
12/17 02:48:43 AM [Supernet Training] lr: 0.01854 epoch: 156/600, step: 521/521, train_loss: 0.383(0.339), train_acc: 88.750(88.022)
12/17 02:48:43 AM [Supernet Training] epoch: 156, train_loss: 0.339, train_acc: 88.022
12/17 02:48:44 AM [Supernet Validation] epoch: 156, val_loss: 0.503, val_acc: 83.480, best_acc: 83.950


12/17 02:48:45 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 001/521, train_loss: 0.374(0.374), train_acc: 85.417(85.417)
12/17 02:48:51 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 101/521, train_loss: 0.340(0.331), train_acc: 85.417(88.366)
12/17 02:48:57 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 201/521, train_loss: 0.329(0.334), train_acc: 86.458(88.246)
12/17 02:49:04 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 301/521, train_loss: 0.331(0.336), train_acc: 87.500(88.130)
12/17 02:49:10 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 401/521, train_loss: 0.295(0.331), train_acc: 89.583(88.316)
12/17 02:49:17 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 501/521, train_loss: 0.291(0.332), train_acc: 89.583(88.263)
12/17 02:49:18 AM [Supernet Training] lr: 0.01850 epoch: 157/600, step: 521/521, train_loss: 0.322(0.332), train_acc: 87.500(88.250)
12/17 02:49:18 AM [Supernet Training] epoch: 157, train_loss: 0.332, train_acc: 88.250
12/17 02:49:20 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:49:20 AM [Supernet Validation] epoch: 157, val_loss: 0.495, val_acc: 84.090, best_acc: 84.090


12/17 02:49:20 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 001/521, train_loss: 0.272(0.272), train_acc: 90.625(90.625)
12/17 02:49:26 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 101/521, train_loss: 0.428(0.317), train_acc: 85.417(88.593)
12/17 02:49:33 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 201/521, train_loss: 0.255(0.327), train_acc: 89.583(88.371)
12/17 02:49:39 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 301/521, train_loss: 0.306(0.328), train_acc: 88.542(88.299)
12/17 02:49:46 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 401/521, train_loss: 0.318(0.327), train_acc: 89.583(88.362)
12/17 02:49:52 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 501/521, train_loss: 0.399(0.326), train_acc: 86.458(88.396)
12/17 02:49:53 AM [Supernet Training] lr: 0.01846 epoch: 158/600, step: 521/521, train_loss: 0.419(0.326), train_acc: 83.750(88.406)
12/17 02:49:53 AM [Supernet Training] epoch: 158, train_loss: 0.326, train_acc: 88.406
12/17 02:49:55 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:49:55 AM [Supernet Validation] epoch: 158, val_loss: 0.491, val_acc: 84.240, best_acc: 84.240


12/17 02:49:55 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 001/521, train_loss: 0.383(0.383), train_acc: 88.542(88.542)
12/17 02:50:02 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 101/521, train_loss: 0.330(0.324), train_acc: 88.542(88.418)
12/17 02:50:08 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 201/521, train_loss: 0.317(0.321), train_acc: 91.667(88.547)
12/17 02:50:15 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 301/521, train_loss: 0.307(0.328), train_acc: 89.583(88.275)
12/17 02:50:21 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 401/521, train_loss: 0.297(0.330), train_acc: 87.500(88.225)
12/17 02:50:27 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 501/521, train_loss: 0.261(0.329), train_acc: 87.500(88.232)
12/17 02:50:29 AM [Supernet Training] lr: 0.01842 epoch: 159/600, step: 521/521, train_loss: 0.312(0.329), train_acc: 88.750(88.250)
12/17 02:50:29 AM [Supernet Training] epoch: 159, train_loss: 0.329, train_acc: 88.250
12/17 02:50:30 AM [Supernet Validation] epoch: 159, val_loss: 0.503, val_acc: 83.740, best_acc: 84.240


12/17 02:50:31 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 001/521, train_loss: 0.324(0.324), train_acc: 89.583(89.583)
12/17 02:50:37 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 101/521, train_loss: 0.313(0.317), train_acc: 90.625(88.593)
12/17 02:50:43 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 201/521, train_loss: 0.255(0.319), train_acc: 91.667(88.682)
12/17 02:50:50 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 301/521, train_loss: 0.315(0.321), train_acc: 90.625(88.735)
12/17 02:50:56 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 401/521, train_loss: 0.399(0.322), train_acc: 86.458(88.614)
12/17 02:51:03 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 501/521, train_loss: 0.307(0.326), train_acc: 86.458(88.506)
12/17 02:51:04 AM [Supernet Training] lr: 0.01837 epoch: 160/600, step: 521/521, train_loss: 0.299(0.326), train_acc: 88.750(88.504)
12/17 02:51:04 AM [Supernet Training] epoch: 160, train_loss: 0.326, train_acc: 88.504
12/17 02:51:05 AM [Supernet Validation] epoch: 160, val_loss: 0.489, val_acc: 83.930, best_acc: 84.240


12/17 02:51:06 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 001/521, train_loss: 0.155(0.155), train_acc: 93.750(93.750)
12/17 02:51:12 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 101/521, train_loss: 0.329(0.327), train_acc: 88.542(88.397)
12/17 02:51:18 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 201/521, train_loss: 0.250(0.331), train_acc: 91.667(88.293)
12/17 02:51:25 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 301/521, train_loss: 0.454(0.324), train_acc: 84.375(88.528)
12/17 02:51:31 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 401/521, train_loss: 0.260(0.324), train_acc: 87.500(88.552)
12/17 02:51:38 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 501/521, train_loss: 0.392(0.322), train_acc: 85.417(88.654)
12/17 02:51:39 AM [Supernet Training] lr: 0.01833 epoch: 161/600, step: 521/521, train_loss: 0.351(0.323), train_acc: 90.000(88.630)
12/17 02:51:39 AM [Supernet Training] epoch: 161, train_loss: 0.323, train_acc: 88.630
12/17 02:51:40 AM [Supernet Validation] epoch: 161, val_loss: 0.495, val_acc: 84.180, best_acc: 84.240


12/17 02:51:41 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 001/521, train_loss: 0.248(0.248), train_acc: 92.708(92.708)
12/17 02:51:47 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 101/521, train_loss: 0.197(0.313), train_acc: 93.750(89.099)
12/17 02:51:54 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 201/521, train_loss: 0.328(0.320), train_acc: 86.458(88.847)
12/17 02:52:00 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 301/521, train_loss: 0.357(0.318), train_acc: 88.542(88.850)
12/17 02:52:07 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 401/521, train_loss: 0.181(0.320), train_acc: 91.667(88.825)
12/17 02:52:13 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 501/521, train_loss: 0.222(0.318), train_acc: 91.667(88.824)
12/17 02:52:14 AM [Supernet Training] lr: 0.01829 epoch: 162/600, step: 521/521, train_loss: 0.292(0.319), train_acc: 87.500(88.784)
12/17 02:52:14 AM [Supernet Training] epoch: 162, train_loss: 0.319, train_acc: 88.784
12/17 02:52:16 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:52:16 AM [Supernet Validation] epoch: 162, val_loss: 0.494, val_acc: 84.370, best_acc: 84.370


12/17 02:52:16 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 001/521, train_loss: 0.316(0.316), train_acc: 88.542(88.542)
12/17 02:52:23 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 101/521, train_loss: 0.420(0.316), train_acc: 84.375(88.542)
12/17 02:52:29 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 201/521, train_loss: 0.437(0.318), train_acc: 85.417(88.531)
12/17 02:52:36 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 301/521, train_loss: 0.397(0.324), train_acc: 87.500(88.376)
12/17 02:52:42 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 401/521, train_loss: 0.526(0.326), train_acc: 82.292(88.448)
12/17 02:52:48 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 501/521, train_loss: 0.210(0.326), train_acc: 94.792(88.392)
12/17 02:52:50 AM [Supernet Training] lr: 0.01825 epoch: 163/600, step: 521/521, train_loss: 0.288(0.327), train_acc: 87.500(88.388)
12/17 02:52:50 AM [Supernet Training] epoch: 163, train_loss: 0.327, train_acc: 88.388
12/17 02:52:51 AM [Supernet Validation] epoch: 163, val_loss: 0.497, val_acc: 83.860, best_acc: 84.370


12/17 02:52:52 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 001/521, train_loss: 0.415(0.415), train_acc: 80.208(80.208)
12/17 02:52:58 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 101/521, train_loss: 0.409(0.312), train_acc: 84.375(89.037)
12/17 02:53:04 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 201/521, train_loss: 0.256(0.317), train_acc: 91.667(88.708)
12/17 02:53:10 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 301/521, train_loss: 0.367(0.321), train_acc: 89.583(88.666)
12/17 02:53:17 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 401/521, train_loss: 0.241(0.321), train_acc: 94.792(88.692)
12/17 02:53:23 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 501/521, train_loss: 0.352(0.320), train_acc: 87.500(88.768)
12/17 02:53:24 AM [Supernet Training] lr: 0.01821 epoch: 164/600, step: 521/521, train_loss: 0.284(0.319), train_acc: 85.000(88.746)
12/17 02:53:24 AM [Supernet Training] epoch: 164, train_loss: 0.319, train_acc: 88.746
12/17 02:53:26 AM [Supernet Validation] epoch: 164, val_loss: 0.492, val_acc: 84.230, best_acc: 84.370


12/17 02:53:26 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 001/521, train_loss: 0.214(0.214), train_acc: 93.750(93.750)
12/17 02:53:33 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 101/521, train_loss: 0.357(0.320), train_acc: 86.458(88.542)
12/17 02:53:39 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 201/521, train_loss: 0.367(0.324), train_acc: 87.500(88.428)
12/17 02:53:45 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 301/521, train_loss: 0.368(0.321), train_acc: 86.458(88.569)
12/17 02:53:51 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 401/521, train_loss: 0.408(0.321), train_acc: 87.500(88.614)
12/17 02:53:58 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 501/521, train_loss: 0.293(0.320), train_acc: 88.542(88.675)
12/17 02:53:59 AM [Supernet Training] lr: 0.01817 epoch: 165/600, step: 521/521, train_loss: 0.251(0.320), train_acc: 91.250(88.658)
12/17 02:53:59 AM [Supernet Training] epoch: 165, train_loss: 0.320, train_acc: 88.658
12/17 02:54:01 AM [Supernet Validation] epoch: 165, val_loss: 0.492, val_acc: 84.350, best_acc: 84.370


12/17 02:54:01 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 001/521, train_loss: 0.294(0.294), train_acc: 90.625(90.625)
12/17 02:54:08 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 101/521, train_loss: 0.385(0.319), train_acc: 89.583(88.717)
12/17 02:54:14 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 201/521, train_loss: 0.407(0.312), train_acc: 87.500(88.936)
12/17 02:54:20 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 301/521, train_loss: 0.234(0.316), train_acc: 91.667(88.798)
12/17 02:54:27 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 401/521, train_loss: 0.233(0.314), train_acc: 89.583(88.830)
12/17 02:54:33 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 501/521, train_loss: 0.499(0.315), train_acc: 82.292(88.758)
12/17 02:54:34 AM [Supernet Training] lr: 0.01812 epoch: 166/600, step: 521/521, train_loss: 0.246(0.314), train_acc: 88.750(88.780)
12/17 02:54:34 AM [Supernet Training] epoch: 166, train_loss: 0.314, train_acc: 88.780
12/17 02:54:36 AM [Supernet Validation] epoch: 166, val_loss: 0.487, val_acc: 83.990, best_acc: 84.370


12/17 02:54:36 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 001/521, train_loss: 0.235(0.235), train_acc: 93.750(93.750)
12/17 02:54:42 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 101/521, train_loss: 0.217(0.308), train_acc: 90.625(89.047)
12/17 02:54:49 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 201/521, train_loss: 0.406(0.303), train_acc: 84.375(89.267)
12/17 02:54:55 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 301/521, train_loss: 0.328(0.309), train_acc: 87.500(89.030)
12/17 02:55:02 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 401/521, train_loss: 0.517(0.306), train_acc: 81.250(89.121)
12/17 02:55:08 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 501/521, train_loss: 0.233(0.309), train_acc: 89.583(88.972)
12/17 02:55:09 AM [Supernet Training] lr: 0.01808 epoch: 167/600, step: 521/521, train_loss: 0.418(0.309), train_acc: 86.250(88.966)
12/17 02:55:09 AM [Supernet Training] epoch: 167, train_loss: 0.309, train_acc: 88.966
12/17 02:55:11 AM [Supernet Validation] epoch: 167, val_loss: 0.499, val_acc: 84.040, best_acc: 84.370


12/17 02:55:11 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 001/521, train_loss: 0.333(0.333), train_acc: 86.458(86.458)
12/17 02:55:17 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 101/521, train_loss: 0.437(0.302), train_acc: 84.375(89.140)
12/17 02:55:24 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 201/521, train_loss: 0.250(0.305), train_acc: 92.708(88.951)
12/17 02:55:30 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 301/521, train_loss: 0.281(0.307), train_acc: 89.583(89.043)
12/17 02:55:36 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 401/521, train_loss: 0.331(0.306), train_acc: 91.667(89.103)
12/17 02:55:43 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 501/521, train_loss: 0.183(0.308), train_acc: 94.792(89.049)
12/17 02:55:44 AM [Supernet Training] lr: 0.01804 epoch: 168/600, step: 521/521, train_loss: 0.547(0.310), train_acc: 78.750(89.006)
12/17 02:55:44 AM [Supernet Training] epoch: 168, train_loss: 0.310, train_acc: 89.006
12/17 02:55:45 AM [Supernet Validation] epoch: 168, val_loss: 0.494, val_acc: 83.860, best_acc: 84.370


12/17 02:55:46 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 001/521, train_loss: 0.338(0.338), train_acc: 87.500(87.500)
12/17 02:55:52 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 101/521, train_loss: 0.274(0.294), train_acc: 88.542(89.408)
12/17 02:55:59 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 201/521, train_loss: 0.389(0.300), train_acc: 85.417(89.252)
12/17 02:56:05 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 301/521, train_loss: 0.217(0.304), train_acc: 92.708(89.206)
12/17 02:56:12 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 401/521, train_loss: 0.131(0.307), train_acc: 95.833(89.066)
12/17 02:56:18 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 501/521, train_loss: 0.236(0.308), train_acc: 90.625(89.093)
12/17 02:56:19 AM [Supernet Training] lr: 0.01800 epoch: 169/600, step: 521/521, train_loss: 0.196(0.309), train_acc: 92.500(89.052)
12/17 02:56:19 AM [Supernet Training] epoch: 169, train_loss: 0.309, train_acc: 89.052
12/17 02:56:21 AM [Supernet Validation] epoch: 169, val_loss: 0.503, val_acc: 83.540, best_acc: 84.370


12/17 02:56:21 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 001/521, train_loss: 0.439(0.439), train_acc: 84.375(84.375)
12/17 02:56:28 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 101/521, train_loss: 0.280(0.307), train_acc: 89.583(88.872)
12/17 02:56:34 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 201/521, train_loss: 0.372(0.312), train_acc: 89.583(88.837)
12/17 02:56:40 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 301/521, train_loss: 0.398(0.309), train_acc: 83.333(88.884)
12/17 02:56:47 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 401/521, train_loss: 0.306(0.311), train_acc: 88.542(88.853)
12/17 02:56:54 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 501/521, train_loss: 0.299(0.309), train_acc: 86.458(88.851)
12/17 02:56:55 AM [Supernet Training] lr: 0.01796 epoch: 170/600, step: 521/521, train_loss: 0.246(0.309), train_acc: 95.000(88.866)
12/17 02:56:55 AM [Supernet Training] epoch: 170, train_loss: 0.309, train_acc: 88.866
12/17 02:56:57 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:56:57 AM [Supernet Validation] epoch: 170, val_loss: 0.489, val_acc: 84.450, best_acc: 84.450


12/17 02:56:57 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 001/521, train_loss: 0.240(0.240), train_acc: 92.708(92.708)
12/17 02:57:03 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 101/521, train_loss: 0.206(0.299), train_acc: 93.750(89.367)
12/17 02:57:10 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 201/521, train_loss: 0.345(0.306), train_acc: 86.458(89.018)
12/17 02:57:16 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 301/521, train_loss: 0.290(0.307), train_acc: 89.583(89.106)
12/17 02:57:23 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 401/521, train_loss: 0.206(0.303), train_acc: 93.750(89.298)
12/17 02:57:29 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 501/521, train_loss: 0.357(0.305), train_acc: 86.458(89.153)
12/17 02:57:30 AM [Supernet Training] lr: 0.01792 epoch: 171/600, step: 521/521, train_loss: 0.285(0.306), train_acc: 88.750(89.128)
12/17 02:57:30 AM [Supernet Training] epoch: 171, train_loss: 0.306, train_acc: 89.128
12/17 02:57:32 AM [Supernet Validation] epoch: 171, val_loss: 0.492, val_acc: 84.250, best_acc: 84.450


12/17 02:57:32 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 001/521, train_loss: 0.264(0.264), train_acc: 93.750(93.750)
12/17 02:57:39 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 101/521, train_loss: 0.372(0.306), train_acc: 87.500(88.965)
12/17 02:57:45 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 201/521, train_loss: 0.136(0.305), train_acc: 93.750(89.034)
12/17 02:57:51 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 301/521, train_loss: 0.320(0.302), train_acc: 89.583(89.175)
12/17 02:57:58 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 401/521, train_loss: 0.446(0.303), train_acc: 83.333(89.155)
12/17 02:58:04 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 501/521, train_loss: 0.357(0.303), train_acc: 85.417(89.242)
12/17 02:58:05 AM [Supernet Training] lr: 0.01788 epoch: 172/600, step: 521/521, train_loss: 0.378(0.304), train_acc: 87.500(89.200)
12/17 02:58:05 AM [Supernet Training] epoch: 172, train_loss: 0.304, train_acc: 89.200
12/17 02:58:07 AM [Supernet Validation] epoch: 172, val_loss: 0.508, val_acc: 83.240, best_acc: 84.450


12/17 02:58:07 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 001/521, train_loss: 0.209(0.209), train_acc: 92.708(92.708)
12/17 02:58:14 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 101/521, train_loss: 0.404(0.309), train_acc: 83.333(89.295)
12/17 02:58:20 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 201/521, train_loss: 0.302(0.297), train_acc: 90.625(89.899)
12/17 02:58:27 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 301/521, train_loss: 0.361(0.299), train_acc: 86.458(89.694)
12/17 02:58:33 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 401/521, train_loss: 0.510(0.303), train_acc: 86.458(89.427)
12/17 02:58:40 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 501/521, train_loss: 0.413(0.303), train_acc: 85.417(89.305)
12/17 02:58:41 AM [Supernet Training] lr: 0.01783 epoch: 173/600, step: 521/521, train_loss: 0.348(0.304), train_acc: 83.750(89.304)
12/17 02:58:41 AM [Supernet Training] epoch: 173, train_loss: 0.304, train_acc: 89.304
12/17 02:58:42 AM [Supernet Validation] epoch: 173, val_loss: 0.492, val_acc: 84.270, best_acc: 84.450


12/17 02:58:43 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 001/521, train_loss: 0.233(0.233), train_acc: 93.750(93.750)
12/17 02:58:49 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 101/521, train_loss: 0.316(0.293), train_acc: 87.500(89.408)
12/17 02:58:56 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 201/521, train_loss: 0.252(0.298), train_acc: 89.583(89.402)
12/17 02:59:02 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 301/521, train_loss: 0.222(0.301), train_acc: 92.708(89.289)
12/17 02:59:08 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 401/521, train_loss: 0.291(0.300), train_acc: 90.625(89.339)
12/17 02:59:15 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 501/521, train_loss: 0.349(0.301), train_acc: 87.500(89.332)
12/17 02:59:16 AM [Supernet Training] lr: 0.01779 epoch: 174/600, step: 521/521, train_loss: 0.279(0.300), train_acc: 91.250(89.358)
12/17 02:59:16 AM [Supernet Training] epoch: 174, train_loss: 0.300, train_acc: 89.358
12/17 02:59:18 AM [Supernet Validation] epoch: 174, val_loss: 0.501, val_acc: 84.020, best_acc: 84.450


12/17 02:59:18 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 001/521, train_loss: 0.294(0.294), train_acc: 89.583(89.583)
12/17 02:59:24 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 101/521, train_loss: 0.410(0.302), train_acc: 89.583(88.779)
12/17 02:59:31 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 201/521, train_loss: 0.256(0.305), train_acc: 91.667(89.013)
12/17 02:59:37 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 301/521, train_loss: 0.196(0.301), train_acc: 93.750(89.203)
12/17 02:59:44 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 401/521, train_loss: 0.331(0.301), train_acc: 91.667(89.199)
12/17 02:59:50 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 501/521, train_loss: 0.325(0.299), train_acc: 86.458(89.240)
12/17 02:59:51 AM [Supernet Training] lr: 0.01775 epoch: 175/600, step: 521/521, train_loss: 0.406(0.298), train_acc: 86.250(89.256)
12/17 02:59:51 AM [Supernet Training] epoch: 175, train_loss: 0.298, train_acc: 89.256
12/17 02:59:53 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 02:59:53 AM [Supernet Validation] epoch: 175, val_loss: 0.488, val_acc: 84.710, best_acc: 84.710


12/17 02:59:53 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 001/521, train_loss: 0.330(0.330), train_acc: 90.625(90.625)
12/17 03:00:00 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 101/521, train_loss: 0.343(0.302), train_acc: 88.542(89.573)
12/17 03:00:06 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 201/521, train_loss: 0.316(0.293), train_acc: 89.583(89.728)
12/17 03:00:12 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 301/521, train_loss: 0.265(0.294), train_acc: 88.542(89.621)
12/17 03:00:19 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 401/521, train_loss: 0.358(0.294), train_acc: 87.500(89.609)
12/17 03:00:25 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 501/521, train_loss: 0.249(0.296), train_acc: 91.667(89.500)
12/17 03:00:26 AM [Supernet Training] lr: 0.01771 epoch: 176/600, step: 521/521, train_loss: 0.434(0.298), train_acc: 88.750(89.492)
12/17 03:00:26 AM [Supernet Training] epoch: 176, train_loss: 0.298, train_acc: 89.492
12/17 03:00:28 AM [Supernet Validation] epoch: 176, val_loss: 0.480, val_acc: 84.710, best_acc: 84.710


12/17 03:00:28 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 001/521, train_loss: 0.319(0.319), train_acc: 87.500(87.500)
12/17 03:00:35 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 101/521, train_loss: 0.209(0.289), train_acc: 93.750(89.831)
12/17 03:00:41 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 201/521, train_loss: 0.218(0.292), train_acc: 88.542(89.604)
12/17 03:00:47 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 301/521, train_loss: 0.299(0.293), train_acc: 90.625(89.542)
12/17 03:00:54 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 401/521, train_loss: 0.477(0.297), train_acc: 87.500(89.396)
12/17 03:01:00 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 501/521, train_loss: 0.464(0.298), train_acc: 85.417(89.328)
12/17 03:01:01 AM [Supernet Training] lr: 0.01767 epoch: 177/600, step: 521/521, train_loss: 0.321(0.297), train_acc: 90.000(89.358)
12/17 03:01:01 AM [Supernet Training] epoch: 177, train_loss: 0.297, train_acc: 89.358
12/17 03:01:03 AM [Supernet Validation] epoch: 177, val_loss: 0.486, val_acc: 84.310, best_acc: 84.710


12/17 03:01:03 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 001/521, train_loss: 0.363(0.363), train_acc: 89.583(89.583)
12/17 03:01:10 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 101/521, train_loss: 0.316(0.282), train_acc: 89.583(90.006)
12/17 03:01:16 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 201/521, train_loss: 0.155(0.291), train_acc: 94.792(89.630)
12/17 03:01:23 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 301/521, train_loss: 0.208(0.290), train_acc: 90.625(89.594)
12/17 03:01:29 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 401/521, train_loss: 0.322(0.290), train_acc: 89.583(89.643)
12/17 03:01:36 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 501/521, train_loss: 0.200(0.295), train_acc: 92.708(89.486)
12/17 03:01:37 AM [Supernet Training] lr: 0.01763 epoch: 178/600, step: 521/521, train_loss: 0.366(0.295), train_acc: 86.250(89.486)
12/17 03:01:37 AM [Supernet Training] epoch: 178, train_loss: 0.295, train_acc: 89.486
12/17 03:01:38 AM [Supernet Validation] epoch: 178, val_loss: 0.484, val_acc: 84.440, best_acc: 84.710


12/17 03:01:39 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 001/521, train_loss: 0.194(0.194), train_acc: 93.750(93.750)
12/17 03:01:45 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 101/521, train_loss: 0.213(0.291), train_acc: 91.667(89.614)
12/17 03:01:52 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 201/521, train_loss: 0.347(0.292), train_acc: 88.542(89.661)
12/17 03:01:58 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 301/521, train_loss: 0.273(0.287), train_acc: 90.625(89.736)
12/17 03:02:05 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 401/521, train_loss: 0.363(0.290), train_acc: 85.417(89.669)
12/17 03:02:11 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 501/521, train_loss: 0.283(0.293), train_acc: 89.583(89.592)
12/17 03:02:12 AM [Supernet Training] lr: 0.01758 epoch: 179/600, step: 521/521, train_loss: 0.251(0.293), train_acc: 88.750(89.566)
12/17 03:02:12 AM [Supernet Training] epoch: 179, train_loss: 0.293, train_acc: 89.566
12/17 03:02:14 AM [Supernet Validation] epoch: 179, val_loss: 0.486, val_acc: 84.430, best_acc: 84.710


12/17 03:02:14 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 001/521, train_loss: 0.259(0.259), train_acc: 91.667(91.667)
12/17 03:02:21 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 101/521, train_loss: 0.239(0.283), train_acc: 91.667(90.233)
12/17 03:02:27 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 201/521, train_loss: 0.290(0.288), train_acc: 90.625(89.941)
12/17 03:02:33 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 301/521, train_loss: 0.423(0.288), train_acc: 88.542(89.812)
12/17 03:02:40 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 401/521, train_loss: 0.334(0.285), train_acc: 89.583(89.879)
12/17 03:02:46 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 501/521, train_loss: 0.243(0.289), train_acc: 88.542(89.733)
12/17 03:02:47 AM [Supernet Training] lr: 0.01754 epoch: 180/600, step: 521/521, train_loss: 0.314(0.289), train_acc: 88.750(89.758)
12/17 03:02:47 AM [Supernet Training] epoch: 180, train_loss: 0.289, train_acc: 89.758
12/17 03:02:49 AM [Supernet Validation] epoch: 180, val_loss: 0.472, val_acc: 84.650, best_acc: 84.710


12/17 03:02:49 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 001/521, train_loss: 0.209(0.209), train_acc: 95.833(95.833)
12/17 03:02:55 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 101/521, train_loss: 0.428(0.291), train_acc: 79.167(89.583)
12/17 03:03:02 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 201/521, train_loss: 0.342(0.287), train_acc: 90.625(89.863)
12/17 03:03:08 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 301/521, train_loss: 0.337(0.285), train_acc: 90.625(89.947)
12/17 03:03:14 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 401/521, train_loss: 0.234(0.286), train_acc: 91.667(89.895)
12/17 03:03:21 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 501/521, train_loss: 0.182(0.288), train_acc: 93.750(89.781)
12/17 03:03:22 AM [Supernet Training] lr: 0.01750 epoch: 181/600, step: 521/521, train_loss: 0.340(0.288), train_acc: 91.250(89.778)
12/17 03:03:22 AM [Supernet Training] epoch: 181, train_loss: 0.288, train_acc: 89.778
12/17 03:03:23 AM [Supernet Validation] epoch: 181, val_loss: 0.501, val_acc: 84.320, best_acc: 84.710


12/17 03:03:24 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 001/521, train_loss: 0.258(0.258), train_acc: 93.750(93.750)
12/17 03:03:30 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 101/521, train_loss: 0.216(0.282), train_acc: 91.667(90.264)
12/17 03:03:36 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 201/521, train_loss: 0.357(0.283), train_acc: 87.500(90.112)
12/17 03:03:43 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 301/521, train_loss: 0.221(0.287), train_acc: 90.625(89.947)
12/17 03:03:49 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 401/521, train_loss: 0.292(0.289), train_acc: 90.625(89.926)
12/17 03:03:55 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 501/521, train_loss: 0.444(0.287), train_acc: 82.292(89.928)
12/17 03:03:57 AM [Supernet Training] lr: 0.01746 epoch: 182/600, step: 521/521, train_loss: 0.473(0.286), train_acc: 85.000(89.982)
12/17 03:03:57 AM [Supernet Training] epoch: 182, train_loss: 0.286, train_acc: 89.982
12/17 03:03:58 AM [Supernet Validation] epoch: 182, val_loss: 0.495, val_acc: 84.350, best_acc: 84.710


12/17 03:03:59 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 001/521, train_loss: 0.457(0.457), train_acc: 82.292(82.292)
12/17 03:04:05 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 101/521, train_loss: 0.337(0.286), train_acc: 85.417(89.656)
12/17 03:04:11 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 201/521, train_loss: 0.192(0.288), train_acc: 90.625(89.620)
12/17 03:04:18 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 301/521, train_loss: 0.362(0.291), train_acc: 88.542(89.677)
12/17 03:04:24 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 401/521, train_loss: 0.226(0.289), train_acc: 94.792(89.789)
12/17 03:04:30 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 501/521, train_loss: 0.220(0.288), train_acc: 92.708(89.787)
12/17 03:04:32 AM [Supernet Training] lr: 0.01742 epoch: 183/600, step: 521/521, train_loss: 0.246(0.288), train_acc: 91.250(89.796)
12/17 03:04:32 AM [Supernet Training] epoch: 183, train_loss: 0.288, train_acc: 89.796
12/17 03:04:33 AM [Supernet Validation] epoch: 183, val_loss: 0.490, val_acc: 84.450, best_acc: 84.710


12/17 03:04:34 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 001/521, train_loss: 0.358(0.358), train_acc: 88.542(88.542)
12/17 03:04:40 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 101/521, train_loss: 0.210(0.291), train_acc: 92.708(89.449)
12/17 03:04:46 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 201/521, train_loss: 0.235(0.284), train_acc: 89.583(89.723)
12/17 03:04:53 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 301/521, train_loss: 0.253(0.288), train_acc: 90.625(89.698)
12/17 03:04:59 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 401/521, train_loss: 0.208(0.286), train_acc: 92.708(89.731)
12/17 03:05:05 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 501/521, train_loss: 0.155(0.287), train_acc: 95.833(89.706)
12/17 03:05:06 AM [Supernet Training] lr: 0.01738 epoch: 184/600, step: 521/521, train_loss: 0.198(0.286), train_acc: 93.750(89.766)
12/17 03:05:07 AM [Supernet Training] epoch: 184, train_loss: 0.286, train_acc: 89.766
12/17 03:05:08 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:05:08 AM [Supernet Validation] epoch: 184, val_loss: 0.482, val_acc: 84.790, best_acc: 84.790


12/17 03:05:09 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 001/521, train_loss: 0.259(0.259), train_acc: 90.625(90.625)
12/17 03:05:15 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 101/521, train_loss: 0.203(0.283), train_acc: 94.792(89.851)
12/17 03:05:21 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 201/521, train_loss: 0.279(0.277), train_acc: 87.500(90.200)
12/17 03:05:28 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 301/521, train_loss: 0.330(0.280), train_acc: 87.500(90.109)
12/17 03:05:34 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 401/521, train_loss: 0.311(0.283), train_acc: 89.583(89.960)
12/17 03:05:40 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 501/521, train_loss: 0.156(0.284), train_acc: 94.792(89.964)
12/17 03:05:41 AM [Supernet Training] lr: 0.01733 epoch: 185/600, step: 521/521, train_loss: 0.256(0.284), train_acc: 90.000(89.936)
12/17 03:05:41 AM [Supernet Training] epoch: 185, train_loss: 0.284, train_acc: 89.936
12/17 03:05:43 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:05:43 AM [Supernet Validation] epoch: 185, val_loss: 0.480, val_acc: 84.810, best_acc: 84.810


12/17 03:05:43 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 001/521, train_loss: 0.331(0.331), train_acc: 90.625(90.625)
12/17 03:05:50 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 101/521, train_loss: 0.403(0.282), train_acc: 83.333(90.140)
12/17 03:05:56 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 201/521, train_loss: 0.313(0.289), train_acc: 89.583(89.682)
12/17 03:06:02 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 301/521, train_loss: 0.415(0.283), train_acc: 84.375(89.777)
12/17 03:06:09 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 401/521, train_loss: 0.308(0.281), train_acc: 86.458(89.778)
12/17 03:06:15 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 501/521, train_loss: 0.306(0.280), train_acc: 89.583(89.856)
12/17 03:06:16 AM [Supernet Training] lr: 0.01729 epoch: 186/600, step: 521/521, train_loss: 0.245(0.280), train_acc: 91.250(89.852)
12/17 03:06:16 AM [Supernet Training] epoch: 186, train_loss: 0.280, train_acc: 89.852
12/17 03:06:18 AM [Supernet Validation] epoch: 186, val_loss: 0.490, val_acc: 84.650, best_acc: 84.810


12/17 03:06:18 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 001/521, train_loss: 0.450(0.450), train_acc: 86.458(86.458)
12/17 03:06:24 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 101/521, train_loss: 0.261(0.282), train_acc: 88.542(90.006)
12/17 03:06:31 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 201/521, train_loss: 0.273(0.279), train_acc: 91.667(90.003)
12/17 03:06:37 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 301/521, train_loss: 0.423(0.280), train_acc: 82.292(89.992)
12/17 03:06:43 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 401/521, train_loss: 0.384(0.279), train_acc: 88.542(90.012)
12/17 03:06:50 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 501/521, train_loss: 0.198(0.280), train_acc: 91.667(89.978)
12/17 03:06:51 AM [Supernet Training] lr: 0.01725 epoch: 187/600, step: 521/521, train_loss: 0.509(0.278), train_acc: 81.250(90.016)
12/17 03:06:51 AM [Supernet Training] epoch: 187, train_loss: 0.278, train_acc: 90.016
12/17 03:06:53 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:06:53 AM [Supernet Validation] epoch: 187, val_loss: 0.480, val_acc: 85.060, best_acc: 85.060


12/17 03:06:53 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 001/521, train_loss: 0.223(0.223), train_acc: 91.667(91.667)
12/17 03:06:59 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 101/521, train_loss: 0.256(0.267), train_acc: 91.667(90.316)
12/17 03:07:06 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 201/521, train_loss: 0.268(0.274), train_acc: 91.667(90.179)
12/17 03:07:12 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 301/521, train_loss: 0.296(0.273), train_acc: 87.500(90.179)
12/17 03:07:18 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 401/521, train_loss: 0.282(0.275), train_acc: 88.542(90.113)
12/17 03:07:25 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 501/521, train_loss: 0.319(0.277), train_acc: 90.625(90.099)
12/17 03:07:26 AM [Supernet Training] lr: 0.01721 epoch: 188/600, step: 521/521, train_loss: 0.170(0.278), train_acc: 92.500(90.108)
12/17 03:07:26 AM [Supernet Training] epoch: 188, train_loss: 0.278, train_acc: 90.108
12/17 03:07:28 AM [Supernet Validation] epoch: 188, val_loss: 0.478, val_acc: 84.730, best_acc: 85.060


12/17 03:07:28 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 001/521, train_loss: 0.269(0.269), train_acc: 89.583(89.583)
12/17 03:07:34 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 101/521, train_loss: 0.387(0.272), train_acc: 85.417(90.099)
12/17 03:07:41 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 201/521, train_loss: 0.251(0.276), train_acc: 91.667(90.076)
12/17 03:07:47 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 301/521, train_loss: 0.304(0.275), train_acc: 89.583(90.172)
12/17 03:07:53 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 401/521, train_loss: 0.282(0.278), train_acc: 87.500(90.118)
12/17 03:08:00 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 501/521, train_loss: 0.254(0.277), train_acc: 87.500(90.228)
12/17 03:08:01 AM [Supernet Training] lr: 0.01717 epoch: 189/600, step: 521/521, train_loss: 0.330(0.277), train_acc: 90.000(90.220)
12/17 03:08:01 AM [Supernet Training] epoch: 189, train_loss: 0.277, train_acc: 90.220
12/17 03:08:03 AM [Supernet Validation] epoch: 189, val_loss: 0.460, val_acc: 85.020, best_acc: 85.060


12/17 03:08:03 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 001/521, train_loss: 0.346(0.346), train_acc: 88.542(88.542)
12/17 03:08:09 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 101/521, train_loss: 0.279(0.271), train_acc: 88.542(90.274)
12/17 03:08:16 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 201/521, train_loss: 0.190(0.273), train_acc: 94.792(90.361)
12/17 03:08:22 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 301/521, train_loss: 0.261(0.279), train_acc: 89.583(90.089)
12/17 03:08:29 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 401/521, train_loss: 0.307(0.274), train_acc: 88.542(90.295)
12/17 03:08:35 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 501/521, train_loss: 0.294(0.276), train_acc: 88.542(90.201)
12/17 03:08:36 AM [Supernet Training] lr: 0.01713 epoch: 190/600, step: 521/521, train_loss: 0.205(0.277), train_acc: 92.500(90.214)
12/17 03:08:36 AM [Supernet Training] epoch: 190, train_loss: 0.277, train_acc: 90.214
12/17 03:08:38 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:08:38 AM [Supernet Validation] epoch: 190, val_loss: 0.475, val_acc: 85.250, best_acc: 85.250


12/17 03:08:38 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 001/521, train_loss: 0.321(0.321), train_acc: 88.542(88.542)
12/17 03:08:45 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 101/521, train_loss: 0.259(0.261), train_acc: 88.542(90.604)
12/17 03:08:51 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 201/521, train_loss: 0.288(0.267), train_acc: 91.667(90.319)
12/17 03:08:57 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 301/521, train_loss: 0.259(0.269), train_acc: 88.542(90.279)
12/17 03:09:04 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 401/521, train_loss: 0.211(0.270), train_acc: 93.750(90.295)
12/17 03:09:10 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 501/521, train_loss: 0.271(0.271), train_acc: 88.542(90.199)
12/17 03:09:12 AM [Supernet Training] lr: 0.01708 epoch: 191/600, step: 521/521, train_loss: 0.214(0.271), train_acc: 92.500(90.160)
12/17 03:09:12 AM [Supernet Training] epoch: 191, train_loss: 0.271, train_acc: 90.160
12/17 03:09:13 AM [Supernet Validation] epoch: 191, val_loss: 0.506, val_acc: 84.550, best_acc: 85.250


12/17 03:09:14 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 001/521, train_loss: 0.261(0.261), train_acc: 91.667(91.667)
12/17 03:09:20 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 101/521, train_loss: 0.297(0.265), train_acc: 86.458(90.347)
12/17 03:09:26 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 201/521, train_loss: 0.234(0.270), train_acc: 90.625(90.470)
12/17 03:09:33 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 301/521, train_loss: 0.142(0.268), train_acc: 97.917(90.525)
12/17 03:09:39 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 401/521, train_loss: 0.218(0.269), train_acc: 92.708(90.518)
12/17 03:09:45 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 501/521, train_loss: 0.323(0.267), train_acc: 90.625(90.563)
12/17 03:09:47 AM [Supernet Training] lr: 0.01704 epoch: 192/600, step: 521/521, train_loss: 0.234(0.267), train_acc: 91.250(90.538)
12/17 03:09:47 AM [Supernet Training] epoch: 192, train_loss: 0.267, train_acc: 90.538
12/17 03:09:48 AM [Supernet Validation] epoch: 192, val_loss: 0.475, val_acc: 84.920, best_acc: 85.250


12/17 03:09:49 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 98.958(98.958)
12/17 03:09:55 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 101/521, train_loss: 0.300(0.267), train_acc: 87.500(90.326)
12/17 03:10:01 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 201/521, train_loss: 0.367(0.269), train_acc: 89.583(90.324)
12/17 03:10:08 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 301/521, train_loss: 0.228(0.269), train_acc: 92.708(90.334)
12/17 03:10:14 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 401/521, train_loss: 0.431(0.269), train_acc: 85.417(90.350)
12/17 03:10:21 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 501/521, train_loss: 0.262(0.270), train_acc: 89.583(90.361)
12/17 03:10:22 AM [Supernet Training] lr: 0.01700 epoch: 193/600, step: 521/521, train_loss: 0.248(0.270), train_acc: 93.750(90.344)
12/17 03:10:22 AM [Supernet Training] epoch: 193, train_loss: 0.270, train_acc: 90.344
12/17 03:10:23 AM [Supernet Validation] epoch: 193, val_loss: 0.500, val_acc: 84.720, best_acc: 85.250


12/17 03:10:24 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 001/521, train_loss: 0.320(0.320), train_acc: 89.583(89.583)
12/17 03:10:30 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 101/521, train_loss: 0.334(0.262), train_acc: 90.625(90.728)
12/17 03:10:36 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 201/521, train_loss: 0.319(0.267), train_acc: 87.500(90.356)
12/17 03:10:43 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 301/521, train_loss: 0.191(0.267), train_acc: 94.792(90.435)
12/17 03:10:49 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 401/521, train_loss: 0.217(0.268), train_acc: 92.708(90.381)
12/17 03:10:55 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 501/521, train_loss: 0.218(0.270), train_acc: 92.708(90.332)
12/17 03:10:57 AM [Supernet Training] lr: 0.01696 epoch: 194/600, step: 521/521, train_loss: 0.278(0.269), train_acc: 88.750(90.340)
12/17 03:10:57 AM [Supernet Training] epoch: 194, train_loss: 0.269, train_acc: 90.340
12/17 03:10:58 AM [Supernet Validation] epoch: 194, val_loss: 0.484, val_acc: 85.100, best_acc: 85.250


12/17 03:10:59 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 001/521, train_loss: 0.205(0.205), train_acc: 91.667(91.667)
12/17 03:11:05 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 101/521, train_loss: 0.306(0.269), train_acc: 88.542(90.347)
12/17 03:11:11 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 201/521, train_loss: 0.259(0.270), train_acc: 89.583(90.459)
12/17 03:11:18 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 301/521, train_loss: 0.328(0.269), train_acc: 88.542(90.556)
12/17 03:11:24 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 401/521, train_loss: 0.197(0.266), train_acc: 92.708(90.682)
12/17 03:11:30 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 501/521, train_loss: 0.294(0.265), train_acc: 88.542(90.637)
12/17 03:11:31 AM [Supernet Training] lr: 0.01692 epoch: 195/600, step: 521/521, train_loss: 0.225(0.265), train_acc: 92.500(90.622)
12/17 03:11:31 AM [Supernet Training] epoch: 195, train_loss: 0.265, train_acc: 90.622
12/17 03:11:33 AM [Supernet Validation] epoch: 195, val_loss: 0.491, val_acc: 84.990, best_acc: 85.250


12/17 03:11:33 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 001/521, train_loss: 0.288(0.288), train_acc: 90.625(90.625)
12/17 03:11:40 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 101/521, train_loss: 0.211(0.257), train_acc: 91.667(90.738)
12/17 03:11:46 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 201/521, train_loss: 0.382(0.261), train_acc: 84.375(90.651)
12/17 03:11:52 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 301/521, train_loss: 0.300(0.261), train_acc: 89.583(90.639)
12/17 03:11:59 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 401/521, train_loss: 0.203(0.264), train_acc: 93.750(90.612)
12/17 03:12:05 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 501/521, train_loss: 0.273(0.265), train_acc: 91.667(90.565)
12/17 03:12:06 AM [Supernet Training] lr: 0.01688 epoch: 196/600, step: 521/521, train_loss: 0.329(0.266), train_acc: 88.750(90.556)
12/17 03:12:06 AM [Supernet Training] epoch: 196, train_loss: 0.266, train_acc: 90.556
12/17 03:12:08 AM [Supernet Validation] epoch: 196, val_loss: 0.487, val_acc: 84.740, best_acc: 85.250


12/17 03:12:08 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 001/521, train_loss: 0.275(0.275), train_acc: 91.667(91.667)
12/17 03:12:15 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 101/521, train_loss: 0.246(0.258), train_acc: 93.750(90.945)
12/17 03:12:21 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 201/521, train_loss: 0.310(0.259), train_acc: 87.500(90.765)
12/17 03:12:27 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 301/521, train_loss: 0.179(0.260), train_acc: 94.792(90.673)
12/17 03:12:34 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 401/521, train_loss: 0.206(0.264), train_acc: 92.708(90.612)
12/17 03:12:40 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 501/521, train_loss: 0.312(0.263), train_acc: 85.417(90.600)
12/17 03:12:41 AM [Supernet Training] lr: 0.01683 epoch: 197/600, step: 521/521, train_loss: 0.228(0.265), train_acc: 92.500(90.550)
12/17 03:12:41 AM [Supernet Training] epoch: 197, train_loss: 0.265, train_acc: 90.550
12/17 03:12:43 AM [Supernet Validation] epoch: 197, val_loss: 0.491, val_acc: 84.560, best_acc: 85.250


12/17 03:12:43 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 001/521, train_loss: 0.193(0.193), train_acc: 92.708(92.708)
12/17 03:12:50 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 101/521, train_loss: 0.258(0.258), train_acc: 90.625(90.512)
12/17 03:12:56 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 201/521, train_loss: 0.358(0.258), train_acc: 84.375(90.718)
12/17 03:13:02 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 301/521, train_loss: 0.301(0.260), train_acc: 89.583(90.691)
12/17 03:13:09 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 401/521, train_loss: 0.189(0.263), train_acc: 92.708(90.565)
12/17 03:13:15 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 501/521, train_loss: 0.186(0.264), train_acc: 92.708(90.569)
12/17 03:13:16 AM [Supernet Training] lr: 0.01679 epoch: 198/600, step: 521/521, train_loss: 0.288(0.264), train_acc: 92.500(90.580)
12/17 03:13:16 AM [Supernet Training] epoch: 198, train_loss: 0.264, train_acc: 90.580
12/17 03:13:18 AM [Supernet Validation] epoch: 198, val_loss: 0.488, val_acc: 84.800, best_acc: 85.250


12/17 03:13:18 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 001/521, train_loss: 0.326(0.326), train_acc: 89.583(89.583)
12/17 03:13:24 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 101/521, train_loss: 0.146(0.246), train_acc: 93.750(91.192)
12/17 03:13:31 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 201/521, train_loss: 0.223(0.257), train_acc: 91.667(90.796)
12/17 03:13:37 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 301/521, train_loss: 0.394(0.256), train_acc: 87.500(90.812)
12/17 03:13:44 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 401/521, train_loss: 0.326(0.258), train_acc: 85.417(90.724)
12/17 03:13:50 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 501/521, train_loss: 0.385(0.259), train_acc: 86.458(90.694)
12/17 03:13:51 AM [Supernet Training] lr: 0.01675 epoch: 199/600, step: 521/521, train_loss: 0.250(0.258), train_acc: 90.000(90.684)
12/17 03:13:51 AM [Supernet Training] epoch: 199, train_loss: 0.258, train_acc: 90.684
12/17 03:13:53 AM [Supernet Validation] epoch: 199, val_loss: 0.479, val_acc: 85.010, best_acc: 85.250


12/17 03:13:53 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 001/521, train_loss: 0.224(0.224), train_acc: 91.667(91.667)
12/17 03:14:00 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 101/521, train_loss: 0.429(0.264), train_acc: 83.333(90.687)
12/17 03:14:06 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 201/521, train_loss: 0.306(0.262), train_acc: 88.542(90.739)
12/17 03:14:12 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 301/521, train_loss: 0.297(0.261), train_acc: 87.500(90.680)
12/17 03:14:18 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 401/521, train_loss: 0.208(0.260), train_acc: 93.750(90.682)
12/17 03:14:25 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 501/521, train_loss: 0.277(0.260), train_acc: 92.708(90.642)
12/17 03:14:26 AM [Supernet Training] lr: 0.01671 epoch: 200/600, step: 521/521, train_loss: 0.208(0.259), train_acc: 90.000(90.672)
12/17 03:14:26 AM [Supernet Training] epoch: 200, train_loss: 0.259, train_acc: 90.672
12/17 03:14:27 AM [Supernet Validation] epoch: 200, val_loss: 0.488, val_acc: 85.120, best_acc: 85.250


12/17 03:14:28 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 001/521, train_loss: 0.232(0.232), train_acc: 92.708(92.708)
12/17 03:14:34 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 101/521, train_loss: 0.180(0.269), train_acc: 94.792(90.604)
12/17 03:14:41 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 201/521, train_loss: 0.283(0.268), train_acc: 88.542(90.542)
12/17 03:14:47 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 301/521, train_loss: 0.250(0.265), train_acc: 92.708(90.604)
12/17 03:14:53 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 401/521, train_loss: 0.184(0.261), train_acc: 92.708(90.669)
12/17 03:15:00 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 501/521, train_loss: 0.345(0.259), train_acc: 88.542(90.721)
12/17 03:15:01 AM [Supernet Training] lr: 0.01667 epoch: 201/600, step: 521/521, train_loss: 0.215(0.259), train_acc: 91.250(90.740)
12/17 03:15:01 AM [Supernet Training] epoch: 201, train_loss: 0.259, train_acc: 90.740
12/17 03:15:03 AM [Supernet Validation] epoch: 201, val_loss: 0.490, val_acc: 85.090, best_acc: 85.250


12/17 03:15:03 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 001/521, train_loss: 0.225(0.225), train_acc: 92.708(92.708)
12/17 03:15:09 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 101/521, train_loss: 0.247(0.251), train_acc: 92.708(91.110)
12/17 03:15:16 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 201/521, train_loss: 0.359(0.252), train_acc: 89.583(91.066)
12/17 03:15:22 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 301/521, train_loss: 0.247(0.253), train_acc: 89.583(91.009)
12/17 03:15:29 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 401/521, train_loss: 0.356(0.255), train_acc: 86.458(90.911)
12/17 03:15:35 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 501/521, train_loss: 0.241(0.257), train_acc: 91.667(90.868)
12/17 03:15:36 AM [Supernet Training] lr: 0.01663 epoch: 202/600, step: 521/521, train_loss: 0.278(0.257), train_acc: 92.500(90.832)
12/17 03:15:36 AM [Supernet Training] epoch: 202, train_loss: 0.257, train_acc: 90.832
12/17 03:15:38 AM [Supernet Validation] epoch: 202, val_loss: 0.502, val_acc: 84.790, best_acc: 85.250


12/17 03:15:38 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 001/521, train_loss: 0.130(0.130), train_acc: 95.833(95.833)
12/17 03:15:45 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 101/521, train_loss: 0.260(0.256), train_acc: 91.667(91.058)
12/17 03:15:51 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 201/521, train_loss: 0.357(0.259), train_acc: 86.458(90.801)
12/17 03:15:57 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 301/521, train_loss: 0.319(0.257), train_acc: 88.542(90.801)
12/17 03:16:04 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 401/521, train_loss: 0.356(0.258), train_acc: 87.500(90.877)
12/17 03:16:10 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 501/521, train_loss: 0.249(0.258), train_acc: 91.667(90.850)
12/17 03:16:11 AM [Supernet Training] lr: 0.01658 epoch: 203/600, step: 521/521, train_loss: 0.150(0.257), train_acc: 97.500(90.864)
12/17 03:16:11 AM [Supernet Training] epoch: 203, train_loss: 0.257, train_acc: 90.864
12/17 03:16:13 AM [Supernet Validation] epoch: 203, val_loss: 0.495, val_acc: 84.620, best_acc: 85.250


12/17 03:16:13 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 001/521, train_loss: 0.234(0.234), train_acc: 90.625(90.625)
12/17 03:16:20 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 101/521, train_loss: 0.249(0.258), train_acc: 91.667(90.687)
12/17 03:16:26 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 201/521, train_loss: 0.178(0.248), train_acc: 96.875(91.076)
12/17 03:16:32 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 301/521, train_loss: 0.162(0.250), train_acc: 94.792(91.172)
12/17 03:16:38 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 401/521, train_loss: 0.342(0.251), train_acc: 88.542(91.111)
12/17 03:16:45 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 501/521, train_loss: 0.261(0.252), train_acc: 91.667(91.003)
12/17 03:16:46 AM [Supernet Training] lr: 0.01654 epoch: 204/600, step: 521/521, train_loss: 0.166(0.251), train_acc: 96.250(91.036)
12/17 03:16:46 AM [Supernet Training] epoch: 204, train_loss: 0.251, train_acc: 91.036
12/17 03:16:48 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:16:48 AM [Supernet Validation] epoch: 204, val_loss: 0.482, val_acc: 85.290, best_acc: 85.290


12/17 03:16:48 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 001/521, train_loss: 0.232(0.232), train_acc: 90.625(90.625)
12/17 03:16:54 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 101/521, train_loss: 0.285(0.251), train_acc: 90.625(91.223)
12/17 03:17:01 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 201/521, train_loss: 0.423(0.249), train_acc: 84.375(91.060)
12/17 03:17:07 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 301/521, train_loss: 0.241(0.250), train_acc: 91.667(91.009)
12/17 03:17:13 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 401/521, train_loss: 0.417(0.249), train_acc: 84.375(91.043)
12/17 03:17:20 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 501/521, train_loss: 0.192(0.252), train_acc: 91.667(90.929)
12/17 03:17:21 AM [Supernet Training] lr: 0.01650 epoch: 205/600, step: 521/521, train_loss: 0.243(0.252), train_acc: 90.000(90.930)
12/17 03:17:21 AM [Supernet Training] epoch: 205, train_loss: 0.252, train_acc: 90.930
12/17 03:17:23 AM [Supernet Validation] epoch: 205, val_loss: 0.483, val_acc: 85.180, best_acc: 85.290


12/17 03:17:23 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 001/521, train_loss: 0.219(0.219), train_acc: 93.750(93.750)
12/17 03:17:30 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 101/521, train_loss: 0.257(0.248), train_acc: 92.708(91.141)
12/17 03:17:36 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 201/521, train_loss: 0.245(0.250), train_acc: 92.708(91.066)
12/17 03:17:42 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 301/521, train_loss: 0.292(0.249), train_acc: 89.583(91.116)
12/17 03:17:49 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 401/521, train_loss: 0.325(0.250), train_acc: 85.417(91.033)
12/17 03:17:55 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 501/521, train_loss: 0.230(0.251), train_acc: 90.625(90.985)
12/17 03:17:56 AM [Supernet Training] lr: 0.01646 epoch: 206/600, step: 521/521, train_loss: 0.248(0.252), train_acc: 91.250(90.978)
12/17 03:17:56 AM [Supernet Training] epoch: 206, train_loss: 0.252, train_acc: 90.978
12/17 03:17:58 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:17:58 AM [Supernet Validation] epoch: 206, val_loss: 0.471, val_acc: 85.600, best_acc: 85.600


12/17 03:17:58 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 95.833(95.833)
12/17 03:18:05 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 101/521, train_loss: 0.370(0.245), train_acc: 85.417(91.110)
12/17 03:18:11 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 201/521, train_loss: 0.201(0.247), train_acc: 91.667(91.060)
12/17 03:18:17 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 301/521, train_loss: 0.371(0.243), train_acc: 84.375(91.251)
12/17 03:18:23 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 401/521, train_loss: 0.212(0.244), train_acc: 92.708(91.246)
12/17 03:18:30 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 501/521, train_loss: 0.186(0.243), train_acc: 94.792(91.251)
12/17 03:18:31 AM [Supernet Training] lr: 0.01642 epoch: 207/600, step: 521/521, train_loss: 0.225(0.244), train_acc: 90.000(91.230)
12/17 03:18:31 AM [Supernet Training] epoch: 207, train_loss: 0.244, train_acc: 91.230
12/17 03:18:33 AM [Supernet Validation] epoch: 207, val_loss: 0.495, val_acc: 84.660, best_acc: 85.600


12/17 03:18:33 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 001/521, train_loss: 0.125(0.125), train_acc: 97.917(97.917)
12/17 03:18:39 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 101/521, train_loss: 0.232(0.251), train_acc: 95.833(91.007)
12/17 03:18:46 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 201/521, train_loss: 0.336(0.250), train_acc: 88.542(90.936)
12/17 03:18:52 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 301/521, train_loss: 0.268(0.249), train_acc: 91.667(90.978)
12/17 03:18:58 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 401/521, train_loss: 0.265(0.249), train_acc: 91.667(90.976)
12/17 03:19:05 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 501/521, train_loss: 0.287(0.250), train_acc: 87.500(90.958)
12/17 03:19:06 AM [Supernet Training] lr: 0.01638 epoch: 208/600, step: 521/521, train_loss: 0.253(0.250), train_acc: 93.750(90.930)
12/17 03:19:06 AM [Supernet Training] epoch: 208, train_loss: 0.250, train_acc: 90.930
12/17 03:19:08 AM [Supernet Validation] epoch: 208, val_loss: 0.481, val_acc: 85.360, best_acc: 85.600


12/17 03:19:09 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 001/521, train_loss: 0.277(0.277), train_acc: 89.583(89.583)
12/17 03:19:15 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 101/521, train_loss: 0.249(0.241), train_acc: 90.625(91.182)
12/17 03:19:21 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 201/521, train_loss: 0.202(0.249), train_acc: 91.667(91.066)
12/17 03:19:28 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 301/521, train_loss: 0.222(0.250), train_acc: 94.792(91.044)
12/17 03:19:34 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 401/521, train_loss: 0.160(0.250), train_acc: 95.833(91.093)
12/17 03:19:41 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 501/521, train_loss: 0.226(0.249), train_acc: 91.667(91.139)
12/17 03:19:42 AM [Supernet Training] lr: 0.01633 epoch: 209/600, step: 521/521, train_loss: 0.201(0.249), train_acc: 93.750(91.152)
12/17 03:19:42 AM [Supernet Training] epoch: 209, train_loss: 0.249, train_acc: 91.152
12/17 03:19:43 AM [Supernet Validation] epoch: 209, val_loss: 0.481, val_acc: 85.430, best_acc: 85.600


12/17 03:19:44 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 001/521, train_loss: 0.229(0.229), train_acc: 92.708(92.708)
12/17 03:19:50 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 101/521, train_loss: 0.326(0.237), train_acc: 90.625(91.316)
12/17 03:19:56 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 201/521, train_loss: 0.120(0.246), train_acc: 96.875(90.822)
12/17 03:20:03 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 301/521, train_loss: 0.192(0.245), train_acc: 90.625(90.961)
12/17 03:20:09 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 401/521, train_loss: 0.132(0.244), train_acc: 95.833(91.077)
12/17 03:20:15 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 501/521, train_loss: 0.244(0.246), train_acc: 91.667(91.003)
12/17 03:20:17 AM [Supernet Training] lr: 0.01629 epoch: 210/600, step: 521/521, train_loss: 0.262(0.247), train_acc: 91.250(91.014)
12/17 03:20:17 AM [Supernet Training] epoch: 210, train_loss: 0.247, train_acc: 91.014
12/17 03:20:18 AM [Supernet Validation] epoch: 210, val_loss: 0.476, val_acc: 85.270, best_acc: 85.600


12/17 03:20:19 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 001/521, train_loss: 0.179(0.179), train_acc: 93.750(93.750)
12/17 03:20:25 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 101/521, train_loss: 0.238(0.237), train_acc: 89.583(91.471)
12/17 03:20:31 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 201/521, train_loss: 0.348(0.238), train_acc: 88.542(91.444)
12/17 03:20:37 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 301/521, train_loss: 0.182(0.242), train_acc: 91.667(91.276)
12/17 03:20:44 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 401/521, train_loss: 0.230(0.242), train_acc: 92.708(91.261)
12/17 03:20:50 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 501/521, train_loss: 0.162(0.246), train_acc: 94.792(91.161)
12/17 03:20:52 AM [Supernet Training] lr: 0.01625 epoch: 211/600, step: 521/521, train_loss: 0.230(0.246), train_acc: 91.250(91.148)
12/17 03:20:52 AM [Supernet Training] epoch: 211, train_loss: 0.246, train_acc: 91.148
12/17 03:20:53 AM [Supernet Validation] epoch: 211, val_loss: 0.471, val_acc: 85.540, best_acc: 85.600


12/17 03:20:54 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 001/521, train_loss: 0.288(0.288), train_acc: 91.667(91.667)
12/17 03:21:00 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 101/521, train_loss: 0.279(0.245), train_acc: 89.583(91.233)
12/17 03:21:07 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 201/521, train_loss: 0.333(0.247), train_acc: 87.500(91.356)
12/17 03:21:13 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 301/521, train_loss: 0.200(0.247), train_acc: 92.708(91.189)
12/17 03:21:19 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 401/521, train_loss: 0.155(0.245), train_acc: 93.750(91.230)
12/17 03:21:26 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 501/521, train_loss: 0.311(0.246), train_acc: 88.542(91.164)
12/17 03:21:27 AM [Supernet Training] lr: 0.01621 epoch: 212/600, step: 521/521, train_loss: 0.303(0.247), train_acc: 91.250(91.166)
12/17 03:21:27 AM [Supernet Training] epoch: 212, train_loss: 0.247, train_acc: 91.166
12/17 03:21:28 AM [Supernet Validation] epoch: 212, val_loss: 0.478, val_acc: 85.550, best_acc: 85.600


12/17 03:21:29 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 001/521, train_loss: 0.298(0.298), train_acc: 90.625(90.625)
12/17 03:21:35 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 101/521, train_loss: 0.252(0.236), train_acc: 91.667(91.698)
12/17 03:21:41 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 201/521, train_loss: 0.280(0.239), train_acc: 92.708(91.532)
12/17 03:21:48 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 301/521, train_loss: 0.209(0.238), train_acc: 89.583(91.559)
12/17 03:21:54 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 401/521, train_loss: 0.219(0.240), train_acc: 90.625(91.456)
12/17 03:22:00 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 501/521, train_loss: 0.207(0.242), train_acc: 95.833(91.407)
12/17 03:22:02 AM [Supernet Training] lr: 0.01617 epoch: 213/600, step: 521/521, train_loss: 0.488(0.242), train_acc: 85.000(91.410)
12/17 03:22:02 AM [Supernet Training] epoch: 213, train_loss: 0.242, train_acc: 91.410
12/17 03:22:03 AM [Supernet Validation] epoch: 213, val_loss: 0.492, val_acc: 85.140, best_acc: 85.600


12/17 03:22:04 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 001/521, train_loss: 0.199(0.199), train_acc: 92.708(92.708)
12/17 03:22:10 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 101/521, train_loss: 0.281(0.236), train_acc: 89.583(91.625)
12/17 03:22:16 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 201/521, train_loss: 0.227(0.233), train_acc: 91.667(91.744)
12/17 03:22:23 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 301/521, train_loss: 0.274(0.236), train_acc: 87.500(91.532)
12/17 03:22:29 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 401/521, train_loss: 0.093(0.240), train_acc: 98.958(91.386)
12/17 03:22:35 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 501/521, train_loss: 0.183(0.240), train_acc: 94.792(91.455)
12/17 03:22:37 AM [Supernet Training] lr: 0.01613 epoch: 214/600, step: 521/521, train_loss: 0.174(0.239), train_acc: 95.000(91.482)
12/17 03:22:37 AM [Supernet Training] epoch: 214, train_loss: 0.239, train_acc: 91.482
12/17 03:22:38 AM [Supernet Validation] epoch: 214, val_loss: 0.479, val_acc: 85.250, best_acc: 85.600


12/17 03:22:38 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 001/521, train_loss: 0.131(0.131), train_acc: 92.708(92.708)
12/17 03:22:45 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 101/521, train_loss: 0.187(0.236), train_acc: 95.833(91.388)
12/17 03:22:51 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 201/521, train_loss: 0.274(0.225), train_acc: 88.542(91.874)
12/17 03:22:58 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 301/521, train_loss: 0.173(0.229), train_acc: 94.792(91.743)
12/17 03:23:04 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 401/521, train_loss: 0.370(0.231), train_acc: 86.458(91.643)
12/17 03:23:11 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 501/521, train_loss: 0.266(0.232), train_acc: 88.542(91.613)
12/17 03:23:12 AM [Supernet Training] lr: 0.01608 epoch: 215/600, step: 521/521, train_loss: 0.307(0.233), train_acc: 88.750(91.602)
12/17 03:23:12 AM [Supernet Training] epoch: 215, train_loss: 0.233, train_acc: 91.602
12/17 03:23:14 AM [Supernet Validation] epoch: 215, val_loss: 0.483, val_acc: 85.130, best_acc: 85.600


12/17 03:23:14 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 001/521, train_loss: 0.223(0.223), train_acc: 90.625(90.625)
12/17 03:23:20 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 101/521, train_loss: 0.357(0.235), train_acc: 85.417(91.687)
12/17 03:23:27 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 201/521, train_loss: 0.189(0.238), train_acc: 92.708(91.708)
12/17 03:23:33 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 301/521, train_loss: 0.246(0.237), train_acc: 91.667(91.753)
12/17 03:23:39 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 401/521, train_loss: 0.234(0.239), train_acc: 89.583(91.708)
12/17 03:23:45 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 501/521, train_loss: 0.280(0.238), train_acc: 88.542(91.756)
12/17 03:23:47 AM [Supernet Training] lr: 0.01604 epoch: 216/600, step: 521/521, train_loss: 0.255(0.238), train_acc: 91.250(91.722)
12/17 03:23:47 AM [Supernet Training] epoch: 216, train_loss: 0.238, train_acc: 91.722
12/17 03:23:48 AM [Supernet Validation] epoch: 216, val_loss: 0.479, val_acc: 85.370, best_acc: 85.600


12/17 03:23:49 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 001/521, train_loss: 0.148(0.148), train_acc: 95.833(95.833)
12/17 03:23:55 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 101/521, train_loss: 0.274(0.239), train_acc: 86.458(91.502)
12/17 03:24:01 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 201/521, train_loss: 0.207(0.233), train_acc: 89.583(91.760)
12/17 03:24:08 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 301/521, train_loss: 0.320(0.233), train_acc: 85.417(91.674)
12/17 03:24:14 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 401/521, train_loss: 0.308(0.234), train_acc: 89.583(91.638)
12/17 03:24:20 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 501/521, train_loss: 0.201(0.235), train_acc: 93.750(91.621)
12/17 03:24:22 AM [Supernet Training] lr: 0.01600 epoch: 217/600, step: 521/521, train_loss: 0.255(0.236), train_acc: 92.500(91.584)
12/17 03:24:22 AM [Supernet Training] epoch: 217, train_loss: 0.236, train_acc: 91.584
12/17 03:24:24 AM [Supernet Validation] epoch: 217, val_loss: 0.499, val_acc: 85.060, best_acc: 85.600


12/17 03:24:24 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 001/521, train_loss: 0.288(0.288), train_acc: 90.625(90.625)
12/17 03:24:30 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 101/521, train_loss: 0.157(0.234), train_acc: 92.708(91.749)
12/17 03:24:37 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 201/521, train_loss: 0.184(0.226), train_acc: 94.792(91.967)
12/17 03:24:43 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 301/521, train_loss: 0.177(0.227), train_acc: 92.708(92.002)
12/17 03:24:49 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 401/521, train_loss: 0.300(0.227), train_acc: 88.542(91.916)
12/17 03:24:56 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 501/521, train_loss: 0.163(0.228), train_acc: 90.625(91.866)
12/17 03:24:57 AM [Supernet Training] lr: 0.01596 epoch: 218/600, step: 521/521, train_loss: 0.250(0.230), train_acc: 93.750(91.820)
12/17 03:24:57 AM [Supernet Training] epoch: 218, train_loss: 0.230, train_acc: 91.820
12/17 03:24:58 AM [Supernet Validation] epoch: 218, val_loss: 0.480, val_acc: 85.450, best_acc: 85.600


12/17 03:24:59 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 001/521, train_loss: 0.382(0.382), train_acc: 85.417(85.417)
12/17 03:25:05 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 101/521, train_loss: 0.224(0.230), train_acc: 92.708(91.275)
12/17 03:25:12 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 201/521, train_loss: 0.209(0.235), train_acc: 94.792(91.413)
12/17 03:25:19 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 301/521, train_loss: 0.226(0.229), train_acc: 91.667(91.674)
12/17 03:25:25 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 401/521, train_loss: 0.198(0.230), train_acc: 92.708(91.685)
12/17 03:25:31 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 501/521, train_loss: 0.254(0.231), train_acc: 90.625(91.602)
12/17 03:25:33 AM [Supernet Training] lr: 0.01592 epoch: 219/600, step: 521/521, train_loss: 0.287(0.232), train_acc: 92.500(91.580)
12/17 03:25:33 AM [Supernet Training] epoch: 219, train_loss: 0.232, train_acc: 91.580
12/17 03:25:34 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:25:34 AM [Supernet Validation] epoch: 219, val_loss: 0.476, val_acc: 85.640, best_acc: 85.640


12/17 03:25:35 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 001/521, train_loss: 0.172(0.172), train_acc: 96.875(96.875)
12/17 03:25:41 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 101/521, train_loss: 0.238(0.227), train_acc: 89.583(91.842)
12/17 03:25:47 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 201/521, train_loss: 0.253(0.225), train_acc: 91.667(91.858)
12/17 03:25:53 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 301/521, train_loss: 0.344(0.228), train_acc: 86.458(91.736)
12/17 03:26:00 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 401/521, train_loss: 0.324(0.228), train_acc: 86.458(91.789)
12/17 03:26:06 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 501/521, train_loss: 0.256(0.229), train_acc: 89.583(91.708)
12/17 03:26:07 AM [Supernet Training] lr: 0.01588 epoch: 220/600, step: 521/521, train_loss: 0.281(0.229), train_acc: 90.000(91.724)
12/17 03:26:07 AM [Supernet Training] epoch: 220, train_loss: 0.229, train_acc: 91.724
12/17 03:26:09 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:26:09 AM [Supernet Validation] epoch: 220, val_loss: 0.476, val_acc: 85.640, best_acc: 85.640


12/17 03:26:09 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 001/521, train_loss: 0.175(0.175), train_acc: 93.750(93.750)
12/17 03:26:16 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 101/521, train_loss: 0.304(0.219), train_acc: 90.625(92.038)
12/17 03:26:22 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 201/521, train_loss: 0.361(0.221), train_acc: 86.458(92.029)
12/17 03:26:28 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 301/521, train_loss: 0.226(0.221), train_acc: 93.750(91.968)
12/17 03:26:35 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 401/521, train_loss: 0.250(0.224), train_acc: 90.625(91.898)
12/17 03:26:41 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 501/521, train_loss: 0.289(0.227), train_acc: 89.583(91.862)
12/17 03:26:42 AM [Supernet Training] lr: 0.01583 epoch: 221/600, step: 521/521, train_loss: 0.262(0.228), train_acc: 91.250(91.836)
12/17 03:26:42 AM [Supernet Training] epoch: 221, train_loss: 0.228, train_acc: 91.836
12/17 03:26:44 AM [Supernet Validation] epoch: 221, val_loss: 0.494, val_acc: 85.310, best_acc: 85.640


12/17 03:26:44 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 001/521, train_loss: 0.214(0.214), train_acc: 92.708(92.708)
12/17 03:26:50 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 101/521, train_loss: 0.254(0.225), train_acc: 91.667(91.945)
12/17 03:26:57 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 201/521, train_loss: 0.241(0.232), train_acc: 89.583(91.729)
12/17 03:27:03 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 301/521, train_loss: 0.193(0.231), train_acc: 94.792(91.760)
12/17 03:27:10 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 401/521, train_loss: 0.258(0.233), train_acc: 91.667(91.708)
12/17 03:27:16 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 501/521, train_loss: 0.205(0.233), train_acc: 93.750(91.687)
12/17 03:27:17 AM [Supernet Training] lr: 0.01579 epoch: 222/600, step: 521/521, train_loss: 0.160(0.233), train_acc: 92.500(91.674)
12/17 03:27:17 AM [Supernet Training] epoch: 222, train_loss: 0.233, train_acc: 91.674
12/17 03:27:19 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:27:19 AM [Supernet Validation] epoch: 222, val_loss: 0.468, val_acc: 85.660, best_acc: 85.660


12/17 03:27:19 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 001/521, train_loss: 0.199(0.199), train_acc: 95.833(95.833)
12/17 03:27:26 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 101/521, train_loss: 0.105(0.235), train_acc: 97.917(91.543)
12/17 03:27:32 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 201/521, train_loss: 0.222(0.231), train_acc: 93.750(91.729)
12/17 03:27:38 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 301/521, train_loss: 0.116(0.231), train_acc: 96.875(91.604)
12/17 03:27:45 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 401/521, train_loss: 0.168(0.231), train_acc: 93.750(91.623)
12/17 03:27:51 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 501/521, train_loss: 0.422(0.233), train_acc: 81.250(91.575)
12/17 03:27:52 AM [Supernet Training] lr: 0.01575 epoch: 223/600, step: 521/521, train_loss: 0.259(0.232), train_acc: 90.000(91.630)
12/17 03:27:52 AM [Supernet Training] epoch: 223, train_loss: 0.232, train_acc: 91.630
12/17 03:27:54 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:27:54 AM [Supernet Validation] epoch: 223, val_loss: 0.476, val_acc: 85.740, best_acc: 85.740


12/17 03:27:54 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 001/521, train_loss: 0.182(0.182), train_acc: 92.708(92.708)
12/17 03:28:01 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 101/521, train_loss: 0.261(0.218), train_acc: 89.583(92.265)
12/17 03:28:07 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 201/521, train_loss: 0.165(0.232), train_acc: 96.875(91.579)
12/17 03:28:14 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 301/521, train_loss: 0.286(0.234), train_acc: 91.667(91.525)
12/17 03:28:20 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 401/521, train_loss: 0.257(0.234), train_acc: 89.583(91.620)
12/17 03:28:26 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 501/521, train_loss: 0.261(0.231), train_acc: 93.750(91.673)
12/17 03:28:27 AM [Supernet Training] lr: 0.01571 epoch: 224/600, step: 521/521, train_loss: 0.190(0.231), train_acc: 91.250(91.678)
12/17 03:28:27 AM [Supernet Training] epoch: 224, train_loss: 0.231, train_acc: 91.678
12/17 03:28:29 AM [Supernet Validation] epoch: 224, val_loss: 0.474, val_acc: 85.600, best_acc: 85.740


12/17 03:28:29 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 001/521, train_loss: 0.184(0.184), train_acc: 92.708(92.708)
12/17 03:28:36 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 101/521, train_loss: 0.261(0.228), train_acc: 91.667(91.677)
12/17 03:28:42 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 201/521, train_loss: 0.222(0.228), train_acc: 92.708(91.698)
12/17 03:28:49 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 301/521, train_loss: 0.243(0.225), train_acc: 89.583(91.764)
12/17 03:28:55 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 401/521, train_loss: 0.142(0.223), train_acc: 95.833(91.849)
12/17 03:29:01 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 501/521, train_loss: 0.200(0.223), train_acc: 91.667(91.879)
12/17 03:29:02 AM [Supernet Training] lr: 0.01567 epoch: 225/600, step: 521/521, train_loss: 0.246(0.224), train_acc: 91.250(91.876)
12/17 03:29:02 AM [Supernet Training] epoch: 225, train_loss: 0.224, train_acc: 91.876
12/17 03:29:04 AM [Supernet Validation] epoch: 225, val_loss: 0.479, val_acc: 85.620, best_acc: 85.740


12/17 03:29:04 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 001/521, train_loss: 0.259(0.259), train_acc: 90.625(90.625)
12/17 03:29:11 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 101/521, train_loss: 0.180(0.216), train_acc: 94.792(92.069)
12/17 03:29:17 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 201/521, train_loss: 0.320(0.218), train_acc: 84.375(92.118)
12/17 03:29:23 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 301/521, train_loss: 0.166(0.219), train_acc: 92.708(91.992)
12/17 03:29:30 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 401/521, train_loss: 0.134(0.221), train_acc: 95.833(92.002)
12/17 03:29:36 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 501/521, train_loss: 0.091(0.221), train_acc: 97.917(92.058)
12/17 03:29:37 AM [Supernet Training] lr: 0.01562 epoch: 226/600, step: 521/521, train_loss: 0.156(0.222), train_acc: 92.500(91.988)
12/17 03:29:37 AM [Supernet Training] epoch: 226, train_loss: 0.222, train_acc: 91.988
12/17 03:29:39 AM [Supernet Validation] epoch: 226, val_loss: 0.478, val_acc: 85.510, best_acc: 85.740


12/17 03:29:39 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 001/521, train_loss: 0.326(0.326), train_acc: 86.458(86.458)
12/17 03:29:45 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 101/521, train_loss: 0.139(0.217), train_acc: 95.833(92.172)
12/17 03:29:52 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 201/521, train_loss: 0.224(0.219), train_acc: 91.667(92.004)
12/17 03:29:59 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 301/521, train_loss: 0.212(0.221), train_acc: 92.708(92.016)
12/17 03:30:05 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 401/521, train_loss: 0.241(0.223), train_acc: 90.625(91.929)
12/17 03:30:12 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 501/521, train_loss: 0.286(0.223), train_acc: 90.625(91.897)
12/17 03:30:13 AM [Supernet Training] lr: 0.01558 epoch: 227/600, step: 521/521, train_loss: 0.233(0.223), train_acc: 87.500(91.896)
12/17 03:30:13 AM [Supernet Training] epoch: 227, train_loss: 0.223, train_acc: 91.896
12/17 03:30:15 AM [Supernet Validation] epoch: 227, val_loss: 0.494, val_acc: 85.650, best_acc: 85.740


12/17 03:30:15 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 001/521, train_loss: 0.243(0.243), train_acc: 91.667(91.667)
12/17 03:30:22 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 101/521, train_loss: 0.231(0.218), train_acc: 91.667(92.038)
12/17 03:30:28 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 201/521, train_loss: 0.260(0.215), train_acc: 91.667(92.216)
12/17 03:30:34 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 301/521, train_loss: 0.234(0.219), train_acc: 93.750(92.082)
12/17 03:30:41 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 401/521, train_loss: 0.151(0.219), train_acc: 92.708(92.056)
12/17 03:30:47 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 501/521, train_loss: 0.246(0.222), train_acc: 91.667(91.976)
12/17 03:30:49 AM [Supernet Training] lr: 0.01554 epoch: 228/600, step: 521/521, train_loss: 0.131(0.221), train_acc: 97.500(92.024)
12/17 03:30:49 AM [Supernet Training] epoch: 228, train_loss: 0.221, train_acc: 92.024
12/17 03:30:50 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:30:50 AM [Supernet Validation] epoch: 228, val_loss: 0.463, val_acc: 86.010, best_acc: 86.010


12/17 03:30:51 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 001/521, train_loss: 0.306(0.306), train_acc: 89.583(89.583)
12/17 03:30:57 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 101/521, train_loss: 0.194(0.213), train_acc: 92.708(92.657)
12/17 03:31:03 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 201/521, train_loss: 0.120(0.216), train_acc: 94.792(92.377)
12/17 03:31:10 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 301/521, train_loss: 0.286(0.217), train_acc: 91.667(92.362)
12/17 03:31:16 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 401/521, train_loss: 0.196(0.220), train_acc: 94.792(92.300)
12/17 03:31:23 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 501/521, train_loss: 0.253(0.220), train_acc: 91.667(92.286)
12/17 03:31:24 AM [Supernet Training] lr: 0.01550 epoch: 229/600, step: 521/521, train_loss: 0.215(0.218), train_acc: 93.750(92.338)
12/17 03:31:24 AM [Supernet Training] epoch: 229, train_loss: 0.218, train_acc: 92.338
12/17 03:31:26 AM [Supernet Validation] epoch: 229, val_loss: 0.469, val_acc: 85.560, best_acc: 86.010


12/17 03:31:26 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 001/521, train_loss: 0.231(0.231), train_acc: 90.625(90.625)
12/17 03:31:32 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 101/521, train_loss: 0.220(0.206), train_acc: 92.708(92.585)
12/17 03:31:39 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 201/521, train_loss: 0.191(0.215), train_acc: 90.625(92.216)
12/17 03:31:45 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 301/521, train_loss: 0.276(0.214), train_acc: 89.583(92.383)
12/17 03:31:52 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 401/521, train_loss: 0.214(0.217), train_acc: 91.667(92.220)
12/17 03:31:58 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 501/521, train_loss: 0.157(0.218), train_acc: 93.750(92.197)
12/17 03:31:59 AM [Supernet Training] lr: 0.01546 epoch: 230/600, step: 521/521, train_loss: 0.231(0.219), train_acc: 88.750(92.148)
12/17 03:31:59 AM [Supernet Training] epoch: 230, train_loss: 0.219, train_acc: 92.148
12/17 03:32:01 AM [Supernet Validation] epoch: 230, val_loss: 0.471, val_acc: 85.420, best_acc: 86.010


12/17 03:32:01 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 001/521, train_loss: 0.261(0.261), train_acc: 95.833(95.833)
12/17 03:32:07 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 101/521, train_loss: 0.127(0.213), train_acc: 95.833(92.327)
12/17 03:32:14 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 201/521, train_loss: 0.215(0.220), train_acc: 93.750(91.972)
12/17 03:32:20 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 301/521, train_loss: 0.231(0.219), train_acc: 94.792(92.047)
12/17 03:32:26 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 401/521, train_loss: 0.150(0.219), train_acc: 93.750(92.129)
12/17 03:32:33 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 501/521, train_loss: 0.248(0.217), train_acc: 89.583(92.205)
12/17 03:32:34 AM [Supernet Training] lr: 0.01542 epoch: 231/600, step: 521/521, train_loss: 0.186(0.217), train_acc: 93.750(92.216)
12/17 03:32:34 AM [Supernet Training] epoch: 231, train_loss: 0.217, train_acc: 92.216
12/17 03:32:36 AM [Supernet Validation] epoch: 231, val_loss: 0.488, val_acc: 85.770, best_acc: 86.010


12/17 03:32:36 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 001/521, train_loss: 0.204(0.204), train_acc: 92.708(92.708)
12/17 03:32:43 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 101/521, train_loss: 0.172(0.218), train_acc: 91.667(92.079)
12/17 03:32:49 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 201/521, train_loss: 0.087(0.217), train_acc: 97.917(92.112)
12/17 03:32:56 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 301/521, train_loss: 0.262(0.214), train_acc: 90.625(92.196)
12/17 03:33:02 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 401/521, train_loss: 0.308(0.215), train_acc: 88.542(92.194)
12/17 03:33:09 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 501/521, train_loss: 0.168(0.217), train_acc: 94.792(92.213)
12/17 03:33:10 AM [Supernet Training] lr: 0.01537 epoch: 232/600, step: 521/521, train_loss: 0.337(0.217), train_acc: 86.250(92.236)
12/17 03:33:10 AM [Supernet Training] epoch: 232, train_loss: 0.217, train_acc: 92.236
12/17 03:33:11 AM [Supernet Validation] epoch: 232, val_loss: 0.475, val_acc: 85.770, best_acc: 86.010


12/17 03:33:12 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 001/521, train_loss: 0.169(0.169), train_acc: 94.792(94.792)
12/17 03:33:18 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 101/521, train_loss: 0.086(0.207), train_acc: 97.917(92.420)
12/17 03:33:25 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 201/521, train_loss: 0.224(0.213), train_acc: 92.708(92.304)
12/17 03:33:31 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 301/521, train_loss: 0.214(0.216), train_acc: 93.750(92.168)
12/17 03:33:38 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 401/521, train_loss: 0.239(0.215), train_acc: 94.792(92.106)
12/17 03:33:44 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 501/521, train_loss: 0.168(0.215), train_acc: 95.833(92.118)
12/17 03:33:45 AM [Supernet Training] lr: 0.01533 epoch: 233/600, step: 521/521, train_loss: 0.200(0.215), train_acc: 92.500(92.116)
12/17 03:33:45 AM [Supernet Training] epoch: 233, train_loss: 0.215, train_acc: 92.116
12/17 03:33:47 AM [Supernet Validation] epoch: 233, val_loss: 0.475, val_acc: 85.810, best_acc: 86.010


12/17 03:33:47 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 001/521, train_loss: 0.220(0.220), train_acc: 94.792(94.792)
12/17 03:33:53 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 101/521, train_loss: 0.139(0.204), train_acc: 93.750(92.781)
12/17 03:34:00 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 201/521, train_loss: 0.277(0.204), train_acc: 85.417(92.672)
12/17 03:34:06 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 301/521, train_loss: 0.281(0.208), train_acc: 88.542(92.504)
12/17 03:34:13 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 401/521, train_loss: 0.141(0.211), train_acc: 96.875(92.428)
12/17 03:34:19 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 501/521, train_loss: 0.210(0.212), train_acc: 92.708(92.382)
12/17 03:34:21 AM [Supernet Training] lr: 0.01529 epoch: 234/600, step: 521/521, train_loss: 0.336(0.213), train_acc: 91.250(92.348)
12/17 03:34:21 AM [Supernet Training] epoch: 234, train_loss: 0.213, train_acc: 92.348
12/17 03:34:22 AM [Supernet Validation] epoch: 234, val_loss: 0.480, val_acc: 85.890, best_acc: 86.010


12/17 03:34:23 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 001/521, train_loss: 0.211(0.211), train_acc: 92.708(92.708)
12/17 03:34:29 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 101/521, train_loss: 0.263(0.206), train_acc: 91.667(92.770)
12/17 03:34:35 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 201/521, train_loss: 0.219(0.215), train_acc: 94.792(92.325)
12/17 03:34:42 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 301/521, train_loss: 0.252(0.211), train_acc: 90.625(92.570)
12/17 03:34:48 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 401/521, train_loss: 0.182(0.210), train_acc: 92.708(92.602)
12/17 03:34:54 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 501/521, train_loss: 0.291(0.208), train_acc: 89.583(92.638)
12/17 03:34:55 AM [Supernet Training] lr: 0.01525 epoch: 235/600, step: 521/521, train_loss: 0.118(0.208), train_acc: 95.000(92.622)
12/17 03:34:55 AM [Supernet Training] epoch: 235, train_loss: 0.208, train_acc: 92.622
12/17 03:34:57 AM [Supernet Validation] epoch: 235, val_loss: 0.474, val_acc: 85.640, best_acc: 86.010


12/17 03:34:57 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 001/521, train_loss: 0.242(0.242), train_acc: 90.625(90.625)
12/17 03:35:04 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 101/521, train_loss: 0.134(0.209), train_acc: 93.750(92.585)
12/17 03:35:10 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 201/521, train_loss: 0.192(0.209), train_acc: 94.792(92.408)
12/17 03:35:17 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 301/521, train_loss: 0.119(0.210), train_acc: 94.792(92.407)
12/17 03:35:24 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 401/521, train_loss: 0.327(0.210), train_acc: 88.542(92.363)
12/17 03:35:31 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 501/521, train_loss: 0.243(0.210), train_acc: 90.625(92.401)
12/17 03:35:32 AM [Supernet Training] lr: 0.01521 epoch: 236/600, step: 521/521, train_loss: 0.201(0.210), train_acc: 92.500(92.392)
12/17 03:35:32 AM [Supernet Training] epoch: 236, train_loss: 0.210, train_acc: 92.392
12/17 03:35:34 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:35:34 AM [Supernet Validation] epoch: 236, val_loss: 0.485, val_acc: 86.050, best_acc: 86.050


12/17 03:35:34 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 001/521, train_loss: 0.078(0.078), train_acc: 97.917(97.917)
12/17 03:35:41 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 101/521, train_loss: 0.144(0.205), train_acc: 93.750(92.801)
12/17 03:35:47 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 201/521, train_loss: 0.271(0.205), train_acc: 90.625(92.651)
12/17 03:35:53 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 301/521, train_loss: 0.216(0.208), train_acc: 92.708(92.570)
12/17 03:36:00 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 401/521, train_loss: 0.124(0.207), train_acc: 96.875(92.589)
12/17 03:36:06 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 501/521, train_loss: 0.185(0.208), train_acc: 92.708(92.500)
12/17 03:36:08 AM [Supernet Training] lr: 0.01517 epoch: 237/600, step: 521/521, train_loss: 0.240(0.209), train_acc: 95.000(92.500)
12/17 03:36:08 AM [Supernet Training] epoch: 237, train_loss: 0.209, train_acc: 92.500
12/17 03:36:09 AM [Supernet Validation] epoch: 237, val_loss: 0.473, val_acc: 86.010, best_acc: 86.050


12/17 03:36:09 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 001/521, train_loss: 0.119(0.119), train_acc: 95.833(95.833)
12/17 03:36:16 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 101/521, train_loss: 0.174(0.203), train_acc: 94.792(92.894)
12/17 03:36:22 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 201/521, train_loss: 0.178(0.211), train_acc: 92.708(92.542)
12/17 03:36:28 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 301/521, train_loss: 0.273(0.205), train_acc: 88.542(92.771)
12/17 03:36:35 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 401/521, train_loss: 0.256(0.207), train_acc: 88.542(92.555)
12/17 03:36:41 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 501/521, train_loss: 0.143(0.208), train_acc: 96.875(92.505)
12/17 03:36:42 AM [Supernet Training] lr: 0.01512 epoch: 238/600, step: 521/521, train_loss: 0.202(0.207), train_acc: 91.250(92.512)
12/17 03:36:42 AM [Supernet Training] epoch: 238, train_loss: 0.207, train_acc: 92.512
12/17 03:36:44 AM [Supernet Validation] epoch: 238, val_loss: 0.496, val_acc: 85.880, best_acc: 86.050


12/17 03:36:44 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 001/521, train_loss: 0.179(0.179), train_acc: 92.708(92.708)
12/17 03:36:51 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 101/521, train_loss: 0.322(0.206), train_acc: 88.542(92.616)
12/17 03:36:57 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 201/521, train_loss: 0.302(0.206), train_acc: 86.458(92.537)
12/17 03:37:03 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 301/521, train_loss: 0.223(0.211), train_acc: 91.667(92.290)
12/17 03:37:10 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 401/521, train_loss: 0.283(0.209), train_acc: 88.542(92.345)
12/17 03:37:16 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 501/521, train_loss: 0.274(0.211), train_acc: 90.625(92.292)
12/17 03:37:17 AM [Supernet Training] lr: 0.01508 epoch: 239/600, step: 521/521, train_loss: 0.133(0.211), train_acc: 96.250(92.314)
12/17 03:37:17 AM [Supernet Training] epoch: 239, train_loss: 0.211, train_acc: 92.314
12/17 03:37:19 AM [Supernet Validation] epoch: 239, val_loss: 0.471, val_acc: 85.750, best_acc: 86.050


12/17 03:37:19 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 001/521, train_loss: 0.293(0.293), train_acc: 88.542(88.542)
12/17 03:37:26 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 101/521, train_loss: 0.166(0.205), train_acc: 93.750(92.616)
12/17 03:37:32 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 201/521, train_loss: 0.325(0.204), train_acc: 90.625(92.605)
12/17 03:37:38 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 301/521, train_loss: 0.266(0.203), train_acc: 88.542(92.656)
12/17 03:37:45 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 401/521, train_loss: 0.217(0.202), train_acc: 91.667(92.721)
12/17 03:37:51 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 501/521, train_loss: 0.179(0.203), train_acc: 90.625(92.648)
12/17 03:37:52 AM [Supernet Training] lr: 0.01504 epoch: 240/600, step: 521/521, train_loss: 0.320(0.203), train_acc: 91.250(92.650)
12/17 03:37:52 AM [Supernet Training] epoch: 240, train_loss: 0.203, train_acc: 92.650
12/17 03:37:54 AM [Supernet Validation] epoch: 240, val_loss: 0.481, val_acc: 85.800, best_acc: 86.050


12/17 03:37:54 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 001/521, train_loss: 0.174(0.174), train_acc: 93.750(93.750)
12/17 03:38:00 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 101/521, train_loss: 0.211(0.205), train_acc: 94.792(92.585)
12/17 03:38:07 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 201/521, train_loss: 0.244(0.200), train_acc: 92.708(92.734)
12/17 03:38:13 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 301/521, train_loss: 0.233(0.201), train_acc: 92.708(92.771)
12/17 03:38:20 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 401/521, train_loss: 0.190(0.201), train_acc: 94.792(92.799)
12/17 03:38:26 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 501/521, train_loss: 0.224(0.201), train_acc: 92.708(92.831)
12/17 03:38:27 AM [Supernet Training] lr: 0.01500 epoch: 241/600, step: 521/521, train_loss: 0.189(0.202), train_acc: 95.000(92.774)
12/17 03:38:27 AM [Supernet Training] epoch: 241, train_loss: 0.202, train_acc: 92.774
12/17 03:38:29 AM [Supernet Validation] epoch: 241, val_loss: 0.495, val_acc: 85.860, best_acc: 86.050


12/17 03:38:29 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 001/521, train_loss: 0.162(0.162), train_acc: 95.833(95.833)
12/17 03:38:35 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 101/521, train_loss: 0.206(0.199), train_acc: 93.750(92.616)
12/17 03:38:42 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 201/521, train_loss: 0.131(0.205), train_acc: 96.875(92.449)
12/17 03:38:48 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 301/521, train_loss: 0.271(0.210), train_acc: 90.625(92.380)
12/17 03:38:54 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 401/521, train_loss: 0.113(0.210), train_acc: 94.792(92.347)
12/17 03:39:01 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 501/521, train_loss: 0.170(0.208), train_acc: 91.667(92.421)
12/17 03:39:02 AM [Supernet Training] lr: 0.01496 epoch: 242/600, step: 521/521, train_loss: 0.203(0.208), train_acc: 92.500(92.406)
12/17 03:39:02 AM [Supernet Training] epoch: 242, train_loss: 0.208, train_acc: 92.406
12/17 03:39:03 AM [Supernet Validation] epoch: 242, val_loss: 0.479, val_acc: 85.730, best_acc: 86.050


12/17 03:39:04 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 95.833(95.833)
12/17 03:39:10 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 101/521, train_loss: 0.150(0.198), train_acc: 95.833(92.822)
12/17 03:39:17 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 201/521, train_loss: 0.212(0.196), train_acc: 92.708(93.056)
12/17 03:39:23 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 301/521, train_loss: 0.298(0.198), train_acc: 89.583(92.892)
12/17 03:39:29 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 401/521, train_loss: 0.123(0.197), train_acc: 96.875(92.940)
12/17 03:39:36 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 501/521, train_loss: 0.155(0.199), train_acc: 93.750(92.858)
12/17 03:39:37 AM [Supernet Training] lr: 0.01492 epoch: 243/600, step: 521/521, train_loss: 0.281(0.200), train_acc: 93.750(92.826)
12/17 03:39:37 AM [Supernet Training] epoch: 243, train_loss: 0.200, train_acc: 92.826
12/17 03:39:38 AM [Supernet Validation] epoch: 243, val_loss: 0.488, val_acc: 85.890, best_acc: 86.050


12/17 03:39:39 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 001/521, train_loss: 0.365(0.365), train_acc: 88.542(88.542)
12/17 03:39:45 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 101/521, train_loss: 0.196(0.202), train_acc: 91.667(92.698)
12/17 03:39:52 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 201/521, train_loss: 0.182(0.204), train_acc: 93.750(92.579)
12/17 03:39:58 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 301/521, train_loss: 0.131(0.203), train_acc: 96.875(92.615)
12/17 03:40:04 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 401/521, train_loss: 0.326(0.204), train_acc: 87.500(92.539)
12/17 03:40:11 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 501/521, train_loss: 0.149(0.204), train_acc: 94.792(92.550)
12/17 03:40:12 AM [Supernet Training] lr: 0.01487 epoch: 244/600, step: 521/521, train_loss: 0.297(0.204), train_acc: 87.500(92.552)
12/17 03:40:12 AM [Supernet Training] epoch: 244, train_loss: 0.204, train_acc: 92.552
12/17 03:40:14 AM [Supernet Validation] epoch: 244, val_loss: 0.498, val_acc: 85.550, best_acc: 86.050


12/17 03:40:14 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 001/521, train_loss: 0.147(0.147), train_acc: 95.833(95.833)
12/17 03:40:20 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 101/521, train_loss: 0.322(0.198), train_acc: 89.583(92.677)
12/17 03:40:27 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 201/521, train_loss: 0.199(0.201), train_acc: 92.708(92.776)
12/17 03:40:33 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 301/521, train_loss: 0.313(0.202), train_acc: 89.583(92.729)
12/17 03:40:39 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 401/521, train_loss: 0.157(0.205), train_acc: 97.917(92.659)
12/17 03:40:46 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 501/521, train_loss: 0.113(0.202), train_acc: 95.833(92.729)
12/17 03:40:47 AM [Supernet Training] lr: 0.01483 epoch: 245/600, step: 521/521, train_loss: 0.145(0.203), train_acc: 93.750(92.716)
12/17 03:40:47 AM [Supernet Training] epoch: 245, train_loss: 0.203, train_acc: 92.716
12/17 03:40:49 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:40:49 AM [Supernet Validation] epoch: 245, val_loss: 0.469, val_acc: 86.310, best_acc: 86.310


12/17 03:40:49 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 001/521, train_loss: 0.188(0.188), train_acc: 94.792(94.792)
12/17 03:40:56 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 101/521, train_loss: 0.190(0.195), train_acc: 92.708(93.121)
12/17 03:41:02 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 201/521, train_loss: 0.129(0.193), train_acc: 94.792(93.092)
12/17 03:41:08 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 301/521, train_loss: 0.192(0.193), train_acc: 92.708(93.034)
12/17 03:41:15 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 401/521, train_loss: 0.191(0.194), train_acc: 92.708(92.955)
12/17 03:41:21 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 501/521, train_loss: 0.218(0.195), train_acc: 91.667(92.906)
12/17 03:41:22 AM [Supernet Training] lr: 0.01479 epoch: 246/600, step: 521/521, train_loss: 0.163(0.195), train_acc: 95.000(92.906)
12/17 03:41:22 AM [Supernet Training] epoch: 246, train_loss: 0.195, train_acc: 92.906
12/17 03:41:24 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:41:24 AM [Supernet Validation] epoch: 246, val_loss: 0.488, val_acc: 86.390, best_acc: 86.390


12/17 03:41:24 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 001/521, train_loss: 0.136(0.136), train_acc: 94.792(94.792)
12/17 03:41:31 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 101/521, train_loss: 0.223(0.184), train_acc: 90.625(93.214)
12/17 03:41:37 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 201/521, train_loss: 0.320(0.195), train_acc: 91.667(93.035)
12/17 03:41:44 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 301/521, train_loss: 0.297(0.195), train_acc: 89.583(92.968)
12/17 03:41:50 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 401/521, train_loss: 0.357(0.196), train_acc: 88.542(92.901)
12/17 03:41:57 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 501/521, train_loss: 0.264(0.197), train_acc: 89.583(92.841)
12/17 03:41:58 AM [Supernet Training] lr: 0.01475 epoch: 247/600, step: 521/521, train_loss: 0.258(0.198), train_acc: 87.500(92.824)
12/17 03:41:58 AM [Supernet Training] epoch: 247, train_loss: 0.198, train_acc: 92.824
12/17 03:41:59 AM [Supernet Validation] epoch: 247, val_loss: 0.485, val_acc: 85.880, best_acc: 86.390


12/17 03:42:00 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 001/521, train_loss: 0.099(0.099), train_acc: 98.958(98.958)
12/17 03:42:06 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 101/521, train_loss: 0.117(0.194), train_acc: 95.833(93.348)
12/17 03:42:12 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 201/521, train_loss: 0.275(0.196), train_acc: 87.500(93.107)
12/17 03:42:19 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 301/521, train_loss: 0.253(0.198), train_acc: 88.542(92.874)
12/17 03:42:25 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 401/521, train_loss: 0.428(0.202), train_acc: 83.333(92.742)
12/17 03:42:31 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 501/521, train_loss: 0.111(0.203), train_acc: 96.875(92.719)
12/17 03:42:33 AM [Supernet Training] lr: 0.01471 epoch: 248/600, step: 521/521, train_loss: 0.179(0.202), train_acc: 95.000(92.742)
12/17 03:42:33 AM [Supernet Training] epoch: 248, train_loss: 0.202, train_acc: 92.742
12/17 03:42:34 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:42:34 AM [Supernet Validation] epoch: 248, val_loss: 0.466, val_acc: 86.540, best_acc: 86.540


12/17 03:42:35 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 001/521, train_loss: 0.078(0.078), train_acc: 98.958(98.958)
12/17 03:42:41 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 101/521, train_loss: 0.093(0.199), train_acc: 97.917(92.873)
12/17 03:42:48 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 201/521, train_loss: 0.233(0.205), train_acc: 88.542(92.631)
12/17 03:42:54 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 301/521, train_loss: 0.210(0.203), train_acc: 92.708(92.701)
12/17 03:43:00 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 401/521, train_loss: 0.144(0.202), train_acc: 96.875(92.708)
12/17 03:43:07 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 501/521, train_loss: 0.231(0.201), train_acc: 91.667(92.754)
12/17 03:43:08 AM [Supernet Training] lr: 0.01467 epoch: 249/600, step: 521/521, train_loss: 0.244(0.201), train_acc: 93.750(92.770)
12/17 03:43:08 AM [Supernet Training] epoch: 249, train_loss: 0.201, train_acc: 92.770
12/17 03:43:09 AM [Supernet Validation] epoch: 249, val_loss: 0.479, val_acc: 85.800, best_acc: 86.540


12/17 03:43:10 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 001/521, train_loss: 0.286(0.286), train_acc: 87.500(87.500)
12/17 03:43:16 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 101/521, train_loss: 0.279(0.180), train_acc: 89.583(93.482)
12/17 03:43:22 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 201/521, train_loss: 0.228(0.189), train_acc: 92.708(93.066)
12/17 03:43:29 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 301/521, train_loss: 0.131(0.190), train_acc: 94.792(93.086)
12/17 03:43:35 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 401/521, train_loss: 0.101(0.194), train_acc: 95.833(92.919)
12/17 03:43:42 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 501/521, train_loss: 0.231(0.195), train_acc: 89.583(92.833)
12/17 03:43:43 AM [Supernet Training] lr: 0.01462 epoch: 250/600, step: 521/521, train_loss: 0.316(0.195), train_acc: 88.750(92.828)
12/17 03:43:43 AM [Supernet Training] epoch: 250, train_loss: 0.195, train_acc: 92.828
12/17 03:43:44 AM [Supernet Validation] epoch: 250, val_loss: 0.481, val_acc: 86.350, best_acc: 86.540


12/17 03:43:45 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 001/521, train_loss: 0.309(0.309), train_acc: 86.458(86.458)
12/17 03:43:51 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 101/521, train_loss: 0.150(0.188), train_acc: 95.833(93.162)
12/17 03:43:57 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 201/521, train_loss: 0.129(0.197), train_acc: 94.792(92.900)
12/17 03:44:04 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 301/521, train_loss: 0.140(0.196), train_acc: 94.792(92.899)
12/17 03:44:10 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 401/521, train_loss: 0.178(0.197), train_acc: 93.750(92.872)
12/17 03:44:16 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 501/521, train_loss: 0.240(0.197), train_acc: 91.667(92.856)
12/17 03:44:17 AM [Supernet Training] lr: 0.01458 epoch: 251/600, step: 521/521, train_loss: 0.133(0.197), train_acc: 96.250(92.868)
12/17 03:44:17 AM [Supernet Training] epoch: 251, train_loss: 0.197, train_acc: 92.868
12/17 03:44:19 AM [Supernet Validation] epoch: 251, val_loss: 0.477, val_acc: 86.070, best_acc: 86.540


12/17 03:44:19 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 001/521, train_loss: 0.202(0.202), train_acc: 91.667(91.667)
12/17 03:44:26 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 101/521, train_loss: 0.144(0.191), train_acc: 95.833(93.121)
12/17 03:44:32 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 201/521, train_loss: 0.150(0.190), train_acc: 95.833(93.190)
12/17 03:44:38 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 301/521, train_loss: 0.221(0.195), train_acc: 90.625(92.947)
12/17 03:44:44 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 401/521, train_loss: 0.361(0.194), train_acc: 86.458(92.986)
12/17 03:44:51 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 501/521, train_loss: 0.211(0.194), train_acc: 91.667(92.960)
12/17 03:44:52 AM [Supernet Training] lr: 0.01454 epoch: 252/600, step: 521/521, train_loss: 0.187(0.193), train_acc: 93.750(93.030)
12/17 03:44:52 AM [Supernet Training] epoch: 252, train_loss: 0.193, train_acc: 93.030
12/17 03:44:53 AM [Supernet Validation] epoch: 252, val_loss: 0.474, val_acc: 86.000, best_acc: 86.540


12/17 03:44:54 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 001/521, train_loss: 0.164(0.164), train_acc: 92.708(92.708)
12/17 03:45:00 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 101/521, train_loss: 0.221(0.183), train_acc: 88.542(93.286)
12/17 03:45:07 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 201/521, train_loss: 0.294(0.189), train_acc: 90.625(93.237)
12/17 03:45:13 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 301/521, train_loss: 0.222(0.186), train_acc: 89.583(93.362)
12/17 03:45:19 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 401/521, train_loss: 0.174(0.186), train_acc: 93.750(93.355)
12/17 03:45:25 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 501/521, train_loss: 0.168(0.186), train_acc: 95.833(93.351)
12/17 03:45:27 AM [Supernet Training] lr: 0.01450 epoch: 253/600, step: 521/521, train_loss: 0.141(0.187), train_acc: 93.750(93.322)
12/17 03:45:27 AM [Supernet Training] epoch: 253, train_loss: 0.187, train_acc: 93.322
12/17 03:45:28 AM [Supernet Validation] epoch: 253, val_loss: 0.488, val_acc: 85.800, best_acc: 86.540


12/17 03:45:29 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 001/521, train_loss: 0.208(0.208), train_acc: 90.625(90.625)
12/17 03:45:35 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 101/521, train_loss: 0.293(0.191), train_acc: 90.625(93.018)
12/17 03:45:42 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 201/521, train_loss: 0.173(0.189), train_acc: 91.667(93.175)
12/17 03:45:48 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 301/521, train_loss: 0.235(0.188), train_acc: 94.792(93.217)
12/17 03:45:55 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 401/521, train_loss: 0.204(0.190), train_acc: 90.625(93.191)
12/17 03:46:01 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 501/521, train_loss: 0.144(0.190), train_acc: 94.792(93.182)
12/17 03:46:02 AM [Supernet Training] lr: 0.01446 epoch: 254/600, step: 521/521, train_loss: 0.150(0.190), train_acc: 95.000(93.204)
12/17 03:46:02 AM [Supernet Training] epoch: 254, train_loss: 0.190, train_acc: 93.204
12/17 03:46:04 AM [Supernet Validation] epoch: 254, val_loss: 0.479, val_acc: 86.220, best_acc: 86.540


12/17 03:46:04 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 001/521, train_loss: 0.114(0.114), train_acc: 96.875(96.875)
12/17 03:46:10 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 101/521, train_loss: 0.202(0.192), train_acc: 93.750(92.791)
12/17 03:46:17 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 201/521, train_loss: 0.092(0.189), train_acc: 95.833(93.009)
12/17 03:46:23 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 301/521, train_loss: 0.349(0.190), train_acc: 89.583(93.061)
12/17 03:46:30 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 401/521, train_loss: 0.220(0.190), train_acc: 89.583(93.077)
12/17 03:46:36 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 501/521, train_loss: 0.229(0.191), train_acc: 89.583(93.072)
12/17 03:46:38 AM [Supernet Training] lr: 0.01442 epoch: 255/600, step: 521/521, train_loss: 0.282(0.191), train_acc: 90.000(93.078)
12/17 03:46:38 AM [Supernet Training] epoch: 255, train_loss: 0.191, train_acc: 93.078
12/17 03:46:39 AM [Supernet Validation] epoch: 255, val_loss: 0.499, val_acc: 86.040, best_acc: 86.540


12/17 03:46:39 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 001/521, train_loss: 0.154(0.154), train_acc: 92.708(92.708)
12/17 03:46:46 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 101/521, train_loss: 0.211(0.183), train_acc: 91.667(93.059)
12/17 03:46:52 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 201/521, train_loss: 0.284(0.182), train_acc: 88.542(93.190)
12/17 03:46:58 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 301/521, train_loss: 0.124(0.186), train_acc: 95.833(93.155)
12/17 03:47:05 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 401/521, train_loss: 0.301(0.188), train_acc: 91.667(93.194)
12/17 03:47:11 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 501/521, train_loss: 0.176(0.188), train_acc: 91.667(93.187)
12/17 03:47:12 AM [Supernet Training] lr: 0.01437 epoch: 256/600, step: 521/521, train_loss: 0.230(0.189), train_acc: 92.500(93.176)
12/17 03:47:12 AM [Supernet Training] epoch: 256, train_loss: 0.189, train_acc: 93.176
12/17 03:47:14 AM [Supernet Validation] epoch: 256, val_loss: 0.506, val_acc: 85.410, best_acc: 86.540


12/17 03:47:14 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 001/521, train_loss: 0.143(0.143), train_acc: 93.750(93.750)
12/17 03:47:20 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 101/521, train_loss: 0.338(0.183), train_acc: 89.583(93.441)
12/17 03:47:27 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 201/521, train_loss: 0.155(0.187), train_acc: 93.750(93.237)
12/17 03:47:34 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 301/521, train_loss: 0.245(0.186), train_acc: 90.625(93.231)
12/17 03:47:40 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 401/521, train_loss: 0.207(0.188), train_acc: 93.750(93.249)
12/17 03:47:46 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 501/521, train_loss: 0.275(0.190), train_acc: 90.625(93.191)
12/17 03:47:48 AM [Supernet Training] lr: 0.01433 epoch: 257/600, step: 521/521, train_loss: 0.114(0.190), train_acc: 96.250(93.178)
12/17 03:47:48 AM [Supernet Training] epoch: 257, train_loss: 0.190, train_acc: 93.178
12/17 03:47:49 AM [Supernet Validation] epoch: 257, val_loss: 0.500, val_acc: 85.750, best_acc: 86.540


12/17 03:47:49 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 001/521, train_loss: 0.247(0.247), train_acc: 91.667(91.667)
12/17 03:47:56 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 101/521, train_loss: 0.180(0.176), train_acc: 93.750(93.564)
12/17 03:48:02 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 201/521, train_loss: 0.101(0.178), train_acc: 93.750(93.626)
12/17 03:48:08 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 301/521, train_loss: 0.206(0.181), train_acc: 91.667(93.442)
12/17 03:48:15 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 401/521, train_loss: 0.089(0.182), train_acc: 97.917(93.441)
12/17 03:48:21 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 501/521, train_loss: 0.238(0.183), train_acc: 89.583(93.305)
12/17 03:48:22 AM [Supernet Training] lr: 0.01429 epoch: 258/600, step: 521/521, train_loss: 0.086(0.183), train_acc: 98.750(93.348)
12/17 03:48:22 AM [Supernet Training] epoch: 258, train_loss: 0.183, train_acc: 93.348
12/17 03:48:24 AM [Supernet Validation] epoch: 258, val_loss: 0.480, val_acc: 86.300, best_acc: 86.540


12/17 03:48:24 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 001/521, train_loss: 0.190(0.190), train_acc: 89.583(89.583)
12/17 03:48:31 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 101/521, train_loss: 0.176(0.176), train_acc: 91.667(93.760)
12/17 03:48:37 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 201/521, train_loss: 0.150(0.181), train_acc: 94.792(93.563)
12/17 03:48:44 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 301/521, train_loss: 0.244(0.183), train_acc: 88.542(93.504)
12/17 03:48:50 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 401/521, train_loss: 0.169(0.182), train_acc: 95.833(93.490)
12/17 03:48:56 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 501/521, train_loss: 0.205(0.183), train_acc: 92.708(93.432)
12/17 03:48:58 AM [Supernet Training] lr: 0.01425 epoch: 259/600, step: 521/521, train_loss: 0.223(0.184), train_acc: 90.000(93.388)
12/17 03:48:58 AM [Supernet Training] epoch: 259, train_loss: 0.184, train_acc: 93.388
12/17 03:48:59 AM [Supernet Validation] epoch: 259, val_loss: 0.492, val_acc: 85.840, best_acc: 86.540


12/17 03:49:00 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 001/521, train_loss: 0.114(0.114), train_acc: 96.875(96.875)
12/17 03:49:06 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 101/521, train_loss: 0.235(0.180), train_acc: 90.625(93.667)
12/17 03:49:12 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 201/521, train_loss: 0.251(0.186), train_acc: 93.750(93.315)
12/17 03:49:19 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 301/521, train_loss: 0.221(0.181), train_acc: 94.792(93.535)
12/17 03:49:25 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 401/521, train_loss: 0.170(0.184), train_acc: 92.708(93.379)
12/17 03:49:31 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 501/521, train_loss: 0.192(0.186), train_acc: 92.708(93.311)
12/17 03:49:32 AM [Supernet Training] lr: 0.01421 epoch: 260/600, step: 521/521, train_loss: 0.212(0.186), train_acc: 91.250(93.290)
12/17 03:49:32 AM [Supernet Training] epoch: 260, train_loss: 0.186, train_acc: 93.290
12/17 03:49:34 AM [Supernet Validation] epoch: 260, val_loss: 0.479, val_acc: 86.150, best_acc: 86.540


12/17 03:49:34 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 001/521, train_loss: 0.176(0.176), train_acc: 92.708(92.708)
12/17 03:49:41 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 101/521, train_loss: 0.156(0.183), train_acc: 94.792(93.657)
12/17 03:49:47 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 201/521, train_loss: 0.204(0.183), train_acc: 92.708(93.470)
12/17 03:49:53 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 301/521, train_loss: 0.130(0.184), train_acc: 95.833(93.407)
12/17 03:50:00 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 401/521, train_loss: 0.192(0.183), train_acc: 94.792(93.464)
12/17 03:50:06 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 501/521, train_loss: 0.201(0.183), train_acc: 90.625(93.411)
12/17 03:50:07 AM [Supernet Training] lr: 0.01417 epoch: 261/600, step: 521/521, train_loss: 0.146(0.183), train_acc: 95.000(93.396)
12/17 03:50:07 AM [Supernet Training] epoch: 261, train_loss: 0.183, train_acc: 93.396
12/17 03:50:09 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 03:50:09 AM [Supernet Validation] epoch: 261, val_loss: 0.470, val_acc: 86.630, best_acc: 86.630


12/17 03:50:09 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 001/521, train_loss: 0.166(0.166), train_acc: 93.750(93.750)
12/17 03:50:15 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 101/521, train_loss: 0.117(0.174), train_acc: 95.833(93.606)
12/17 03:50:22 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 201/521, train_loss: 0.153(0.181), train_acc: 94.792(93.424)
12/17 03:50:28 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 301/521, train_loss: 0.464(0.182), train_acc: 84.375(93.390)
12/17 03:50:34 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 401/521, train_loss: 0.208(0.181), train_acc: 92.708(93.405)
12/17 03:50:41 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 501/521, train_loss: 0.174(0.184), train_acc: 93.750(93.251)
12/17 03:50:42 AM [Supernet Training] lr: 0.01412 epoch: 262/600, step: 521/521, train_loss: 0.173(0.184), train_acc: 93.750(93.250)
12/17 03:50:42 AM [Supernet Training] epoch: 262, train_loss: 0.184, train_acc: 93.250
12/17 03:50:44 AM [Supernet Validation] epoch: 262, val_loss: 0.483, val_acc: 86.070, best_acc: 86.630


12/17 03:50:44 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 001/521, train_loss: 0.159(0.159), train_acc: 95.833(95.833)
12/17 03:50:50 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 101/521, train_loss: 0.105(0.172), train_acc: 97.917(93.781)
12/17 03:50:57 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 201/521, train_loss: 0.242(0.178), train_acc: 91.667(93.475)
12/17 03:51:03 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 301/521, train_loss: 0.304(0.180), train_acc: 88.542(93.366)
12/17 03:51:09 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 401/521, train_loss: 0.092(0.181), train_acc: 95.833(93.379)
12/17 03:51:16 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 501/521, train_loss: 0.150(0.181), train_acc: 91.667(93.380)
12/17 03:51:17 AM [Supernet Training] lr: 0.01408 epoch: 263/600, step: 521/521, train_loss: 0.238(0.182), train_acc: 93.750(93.372)
12/17 03:51:17 AM [Supernet Training] epoch: 263, train_loss: 0.182, train_acc: 93.372
12/17 03:51:18 AM [Supernet Validation] epoch: 263, val_loss: 0.510, val_acc: 85.860, best_acc: 86.630


12/17 03:51:19 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 001/521, train_loss: 0.130(0.130), train_acc: 94.792(94.792)
12/17 03:51:25 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 101/521, train_loss: 0.212(0.180), train_acc: 93.750(93.533)
12/17 03:51:32 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 201/521, train_loss: 0.203(0.179), train_acc: 90.625(93.501)
12/17 03:51:38 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 301/521, train_loss: 0.119(0.180), train_acc: 94.792(93.428)
12/17 03:51:44 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 401/521, train_loss: 0.207(0.179), train_acc: 94.792(93.516)
12/17 03:51:51 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 501/521, train_loss: 0.206(0.182), train_acc: 93.750(93.388)
12/17 03:51:52 AM [Supernet Training] lr: 0.01404 epoch: 264/600, step: 521/521, train_loss: 0.296(0.183), train_acc: 90.000(93.362)
12/17 03:51:52 AM [Supernet Training] epoch: 264, train_loss: 0.183, train_acc: 93.362
12/17 03:51:53 AM [Supernet Validation] epoch: 264, val_loss: 0.484, val_acc: 86.210, best_acc: 86.630


12/17 03:51:54 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 001/521, train_loss: 0.186(0.186), train_acc: 93.750(93.750)
12/17 03:52:00 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 101/521, train_loss: 0.301(0.169), train_acc: 91.667(93.791)
12/17 03:52:07 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 201/521, train_loss: 0.310(0.182), train_acc: 85.417(93.392)
12/17 03:52:13 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 301/521, train_loss: 0.124(0.180), train_acc: 96.875(93.414)
12/17 03:52:20 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 401/521, train_loss: 0.225(0.179), train_acc: 90.625(93.477)
12/17 03:52:26 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 501/521, train_loss: 0.170(0.179), train_acc: 93.750(93.461)
12/17 03:52:27 AM [Supernet Training] lr: 0.01400 epoch: 265/600, step: 521/521, train_loss: 0.225(0.179), train_acc: 90.000(93.450)
12/17 03:52:27 AM [Supernet Training] epoch: 265, train_loss: 0.179, train_acc: 93.450
12/17 03:52:29 AM [Supernet Validation] epoch: 265, val_loss: 0.502, val_acc: 85.700, best_acc: 86.630


12/17 03:52:29 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 001/521, train_loss: 0.173(0.173), train_acc: 92.708(92.708)
12/17 03:52:35 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 101/521, train_loss: 0.173(0.167), train_acc: 94.792(94.049)
12/17 03:52:42 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 201/521, train_loss: 0.137(0.170), train_acc: 93.750(94.025)
12/17 03:52:48 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 301/521, train_loss: 0.129(0.172), train_acc: 94.792(93.909)
12/17 03:52:54 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 401/521, train_loss: 0.167(0.177), train_acc: 95.833(93.677)
12/17 03:53:01 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 501/521, train_loss: 0.251(0.177), train_acc: 92.708(93.604)
12/17 03:53:02 AM [Supernet Training] lr: 0.01396 epoch: 266/600, step: 521/521, train_loss: 0.182(0.178), train_acc: 93.750(93.604)
12/17 03:53:02 AM [Supernet Training] epoch: 266, train_loss: 0.178, train_acc: 93.604
12/17 03:53:04 AM [Supernet Validation] epoch: 266, val_loss: 0.482, val_acc: 86.470, best_acc: 86.630


12/17 03:53:04 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 001/521, train_loss: 0.209(0.209), train_acc: 91.667(91.667)
12/17 03:53:10 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 101/521, train_loss: 0.089(0.177), train_acc: 98.958(93.430)
12/17 03:53:17 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 201/521, train_loss: 0.151(0.180), train_acc: 94.792(93.470)
12/17 03:53:23 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 301/521, train_loss: 0.214(0.175), train_acc: 91.667(93.702)
12/17 03:53:30 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 401/521, train_loss: 0.169(0.174), train_acc: 92.708(93.753)
12/17 03:53:36 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 501/521, train_loss: 0.171(0.175), train_acc: 93.750(93.710)
12/17 03:53:37 AM [Supernet Training] lr: 0.01392 epoch: 267/600, step: 521/521, train_loss: 0.183(0.175), train_acc: 92.500(93.704)
12/17 03:53:37 AM [Supernet Training] epoch: 267, train_loss: 0.175, train_acc: 93.704
12/17 03:53:39 AM [Supernet Validation] epoch: 267, val_loss: 0.482, val_acc: 86.560, best_acc: 86.630


12/17 03:53:39 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 001/521, train_loss: 0.093(0.093), train_acc: 97.917(97.917)
12/17 03:53:46 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 101/521, train_loss: 0.191(0.173), train_acc: 93.750(93.812)
12/17 03:53:52 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 201/521, train_loss: 0.255(0.180), train_acc: 90.625(93.574)
12/17 03:53:59 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 301/521, train_loss: 0.252(0.177), train_acc: 89.583(93.636)
12/17 03:54:05 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 401/521, train_loss: 0.220(0.177), train_acc: 91.667(93.680)
12/17 03:54:11 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 501/521, train_loss: 0.193(0.174), train_acc: 93.750(93.787)
12/17 03:54:12 AM [Supernet Training] lr: 0.01387 epoch: 268/600, step: 521/521, train_loss: 0.269(0.174), train_acc: 90.000(93.770)
12/17 03:54:12 AM [Supernet Training] epoch: 268, train_loss: 0.174, train_acc: 93.770
12/17 03:54:14 AM [Supernet Validation] epoch: 268, val_loss: 0.495, val_acc: 86.360, best_acc: 86.630


12/17 03:54:14 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 001/521, train_loss: 0.265(0.265), train_acc: 92.708(92.708)
12/17 03:54:21 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 101/521, train_loss: 0.116(0.181), train_acc: 95.833(93.410)
12/17 03:54:27 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 201/521, train_loss: 0.221(0.175), train_acc: 92.708(93.563)
12/17 03:54:34 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 301/521, train_loss: 0.147(0.176), train_acc: 93.750(93.580)
12/17 03:54:41 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 401/521, train_loss: 0.182(0.177), train_acc: 90.625(93.529)
12/17 03:54:47 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 501/521, train_loss: 0.272(0.177), train_acc: 90.625(93.550)
12/17 03:54:49 AM [Supernet Training] lr: 0.01383 epoch: 269/600, step: 521/521, train_loss: 0.137(0.176), train_acc: 93.750(93.574)
12/17 03:54:49 AM [Supernet Training] epoch: 269, train_loss: 0.176, train_acc: 93.574
12/17 03:54:50 AM [Supernet Validation] epoch: 269, val_loss: 0.491, val_acc: 85.780, best_acc: 86.630


12/17 03:54:51 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 001/521, train_loss: 0.168(0.168), train_acc: 94.792(94.792)
12/17 03:54:57 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 101/521, train_loss: 0.287(0.172), train_acc: 88.542(93.585)
12/17 03:55:03 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 201/521, train_loss: 0.069(0.170), train_acc: 96.875(93.812)
12/17 03:55:10 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 301/521, train_loss: 0.171(0.171), train_acc: 93.750(93.753)
12/17 03:55:16 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 401/521, train_loss: 0.254(0.172), train_acc: 89.583(93.797)
12/17 03:55:22 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 501/521, train_loss: 0.123(0.173), train_acc: 93.750(93.773)
12/17 03:55:23 AM [Supernet Training] lr: 0.01379 epoch: 270/600, step: 521/521, train_loss: 0.233(0.173), train_acc: 91.250(93.716)
12/17 03:55:23 AM [Supernet Training] epoch: 270, train_loss: 0.173, train_acc: 93.716
12/17 03:55:25 AM [Supernet Validation] epoch: 270, val_loss: 0.491, val_acc: 86.220, best_acc: 86.630


12/17 03:55:25 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 001/521, train_loss: 0.100(0.100), train_acc: 96.875(96.875)
12/17 03:55:32 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 101/521, train_loss: 0.180(0.169), train_acc: 94.792(93.998)
12/17 03:55:38 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 201/521, train_loss: 0.180(0.168), train_acc: 93.750(93.905)
12/17 03:55:45 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 301/521, train_loss: 0.144(0.171), train_acc: 94.792(93.826)
12/17 03:55:51 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 401/521, train_loss: 0.211(0.170), train_acc: 93.750(93.862)
12/17 03:55:57 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 501/521, train_loss: 0.132(0.170), train_acc: 93.750(93.798)
12/17 03:55:58 AM [Supernet Training] lr: 0.01375 epoch: 271/600, step: 521/521, train_loss: 0.234(0.170), train_acc: 87.500(93.794)
12/17 03:55:58 AM [Supernet Training] epoch: 271, train_loss: 0.170, train_acc: 93.794
12/17 03:56:00 AM [Supernet Validation] epoch: 271, val_loss: 0.497, val_acc: 86.240, best_acc: 86.630


12/17 03:56:00 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 001/521, train_loss: 0.208(0.208), train_acc: 94.792(94.792)
12/17 03:56:07 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 101/521, train_loss: 0.135(0.169), train_acc: 95.833(93.956)
12/17 03:56:13 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 201/521, train_loss: 0.149(0.172), train_acc: 95.833(93.854)
12/17 03:56:19 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 301/521, train_loss: 0.191(0.173), train_acc: 92.708(93.788)
12/17 03:56:26 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 401/521, train_loss: 0.162(0.173), train_acc: 92.708(93.737)
12/17 03:56:32 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 501/521, train_loss: 0.165(0.174), train_acc: 93.750(93.704)
12/17 03:56:34 AM [Supernet Training] lr: 0.01371 epoch: 272/600, step: 521/521, train_loss: 0.250(0.174), train_acc: 90.000(93.704)
12/17 03:56:34 AM [Supernet Training] epoch: 272, train_loss: 0.174, train_acc: 93.704
12/17 03:56:35 AM [Supernet Validation] epoch: 272, val_loss: 0.488, val_acc: 86.160, best_acc: 86.630


12/17 03:56:36 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 94.792(94.792)
12/17 03:56:42 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 101/521, train_loss: 0.146(0.162), train_acc: 95.833(94.204)
12/17 03:56:48 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 201/521, train_loss: 0.157(0.165), train_acc: 93.750(94.108)
12/17 03:56:54 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 301/521, train_loss: 0.173(0.166), train_acc: 93.750(93.999)
12/17 03:57:01 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 401/521, train_loss: 0.136(0.169), train_acc: 93.750(93.903)
12/17 03:57:07 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 501/521, train_loss: 0.116(0.168), train_acc: 96.875(93.937)
12/17 03:57:08 AM [Supernet Training] lr: 0.01367 epoch: 273/600, step: 521/521, train_loss: 0.171(0.168), train_acc: 96.250(93.964)
12/17 03:57:09 AM [Supernet Training] epoch: 273, train_loss: 0.168, train_acc: 93.964
12/17 03:57:10 AM [Supernet Validation] epoch: 273, val_loss: 0.505, val_acc: 85.980, best_acc: 86.630


12/17 03:57:10 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 001/521, train_loss: 0.148(0.148), train_acc: 93.750(93.750)
12/17 03:57:17 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 101/521, train_loss: 0.245(0.172), train_acc: 89.583(93.874)
12/17 03:57:23 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 201/521, train_loss: 0.205(0.166), train_acc: 92.708(94.071)
12/17 03:57:30 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 301/521, train_loss: 0.173(0.169), train_acc: 92.708(93.968)
12/17 03:57:36 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 401/521, train_loss: 0.185(0.169), train_acc: 91.667(93.973)
12/17 03:57:42 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 501/521, train_loss: 0.121(0.169), train_acc: 95.833(93.962)
12/17 03:57:44 AM [Supernet Training] lr: 0.01362 epoch: 274/600, step: 521/521, train_loss: 0.149(0.169), train_acc: 95.000(93.954)
12/17 03:57:44 AM [Supernet Training] epoch: 274, train_loss: 0.169, train_acc: 93.954
12/17 03:57:45 AM [Supernet Validation] epoch: 274, val_loss: 0.504, val_acc: 86.230, best_acc: 86.630


12/17 03:57:46 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 001/521, train_loss: 0.202(0.202), train_acc: 93.750(93.750)
12/17 03:57:52 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 101/521, train_loss: 0.270(0.167), train_acc: 90.625(93.987)
12/17 03:57:58 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 201/521, train_loss: 0.118(0.166), train_acc: 95.833(94.009)
12/17 03:58:05 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 301/521, train_loss: 0.152(0.170), train_acc: 94.792(93.816)
12/17 03:58:12 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 401/521, train_loss: 0.123(0.168), train_acc: 94.792(93.937)
12/17 03:58:18 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 501/521, train_loss: 0.127(0.170), train_acc: 95.833(93.852)
12/17 03:58:19 AM [Supernet Training] lr: 0.01358 epoch: 275/600, step: 521/521, train_loss: 0.145(0.171), train_acc: 95.000(93.812)
12/17 03:58:19 AM [Supernet Training] epoch: 275, train_loss: 0.171, train_acc: 93.812
12/17 03:58:21 AM [Supernet Validation] epoch: 275, val_loss: 0.495, val_acc: 85.880, best_acc: 86.630


12/17 03:58:21 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 001/521, train_loss: 0.125(0.125), train_acc: 95.833(95.833)
12/17 03:58:28 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 101/521, train_loss: 0.204(0.153), train_acc: 92.708(94.410)
12/17 03:58:34 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 201/521, train_loss: 0.135(0.160), train_acc: 93.750(94.128)
12/17 03:58:40 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 301/521, train_loss: 0.212(0.162), train_acc: 91.667(94.072)
12/17 03:58:47 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 401/521, train_loss: 0.189(0.165), train_acc: 92.708(93.997)
12/17 03:58:53 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 501/521, train_loss: 0.178(0.166), train_acc: 92.708(93.956)
12/17 03:58:54 AM [Supernet Training] lr: 0.01354 epoch: 276/600, step: 521/521, train_loss: 0.130(0.166), train_acc: 93.750(93.950)
12/17 03:58:54 AM [Supernet Training] epoch: 276, train_loss: 0.166, train_acc: 93.950
12/17 03:58:56 AM [Supernet Validation] epoch: 276, val_loss: 0.484, val_acc: 86.410, best_acc: 86.630


12/17 03:58:56 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 001/521, train_loss: 0.186(0.186), train_acc: 92.708(92.708)
12/17 03:59:02 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 101/521, train_loss: 0.136(0.167), train_acc: 92.708(93.833)
12/17 03:59:09 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 201/521, train_loss: 0.178(0.169), train_acc: 93.750(93.812)
12/17 03:59:15 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 301/521, train_loss: 0.159(0.168), train_acc: 92.708(93.926)
12/17 03:59:22 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 401/521, train_loss: 0.118(0.169), train_acc: 95.833(93.872)
12/17 03:59:28 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 501/521, train_loss: 0.142(0.170), train_acc: 93.750(93.850)
12/17 03:59:30 AM [Supernet Training] lr: 0.01350 epoch: 277/600, step: 521/521, train_loss: 0.242(0.170), train_acc: 90.000(93.842)
12/17 03:59:30 AM [Supernet Training] epoch: 277, train_loss: 0.170, train_acc: 93.842
12/17 03:59:31 AM [Supernet Validation] epoch: 277, val_loss: 0.500, val_acc: 85.960, best_acc: 86.630


12/17 03:59:32 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 001/521, train_loss: 0.258(0.258), train_acc: 89.583(89.583)
12/17 03:59:38 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 101/521, train_loss: 0.222(0.159), train_acc: 90.625(94.142)
12/17 03:59:44 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 201/521, train_loss: 0.156(0.164), train_acc: 94.792(93.983)
12/17 03:59:51 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 301/521, train_loss: 0.205(0.164), train_acc: 94.792(94.023)
12/17 03:59:57 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 401/521, train_loss: 0.172(0.165), train_acc: 95.833(94.010)
12/17 04:00:03 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 501/521, train_loss: 0.230(0.166), train_acc: 93.750(93.983)
12/17 04:00:05 AM [Supernet Training] lr: 0.01346 epoch: 278/600, step: 521/521, train_loss: 0.205(0.166), train_acc: 93.750(94.004)
12/17 04:00:05 AM [Supernet Training] epoch: 278, train_loss: 0.166, train_acc: 94.004
12/17 04:00:06 AM [Supernet Validation] epoch: 278, val_loss: 0.487, val_acc: 86.380, best_acc: 86.630


12/17 04:00:06 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 001/521, train_loss: 0.434(0.434), train_acc: 86.458(86.458)
12/17 04:00:13 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 101/521, train_loss: 0.169(0.157), train_acc: 93.750(94.431)
12/17 04:00:19 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 201/521, train_loss: 0.178(0.157), train_acc: 94.792(94.258)
12/17 04:00:26 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 301/521, train_loss: 0.173(0.159), train_acc: 90.625(94.245)
12/17 04:00:32 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 401/521, train_loss: 0.121(0.161), train_acc: 95.833(94.254)
12/17 04:00:38 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 501/521, train_loss: 0.246(0.161), train_acc: 91.667(94.230)
12/17 04:00:40 AM [Supernet Training] lr: 0.01342 epoch: 279/600, step: 521/521, train_loss: 0.206(0.161), train_acc: 91.250(94.222)
12/17 04:00:40 AM [Supernet Training] epoch: 279, train_loss: 0.161, train_acc: 94.222
12/17 04:00:41 AM [Supernet Validation] epoch: 279, val_loss: 0.496, val_acc: 86.280, best_acc: 86.630


12/17 04:00:42 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 93.750(93.750)
12/17 04:00:48 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 101/521, train_loss: 0.078(0.162), train_acc: 97.917(94.173)
12/17 04:00:55 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 201/521, train_loss: 0.070(0.157), train_acc: 97.917(94.341)
12/17 04:01:01 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 301/521, train_loss: 0.151(0.161), train_acc: 93.750(94.190)
12/17 04:01:07 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 401/521, train_loss: 0.220(0.164), train_acc: 91.667(94.036)
12/17 04:01:14 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 501/521, train_loss: 0.142(0.165), train_acc: 94.792(93.983)
12/17 04:01:15 AM [Supernet Training] lr: 0.01337 epoch: 280/600, step: 521/521, train_loss: 0.210(0.164), train_acc: 92.500(94.002)
12/17 04:01:15 AM [Supernet Training] epoch: 280, train_loss: 0.164, train_acc: 94.002
12/17 04:01:17 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:01:17 AM [Supernet Validation] epoch: 280, val_loss: 0.474, val_acc: 86.770, best_acc: 86.770


12/17 04:01:17 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 001/521, train_loss: 0.193(0.193), train_acc: 92.708(92.708)
12/17 04:01:24 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 101/521, train_loss: 0.085(0.159), train_acc: 96.875(94.193)
12/17 04:01:30 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 201/521, train_loss: 0.087(0.160), train_acc: 97.917(94.211)
12/17 04:01:36 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 301/521, train_loss: 0.085(0.159), train_acc: 97.917(94.245)
12/17 04:01:43 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 401/521, train_loss: 0.179(0.161), train_acc: 93.750(94.119)
12/17 04:01:49 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 501/521, train_loss: 0.269(0.164), train_acc: 92.708(94.014)
12/17 04:01:50 AM [Supernet Training] lr: 0.01333 epoch: 281/600, step: 521/521, train_loss: 0.175(0.164), train_acc: 92.500(94.004)
12/17 04:01:50 AM [Supernet Training] epoch: 281, train_loss: 0.164, train_acc: 94.004
12/17 04:01:52 AM [Supernet Validation] epoch: 281, val_loss: 0.486, val_acc: 86.630, best_acc: 86.770


12/17 04:01:52 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 001/521, train_loss: 0.160(0.160), train_acc: 92.708(92.708)
12/17 04:01:58 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 101/521, train_loss: 0.195(0.157), train_acc: 92.708(94.493)
12/17 04:02:05 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 201/521, train_loss: 0.128(0.156), train_acc: 95.833(94.647)
12/17 04:02:11 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 301/521, train_loss: 0.142(0.158), train_acc: 94.792(94.536)
12/17 04:02:18 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 401/521, train_loss: 0.173(0.159), train_acc: 92.708(94.399)
12/17 04:02:24 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 501/521, train_loss: 0.212(0.160), train_acc: 92.708(94.343)
12/17 04:02:25 AM [Supernet Training] lr: 0.01329 epoch: 282/600, step: 521/521, train_loss: 0.167(0.160), train_acc: 92.500(94.350)
12/17 04:02:25 AM [Supernet Training] epoch: 282, train_loss: 0.160, train_acc: 94.350
12/17 04:02:27 AM [Supernet Validation] epoch: 282, val_loss: 0.498, val_acc: 86.280, best_acc: 86.770


12/17 04:02:27 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 001/521, train_loss: 0.184(0.184), train_acc: 93.750(93.750)
12/17 04:02:34 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 101/521, train_loss: 0.178(0.154), train_acc: 91.667(94.379)
12/17 04:02:40 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 201/521, train_loss: 0.195(0.153), train_acc: 92.708(94.377)
12/17 04:02:47 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 301/521, train_loss: 0.070(0.154), train_acc: 98.958(94.356)
12/17 04:02:53 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 401/521, train_loss: 0.145(0.159), train_acc: 96.875(94.205)
12/17 04:02:59 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 501/521, train_loss: 0.145(0.159), train_acc: 94.792(94.137)
12/17 04:03:00 AM [Supernet Training] lr: 0.01325 epoch: 283/600, step: 521/521, train_loss: 0.151(0.158), train_acc: 95.000(94.152)
12/17 04:03:01 AM [Supernet Training] epoch: 283, train_loss: 0.158, train_acc: 94.152
12/17 04:03:02 AM [Supernet Validation] epoch: 283, val_loss: 0.486, val_acc: 86.450, best_acc: 86.770


12/17 04:03:02 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 001/521, train_loss: 0.190(0.190), train_acc: 93.750(93.750)
12/17 04:03:09 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 101/521, train_loss: 0.126(0.168), train_acc: 95.833(94.080)
12/17 04:03:15 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 201/521, train_loss: 0.183(0.164), train_acc: 93.750(94.108)
12/17 04:03:22 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 301/521, train_loss: 0.126(0.158), train_acc: 94.792(94.234)
12/17 04:03:28 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 401/521, train_loss: 0.158(0.159), train_acc: 92.708(94.163)
12/17 04:03:34 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 501/521, train_loss: 0.154(0.161), train_acc: 95.833(94.126)
12/17 04:03:36 AM [Supernet Training] lr: 0.01321 epoch: 284/600, step: 521/521, train_loss: 0.199(0.162), train_acc: 92.500(94.108)
12/17 04:03:36 AM [Supernet Training] epoch: 284, train_loss: 0.162, train_acc: 94.108
12/17 04:03:37 AM [Supernet Validation] epoch: 284, val_loss: 0.490, val_acc: 86.430, best_acc: 86.770


12/17 04:03:38 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 001/521, train_loss: 0.108(0.108), train_acc: 96.875(96.875)
12/17 04:03:44 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 101/521, train_loss: 0.081(0.154), train_acc: 96.875(94.348)
12/17 04:03:51 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 201/521, train_loss: 0.172(0.160), train_acc: 91.667(94.154)
12/17 04:03:57 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 301/521, train_loss: 0.153(0.162), train_acc: 92.708(94.055)
12/17 04:04:03 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 401/521, train_loss: 0.208(0.163), train_acc: 91.667(93.973)
12/17 04:04:10 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 501/521, train_loss: 0.116(0.161), train_acc: 94.792(94.054)
12/17 04:04:11 AM [Supernet Training] lr: 0.01317 epoch: 285/600, step: 521/521, train_loss: 0.331(0.161), train_acc: 90.000(94.054)
12/17 04:04:11 AM [Supernet Training] epoch: 285, train_loss: 0.161, train_acc: 94.054
12/17 04:04:13 AM [Supernet Validation] epoch: 285, val_loss: 0.501, val_acc: 85.980, best_acc: 86.770


12/17 04:04:13 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 001/521, train_loss: 0.200(0.200), train_acc: 90.625(90.625)
12/17 04:04:19 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 101/521, train_loss: 0.194(0.153), train_acc: 92.708(94.173)
12/17 04:04:26 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 201/521, train_loss: 0.249(0.155), train_acc: 92.708(94.268)
12/17 04:04:32 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 301/521, train_loss: 0.164(0.157), train_acc: 94.792(94.245)
12/17 04:04:38 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 401/521, train_loss: 0.222(0.158), train_acc: 91.667(94.259)
12/17 04:04:45 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 501/521, train_loss: 0.078(0.157), train_acc: 97.917(94.309)
12/17 04:04:46 AM [Supernet Training] lr: 0.01313 epoch: 286/600, step: 521/521, train_loss: 0.170(0.156), train_acc: 93.750(94.284)
12/17 04:04:46 AM [Supernet Training] epoch: 286, train_loss: 0.156, train_acc: 94.284
12/17 04:04:47 AM [Supernet Validation] epoch: 286, val_loss: 0.481, val_acc: 86.570, best_acc: 86.770


12/17 04:04:48 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 001/521, train_loss: 0.184(0.184), train_acc: 89.583(89.583)
12/17 04:04:54 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 101/521, train_loss: 0.167(0.160), train_acc: 92.708(94.132)
12/17 04:05:00 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 201/521, train_loss: 0.113(0.160), train_acc: 95.833(94.191)
12/17 04:05:07 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 301/521, train_loss: 0.233(0.162), train_acc: 92.708(94.075)
12/17 04:05:13 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 401/521, train_loss: 0.100(0.160), train_acc: 97.917(94.168)
12/17 04:05:20 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 501/521, train_loss: 0.119(0.159), train_acc: 97.917(94.232)
12/17 04:05:21 AM [Supernet Training] lr: 0.01308 epoch: 287/600, step: 521/521, train_loss: 0.063(0.159), train_acc: 98.750(94.246)
12/17 04:05:21 AM [Supernet Training] epoch: 287, train_loss: 0.159, train_acc: 94.246
12/17 04:05:23 AM [Supernet Validation] epoch: 287, val_loss: 0.502, val_acc: 86.330, best_acc: 86.770


12/17 04:05:23 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 001/521, train_loss: 0.186(0.186), train_acc: 92.708(92.708)
12/17 04:05:30 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 101/521, train_loss: 0.129(0.164), train_acc: 95.833(94.173)
12/17 04:05:36 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 201/521, train_loss: 0.135(0.161), train_acc: 95.833(94.175)
12/17 04:05:42 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 301/521, train_loss: 0.301(0.158), train_acc: 89.583(94.318)
12/17 04:05:49 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 401/521, train_loss: 0.235(0.157), train_acc: 90.625(94.363)
12/17 04:05:55 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 501/521, train_loss: 0.195(0.157), train_acc: 93.750(94.322)
12/17 04:05:56 AM [Supernet Training] lr: 0.01304 epoch: 288/600, step: 521/521, train_loss: 0.131(0.157), train_acc: 93.750(94.310)
12/17 04:05:56 AM [Supernet Training] epoch: 288, train_loss: 0.157, train_acc: 94.310
12/17 04:05:58 AM [Supernet Validation] epoch: 288, val_loss: 0.481, val_acc: 86.660, best_acc: 86.770


12/17 04:05:58 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 001/521, train_loss: 0.212(0.212), train_acc: 90.625(90.625)
12/17 04:06:05 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 101/521, train_loss: 0.283(0.151), train_acc: 90.625(94.493)
12/17 04:06:11 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 201/521, train_loss: 0.166(0.150), train_acc: 94.792(94.517)
12/17 04:06:18 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 301/521, train_loss: 0.244(0.154), train_acc: 89.583(94.446)
12/17 04:06:24 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 401/521, train_loss: 0.142(0.157), train_acc: 92.708(94.308)
12/17 04:06:31 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 501/521, train_loss: 0.188(0.157), train_acc: 92.708(94.322)
12/17 04:06:32 AM [Supernet Training] lr: 0.01300 epoch: 289/600, step: 521/521, train_loss: 0.128(0.157), train_acc: 96.250(94.318)
12/17 04:06:32 AM [Supernet Training] epoch: 289, train_loss: 0.157, train_acc: 94.318
12/17 04:06:33 AM [Supernet Validation] epoch: 289, val_loss: 0.493, val_acc: 86.550, best_acc: 86.770


12/17 04:06:34 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 001/521, train_loss: 0.129(0.129), train_acc: 95.833(95.833)
12/17 04:06:40 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 101/521, train_loss: 0.140(0.157), train_acc: 93.750(94.472)
12/17 04:06:46 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 201/521, train_loss: 0.223(0.153), train_acc: 93.750(94.512)
12/17 04:06:53 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 301/521, train_loss: 0.129(0.153), train_acc: 94.792(94.477)
12/17 04:06:59 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 401/521, train_loss: 0.112(0.155), train_acc: 94.792(94.386)
12/17 04:07:06 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 501/521, train_loss: 0.210(0.154), train_acc: 91.667(94.465)
12/17 04:07:07 AM [Supernet Training] lr: 0.01296 epoch: 290/600, step: 521/521, train_loss: 0.167(0.154), train_acc: 92.500(94.478)
12/17 04:07:07 AM [Supernet Training] epoch: 290, train_loss: 0.154, train_acc: 94.478
12/17 04:07:08 AM [Supernet Validation] epoch: 290, val_loss: 0.506, val_acc: 86.380, best_acc: 86.770


12/17 04:07:09 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 001/521, train_loss: 0.150(0.150), train_acc: 94.792(94.792)
12/17 04:07:15 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 101/521, train_loss: 0.105(0.149), train_acc: 94.792(94.596)
12/17 04:07:22 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 201/521, train_loss: 0.088(0.148), train_acc: 95.833(94.595)
12/17 04:07:28 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 301/521, train_loss: 0.185(0.150), train_acc: 93.750(94.504)
12/17 04:07:34 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 401/521, train_loss: 0.154(0.151), train_acc: 93.750(94.493)
12/17 04:07:40 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 501/521, train_loss: 0.194(0.152), train_acc: 91.667(94.424)
12/17 04:07:42 AM [Supernet Training] lr: 0.01292 epoch: 291/600, step: 521/521, train_loss: 0.206(0.153), train_acc: 91.250(94.416)
12/17 04:07:42 AM [Supernet Training] epoch: 291, train_loss: 0.153, train_acc: 94.416
12/17 04:07:44 AM [Supernet Validation] epoch: 291, val_loss: 0.483, val_acc: 86.770, best_acc: 86.770


12/17 04:07:44 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 001/521, train_loss: 0.196(0.196), train_acc: 93.750(93.750)
12/17 04:07:50 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 101/521, train_loss: 0.085(0.150), train_acc: 94.792(94.431)
12/17 04:07:57 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 201/521, train_loss: 0.277(0.154), train_acc: 90.625(94.382)
12/17 04:08:03 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 301/521, train_loss: 0.225(0.153), train_acc: 92.708(94.442)
12/17 04:08:09 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 401/521, train_loss: 0.148(0.150), train_acc: 92.708(94.524)
12/17 04:08:16 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 501/521, train_loss: 0.161(0.150), train_acc: 94.792(94.507)
12/17 04:08:17 AM [Supernet Training] lr: 0.01288 epoch: 292/600, step: 521/521, train_loss: 0.116(0.151), train_acc: 96.250(94.462)
12/17 04:08:17 AM [Supernet Training] epoch: 292, train_loss: 0.151, train_acc: 94.462
12/17 04:08:19 AM [Supernet Validation] epoch: 292, val_loss: 0.495, val_acc: 86.300, best_acc: 86.770


12/17 04:08:19 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 001/521, train_loss: 0.108(0.108), train_acc: 96.875(96.875)
12/17 04:08:25 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 101/521, train_loss: 0.152(0.142), train_acc: 93.750(94.658)
12/17 04:08:32 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 201/521, train_loss: 0.167(0.143), train_acc: 94.792(94.657)
12/17 04:08:38 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 301/521, train_loss: 0.092(0.147), train_acc: 97.917(94.539)
12/17 04:08:44 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 401/521, train_loss: 0.137(0.149), train_acc: 95.833(94.527)
12/17 04:08:51 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 501/521, train_loss: 0.166(0.151), train_acc: 94.792(94.380)
12/17 04:08:52 AM [Supernet Training] lr: 0.01283 epoch: 293/600, step: 521/521, train_loss: 0.051(0.152), train_acc: 100.000(94.350)
12/17 04:08:52 AM [Supernet Training] epoch: 293, train_loss: 0.152, train_acc: 94.350
12/17 04:08:53 AM [Supernet Validation] epoch: 293, val_loss: 0.501, val_acc: 86.710, best_acc: 86.770


12/17 04:08:54 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 001/521, train_loss: 0.211(0.211), train_acc: 91.667(91.667)
12/17 04:09:00 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 101/521, train_loss: 0.187(0.151), train_acc: 92.708(94.348)
12/17 04:09:06 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 201/521, train_loss: 0.122(0.153), train_acc: 95.833(94.351)
12/17 04:09:13 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 301/521, train_loss: 0.186(0.154), train_acc: 93.750(94.352)
12/17 04:09:19 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 401/521, train_loss: 0.151(0.152), train_acc: 93.750(94.477)
12/17 04:09:25 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 501/521, train_loss: 0.193(0.154), train_acc: 94.792(94.395)
12/17 04:09:27 AM [Supernet Training] lr: 0.01279 epoch: 294/600, step: 521/521, train_loss: 0.058(0.154), train_acc: 100.000(94.412)
12/17 04:09:27 AM [Supernet Training] epoch: 294, train_loss: 0.154, train_acc: 94.412
12/17 04:09:28 AM [Supernet Validation] epoch: 294, val_loss: 0.494, val_acc: 86.500, best_acc: 86.770


12/17 04:09:29 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 94.792(94.792)
12/17 04:09:35 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 101/521, train_loss: 0.258(0.153), train_acc: 90.625(94.554)
12/17 04:09:41 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 201/521, train_loss: 0.070(0.152), train_acc: 97.917(94.590)
12/17 04:09:47 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 301/521, train_loss: 0.095(0.149), train_acc: 94.792(94.615)
12/17 04:09:54 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 401/521, train_loss: 0.181(0.151), train_acc: 93.750(94.602)
12/17 04:10:00 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 501/521, train_loss: 0.166(0.152), train_acc: 91.667(94.559)
12/17 04:10:01 AM [Supernet Training] lr: 0.01275 epoch: 295/600, step: 521/521, train_loss: 0.235(0.151), train_acc: 92.500(94.574)
12/17 04:10:01 AM [Supernet Training] epoch: 295, train_loss: 0.151, train_acc: 94.574
12/17 04:10:03 AM [Supernet Validation] epoch: 295, val_loss: 0.478, val_acc: 86.770, best_acc: 86.770


12/17 04:10:03 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 001/521, train_loss: 0.104(0.104), train_acc: 95.833(95.833)
12/17 04:10:10 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 101/521, train_loss: 0.107(0.148), train_acc: 96.875(94.750)
12/17 04:10:16 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 201/521, train_loss: 0.087(0.148), train_acc: 97.917(94.709)
12/17 04:10:22 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 301/521, train_loss: 0.148(0.150), train_acc: 95.833(94.605)
12/17 04:10:29 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 401/521, train_loss: 0.161(0.153), train_acc: 91.667(94.501)
12/17 04:10:35 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 501/521, train_loss: 0.117(0.152), train_acc: 95.833(94.542)
12/17 04:10:36 AM [Supernet Training] lr: 0.01271 epoch: 296/600, step: 521/521, train_loss: 0.166(0.153), train_acc: 96.250(94.516)
12/17 04:10:36 AM [Supernet Training] epoch: 296, train_loss: 0.153, train_acc: 94.516
12/17 04:10:38 AM [Supernet Validation] epoch: 296, val_loss: 0.477, val_acc: 86.650, best_acc: 86.770


12/17 04:10:38 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 001/521, train_loss: 0.136(0.136), train_acc: 94.792(94.792)
12/17 04:10:45 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 101/521, train_loss: 0.144(0.144), train_acc: 94.792(94.730)
12/17 04:10:51 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 201/521, train_loss: 0.206(0.142), train_acc: 90.625(94.838)
12/17 04:10:57 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 301/521, train_loss: 0.109(0.142), train_acc: 95.833(94.854)
12/17 04:11:04 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 401/521, train_loss: 0.188(0.145), train_acc: 91.667(94.711)
12/17 04:11:10 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 501/521, train_loss: 0.284(0.147), train_acc: 91.667(94.638)
12/17 04:11:12 AM [Supernet Training] lr: 0.01267 epoch: 297/600, step: 521/521, train_loss: 0.188(0.147), train_acc: 91.250(94.626)
12/17 04:11:12 AM [Supernet Training] epoch: 297, train_loss: 0.147, train_acc: 94.626
12/17 04:11:13 AM [Supernet Validation] epoch: 297, val_loss: 0.493, val_acc: 86.420, best_acc: 86.770


12/17 04:11:13 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 001/521, train_loss: 0.152(0.152), train_acc: 94.792(94.792)
12/17 04:11:20 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 101/521, train_loss: 0.153(0.143), train_acc: 95.833(94.761)
12/17 04:11:26 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 201/521, train_loss: 0.071(0.140), train_acc: 96.875(94.916)
12/17 04:11:33 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 301/521, train_loss: 0.224(0.142), train_acc: 92.708(94.785)
12/17 04:11:39 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 401/521, train_loss: 0.049(0.144), train_acc: 97.917(94.745)
12/17 04:11:46 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 501/521, train_loss: 0.113(0.145), train_acc: 96.875(94.681)
12/17 04:11:47 AM [Supernet Training] lr: 0.01263 epoch: 298/600, step: 521/521, train_loss: 0.181(0.145), train_acc: 95.000(94.674)
12/17 04:11:47 AM [Supernet Training] epoch: 298, train_loss: 0.145, train_acc: 94.674
12/17 04:11:49 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:11:49 AM [Supernet Validation] epoch: 298, val_loss: 0.488, val_acc: 87.040, best_acc: 87.040


12/17 04:11:49 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 001/521, train_loss: 0.128(0.128), train_acc: 93.750(93.750)
12/17 04:11:55 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 101/521, train_loss: 0.220(0.149), train_acc: 93.750(94.637)
12/17 04:12:02 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 201/521, train_loss: 0.085(0.146), train_acc: 96.875(94.647)
12/17 04:12:08 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 301/521, train_loss: 0.090(0.145), train_acc: 96.875(94.698)
12/17 04:12:15 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 401/521, train_loss: 0.144(0.146), train_acc: 94.792(94.638)
12/17 04:12:21 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 501/521, train_loss: 0.119(0.146), train_acc: 94.792(94.663)
12/17 04:12:22 AM [Supernet Training] lr: 0.01258 epoch: 299/600, step: 521/521, train_loss: 0.122(0.146), train_acc: 96.250(94.644)
12/17 04:12:23 AM [Supernet Training] epoch: 299, train_loss: 0.146, train_acc: 94.644
12/17 04:12:24 AM [Supernet Validation] epoch: 299, val_loss: 0.490, val_acc: 86.490, best_acc: 87.040


12/17 04:12:24 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 001/521, train_loss: 0.154(0.154), train_acc: 94.792(94.792)
12/17 04:12:31 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 101/521, train_loss: 0.059(0.139), train_acc: 98.958(94.905)
12/17 04:12:37 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 201/521, train_loss: 0.206(0.144), train_acc: 91.667(94.735)
12/17 04:12:44 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 301/521, train_loss: 0.298(0.143), train_acc: 91.667(94.774)
12/17 04:12:50 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 401/521, train_loss: 0.214(0.143), train_acc: 91.667(94.760)
12/17 04:12:57 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 501/521, train_loss: 0.120(0.145), train_acc: 96.875(94.773)
12/17 04:12:58 AM [Supernet Training] lr: 0.01254 epoch: 300/600, step: 521/521, train_loss: 0.186(0.144), train_acc: 91.250(94.804)
12/17 04:12:58 AM [Supernet Training] epoch: 300, train_loss: 0.144, train_acc: 94.804
12/17 04:12:59 AM [Supernet Validation] epoch: 300, val_loss: 0.489, val_acc: 86.410, best_acc: 87.040


12/17 04:13:00 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 001/521, train_loss: 0.157(0.157), train_acc: 92.708(92.708)
12/17 04:13:06 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 101/521, train_loss: 0.088(0.149), train_acc: 96.875(94.647)
12/17 04:13:13 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 201/521, train_loss: 0.115(0.146), train_acc: 93.750(94.771)
12/17 04:13:19 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 301/521, train_loss: 0.114(0.146), train_acc: 95.833(94.767)
12/17 04:13:25 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 401/521, train_loss: 0.084(0.146), train_acc: 98.958(94.771)
12/17 04:13:32 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 501/521, train_loss: 0.110(0.146), train_acc: 94.792(94.736)
12/17 04:13:33 AM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 521/521, train_loss: 0.198(0.146), train_acc: 93.750(94.762)
12/17 04:13:33 AM [Supernet Training] epoch: 301, train_loss: 0.146, train_acc: 94.762
12/17 04:13:35 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:13:35 AM [Supernet Validation] epoch: 301, val_loss: 0.474, val_acc: 87.200, best_acc: 87.200


12/17 04:13:35 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 001/521, train_loss: 0.095(0.095), train_acc: 97.917(97.917)
12/17 04:13:41 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 101/521, train_loss: 0.138(0.152), train_acc: 95.833(94.297)
12/17 04:13:48 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 201/521, train_loss: 0.119(0.149), train_acc: 96.875(94.564)
12/17 04:13:54 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 301/521, train_loss: 0.117(0.146), train_acc: 95.833(94.629)
12/17 04:14:00 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 401/521, train_loss: 0.113(0.146), train_acc: 94.792(94.618)
12/17 04:14:07 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 501/521, train_loss: 0.242(0.145), train_acc: 90.625(94.661)
12/17 04:14:08 AM [Supernet Training] lr: 0.01246 epoch: 302/600, step: 521/521, train_loss: 0.179(0.145), train_acc: 91.250(94.666)
12/17 04:14:08 AM [Supernet Training] epoch: 302, train_loss: 0.145, train_acc: 94.666
12/17 04:14:09 AM [Supernet Validation] epoch: 302, val_loss: 0.503, val_acc: 86.500, best_acc: 87.200


12/17 04:14:10 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 95.833(95.833)
12/17 04:14:16 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 101/521, train_loss: 0.158(0.133), train_acc: 94.792(94.905)
12/17 04:14:22 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 201/521, train_loss: 0.059(0.138), train_acc: 96.875(94.885)
12/17 04:14:29 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 301/521, train_loss: 0.142(0.139), train_acc: 95.833(94.854)
12/17 04:14:35 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 401/521, train_loss: 0.101(0.139), train_acc: 95.833(94.877)
12/17 04:14:42 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 501/521, train_loss: 0.086(0.141), train_acc: 96.875(94.810)
12/17 04:14:43 AM [Supernet Training] lr: 0.01242 epoch: 303/600, step: 521/521, train_loss: 0.148(0.141), train_acc: 93.750(94.786)
12/17 04:14:43 AM [Supernet Training] epoch: 303, train_loss: 0.141, train_acc: 94.786
12/17 04:14:45 AM [Supernet Validation] epoch: 303, val_loss: 0.480, val_acc: 86.660, best_acc: 87.200


12/17 04:14:45 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 001/521, train_loss: 0.151(0.151), train_acc: 92.708(92.708)
12/17 04:14:51 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 101/521, train_loss: 0.158(0.130), train_acc: 92.708(95.256)
12/17 04:14:58 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 201/521, train_loss: 0.239(0.137), train_acc: 90.625(94.978)
12/17 04:15:04 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 301/521, train_loss: 0.135(0.138), train_acc: 95.833(95.048)
12/17 04:15:10 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 401/521, train_loss: 0.302(0.138), train_acc: 90.625(95.054)
12/17 04:15:17 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 501/521, train_loss: 0.086(0.139), train_acc: 96.875(95.025)
12/17 04:15:18 AM [Supernet Training] lr: 0.01238 epoch: 304/600, step: 521/521, train_loss: 0.231(0.139), train_acc: 92.500(95.020)
12/17 04:15:18 AM [Supernet Training] epoch: 304, train_loss: 0.139, train_acc: 95.020
12/17 04:15:20 AM [Supernet Validation] epoch: 304, val_loss: 0.498, val_acc: 86.740, best_acc: 87.200


12/17 04:15:20 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 001/521, train_loss: 0.090(0.090), train_acc: 98.958(98.958)
12/17 04:15:26 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 101/521, train_loss: 0.075(0.134), train_acc: 96.875(95.256)
12/17 04:15:33 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 201/521, train_loss: 0.153(0.137), train_acc: 94.792(95.191)
12/17 04:15:39 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 301/521, train_loss: 0.128(0.141), train_acc: 97.917(95.103)
12/17 04:15:45 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 401/521, train_loss: 0.193(0.143), train_acc: 92.708(94.966)
12/17 04:15:52 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 501/521, train_loss: 0.215(0.142), train_acc: 93.750(95.018)
12/17 04:15:53 AM [Supernet Training] lr: 0.01233 epoch: 305/600, step: 521/521, train_loss: 0.232(0.142), train_acc: 91.250(95.008)
12/17 04:15:53 AM [Supernet Training] epoch: 305, train_loss: 0.142, train_acc: 95.008
12/17 04:15:55 AM [Supernet Validation] epoch: 305, val_loss: 0.483, val_acc: 86.690, best_acc: 87.200


12/17 04:15:55 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 001/521, train_loss: 0.130(0.130), train_acc: 96.875(96.875)
12/17 04:16:01 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 101/521, train_loss: 0.194(0.143), train_acc: 93.750(94.719)
12/17 04:16:08 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 201/521, train_loss: 0.167(0.143), train_acc: 93.750(94.667)
12/17 04:16:14 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 301/521, train_loss: 0.132(0.141), train_acc: 96.875(94.750)
12/17 04:16:20 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 401/521, train_loss: 0.110(0.140), train_acc: 94.792(94.794)
12/17 04:16:27 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 501/521, train_loss: 0.209(0.138), train_acc: 90.625(94.869)
12/17 04:16:28 AM [Supernet Training] lr: 0.01229 epoch: 306/600, step: 521/521, train_loss: 0.170(0.139), train_acc: 91.250(94.872)
12/17 04:16:28 AM [Supernet Training] epoch: 306, train_loss: 0.139, train_acc: 94.872
12/17 04:16:30 AM [Supernet Validation] epoch: 306, val_loss: 0.503, val_acc: 86.780, best_acc: 87.200


12/17 04:16:30 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 96.875(96.875)
12/17 04:16:36 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 101/521, train_loss: 0.072(0.133), train_acc: 96.875(95.225)
12/17 04:16:43 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 201/521, train_loss: 0.048(0.133), train_acc: 100.000(95.196)
12/17 04:16:49 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 301/521, train_loss: 0.135(0.136), train_acc: 95.833(95.079)
12/17 04:16:55 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 401/521, train_loss: 0.101(0.140), train_acc: 94.792(94.924)
12/17 04:17:02 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 501/521, train_loss: 0.199(0.143), train_acc: 91.667(94.810)
12/17 04:17:03 AM [Supernet Training] lr: 0.01225 epoch: 307/600, step: 521/521, train_loss: 0.151(0.143), train_acc: 90.000(94.794)
12/17 04:17:03 AM [Supernet Training] epoch: 307, train_loss: 0.143, train_acc: 94.794
12/17 04:17:05 AM [Supernet Validation] epoch: 307, val_loss: 0.489, val_acc: 86.810, best_acc: 87.200


12/17 04:17:05 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 001/521, train_loss: 0.140(0.140), train_acc: 95.833(95.833)
12/17 04:17:12 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 101/521, train_loss: 0.167(0.138), train_acc: 94.792(94.998)
12/17 04:17:18 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 201/521, train_loss: 0.140(0.138), train_acc: 94.792(95.040)
12/17 04:17:25 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 301/521, train_loss: 0.077(0.141), train_acc: 96.875(94.875)
12/17 04:17:32 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 401/521, train_loss: 0.167(0.140), train_acc: 96.875(94.890)
12/17 04:17:38 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 501/521, train_loss: 0.281(0.140), train_acc: 88.542(94.850)
12/17 04:17:39 AM [Supernet Training] lr: 0.01221 epoch: 308/600, step: 521/521, train_loss: 0.159(0.140), train_acc: 95.000(94.832)
12/17 04:17:39 AM [Supernet Training] epoch: 308, train_loss: 0.140, train_acc: 94.832
12/17 04:17:41 AM [Supernet Validation] epoch: 308, val_loss: 0.501, val_acc: 86.630, best_acc: 87.200


12/17 04:17:41 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 001/521, train_loss: 0.169(0.169), train_acc: 91.667(91.667)
12/17 04:17:48 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 101/521, train_loss: 0.124(0.127), train_acc: 96.875(95.400)
12/17 04:17:54 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 201/521, train_loss: 0.129(0.131), train_acc: 95.833(95.274)
12/17 04:18:00 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 301/521, train_loss: 0.168(0.135), train_acc: 94.792(95.165)
12/17 04:18:07 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 401/521, train_loss: 0.145(0.135), train_acc: 93.750(95.158)
12/17 04:18:13 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 501/521, train_loss: 0.072(0.135), train_acc: 98.958(95.187)
12/17 04:18:14 AM [Supernet Training] lr: 0.01217 epoch: 309/600, step: 521/521, train_loss: 0.072(0.134), train_acc: 97.500(95.214)
12/17 04:18:14 AM [Supernet Training] epoch: 309, train_loss: 0.134, train_acc: 95.214
12/17 04:18:16 AM [Supernet Validation] epoch: 309, val_loss: 0.495, val_acc: 86.780, best_acc: 87.200


12/17 04:18:16 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 001/521, train_loss: 0.164(0.164), train_acc: 95.833(95.833)
12/17 04:18:23 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 101/521, train_loss: 0.140(0.130), train_acc: 93.750(95.287)
12/17 04:18:29 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 201/521, train_loss: 0.132(0.132), train_acc: 94.792(95.217)
12/17 04:18:35 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 301/521, train_loss: 0.167(0.136), train_acc: 92.708(95.065)
12/17 04:18:42 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 401/521, train_loss: 0.081(0.137), train_acc: 97.917(95.012)
12/17 04:18:48 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 501/521, train_loss: 0.214(0.138), train_acc: 92.708(95.033)
12/17 04:18:49 AM [Supernet Training] lr: 0.01213 epoch: 310/600, step: 521/521, train_loss: 0.085(0.138), train_acc: 96.250(95.020)
12/17 04:18:49 AM [Supernet Training] epoch: 310, train_loss: 0.138, train_acc: 95.020
12/17 04:18:51 AM [Supernet Validation] epoch: 310, val_loss: 0.490, val_acc: 86.960, best_acc: 87.200


12/17 04:18:51 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 001/521, train_loss: 0.091(0.091), train_acc: 94.792(94.792)
12/17 04:18:57 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 101/521, train_loss: 0.174(0.137), train_acc: 95.833(94.977)
12/17 04:19:04 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 201/521, train_loss: 0.136(0.137), train_acc: 93.750(94.952)
12/17 04:19:10 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 301/521, train_loss: 0.117(0.137), train_acc: 95.833(94.954)
12/17 04:19:16 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 401/521, train_loss: 0.124(0.137), train_acc: 94.792(94.929)
12/17 04:19:23 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 501/521, train_loss: 0.097(0.138), train_acc: 96.875(94.960)
12/17 04:19:24 AM [Supernet Training] lr: 0.01208 epoch: 311/600, step: 521/521, train_loss: 0.038(0.138), train_acc: 100.000(94.992)
12/17 04:19:24 AM [Supernet Training] epoch: 311, train_loss: 0.138, train_acc: 94.992
12/17 04:19:26 AM [Supernet Validation] epoch: 311, val_loss: 0.494, val_acc: 86.570, best_acc: 87.200


12/17 04:19:26 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 001/521, train_loss: 0.087(0.087), train_acc: 96.875(96.875)
12/17 04:19:32 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 101/521, train_loss: 0.206(0.120), train_acc: 90.625(95.699)
12/17 04:19:39 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 201/521, train_loss: 0.142(0.124), train_acc: 92.708(95.517)
12/17 04:19:45 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 301/521, train_loss: 0.144(0.129), train_acc: 94.792(95.380)
12/17 04:19:51 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 401/521, train_loss: 0.139(0.131), train_acc: 94.792(95.262)
12/17 04:19:58 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 501/521, train_loss: 0.057(0.130), train_acc: 98.958(95.282)
12/17 04:19:59 AM [Supernet Training] lr: 0.01204 epoch: 312/600, step: 521/521, train_loss: 0.085(0.130), train_acc: 96.250(95.292)
12/17 04:19:59 AM [Supernet Training] epoch: 312, train_loss: 0.130, train_acc: 95.292
12/17 04:20:00 AM [Supernet Validation] epoch: 312, val_loss: 0.486, val_acc: 87.200, best_acc: 87.200


12/17 04:20:01 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 001/521, train_loss: 0.028(0.028), train_acc: 100.000(100.000)
12/17 04:20:07 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 101/521, train_loss: 0.031(0.134), train_acc: 100.000(95.184)
12/17 04:20:14 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 201/521, train_loss: 0.144(0.134), train_acc: 92.708(95.227)
12/17 04:20:20 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 301/521, train_loss: 0.146(0.134), train_acc: 93.750(95.266)
12/17 04:20:27 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 401/521, train_loss: 0.086(0.136), train_acc: 96.875(95.142)
12/17 04:20:33 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 501/521, train_loss: 0.114(0.135), train_acc: 96.875(95.128)
12/17 04:20:34 AM [Supernet Training] lr: 0.01200 epoch: 313/600, step: 521/521, train_loss: 0.150(0.135), train_acc: 95.000(95.130)
12/17 04:20:34 AM [Supernet Training] epoch: 313, train_loss: 0.135, train_acc: 95.130
12/17 04:20:36 AM [Supernet Validation] epoch: 313, val_loss: 0.494, val_acc: 87.070, best_acc: 87.200


12/17 04:20:36 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 001/521, train_loss: 0.222(0.222), train_acc: 92.708(92.708)
12/17 04:20:43 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 101/521, train_loss: 0.121(0.135), train_acc: 95.833(95.060)
12/17 04:20:49 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 201/521, train_loss: 0.140(0.131), train_acc: 93.750(95.237)
12/17 04:20:55 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 301/521, train_loss: 0.154(0.130), train_acc: 94.792(95.345)
12/17 04:21:02 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 401/521, train_loss: 0.096(0.130), train_acc: 96.875(95.322)
12/17 04:21:08 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 501/521, train_loss: 0.224(0.133), train_acc: 92.708(95.220)
12/17 04:21:09 AM [Supernet Training] lr: 0.01196 epoch: 314/600, step: 521/521, train_loss: 0.123(0.133), train_acc: 95.000(95.228)
12/17 04:21:09 AM [Supernet Training] epoch: 314, train_loss: 0.133, train_acc: 95.228
12/17 04:21:11 AM [Supernet Validation] epoch: 314, val_loss: 0.507, val_acc: 86.480, best_acc: 87.200


12/17 04:21:11 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 001/521, train_loss: 0.082(0.082), train_acc: 96.875(96.875)
12/17 04:21:18 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 101/521, train_loss: 0.120(0.128), train_acc: 95.833(95.235)
12/17 04:21:24 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 201/521, train_loss: 0.153(0.129), train_acc: 93.750(95.206)
12/17 04:21:30 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 301/521, train_loss: 0.113(0.132), train_acc: 94.792(95.093)
12/17 04:21:37 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 401/521, train_loss: 0.102(0.132), train_acc: 95.833(95.103)
12/17 04:21:43 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 501/521, train_loss: 0.131(0.132), train_acc: 95.833(95.137)
12/17 04:21:44 AM [Supernet Training] lr: 0.01192 epoch: 315/600, step: 521/521, train_loss: 0.182(0.131), train_acc: 93.750(95.154)
12/17 04:21:44 AM [Supernet Training] epoch: 315, train_loss: 0.131, train_acc: 95.154
12/17 04:21:46 AM [Supernet Validation] epoch: 315, val_loss: 0.496, val_acc: 86.660, best_acc: 87.200


12/17 04:21:46 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 001/521, train_loss: 0.065(0.065), train_acc: 98.958(98.958)
12/17 04:21:53 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 101/521, train_loss: 0.098(0.129), train_acc: 95.833(95.318)
12/17 04:21:59 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 201/521, train_loss: 0.139(0.127), train_acc: 95.833(95.279)
12/17 04:22:05 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 301/521, train_loss: 0.053(0.128), train_acc: 100.000(95.307)
12/17 04:22:12 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 401/521, train_loss: 0.214(0.130), train_acc: 91.667(95.192)
12/17 04:22:18 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 501/521, train_loss: 0.125(0.131), train_acc: 95.833(95.158)
12/17 04:22:19 AM [Supernet Training] lr: 0.01188 epoch: 316/600, step: 521/521, train_loss: 0.110(0.131), train_acc: 97.500(95.176)
12/17 04:22:19 AM [Supernet Training] epoch: 316, train_loss: 0.131, train_acc: 95.176
12/17 04:22:21 AM [Supernet Validation] epoch: 316, val_loss: 0.496, val_acc: 86.910, best_acc: 87.200


12/17 04:22:21 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 96.875(96.875)
12/17 04:22:27 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 101/521, train_loss: 0.072(0.126), train_acc: 98.958(95.297)
12/17 04:22:34 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 201/521, train_loss: 0.198(0.127), train_acc: 92.708(95.300)
12/17 04:22:40 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 301/521, train_loss: 0.087(0.129), train_acc: 96.875(95.238)
12/17 04:22:46 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 401/521, train_loss: 0.168(0.132), train_acc: 93.750(95.129)
12/17 04:22:52 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 501/521, train_loss: 0.251(0.133), train_acc: 93.750(95.072)
12/17 04:22:54 AM [Supernet Training] lr: 0.01183 epoch: 317/600, step: 521/521, train_loss: 0.159(0.132), train_acc: 93.750(95.080)
12/17 04:22:54 AM [Supernet Training] epoch: 317, train_loss: 0.132, train_acc: 95.080
12/17 04:22:55 AM [Supernet Validation] epoch: 317, val_loss: 0.496, val_acc: 87.050, best_acc: 87.200


12/17 04:22:56 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 001/521, train_loss: 0.152(0.152), train_acc: 95.833(95.833)
12/17 04:23:02 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 101/521, train_loss: 0.071(0.129), train_acc: 98.958(95.276)
12/17 04:23:08 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 201/521, train_loss: 0.137(0.130), train_acc: 93.750(95.268)
12/17 04:23:14 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 301/521, train_loss: 0.070(0.128), train_acc: 96.875(95.366)
12/17 04:23:21 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 401/521, train_loss: 0.088(0.130), train_acc: 96.875(95.301)
12/17 04:23:27 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 501/521, train_loss: 0.096(0.129), train_acc: 96.875(95.363)
12/17 04:23:28 AM [Supernet Training] lr: 0.01179 epoch: 318/600, step: 521/521, train_loss: 0.199(0.129), train_acc: 92.500(95.374)
12/17 04:23:28 AM [Supernet Training] epoch: 318, train_loss: 0.129, train_acc: 95.374
12/17 04:23:30 AM [Supernet Validation] epoch: 318, val_loss: 0.493, val_acc: 87.150, best_acc: 87.200


12/17 04:23:30 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 001/521, train_loss: 0.050(0.050), train_acc: 98.958(98.958)
12/17 04:23:37 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 101/521, train_loss: 0.081(0.123), train_acc: 97.917(95.627)
12/17 04:23:43 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 201/521, train_loss: 0.115(0.127), train_acc: 97.917(95.460)
12/17 04:23:50 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 301/521, train_loss: 0.057(0.129), train_acc: 97.917(95.335)
12/17 04:23:56 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 401/521, train_loss: 0.197(0.130), train_acc: 89.583(95.277)
12/17 04:24:02 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 501/521, train_loss: 0.143(0.129), train_acc: 92.708(95.309)
12/17 04:24:03 AM [Supernet Training] lr: 0.01175 epoch: 319/600, step: 521/521, train_loss: 0.118(0.129), train_acc: 96.250(95.296)
12/17 04:24:03 AM [Supernet Training] epoch: 319, train_loss: 0.129, train_acc: 95.296
12/17 04:24:05 AM [Supernet Validation] epoch: 319, val_loss: 0.492, val_acc: 86.430, best_acc: 87.200


12/17 04:24:05 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 001/521, train_loss: 0.176(0.176), train_acc: 93.750(93.750)
12/17 04:24:12 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 101/521, train_loss: 0.133(0.125), train_acc: 91.667(95.276)
12/17 04:24:18 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 201/521, train_loss: 0.182(0.126), train_acc: 93.750(95.325)
12/17 04:24:25 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 301/521, train_loss: 0.107(0.128), train_acc: 96.875(95.304)
12/17 04:24:31 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 401/521, train_loss: 0.234(0.129), train_acc: 92.708(95.290)
12/17 04:24:37 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 501/521, train_loss: 0.135(0.129), train_acc: 93.750(95.316)
12/17 04:24:39 AM [Supernet Training] lr: 0.01171 epoch: 320/600, step: 521/521, train_loss: 0.118(0.130), train_acc: 97.500(95.302)
12/17 04:24:39 AM [Supernet Training] epoch: 320, train_loss: 0.130, train_acc: 95.302
12/17 04:24:40 AM [Supernet Validation] epoch: 320, val_loss: 0.493, val_acc: 86.510, best_acc: 87.200


12/17 04:24:40 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 001/521, train_loss: 0.084(0.084), train_acc: 96.875(96.875)
12/17 04:24:47 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 101/521, train_loss: 0.104(0.128), train_acc: 95.833(95.008)
12/17 04:24:53 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 201/521, train_loss: 0.220(0.123), train_acc: 91.667(95.331)
12/17 04:24:59 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 301/521, train_loss: 0.058(0.123), train_acc: 96.875(95.446)
12/17 04:25:06 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 401/521, train_loss: 0.144(0.123), train_acc: 94.792(95.410)
12/17 04:25:12 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 501/521, train_loss: 0.152(0.124), train_acc: 91.667(95.399)
12/17 04:25:14 AM [Supernet Training] lr: 0.01167 epoch: 321/600, step: 521/521, train_loss: 0.142(0.124), train_acc: 96.250(95.382)
12/17 04:25:14 AM [Supernet Training] epoch: 321, train_loss: 0.124, train_acc: 95.382
12/17 04:25:15 AM [Supernet Validation] epoch: 321, val_loss: 0.494, val_acc: 86.580, best_acc: 87.200


12/17 04:25:15 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 001/521, train_loss: 0.140(0.140), train_acc: 96.875(96.875)
12/17 04:25:22 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 101/521, train_loss: 0.099(0.125), train_acc: 96.875(95.483)
12/17 04:25:28 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 201/521, train_loss: 0.273(0.126), train_acc: 91.667(95.346)
12/17 04:25:34 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 301/521, train_loss: 0.213(0.126), train_acc: 90.625(95.352)
12/17 04:25:41 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 401/521, train_loss: 0.176(0.128), train_acc: 95.833(95.293)
12/17 04:25:47 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 501/521, train_loss: 0.075(0.127), train_acc: 96.875(95.289)
12/17 04:25:48 AM [Supernet Training] lr: 0.01162 epoch: 322/600, step: 521/521, train_loss: 0.262(0.127), train_acc: 91.250(95.286)
12/17 04:25:48 AM [Supernet Training] epoch: 322, train_loss: 0.127, train_acc: 95.286
12/17 04:25:50 AM [Supernet Validation] epoch: 322, val_loss: 0.486, val_acc: 86.990, best_acc: 87.200


12/17 04:25:50 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 95.833(95.833)
12/17 04:25:56 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 101/521, train_loss: 0.118(0.129), train_acc: 96.875(95.276)
12/17 04:26:03 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 201/521, train_loss: 0.092(0.128), train_acc: 95.833(95.305)
12/17 04:26:09 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 301/521, train_loss: 0.101(0.125), train_acc: 97.917(95.397)
12/17 04:26:15 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 401/521, train_loss: 0.111(0.128), train_acc: 95.833(95.311)
12/17 04:26:22 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 501/521, train_loss: 0.085(0.127), train_acc: 95.833(95.357)
12/17 04:26:23 AM [Supernet Training] lr: 0.01158 epoch: 323/600, step: 521/521, train_loss: 0.229(0.127), train_acc: 92.500(95.362)
12/17 04:26:23 AM [Supernet Training] epoch: 323, train_loss: 0.127, train_acc: 95.362
12/17 04:26:25 AM [Supernet Validation] epoch: 323, val_loss: 0.523, val_acc: 86.590, best_acc: 87.200


12/17 04:26:25 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 001/521, train_loss: 0.108(0.108), train_acc: 96.875(96.875)
12/17 04:26:31 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 101/521, train_loss: 0.075(0.119), train_acc: 97.917(95.730)
12/17 04:26:38 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 201/521, train_loss: 0.145(0.122), train_acc: 94.792(95.595)
12/17 04:26:44 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 301/521, train_loss: 0.225(0.122), train_acc: 91.667(95.653)
12/17 04:26:51 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 401/521, train_loss: 0.090(0.123), train_acc: 95.833(95.558)
12/17 04:26:57 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 501/521, train_loss: 0.097(0.124), train_acc: 95.833(95.497)
12/17 04:26:58 AM [Supernet Training] lr: 0.01154 epoch: 324/600, step: 521/521, train_loss: 0.134(0.125), train_acc: 95.000(95.462)
12/17 04:26:58 AM [Supernet Training] epoch: 324, train_loss: 0.125, train_acc: 95.462
12/17 04:27:00 AM [Supernet Validation] epoch: 324, val_loss: 0.516, val_acc: 86.620, best_acc: 87.200


12/17 04:27:00 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 001/521, train_loss: 0.169(0.169), train_acc: 92.708(92.708)
12/17 04:27:06 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 101/521, train_loss: 0.106(0.116), train_acc: 96.875(95.885)
12/17 04:27:13 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 201/521, train_loss: 0.065(0.114), train_acc: 97.917(95.932)
12/17 04:27:19 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 301/521, train_loss: 0.119(0.117), train_acc: 93.750(95.774)
12/17 04:27:26 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 401/521, train_loss: 0.054(0.121), train_acc: 98.958(95.602)
12/17 04:27:32 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 501/521, train_loss: 0.072(0.123), train_acc: 96.875(95.519)
12/17 04:27:33 AM [Supernet Training] lr: 0.01150 epoch: 325/600, step: 521/521, train_loss: 0.121(0.123), train_acc: 96.250(95.512)
12/17 04:27:34 AM [Supernet Training] epoch: 325, train_loss: 0.123, train_acc: 95.512
12/17 04:27:35 AM [Supernet Validation] epoch: 325, val_loss: 0.481, val_acc: 87.190, best_acc: 87.200


12/17 04:27:35 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 001/521, train_loss: 0.119(0.119), train_acc: 95.833(95.833)
12/17 04:27:42 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 101/521, train_loss: 0.157(0.121), train_acc: 92.708(95.699)
12/17 04:27:48 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 201/521, train_loss: 0.116(0.119), train_acc: 95.833(95.797)
12/17 04:27:54 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 301/521, train_loss: 0.108(0.120), train_acc: 95.833(95.726)
12/17 04:28:01 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 401/521, train_loss: 0.089(0.121), train_acc: 95.833(95.649)
12/17 04:28:07 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 501/521, train_loss: 0.218(0.122), train_acc: 89.583(95.617)
12/17 04:28:08 AM [Supernet Training] lr: 0.01146 epoch: 326/600, step: 521/521, train_loss: 0.104(0.122), train_acc: 96.250(95.620)
12/17 04:28:09 AM [Supernet Training] epoch: 326, train_loss: 0.122, train_acc: 95.620
12/17 04:28:10 AM [Supernet Validation] epoch: 326, val_loss: 0.514, val_acc: 86.640, best_acc: 87.200


12/17 04:28:10 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 001/521, train_loss: 0.181(0.181), train_acc: 92.708(92.708)
12/17 04:28:17 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 101/521, train_loss: 0.107(0.122), train_acc: 93.750(95.637)
12/17 04:28:23 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 201/521, train_loss: 0.044(0.121), train_acc: 97.917(95.610)
12/17 04:28:30 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 301/521, train_loss: 0.072(0.124), train_acc: 97.917(95.480)
12/17 04:28:36 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 401/521, train_loss: 0.123(0.123), train_acc: 96.875(95.529)
12/17 04:28:43 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 501/521, train_loss: 0.128(0.124), train_acc: 94.792(95.476)
12/17 04:28:44 AM [Supernet Training] lr: 0.01142 epoch: 327/600, step: 521/521, train_loss: 0.148(0.124), train_acc: 96.250(95.482)
12/17 04:28:44 AM [Supernet Training] epoch: 327, train_loss: 0.124, train_acc: 95.482
12/17 04:28:46 AM [Supernet Validation] epoch: 327, val_loss: 0.530, val_acc: 86.490, best_acc: 87.200


12/17 04:28:46 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 001/521, train_loss: 0.143(0.143), train_acc: 94.792(94.792)
12/17 04:28:52 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 101/521, train_loss: 0.265(0.118), train_acc: 90.625(95.699)
12/17 04:28:59 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 201/521, train_loss: 0.134(0.117), train_acc: 92.708(95.875)
12/17 04:29:05 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 301/521, train_loss: 0.103(0.120), train_acc: 97.917(95.736)
12/17 04:29:11 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 401/521, train_loss: 0.099(0.119), train_acc: 94.792(95.776)
12/17 04:29:18 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 501/521, train_loss: 0.114(0.121), train_acc: 95.833(95.715)
12/17 04:29:19 AM [Supernet Training] lr: 0.01137 epoch: 328/600, step: 521/521, train_loss: 0.104(0.122), train_acc: 95.000(95.694)
12/17 04:29:19 AM [Supernet Training] epoch: 328, train_loss: 0.122, train_acc: 95.694
12/17 04:29:21 AM [Supernet Validation] epoch: 328, val_loss: 0.520, val_acc: 86.970, best_acc: 87.200


12/17 04:29:21 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 001/521, train_loss: 0.147(0.147), train_acc: 94.792(94.792)
12/17 04:29:27 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 101/521, train_loss: 0.092(0.113), train_acc: 96.875(95.864)
12/17 04:29:34 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 201/521, train_loss: 0.141(0.119), train_acc: 92.708(95.652)
12/17 04:29:40 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 301/521, train_loss: 0.114(0.118), train_acc: 94.792(95.674)
12/17 04:29:46 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 401/521, train_loss: 0.119(0.117), train_acc: 96.875(95.711)
12/17 04:29:53 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 501/521, train_loss: 0.102(0.119), train_acc: 96.875(95.655)
12/17 04:29:54 AM [Supernet Training] lr: 0.01133 epoch: 329/600, step: 521/521, train_loss: 0.089(0.118), train_acc: 96.250(95.644)
12/17 04:29:54 AM [Supernet Training] epoch: 329, train_loss: 0.118, train_acc: 95.644
12/17 04:29:55 AM [Supernet Validation] epoch: 329, val_loss: 0.506, val_acc: 86.950, best_acc: 87.200


12/17 04:29:56 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 001/521, train_loss: 0.035(0.035), train_acc: 98.958(98.958)
12/17 04:30:02 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 101/521, train_loss: 0.069(0.112), train_acc: 96.875(95.844)
12/17 04:30:08 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 201/521, train_loss: 0.061(0.115), train_acc: 98.958(95.797)
12/17 04:30:15 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 301/521, train_loss: 0.081(0.118), train_acc: 97.917(95.719)
12/17 04:30:21 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 401/521, train_loss: 0.127(0.119), train_acc: 95.833(95.667)
12/17 04:30:27 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 501/521, train_loss: 0.056(0.121), train_acc: 100.000(95.663)
12/17 04:30:29 AM [Supernet Training] lr: 0.01129 epoch: 330/600, step: 521/521, train_loss: 0.239(0.121), train_acc: 93.750(95.650)
12/17 04:30:29 AM [Supernet Training] epoch: 330, train_loss: 0.121, train_acc: 95.650
12/17 04:30:30 AM [Supernet Validation] epoch: 330, val_loss: 0.497, val_acc: 86.940, best_acc: 87.200


12/17 04:30:31 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 001/521, train_loss: 0.201(0.201), train_acc: 92.708(92.708)
12/17 04:30:37 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 101/521, train_loss: 0.086(0.122), train_acc: 97.917(95.864)
12/17 04:30:43 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 201/521, train_loss: 0.091(0.118), train_acc: 95.833(95.839)
12/17 04:30:50 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 301/521, train_loss: 0.151(0.118), train_acc: 92.708(95.788)
12/17 04:30:56 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 401/521, train_loss: 0.123(0.120), train_acc: 96.875(95.750)
12/17 04:31:02 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 501/521, train_loss: 0.092(0.121), train_acc: 97.917(95.707)
12/17 04:31:04 AM [Supernet Training] lr: 0.01125 epoch: 331/600, step: 521/521, train_loss: 0.057(0.121), train_acc: 98.750(95.682)
12/17 04:31:04 AM [Supernet Training] epoch: 331, train_loss: 0.121, train_acc: 95.682
12/17 04:31:05 AM [Supernet Validation] epoch: 331, val_loss: 0.510, val_acc: 86.830, best_acc: 87.200


12/17 04:31:06 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 001/521, train_loss: 0.160(0.160), train_acc: 94.792(94.792)
12/17 04:31:12 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 101/521, train_loss: 0.213(0.113), train_acc: 91.667(95.926)
12/17 04:31:18 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 201/521, train_loss: 0.109(0.119), train_acc: 95.833(95.616)
12/17 04:31:24 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 301/521, train_loss: 0.178(0.120), train_acc: 95.833(95.615)
12/17 04:31:31 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 401/521, train_loss: 0.107(0.120), train_acc: 95.833(95.587)
12/17 04:31:37 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 501/521, train_loss: 0.097(0.119), train_acc: 95.833(95.621)
12/17 04:31:38 AM [Supernet Training] lr: 0.01121 epoch: 332/600, step: 521/521, train_loss: 0.124(0.120), train_acc: 96.250(95.620)
12/17 04:31:38 AM [Supernet Training] epoch: 332, train_loss: 0.120, train_acc: 95.620
12/17 04:31:40 AM [Supernet Validation] epoch: 332, val_loss: 0.502, val_acc: 86.700, best_acc: 87.200


12/17 04:31:40 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 001/521, train_loss: 0.233(0.233), train_acc: 91.667(91.667)
12/17 04:31:46 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 101/521, train_loss: 0.084(0.120), train_acc: 97.917(95.802)
12/17 04:31:53 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 201/521, train_loss: 0.125(0.119), train_acc: 96.875(95.688)
12/17 04:31:59 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 301/521, train_loss: 0.149(0.121), train_acc: 92.708(95.577)
12/17 04:32:05 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 401/521, train_loss: 0.078(0.120), train_acc: 96.875(95.667)
12/17 04:32:11 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 501/521, train_loss: 0.162(0.119), train_acc: 93.750(95.715)
12/17 04:32:13 AM [Supernet Training] lr: 0.01117 epoch: 333/600, step: 521/521, train_loss: 0.198(0.120), train_acc: 95.000(95.664)
12/17 04:32:13 AM [Supernet Training] epoch: 333, train_loss: 0.120, train_acc: 95.664
12/17 04:32:14 AM [Supernet Validation] epoch: 333, val_loss: 0.501, val_acc: 86.810, best_acc: 87.200


12/17 04:32:15 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 001/521, train_loss: 0.145(0.145), train_acc: 94.792(94.792)
12/17 04:32:21 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 101/521, train_loss: 0.110(0.128), train_acc: 93.750(95.462)
12/17 04:32:28 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 201/521, train_loss: 0.045(0.123), train_acc: 97.917(95.424)
12/17 04:32:34 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 301/521, train_loss: 0.132(0.121), train_acc: 95.833(95.546)
12/17 04:32:40 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 401/521, train_loss: 0.070(0.120), train_acc: 96.875(95.605)
12/17 04:32:46 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 501/521, train_loss: 0.144(0.120), train_acc: 92.708(95.565)
12/17 04:32:48 AM [Supernet Training] lr: 0.01112 epoch: 334/600, step: 521/521, train_loss: 0.233(0.121), train_acc: 93.750(95.580)
12/17 04:32:48 AM [Supernet Training] epoch: 334, train_loss: 0.121, train_acc: 95.580
12/17 04:32:49 AM [Supernet Validation] epoch: 334, val_loss: 0.522, val_acc: 86.620, best_acc: 87.200


12/17 04:32:50 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 001/521, train_loss: 0.086(0.086), train_acc: 98.958(98.958)
12/17 04:32:56 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 101/521, train_loss: 0.187(0.110), train_acc: 95.833(96.112)
12/17 04:33:02 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 201/521, train_loss: 0.087(0.113), train_acc: 95.833(95.978)
12/17 04:33:09 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 301/521, train_loss: 0.174(0.114), train_acc: 93.750(95.937)
12/17 04:33:15 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 401/521, train_loss: 0.119(0.116), train_acc: 96.875(95.844)
12/17 04:33:21 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 501/521, train_loss: 0.130(0.117), train_acc: 92.708(95.769)
12/17 04:33:22 AM [Supernet Training] lr: 0.01108 epoch: 335/600, step: 521/521, train_loss: 0.079(0.117), train_acc: 97.500(95.774)
12/17 04:33:23 AM [Supernet Training] epoch: 335, train_loss: 0.117, train_acc: 95.774
12/17 04:33:24 AM [Supernet Validation] epoch: 335, val_loss: 0.524, val_acc: 86.440, best_acc: 87.200


12/17 04:33:24 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 001/521, train_loss: 0.188(0.188), train_acc: 95.833(95.833)
12/17 04:33:31 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 101/521, train_loss: 0.190(0.120), train_acc: 93.750(95.668)
12/17 04:33:37 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 201/521, train_loss: 0.137(0.116), train_acc: 93.750(95.906)
12/17 04:33:43 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 301/521, train_loss: 0.099(0.115), train_acc: 95.833(95.819)
12/17 04:33:50 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 401/521, train_loss: 0.052(0.115), train_acc: 96.875(95.844)
12/17 04:33:56 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 501/521, train_loss: 0.091(0.117), train_acc: 95.833(95.798)
12/17 04:33:57 AM [Supernet Training] lr: 0.01104 epoch: 336/600, step: 521/521, train_loss: 0.153(0.117), train_acc: 93.750(95.770)
12/17 04:33:57 AM [Supernet Training] epoch: 336, train_loss: 0.117, train_acc: 95.770
12/17 04:33:59 AM [Supernet Validation] epoch: 336, val_loss: 0.508, val_acc: 86.990, best_acc: 87.200


12/17 04:33:59 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 92.708(92.708)
12/17 04:34:05 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 101/521, train_loss: 0.137(0.111), train_acc: 94.792(95.967)
12/17 04:34:12 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 201/521, train_loss: 0.078(0.115), train_acc: 97.917(95.813)
12/17 04:34:18 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 301/521, train_loss: 0.085(0.116), train_acc: 96.875(95.809)
12/17 04:34:24 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 401/521, train_loss: 0.125(0.117), train_acc: 96.875(95.766)
12/17 04:34:31 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 501/521, train_loss: 0.064(0.118), train_acc: 96.875(95.725)
12/17 04:34:32 AM [Supernet Training] lr: 0.01100 epoch: 337/600, step: 521/521, train_loss: 0.104(0.118), train_acc: 96.250(95.722)
12/17 04:34:32 AM [Supernet Training] epoch: 337, train_loss: 0.118, train_acc: 95.722
12/17 04:34:34 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:34:34 AM [Supernet Validation] epoch: 337, val_loss: 0.486, val_acc: 87.450, best_acc: 87.450


12/17 04:34:34 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 95.833(95.833)
12/17 04:34:40 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 101/521, train_loss: 0.103(0.114), train_acc: 95.833(95.699)
12/17 04:34:47 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 201/521, train_loss: 0.203(0.115), train_acc: 95.833(95.771)
12/17 04:34:53 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 301/521, train_loss: 0.070(0.116), train_acc: 97.917(95.743)
12/17 04:35:00 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 401/521, train_loss: 0.028(0.118), train_acc: 100.000(95.680)
12/17 04:35:06 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 501/521, train_loss: 0.131(0.117), train_acc: 93.750(95.686)
12/17 04:35:08 AM [Supernet Training] lr: 0.01096 epoch: 338/600, step: 521/521, train_loss: 0.120(0.117), train_acc: 97.500(95.700)
12/17 04:35:08 AM [Supernet Training] epoch: 338, train_loss: 0.117, train_acc: 95.700
12/17 04:35:10 AM [Supernet Validation] epoch: 338, val_loss: 0.485, val_acc: 87.290, best_acc: 87.450


12/17 04:35:10 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 001/521, train_loss: 0.101(0.101), train_acc: 97.917(97.917)
12/17 04:35:16 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 101/521, train_loss: 0.087(0.117), train_acc: 98.958(95.906)
12/17 04:35:23 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 201/521, train_loss: 0.070(0.116), train_acc: 98.958(95.833)
12/17 04:35:29 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 301/521, train_loss: 0.033(0.115), train_acc: 100.000(95.909)
12/17 04:35:35 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 401/521, train_loss: 0.071(0.116), train_acc: 97.917(95.813)
12/17 04:35:42 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 501/521, train_loss: 0.060(0.115), train_acc: 96.875(95.835)
12/17 04:35:43 AM [Supernet Training] lr: 0.01092 epoch: 339/600, step: 521/521, train_loss: 0.089(0.115), train_acc: 95.000(95.842)
12/17 04:35:43 AM [Supernet Training] epoch: 339, train_loss: 0.115, train_acc: 95.842
12/17 04:35:44 AM [Supernet Validation] epoch: 339, val_loss: 0.490, val_acc: 87.090, best_acc: 87.450


12/17 04:35:45 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 001/521, train_loss: 0.127(0.127), train_acc: 96.875(96.875)
12/17 04:35:51 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 101/521, train_loss: 0.080(0.103), train_acc: 97.917(96.401)
12/17 04:35:58 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 201/521, train_loss: 0.115(0.109), train_acc: 94.792(96.243)
12/17 04:36:04 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 301/521, train_loss: 0.197(0.112), train_acc: 95.833(96.065)
12/17 04:36:10 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 401/521, train_loss: 0.098(0.110), train_acc: 95.833(96.070)
12/17 04:36:17 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 501/521, train_loss: 0.054(0.113), train_acc: 96.875(95.987)
12/17 04:36:18 AM [Supernet Training] lr: 0.01088 epoch: 340/600, step: 521/521, train_loss: 0.073(0.112), train_acc: 97.500(96.008)
12/17 04:36:18 AM [Supernet Training] epoch: 340, train_loss: 0.112, train_acc: 96.008
12/17 04:36:20 AM [Supernet Validation] epoch: 340, val_loss: 0.514, val_acc: 86.930, best_acc: 87.450


12/17 04:36:20 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 001/521, train_loss: 0.059(0.059), train_acc: 97.917(97.917)
12/17 04:36:27 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 101/521, train_loss: 0.067(0.108), train_acc: 98.958(96.225)
12/17 04:36:33 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 201/521, train_loss: 0.117(0.109), train_acc: 96.875(96.165)
12/17 04:36:40 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 301/521, train_loss: 0.029(0.110), train_acc: 100.000(96.176)
12/17 04:36:46 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 401/521, train_loss: 0.216(0.111), train_acc: 93.750(96.101)
12/17 04:36:52 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 501/521, train_loss: 0.034(0.113), train_acc: 98.958(96.037)
12/17 04:36:54 AM [Supernet Training] lr: 0.01083 epoch: 341/600, step: 521/521, train_loss: 0.243(0.113), train_acc: 93.750(96.018)
12/17 04:36:54 AM [Supernet Training] epoch: 341, train_loss: 0.113, train_acc: 96.018
12/17 04:36:55 AM [Supernet Validation] epoch: 341, val_loss: 0.496, val_acc: 86.900, best_acc: 87.450


12/17 04:36:55 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 95.833(95.833)
12/17 04:37:02 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 101/521, train_loss: 0.051(0.107), train_acc: 98.958(96.132)
12/17 04:37:09 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 201/521, train_loss: 0.134(0.111), train_acc: 92.708(95.921)
12/17 04:37:15 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 301/521, train_loss: 0.090(0.111), train_acc: 96.875(95.948)
12/17 04:37:21 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 401/521, train_loss: 0.081(0.112), train_acc: 97.917(95.935)
12/17 04:37:28 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 501/521, train_loss: 0.149(0.112), train_acc: 95.833(95.927)
12/17 04:37:29 AM [Supernet Training] lr: 0.01079 epoch: 342/600, step: 521/521, train_loss: 0.082(0.112), train_acc: 96.250(95.948)
12/17 04:37:29 AM [Supernet Training] epoch: 342, train_loss: 0.112, train_acc: 95.948
12/17 04:37:30 AM [Supernet Validation] epoch: 342, val_loss: 0.508, val_acc: 87.160, best_acc: 87.450


12/17 04:37:31 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 001/521, train_loss: 0.070(0.070), train_acc: 96.875(96.875)
12/17 04:37:37 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 101/521, train_loss: 0.112(0.110), train_acc: 96.875(95.823)
12/17 04:37:44 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 201/521, train_loss: 0.113(0.108), train_acc: 95.833(95.958)
12/17 04:37:50 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 301/521, train_loss: 0.074(0.110), train_acc: 97.917(95.909)
12/17 04:37:56 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 401/521, train_loss: 0.054(0.114), train_acc: 98.958(95.768)
12/17 04:38:03 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 501/521, train_loss: 0.182(0.115), train_acc: 92.708(95.738)
12/17 04:38:04 AM [Supernet Training] lr: 0.01075 epoch: 343/600, step: 521/521, train_loss: 0.074(0.115), train_acc: 97.500(95.736)
12/17 04:38:04 AM [Supernet Training] epoch: 343, train_loss: 0.115, train_acc: 95.736
12/17 04:38:06 AM [Supernet Validation] epoch: 343, val_loss: 0.493, val_acc: 86.990, best_acc: 87.450


12/17 04:38:06 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 97.917(97.917)
12/17 04:38:12 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 101/521, train_loss: 0.190(0.117), train_acc: 90.625(95.658)
12/17 04:38:18 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 201/521, train_loss: 0.068(0.113), train_acc: 96.875(95.890)
12/17 04:38:25 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 301/521, train_loss: 0.066(0.112), train_acc: 96.875(95.896)
12/17 04:38:31 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 401/521, train_loss: 0.097(0.111), train_acc: 95.833(95.955)
12/17 04:38:38 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 501/521, train_loss: 0.093(0.112), train_acc: 95.833(95.929)
12/17 04:38:39 AM [Supernet Training] lr: 0.01071 epoch: 344/600, step: 521/521, train_loss: 0.095(0.112), train_acc: 96.250(95.908)
12/17 04:38:39 AM [Supernet Training] epoch: 344, train_loss: 0.112, train_acc: 95.908
12/17 04:38:40 AM [Supernet Validation] epoch: 344, val_loss: 0.504, val_acc: 87.130, best_acc: 87.450


12/17 04:38:41 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 001/521, train_loss: 0.153(0.153), train_acc: 94.792(94.792)
12/17 04:38:47 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 101/521, train_loss: 0.131(0.117), train_acc: 93.750(95.916)
12/17 04:38:53 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 201/521, train_loss: 0.136(0.111), train_acc: 95.833(96.082)
12/17 04:39:00 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 301/521, train_loss: 0.114(0.111), train_acc: 97.917(96.027)
12/17 04:39:06 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 401/521, train_loss: 0.136(0.112), train_acc: 95.833(95.932)
12/17 04:39:12 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 501/521, train_loss: 0.138(0.113), train_acc: 95.833(95.896)
12/17 04:39:14 AM [Supernet Training] lr: 0.01067 epoch: 345/600, step: 521/521, train_loss: 0.054(0.113), train_acc: 97.500(95.902)
12/17 04:39:14 AM [Supernet Training] epoch: 345, train_loss: 0.113, train_acc: 95.902
12/17 04:39:15 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:39:15 AM [Supernet Validation] epoch: 345, val_loss: 0.502, val_acc: 87.720, best_acc: 87.720


12/17 04:39:16 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 98.958(98.958)
12/17 04:39:22 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 101/521, train_loss: 0.073(0.101), train_acc: 98.958(96.236)
12/17 04:39:29 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 201/521, train_loss: 0.079(0.108), train_acc: 97.917(96.020)
12/17 04:39:35 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 301/521, train_loss: 0.079(0.111), train_acc: 97.917(95.958)
12/17 04:39:41 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 401/521, train_loss: 0.110(0.111), train_acc: 96.875(95.966)
12/17 04:39:48 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 501/521, train_loss: 0.068(0.112), train_acc: 98.958(95.929)
12/17 04:39:49 AM [Supernet Training] lr: 0.01063 epoch: 346/600, step: 521/521, train_loss: 0.027(0.112), train_acc: 98.750(95.916)
12/17 04:39:49 AM [Supernet Training] epoch: 346, train_loss: 0.112, train_acc: 95.916
12/17 04:39:51 AM [Supernet Validation] epoch: 346, val_loss: 0.500, val_acc: 87.330, best_acc: 87.720


12/17 04:39:51 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 001/521, train_loss: 0.142(0.142), train_acc: 96.875(96.875)
12/17 04:39:57 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 101/521, train_loss: 0.163(0.102), train_acc: 93.750(96.514)
12/17 04:40:04 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 201/521, train_loss: 0.136(0.106), train_acc: 94.792(96.191)
12/17 04:40:10 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 301/521, train_loss: 0.105(0.106), train_acc: 96.875(96.159)
12/17 04:40:16 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 401/521, train_loss: 0.107(0.107), train_acc: 95.833(96.132)
12/17 04:40:23 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 501/521, train_loss: 0.095(0.107), train_acc: 95.833(96.170)
12/17 04:40:24 AM [Supernet Training] lr: 0.01058 epoch: 347/600, step: 521/521, train_loss: 0.100(0.107), train_acc: 96.250(96.178)
12/17 04:40:24 AM [Supernet Training] epoch: 347, train_loss: 0.107, train_acc: 96.178
12/17 04:40:26 AM [Supernet Validation] epoch: 347, val_loss: 0.510, val_acc: 87.080, best_acc: 87.720


12/17 04:40:26 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 001/521, train_loss: 0.192(0.192), train_acc: 92.708(92.708)
12/17 04:40:33 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 101/521, train_loss: 0.055(0.105), train_acc: 96.875(96.143)
12/17 04:40:39 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 201/521, train_loss: 0.133(0.108), train_acc: 95.833(95.953)
12/17 04:40:45 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 301/521, train_loss: 0.101(0.109), train_acc: 97.917(95.934)
12/17 04:40:52 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 401/521, train_loss: 0.110(0.109), train_acc: 94.792(96.005)
12/17 04:40:58 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 501/521, train_loss: 0.095(0.110), train_acc: 95.833(95.958)
12/17 04:40:59 AM [Supernet Training] lr: 0.01054 epoch: 348/600, step: 521/521, train_loss: 0.098(0.110), train_acc: 96.250(95.948)
12/17 04:40:59 AM [Supernet Training] epoch: 348, train_loss: 0.110, train_acc: 95.948
12/17 04:41:01 AM [Supernet Validation] epoch: 348, val_loss: 0.517, val_acc: 87.030, best_acc: 87.720


12/17 04:41:01 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 001/521, train_loss: 0.108(0.108), train_acc: 95.833(95.833)
12/17 04:41:08 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 101/521, train_loss: 0.062(0.109), train_acc: 97.917(96.122)
12/17 04:41:14 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 201/521, train_loss: 0.070(0.107), train_acc: 97.917(96.077)
12/17 04:41:21 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 301/521, train_loss: 0.081(0.107), train_acc: 96.875(96.176)
12/17 04:41:27 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 401/521, train_loss: 0.069(0.106), train_acc: 97.917(96.163)
12/17 04:41:33 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 501/521, train_loss: 0.163(0.108), train_acc: 95.833(96.112)
12/17 04:41:35 AM [Supernet Training] lr: 0.01050 epoch: 349/600, step: 521/521, train_loss: 0.196(0.109), train_acc: 92.500(96.072)
12/17 04:41:35 AM [Supernet Training] epoch: 349, train_loss: 0.109, train_acc: 96.072
12/17 04:41:36 AM [Supernet Validation] epoch: 349, val_loss: 0.512, val_acc: 87.050, best_acc: 87.720


12/17 04:41:36 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 001/521, train_loss: 0.167(0.167), train_acc: 91.667(91.667)
12/17 04:41:43 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 101/521, train_loss: 0.082(0.108), train_acc: 98.958(96.040)
12/17 04:41:49 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 201/521, train_loss: 0.122(0.106), train_acc: 96.875(96.191)
12/17 04:41:55 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 301/521, train_loss: 0.088(0.107), train_acc: 94.792(96.134)
12/17 04:42:02 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 401/521, train_loss: 0.086(0.106), train_acc: 95.833(96.163)
12/17 04:42:08 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 501/521, train_loss: 0.255(0.106), train_acc: 91.667(96.201)
12/17 04:42:09 AM [Supernet Training] lr: 0.01046 epoch: 350/600, step: 521/521, train_loss: 0.193(0.106), train_acc: 93.750(96.178)
12/17 04:42:09 AM [Supernet Training] epoch: 350, train_loss: 0.106, train_acc: 96.178
12/17 04:42:11 AM [Supernet Validation] epoch: 350, val_loss: 0.516, val_acc: 86.350, best_acc: 87.720


12/17 04:42:11 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 96.875(96.875)
12/17 04:42:18 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 101/521, train_loss: 0.123(0.105), train_acc: 96.875(96.266)
12/17 04:42:24 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 201/521, train_loss: 0.029(0.104), train_acc: 100.000(96.300)
12/17 04:42:30 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 301/521, train_loss: 0.076(0.106), train_acc: 95.833(96.221)
12/17 04:42:37 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 401/521, train_loss: 0.102(0.105), train_acc: 96.875(96.296)
12/17 04:42:43 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 501/521, train_loss: 0.073(0.105), train_acc: 97.917(96.285)
12/17 04:42:44 AM [Supernet Training] lr: 0.01042 epoch: 351/600, step: 521/521, train_loss: 0.113(0.105), train_acc: 97.500(96.286)
12/17 04:42:44 AM [Supernet Training] epoch: 351, train_loss: 0.105, train_acc: 96.286
12/17 04:42:46 AM [Supernet Validation] epoch: 351, val_loss: 0.522, val_acc: 87.100, best_acc: 87.720


12/17 04:42:46 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 001/521, train_loss: 0.205(0.205), train_acc: 94.792(94.792)
12/17 04:42:53 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 101/521, train_loss: 0.079(0.104), train_acc: 98.958(96.462)
12/17 04:42:59 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 201/521, train_loss: 0.141(0.106), train_acc: 92.708(96.300)
12/17 04:43:05 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 301/521, train_loss: 0.124(0.105), train_acc: 95.833(96.266)
12/17 04:43:12 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 401/521, train_loss: 0.109(0.106), train_acc: 95.833(96.257)
12/17 04:43:18 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 501/521, train_loss: 0.103(0.110), train_acc: 95.833(96.139)
12/17 04:43:19 AM [Supernet Training] lr: 0.01038 epoch: 352/600, step: 521/521, train_loss: 0.118(0.109), train_acc: 95.000(96.144)
12/17 04:43:19 AM [Supernet Training] epoch: 352, train_loss: 0.109, train_acc: 96.144
12/17 04:43:21 AM [Supernet Validation] epoch: 352, val_loss: 0.512, val_acc: 87.190, best_acc: 87.720


12/17 04:43:21 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 001/521, train_loss: 0.154(0.154), train_acc: 94.792(94.792)
12/17 04:43:28 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 101/521, train_loss: 0.085(0.104), train_acc: 97.917(96.246)
12/17 04:43:34 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 201/521, train_loss: 0.055(0.104), train_acc: 97.917(96.279)
12/17 04:43:41 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 301/521, train_loss: 0.075(0.108), train_acc: 98.958(96.200)
12/17 04:43:47 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 401/521, train_loss: 0.089(0.107), train_acc: 96.875(96.184)
12/17 04:43:53 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 501/521, train_loss: 0.054(0.106), train_acc: 97.917(96.214)
12/17 04:43:54 AM [Supernet Training] lr: 0.01033 epoch: 353/600, step: 521/521, train_loss: 0.237(0.107), train_acc: 91.250(96.194)
12/17 04:43:54 AM [Supernet Training] epoch: 353, train_loss: 0.107, train_acc: 96.194
12/17 04:43:56 AM [Supernet Validation] epoch: 353, val_loss: 0.528, val_acc: 86.870, best_acc: 87.720


12/17 04:43:56 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 001/521, train_loss: 0.116(0.116), train_acc: 95.833(95.833)
12/17 04:44:03 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 101/521, train_loss: 0.085(0.109), train_acc: 94.792(96.050)
12/17 04:44:09 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 201/521, train_loss: 0.043(0.107), train_acc: 100.000(96.165)
12/17 04:44:16 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 301/521, train_loss: 0.117(0.104), train_acc: 95.833(96.252)
12/17 04:44:22 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 401/521, train_loss: 0.108(0.107), train_acc: 95.833(96.145)
12/17 04:44:28 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 501/521, train_loss: 0.183(0.108), train_acc: 92.708(96.126)
12/17 04:44:30 AM [Supernet Training] lr: 0.01029 epoch: 354/600, step: 521/521, train_loss: 0.051(0.108), train_acc: 98.750(96.128)
12/17 04:44:30 AM [Supernet Training] epoch: 354, train_loss: 0.108, train_acc: 96.128
12/17 04:44:31 AM [Supernet Validation] epoch: 354, val_loss: 0.516, val_acc: 86.860, best_acc: 87.720


12/17 04:44:31 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 001/521, train_loss: 0.112(0.112), train_acc: 95.833(95.833)
12/17 04:44:38 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 101/521, train_loss: 0.139(0.103), train_acc: 95.833(96.308)
12/17 04:44:44 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 201/521, train_loss: 0.128(0.104), train_acc: 94.792(96.170)
12/17 04:44:50 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 301/521, train_loss: 0.088(0.105), train_acc: 97.917(96.176)
12/17 04:44:57 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 401/521, train_loss: 0.129(0.105), train_acc: 97.917(96.223)
12/17 04:45:03 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 501/521, train_loss: 0.055(0.105), train_acc: 97.917(96.224)
12/17 04:45:05 AM [Supernet Training] lr: 0.01025 epoch: 355/600, step: 521/521, train_loss: 0.064(0.106), train_acc: 98.750(96.208)
12/17 04:45:05 AM [Supernet Training] epoch: 355, train_loss: 0.106, train_acc: 96.208
12/17 04:45:06 AM [Supernet Validation] epoch: 355, val_loss: 0.496, val_acc: 87.250, best_acc: 87.720


12/17 04:45:07 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 96.875(96.875)
12/17 04:45:13 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 101/521, train_loss: 0.021(0.099), train_acc: 100.000(96.555)
12/17 04:45:20 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 201/521, train_loss: 0.062(0.101), train_acc: 95.833(96.409)
12/17 04:45:26 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 301/521, train_loss: 0.156(0.102), train_acc: 91.667(96.346)
12/17 04:45:33 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 401/521, train_loss: 0.162(0.101), train_acc: 91.667(96.322)
12/17 04:45:39 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 501/521, train_loss: 0.112(0.104), train_acc: 95.833(96.264)
12/17 04:45:40 AM [Supernet Training] lr: 0.01021 epoch: 356/600, step: 521/521, train_loss: 0.216(0.104), train_acc: 92.500(96.260)
12/17 04:45:40 AM [Supernet Training] epoch: 356, train_loss: 0.104, train_acc: 96.260
12/17 04:45:42 AM [Supernet Validation] epoch: 356, val_loss: 0.503, val_acc: 87.350, best_acc: 87.720


12/17 04:45:42 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 001/521, train_loss: 0.089(0.089), train_acc: 95.833(95.833)
12/17 04:45:49 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 101/521, train_loss: 0.069(0.103), train_acc: 97.917(96.308)
12/17 04:45:55 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 201/521, train_loss: 0.095(0.101), train_acc: 96.875(96.372)
12/17 04:46:02 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 301/521, train_loss: 0.087(0.102), train_acc: 96.875(96.363)
12/17 04:46:08 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 401/521, train_loss: 0.155(0.102), train_acc: 93.750(96.374)
12/17 04:46:14 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 501/521, train_loss: 0.080(0.104), train_acc: 96.875(96.285)
12/17 04:46:15 AM [Supernet Training] lr: 0.01017 epoch: 357/600, step: 521/521, train_loss: 0.096(0.104), train_acc: 96.250(96.272)
12/17 04:46:15 AM [Supernet Training] epoch: 357, train_loss: 0.104, train_acc: 96.272
12/17 04:46:17 AM [Supernet Validation] epoch: 357, val_loss: 0.489, val_acc: 87.470, best_acc: 87.720


12/17 04:46:17 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 001/521, train_loss: 0.103(0.103), train_acc: 94.792(94.792)
12/17 04:46:24 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 101/521, train_loss: 0.080(0.099), train_acc: 95.833(96.504)
12/17 04:46:30 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 201/521, train_loss: 0.089(0.103), train_acc: 97.917(96.346)
12/17 04:46:36 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 301/521, train_loss: 0.066(0.101), train_acc: 97.917(96.325)
12/17 04:46:43 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 401/521, train_loss: 0.074(0.101), train_acc: 97.917(96.345)
12/17 04:46:49 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 501/521, train_loss: 0.135(0.101), train_acc: 94.792(96.351)
12/17 04:46:51 AM [Supernet Training] lr: 0.01013 epoch: 358/600, step: 521/521, train_loss: 0.065(0.101), train_acc: 97.500(96.360)
12/17 04:46:51 AM [Supernet Training] epoch: 358, train_loss: 0.101, train_acc: 96.360
12/17 04:46:52 AM [Supernet Validation] epoch: 358, val_loss: 0.508, val_acc: 87.130, best_acc: 87.720


12/17 04:46:53 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 98.958(98.958)
12/17 04:46:59 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 101/521, train_loss: 0.243(0.103), train_acc: 92.708(96.555)
12/17 04:47:05 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 201/521, train_loss: 0.048(0.101), train_acc: 97.917(96.424)
12/17 04:47:12 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 301/521, train_loss: 0.050(0.102), train_acc: 97.917(96.339)
12/17 04:47:18 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 401/521, train_loss: 0.075(0.102), train_acc: 95.833(96.329)
12/17 04:47:24 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 501/521, train_loss: 0.106(0.102), train_acc: 93.750(96.382)
12/17 04:47:25 AM [Supernet Training] lr: 0.01008 epoch: 359/600, step: 521/521, train_loss: 0.102(0.102), train_acc: 98.750(96.400)
12/17 04:47:25 AM [Supernet Training] epoch: 359, train_loss: 0.102, train_acc: 96.400
12/17 04:47:27 AM [Supernet Validation] epoch: 359, val_loss: 0.512, val_acc: 87.440, best_acc: 87.720


12/17 04:47:27 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
12/17 04:47:34 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 101/521, train_loss: 0.061(0.093), train_acc: 98.958(96.700)
12/17 04:47:40 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 201/521, train_loss: 0.075(0.098), train_acc: 95.833(96.419)
12/17 04:47:46 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 301/521, train_loss: 0.074(0.100), train_acc: 96.875(96.384)
12/17 04:47:53 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 401/521, train_loss: 0.050(0.100), train_acc: 97.917(96.358)
12/17 04:47:59 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 501/521, train_loss: 0.095(0.101), train_acc: 95.833(96.322)
12/17 04:48:00 AM [Supernet Training] lr: 0.01004 epoch: 360/600, step: 521/521, train_loss: 0.119(0.102), train_acc: 95.000(96.308)
12/17 04:48:00 AM [Supernet Training] epoch: 360, train_loss: 0.102, train_acc: 96.308
12/17 04:48:02 AM [Supernet Validation] epoch: 360, val_loss: 0.525, val_acc: 86.870, best_acc: 87.720


12/17 04:48:02 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 001/521, train_loss: 0.119(0.119), train_acc: 94.792(94.792)
12/17 04:48:09 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 101/521, train_loss: 0.077(0.100), train_acc: 97.917(96.545)
12/17 04:48:15 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 201/521, train_loss: 0.109(0.102), train_acc: 96.875(96.414)
12/17 04:48:21 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 301/521, train_loss: 0.145(0.105), train_acc: 93.750(96.283)
12/17 04:48:28 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 401/521, train_loss: 0.113(0.104), train_acc: 94.792(96.311)
12/17 04:48:34 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 501/521, train_loss: 0.173(0.104), train_acc: 95.833(96.312)
12/17 04:48:35 AM [Supernet Training] lr: 0.01000 epoch: 361/600, step: 521/521, train_loss: 0.056(0.104), train_acc: 97.500(96.294)
12/17 04:48:35 AM [Supernet Training] epoch: 361, train_loss: 0.104, train_acc: 96.294
12/17 04:48:37 AM [Supernet Validation] epoch: 361, val_loss: 0.516, val_acc: 87.250, best_acc: 87.720


12/17 04:48:37 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 98.958(98.958)
12/17 04:48:43 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 101/521, train_loss: 0.068(0.100), train_acc: 97.917(96.535)
12/17 04:48:50 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 201/521, train_loss: 0.167(0.099), train_acc: 93.750(96.595)
12/17 04:48:56 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 301/521, train_loss: 0.080(0.100), train_acc: 96.875(96.480)
12/17 04:49:03 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 401/521, train_loss: 0.114(0.099), train_acc: 95.833(96.514)
12/17 04:49:09 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 501/521, train_loss: 0.174(0.098), train_acc: 92.708(96.569)
12/17 04:49:10 AM [Supernet Training] lr: 0.00996 epoch: 362/600, step: 521/521, train_loss: 0.031(0.097), train_acc: 98.750(96.594)
12/17 04:49:10 AM [Supernet Training] epoch: 362, train_loss: 0.097, train_acc: 96.594
12/17 04:49:12 AM [Supernet Validation] epoch: 362, val_loss: 0.520, val_acc: 87.060, best_acc: 87.720


12/17 04:49:12 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 001/521, train_loss: 0.143(0.143), train_acc: 95.833(95.833)
12/17 04:49:18 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 101/521, train_loss: 0.076(0.098), train_acc: 96.875(96.411)
12/17 04:49:25 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 201/521, train_loss: 0.061(0.094), train_acc: 98.958(96.580)
12/17 04:49:31 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 301/521, train_loss: 0.091(0.095), train_acc: 96.875(96.550)
12/17 04:49:37 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 401/521, train_loss: 0.111(0.096), train_acc: 96.875(96.470)
12/17 04:49:44 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 501/521, train_loss: 0.039(0.095), train_acc: 97.917(96.505)
12/17 04:49:45 AM [Supernet Training] lr: 0.00992 epoch: 363/600, step: 521/521, train_loss: 0.123(0.096), train_acc: 96.250(96.488)
12/17 04:49:45 AM [Supernet Training] epoch: 363, train_loss: 0.096, train_acc: 96.488
12/17 04:49:47 AM [Supernet Validation] epoch: 363, val_loss: 0.526, val_acc: 86.940, best_acc: 87.720


12/17 04:49:47 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 001/521, train_loss: 0.094(0.094), train_acc: 96.875(96.875)
12/17 04:49:53 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 101/521, train_loss: 0.095(0.096), train_acc: 95.833(96.658)
12/17 04:50:00 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 201/521, train_loss: 0.085(0.100), train_acc: 94.792(96.424)
12/17 04:50:06 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 301/521, train_loss: 0.106(0.096), train_acc: 96.875(96.550)
12/17 04:50:12 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 401/521, train_loss: 0.247(0.099), train_acc: 91.667(96.444)
12/17 04:50:19 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 501/521, train_loss: 0.038(0.099), train_acc: 100.000(96.438)
12/17 04:50:20 AM [Supernet Training] lr: 0.00988 epoch: 364/600, step: 521/521, train_loss: 0.065(0.100), train_acc: 96.250(96.422)
12/17 04:50:20 AM [Supernet Training] epoch: 364, train_loss: 0.100, train_acc: 96.422
12/17 04:50:22 AM [Supernet Validation] epoch: 364, val_loss: 0.529, val_acc: 86.960, best_acc: 87.720


12/17 04:50:22 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 95.833(95.833)
12/17 04:50:28 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 101/521, train_loss: 0.116(0.100), train_acc: 94.792(96.566)
12/17 04:50:35 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 201/521, train_loss: 0.139(0.098), train_acc: 95.833(96.585)
12/17 04:50:41 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 301/521, train_loss: 0.159(0.097), train_acc: 93.750(96.581)
12/17 04:50:47 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 401/521, train_loss: 0.111(0.097), train_acc: 97.917(96.542)
12/17 04:50:54 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 501/521, train_loss: 0.066(0.098), train_acc: 96.875(96.492)
12/17 04:50:55 AM [Supernet Training] lr: 0.00983 epoch: 365/600, step: 521/521, train_loss: 0.066(0.099), train_acc: 96.250(96.486)
12/17 04:50:55 AM [Supernet Training] epoch: 365, train_loss: 0.099, train_acc: 96.486
12/17 04:50:57 AM [Supernet Validation] epoch: 365, val_loss: 0.526, val_acc: 87.040, best_acc: 87.720


12/17 04:50:57 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 001/521, train_loss: 0.112(0.112), train_acc: 95.833(95.833)
12/17 04:51:03 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 101/521, train_loss: 0.115(0.089), train_acc: 94.792(96.700)
12/17 04:51:10 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 201/521, train_loss: 0.060(0.094), train_acc: 96.875(96.600)
12/17 04:51:16 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 301/521, train_loss: 0.058(0.096), train_acc: 96.875(96.477)
12/17 04:51:22 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 401/521, train_loss: 0.133(0.098), train_acc: 96.875(96.449)
12/17 04:51:29 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 501/521, train_loss: 0.092(0.098), train_acc: 95.833(96.455)
12/17 04:51:30 AM [Supernet Training] lr: 0.00979 epoch: 366/600, step: 521/521, train_loss: 0.031(0.097), train_acc: 100.000(96.468)
12/17 04:51:30 AM [Supernet Training] epoch: 366, train_loss: 0.097, train_acc: 96.468
12/17 04:51:32 AM [Supernet Validation] epoch: 366, val_loss: 0.507, val_acc: 87.580, best_acc: 87.720


12/17 04:51:32 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 001/521, train_loss: 0.080(0.080), train_acc: 97.917(97.917)
12/17 04:51:39 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 101/521, train_loss: 0.122(0.094), train_acc: 95.833(96.462)
12/17 04:51:45 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 201/521, train_loss: 0.130(0.094), train_acc: 93.750(96.533)
12/17 04:51:51 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 301/521, train_loss: 0.089(0.095), train_acc: 97.917(96.522)
12/17 04:51:58 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 401/521, train_loss: 0.119(0.096), train_acc: 96.875(96.563)
12/17 04:52:04 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 501/521, train_loss: 0.111(0.095), train_acc: 93.750(96.530)
12/17 04:52:05 AM [Supernet Training] lr: 0.00975 epoch: 367/600, step: 521/521, train_loss: 0.026(0.096), train_acc: 100.000(96.524)
12/17 04:52:05 AM [Supernet Training] epoch: 367, train_loss: 0.096, train_acc: 96.524
12/17 04:52:07 AM [Supernet Validation] epoch: 367, val_loss: 0.509, val_acc: 87.280, best_acc: 87.720


12/17 04:52:07 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 001/521, train_loss: 0.092(0.092), train_acc: 96.875(96.875)
12/17 04:52:13 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 101/521, train_loss: 0.105(0.093), train_acc: 94.792(96.844)
12/17 04:52:20 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 201/521, train_loss: 0.078(0.094), train_acc: 95.833(96.699)
12/17 04:52:26 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 301/521, train_loss: 0.091(0.094), train_acc: 96.875(96.695)
12/17 04:52:32 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 401/521, train_loss: 0.041(0.094), train_acc: 98.958(96.678)
12/17 04:52:39 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 501/521, train_loss: 0.075(0.094), train_acc: 97.917(96.653)
12/17 04:52:40 AM [Supernet Training] lr: 0.00971 epoch: 368/600, step: 521/521, train_loss: 0.080(0.094), train_acc: 96.250(96.638)
12/17 04:52:40 AM [Supernet Training] epoch: 368, train_loss: 0.094, train_acc: 96.638
12/17 04:52:41 AM [Supernet Validation] epoch: 368, val_loss: 0.520, val_acc: 87.250, best_acc: 87.720


12/17 04:52:42 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 001/521, train_loss: 0.082(0.082), train_acc: 97.917(97.917)
12/17 04:52:48 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 101/521, train_loss: 0.111(0.097), train_acc: 94.792(96.586)
12/17 04:52:55 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 201/521, train_loss: 0.202(0.098), train_acc: 93.750(96.517)
12/17 04:53:01 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 301/521, train_loss: 0.169(0.096), train_acc: 94.792(96.612)
12/17 04:53:08 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 401/521, train_loss: 0.149(0.096), train_acc: 94.792(96.633)
12/17 04:53:14 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 501/521, train_loss: 0.156(0.096), train_acc: 92.708(96.565)
12/17 04:53:15 AM [Supernet Training] lr: 0.00967 epoch: 369/600, step: 521/521, train_loss: 0.060(0.096), train_acc: 98.750(96.588)
12/17 04:53:15 AM [Supernet Training] epoch: 369, train_loss: 0.096, train_acc: 96.588
12/17 04:53:17 AM [Supernet Validation] epoch: 369, val_loss: 0.524, val_acc: 87.010, best_acc: 87.720


12/17 04:53:17 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 001/521, train_loss: 0.079(0.079), train_acc: 96.875(96.875)
12/17 04:53:23 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 101/521, train_loss: 0.130(0.095), train_acc: 93.750(96.504)
12/17 04:53:30 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 201/521, train_loss: 0.085(0.094), train_acc: 96.875(96.533)
12/17 04:53:36 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 301/521, train_loss: 0.082(0.093), train_acc: 95.833(96.567)
12/17 04:53:42 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 401/521, train_loss: 0.136(0.093), train_acc: 93.750(96.600)
12/17 04:53:49 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 501/521, train_loss: 0.142(0.094), train_acc: 94.792(96.609)
12/17 04:53:50 AM [Supernet Training] lr: 0.00963 epoch: 370/600, step: 521/521, train_loss: 0.132(0.094), train_acc: 95.000(96.610)
12/17 04:53:50 AM [Supernet Training] epoch: 370, train_loss: 0.094, train_acc: 96.610
12/17 04:53:52 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:53:52 AM [Supernet Validation] epoch: 370, val_loss: 0.512, val_acc: 87.730, best_acc: 87.730


12/17 04:53:52 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 001/521, train_loss: 0.042(0.042), train_acc: 98.958(98.958)
12/17 04:53:58 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 101/521, train_loss: 0.077(0.090), train_acc: 96.875(96.968)
12/17 04:54:05 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 201/521, train_loss: 0.066(0.093), train_acc: 97.917(96.745)
12/17 04:54:11 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 301/521, train_loss: 0.065(0.095), train_acc: 96.875(96.674)
12/17 04:54:18 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 401/521, train_loss: 0.049(0.095), train_acc: 96.875(96.646)
12/17 04:54:24 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 501/521, train_loss: 0.150(0.095), train_acc: 95.833(96.628)
12/17 04:54:25 AM [Supernet Training] lr: 0.00958 epoch: 371/600, step: 521/521, train_loss: 0.042(0.095), train_acc: 97.500(96.648)
12/17 04:54:25 AM [Supernet Training] epoch: 371, train_loss: 0.095, train_acc: 96.648
12/17 04:54:27 AM [Supernet Validation] epoch: 371, val_loss: 0.509, val_acc: 87.300, best_acc: 87.730


12/17 04:54:27 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 001/521, train_loss: 0.071(0.071), train_acc: 97.917(97.917)
12/17 04:54:34 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 101/521, train_loss: 0.117(0.092), train_acc: 92.708(96.669)
12/17 04:54:40 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 201/521, train_loss: 0.024(0.091), train_acc: 98.958(96.787)
12/17 04:54:46 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 301/521, train_loss: 0.070(0.090), train_acc: 97.917(96.823)
12/17 04:54:52 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 401/521, train_loss: 0.073(0.090), train_acc: 97.917(96.789)
12/17 04:54:59 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 501/521, train_loss: 0.035(0.091), train_acc: 98.958(96.754)
12/17 04:55:00 AM [Supernet Training] lr: 0.00954 epoch: 372/600, step: 521/521, train_loss: 0.071(0.091), train_acc: 98.750(96.752)
12/17 04:55:00 AM [Supernet Training] epoch: 372, train_loss: 0.091, train_acc: 96.752
12/17 04:55:01 AM [Supernet Validation] epoch: 372, val_loss: 0.517, val_acc: 87.280, best_acc: 87.730


12/17 04:55:02 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 001/521, train_loss: 0.119(0.119), train_acc: 94.792(94.792)
12/17 04:55:08 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 101/521, train_loss: 0.066(0.096), train_acc: 97.917(96.524)
12/17 04:55:14 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 201/521, train_loss: 0.087(0.093), train_acc: 97.917(96.631)
12/17 04:55:21 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 301/521, train_loss: 0.082(0.093), train_acc: 96.875(96.640)
12/17 04:55:27 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 401/521, train_loss: 0.118(0.096), train_acc: 94.792(96.535)
12/17 04:55:33 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 501/521, train_loss: 0.066(0.096), train_acc: 95.833(96.538)
12/17 04:55:35 AM [Supernet Training] lr: 0.00950 epoch: 373/600, step: 521/521, train_loss: 0.118(0.096), train_acc: 95.000(96.536)
12/17 04:55:35 AM [Supernet Training] epoch: 373, train_loss: 0.096, train_acc: 96.536
12/17 04:55:36 AM [Supernet Validation] epoch: 373, val_loss: 0.523, val_acc: 87.300, best_acc: 87.730


12/17 04:55:37 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 001/521, train_loss: 0.081(0.081), train_acc: 97.917(97.917)
12/17 04:55:43 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 101/521, train_loss: 0.076(0.090), train_acc: 98.958(96.875)
12/17 04:55:50 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 201/521, train_loss: 0.033(0.092), train_acc: 98.958(96.777)
12/17 04:55:56 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 301/521, train_loss: 0.108(0.093), train_acc: 96.875(96.792)
12/17 04:56:02 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 401/521, train_loss: 0.051(0.094), train_acc: 97.917(96.766)
12/17 04:56:09 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 501/521, train_loss: 0.030(0.093), train_acc: 100.000(96.763)
12/17 04:56:10 AM [Supernet Training] lr: 0.00946 epoch: 374/600, step: 521/521, train_loss: 0.084(0.093), train_acc: 96.250(96.750)
12/17 04:56:10 AM [Supernet Training] epoch: 374, train_loss: 0.093, train_acc: 96.750
12/17 04:56:12 AM [Supernet Validation] epoch: 374, val_loss: 0.505, val_acc: 87.400, best_acc: 87.730


12/17 04:56:12 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 001/521, train_loss: 0.069(0.069), train_acc: 96.875(96.875)
12/17 04:56:18 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 101/521, train_loss: 0.031(0.087), train_acc: 100.000(96.834)
12/17 04:56:25 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 201/521, train_loss: 0.055(0.088), train_acc: 97.917(96.797)
12/17 04:56:31 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 301/521, train_loss: 0.123(0.089), train_acc: 94.792(96.771)
12/17 04:56:37 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 401/521, train_loss: 0.101(0.090), train_acc: 94.792(96.761)
12/17 04:56:44 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 501/521, train_loss: 0.154(0.091), train_acc: 94.792(96.684)
12/17 04:56:45 AM [Supernet Training] lr: 0.00942 epoch: 375/600, step: 521/521, train_loss: 0.083(0.091), train_acc: 97.500(96.682)
12/17 04:56:45 AM [Supernet Training] epoch: 375, train_loss: 0.091, train_acc: 96.682
12/17 04:56:47 AM [Supernet Validation] epoch: 375, val_loss: 0.536, val_acc: 87.230, best_acc: 87.730


12/17 04:56:47 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 001/521, train_loss: 0.140(0.140), train_acc: 95.833(95.833)
12/17 04:56:53 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 101/521, train_loss: 0.056(0.085), train_acc: 97.917(97.061)
12/17 04:57:00 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 201/521, train_loss: 0.070(0.087), train_acc: 97.917(97.030)
12/17 04:57:06 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 301/521, train_loss: 0.051(0.088), train_acc: 100.000(96.923)
12/17 04:57:13 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 401/521, train_loss: 0.078(0.087), train_acc: 97.917(96.956)
12/17 04:57:19 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 501/521, train_loss: 0.115(0.089), train_acc: 93.750(96.863)
12/17 04:57:20 AM [Supernet Training] lr: 0.00938 epoch: 376/600, step: 521/521, train_loss: 0.207(0.089), train_acc: 90.000(96.842)
12/17 04:57:20 AM [Supernet Training] epoch: 376, train_loss: 0.089, train_acc: 96.842
12/17 04:57:22 AM [Supernet Validation] epoch: 376, val_loss: 0.512, val_acc: 87.510, best_acc: 87.730


12/17 04:57:22 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 001/521, train_loss: 0.177(0.177), train_acc: 93.750(93.750)
12/17 04:57:29 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 101/521, train_loss: 0.128(0.093), train_acc: 97.917(96.669)
12/17 04:57:35 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 201/521, train_loss: 0.084(0.093), train_acc: 98.958(96.673)
12/17 04:57:41 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 301/521, train_loss: 0.082(0.092), train_acc: 96.875(96.660)
12/17 04:57:48 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 401/521, train_loss: 0.161(0.092), train_acc: 94.792(96.654)
12/17 04:57:54 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 501/521, train_loss: 0.042(0.092), train_acc: 98.958(96.628)
12/17 04:57:55 AM [Supernet Training] lr: 0.00933 epoch: 377/600, step: 521/521, train_loss: 0.114(0.092), train_acc: 96.250(96.630)
12/17 04:57:55 AM [Supernet Training] epoch: 377, train_loss: 0.092, train_acc: 96.630
12/17 04:57:57 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 04:57:57 AM [Supernet Validation] epoch: 377, val_loss: 0.515, val_acc: 87.880, best_acc: 87.880


12/17 04:57:57 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 001/521, train_loss: 0.128(0.128), train_acc: 94.792(94.792)
12/17 04:58:04 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 101/521, train_loss: 0.111(0.088), train_acc: 93.750(96.854)
12/17 04:58:10 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 201/521, train_loss: 0.053(0.092), train_acc: 98.958(96.725)
12/17 04:58:17 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 301/521, train_loss: 0.049(0.092), train_acc: 98.958(96.705)
12/17 04:58:23 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 401/521, train_loss: 0.076(0.091), train_acc: 97.917(96.732)
12/17 04:58:29 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 501/521, train_loss: 0.080(0.090), train_acc: 95.833(96.742)
12/17 04:58:31 AM [Supernet Training] lr: 0.00929 epoch: 378/600, step: 521/521, train_loss: 0.092(0.090), train_acc: 95.000(96.748)
12/17 04:58:31 AM [Supernet Training] epoch: 378, train_loss: 0.090, train_acc: 96.748
12/17 04:58:32 AM [Supernet Validation] epoch: 378, val_loss: 0.502, val_acc: 87.790, best_acc: 87.880


12/17 04:58:33 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 001/521, train_loss: 0.089(0.089), train_acc: 97.917(97.917)
12/17 04:58:39 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 101/521, train_loss: 0.061(0.084), train_acc: 98.958(97.174)
12/17 04:58:46 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 201/521, train_loss: 0.041(0.089), train_acc: 98.958(96.844)
12/17 04:58:52 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 301/521, train_loss: 0.098(0.090), train_acc: 95.833(96.847)
12/17 04:58:58 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 401/521, train_loss: 0.099(0.091), train_acc: 97.917(96.797)
12/17 04:59:05 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 501/521, train_loss: 0.151(0.090), train_acc: 94.792(96.817)
12/17 04:59:06 AM [Supernet Training] lr: 0.00925 epoch: 379/600, step: 521/521, train_loss: 0.036(0.091), train_acc: 98.750(96.792)
12/17 04:59:06 AM [Supernet Training] epoch: 379, train_loss: 0.091, train_acc: 96.792
12/17 04:59:07 AM [Supernet Validation] epoch: 379, val_loss: 0.522, val_acc: 87.180, best_acc: 87.880


12/17 04:59:08 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 001/521, train_loss: 0.089(0.089), train_acc: 95.833(95.833)
12/17 04:59:14 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 101/521, train_loss: 0.026(0.083), train_acc: 98.958(97.277)
12/17 04:59:21 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 201/521, train_loss: 0.049(0.084), train_acc: 97.917(97.212)
12/17 04:59:27 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 301/521, train_loss: 0.028(0.086), train_acc: 98.958(97.055)
12/17 04:59:34 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 401/521, train_loss: 0.039(0.086), train_acc: 98.958(96.994)
12/17 04:59:40 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 501/521, train_loss: 0.076(0.086), train_acc: 95.833(96.958)
12/17 04:59:41 AM [Supernet Training] lr: 0.00921 epoch: 380/600, step: 521/521, train_loss: 0.091(0.086), train_acc: 96.250(96.952)
12/17 04:59:41 AM [Supernet Training] epoch: 380, train_loss: 0.086, train_acc: 96.952
12/17 04:59:43 AM [Supernet Validation] epoch: 380, val_loss: 0.510, val_acc: 87.490, best_acc: 87.880


12/17 04:59:43 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 98.958(98.958)
12/17 04:59:50 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 101/521, train_loss: 0.044(0.083), train_acc: 98.958(97.153)
12/17 04:59:56 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 201/521, train_loss: 0.033(0.084), train_acc: 98.958(97.046)
12/17 05:00:03 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 301/521, train_loss: 0.106(0.088), train_acc: 96.875(96.844)
12/17 05:00:09 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 401/521, train_loss: 0.105(0.088), train_acc: 95.833(96.826)
12/17 05:00:15 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 501/521, train_loss: 0.087(0.089), train_acc: 96.875(96.786)
12/17 05:00:17 AM [Supernet Training] lr: 0.00917 epoch: 381/600, step: 521/521, train_loss: 0.076(0.089), train_acc: 97.500(96.774)
12/17 05:00:17 AM [Supernet Training] epoch: 381, train_loss: 0.089, train_acc: 96.774
12/17 05:00:18 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 05:00:18 AM [Supernet Validation] epoch: 381, val_loss: 0.497, val_acc: 87.990, best_acc: 87.990


12/17 05:00:19 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 001/521, train_loss: 0.050(0.050), train_acc: 98.958(98.958)
12/17 05:00:25 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 101/521, train_loss: 0.166(0.088), train_acc: 94.792(96.772)
12/17 05:00:32 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 201/521, train_loss: 0.091(0.085), train_acc: 97.917(96.916)
12/17 05:00:38 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 301/521, train_loss: 0.018(0.087), train_acc: 100.000(96.872)
12/17 05:00:44 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 401/521, train_loss: 0.104(0.085), train_acc: 94.792(96.943)
12/17 05:00:51 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 501/521, train_loss: 0.077(0.086), train_acc: 96.875(96.933)
12/17 05:00:52 AM [Supernet Training] lr: 0.00912 epoch: 382/600, step: 521/521, train_loss: 0.118(0.087), train_acc: 92.500(96.904)
12/17 05:00:52 AM [Supernet Training] epoch: 382, train_loss: 0.087, train_acc: 96.904
12/17 05:00:53 AM [Supernet Validation] epoch: 382, val_loss: 0.503, val_acc: 87.740, best_acc: 87.990


12/17 05:00:54 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 001/521, train_loss: 0.071(0.071), train_acc: 96.875(96.875)
12/17 05:01:00 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 101/521, train_loss: 0.152(0.077), train_acc: 95.833(97.246)
12/17 05:01:06 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 201/521, train_loss: 0.120(0.084), train_acc: 94.792(96.984)
12/17 05:01:13 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 301/521, train_loss: 0.041(0.085), train_acc: 98.958(96.968)
12/17 05:01:19 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 401/521, train_loss: 0.065(0.086), train_acc: 98.958(96.893)
12/17 05:01:25 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 501/521, train_loss: 0.079(0.086), train_acc: 96.875(96.875)
12/17 05:01:27 AM [Supernet Training] lr: 0.00908 epoch: 383/600, step: 521/521, train_loss: 0.110(0.087), train_acc: 96.250(96.854)
12/17 05:01:27 AM [Supernet Training] epoch: 383, train_loss: 0.087, train_acc: 96.854
12/17 05:01:28 AM [Supernet Validation] epoch: 383, val_loss: 0.521, val_acc: 87.280, best_acc: 87.990


12/17 05:01:29 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 001/521, train_loss: 0.085(0.085), train_acc: 96.875(96.875)
12/17 05:01:35 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 101/521, train_loss: 0.042(0.084), train_acc: 97.917(96.978)
12/17 05:01:41 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 201/521, train_loss: 0.036(0.085), train_acc: 98.958(96.968)
12/17 05:01:48 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 301/521, train_loss: 0.081(0.086), train_acc: 96.875(96.934)
12/17 05:01:54 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 401/521, train_loss: 0.061(0.087), train_acc: 97.917(96.857)
12/17 05:02:00 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 501/521, train_loss: 0.063(0.087), train_acc: 96.875(96.875)
12/17 05:02:02 AM [Supernet Training] lr: 0.00904 epoch: 384/600, step: 521/521, train_loss: 0.064(0.087), train_acc: 97.500(96.862)
12/17 05:02:02 AM [Supernet Training] epoch: 384, train_loss: 0.087, train_acc: 96.862
12/17 05:02:03 AM [Supernet Validation] epoch: 384, val_loss: 0.497, val_acc: 87.600, best_acc: 87.990


12/17 05:02:04 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 96.875(96.875)
12/17 05:02:10 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 101/521, train_loss: 0.046(0.087), train_acc: 98.958(96.937)
12/17 05:02:16 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 201/521, train_loss: 0.063(0.084), train_acc: 98.958(97.041)
12/17 05:02:23 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 301/521, train_loss: 0.080(0.085), train_acc: 98.958(97.058)
12/17 05:02:29 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 401/521, train_loss: 0.090(0.086), train_acc: 96.875(96.958)
12/17 05:02:35 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 501/521, train_loss: 0.032(0.087), train_acc: 98.958(96.908)
12/17 05:02:37 AM [Supernet Training] lr: 0.00900 epoch: 385/600, step: 521/521, train_loss: 0.198(0.087), train_acc: 93.750(96.918)
12/17 05:02:37 AM [Supernet Training] epoch: 385, train_loss: 0.087, train_acc: 96.918
12/17 05:02:38 AM [Supernet Validation] epoch: 385, val_loss: 0.524, val_acc: 87.250, best_acc: 87.990


12/17 05:02:38 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 001/521, train_loss: 0.148(0.148), train_acc: 93.750(93.750)
12/17 05:02:45 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 101/521, train_loss: 0.063(0.088), train_acc: 96.875(96.927)
12/17 05:02:51 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 201/521, train_loss: 0.038(0.088), train_acc: 98.958(96.999)
12/17 05:02:58 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 301/521, train_loss: 0.027(0.087), train_acc: 100.000(96.948)
12/17 05:03:04 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 401/521, train_loss: 0.106(0.088), train_acc: 93.750(96.865)
12/17 05:03:10 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 501/521, train_loss: 0.094(0.089), train_acc: 94.792(96.825)
12/17 05:03:11 AM [Supernet Training] lr: 0.00896 epoch: 386/600, step: 521/521, train_loss: 0.123(0.089), train_acc: 93.750(96.828)
12/17 05:03:11 AM [Supernet Training] epoch: 386, train_loss: 0.089, train_acc: 96.828
12/17 05:03:13 AM [Supernet Validation] epoch: 386, val_loss: 0.517, val_acc: 86.960, best_acc: 87.990


12/17 05:03:13 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 001/521, train_loss: 0.065(0.065), train_acc: 98.958(98.958)
12/17 05:03:20 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 101/521, train_loss: 0.120(0.082), train_acc: 92.708(97.277)
12/17 05:03:26 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 201/521, train_loss: 0.037(0.083), train_acc: 98.958(97.150)
12/17 05:03:32 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 301/521, train_loss: 0.043(0.085), train_acc: 97.917(96.965)
12/17 05:03:39 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 401/521, train_loss: 0.100(0.086), train_acc: 95.833(96.956)
12/17 05:03:45 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 501/521, train_loss: 0.109(0.086), train_acc: 94.792(96.950)
12/17 05:03:46 AM [Supernet Training] lr: 0.00892 epoch: 387/600, step: 521/521, train_loss: 0.081(0.085), train_acc: 95.000(96.976)
12/17 05:03:46 AM [Supernet Training] epoch: 387, train_loss: 0.085, train_acc: 96.976
12/17 05:03:48 AM [Supernet Validation] epoch: 387, val_loss: 0.517, val_acc: 87.340, best_acc: 87.990


12/17 05:03:48 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 98.958(98.958)
12/17 05:03:54 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 101/521, train_loss: 0.056(0.075), train_acc: 98.958(97.236)
12/17 05:04:01 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 201/521, train_loss: 0.042(0.078), train_acc: 98.958(97.155)
12/17 05:04:07 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 301/521, train_loss: 0.081(0.078), train_acc: 95.833(97.110)
12/17 05:04:13 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 401/521, train_loss: 0.115(0.080), train_acc: 93.750(97.067)
12/17 05:04:20 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 501/521, train_loss: 0.066(0.082), train_acc: 96.875(96.983)
12/17 05:04:21 AM [Supernet Training] lr: 0.00887 epoch: 388/600, step: 521/521, train_loss: 0.062(0.082), train_acc: 98.750(96.986)
12/17 05:04:21 AM [Supernet Training] epoch: 388, train_loss: 0.082, train_acc: 96.986
12/17 05:04:23 AM [Supernet Validation] epoch: 388, val_loss: 0.509, val_acc: 87.550, best_acc: 87.990


12/17 05:04:23 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 001/521, train_loss: 0.149(0.149), train_acc: 92.708(92.708)
12/17 05:04:29 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 101/521, train_loss: 0.137(0.083), train_acc: 93.750(96.999)
12/17 05:04:36 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 201/521, train_loss: 0.055(0.084), train_acc: 97.917(96.963)
12/17 05:04:42 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 301/521, train_loss: 0.151(0.086), train_acc: 95.833(96.861)
12/17 05:04:48 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 401/521, train_loss: 0.135(0.086), train_acc: 96.875(96.906)
12/17 05:04:55 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 501/521, train_loss: 0.072(0.087), train_acc: 97.917(96.929)
12/17 05:04:56 AM [Supernet Training] lr: 0.00883 epoch: 389/600, step: 521/521, train_loss: 0.025(0.087), train_acc: 100.000(96.916)
12/17 05:04:56 AM [Supernet Training] epoch: 389, train_loss: 0.087, train_acc: 96.916
12/17 05:04:58 AM [Supernet Validation] epoch: 389, val_loss: 0.524, val_acc: 87.620, best_acc: 87.990


12/17 05:04:58 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 001/521, train_loss: 0.059(0.059), train_acc: 97.917(97.917)
12/17 05:05:04 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 101/521, train_loss: 0.081(0.081), train_acc: 96.875(97.102)
12/17 05:05:11 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 201/521, train_loss: 0.050(0.082), train_acc: 97.917(96.994)
12/17 05:05:17 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 301/521, train_loss: 0.058(0.083), train_acc: 96.875(96.993)
12/17 05:05:24 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 401/521, train_loss: 0.042(0.084), train_acc: 98.958(96.984)
12/17 05:05:30 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 501/521, train_loss: 0.064(0.084), train_acc: 97.917(96.979)
12/17 05:05:31 AM [Supernet Training] lr: 0.00879 epoch: 390/600, step: 521/521, train_loss: 0.049(0.084), train_acc: 98.750(96.994)
12/17 05:05:31 AM [Supernet Training] epoch: 390, train_loss: 0.084, train_acc: 96.994
12/17 05:05:33 AM [Supernet Validation] epoch: 390, val_loss: 0.536, val_acc: 87.140, best_acc: 87.990


12/17 05:05:33 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 98.958(98.958)
12/17 05:05:39 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 101/521, train_loss: 0.064(0.079), train_acc: 96.875(97.308)
12/17 05:05:46 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 201/521, train_loss: 0.083(0.079), train_acc: 95.833(97.222)
12/17 05:05:52 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 301/521, train_loss: 0.142(0.082), train_acc: 96.875(97.186)
12/17 05:05:59 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 401/521, train_loss: 0.063(0.082), train_acc: 96.875(97.135)
12/17 05:06:05 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 501/521, train_loss: 0.121(0.082), train_acc: 93.750(97.120)
12/17 05:06:06 AM [Supernet Training] lr: 0.00875 epoch: 391/600, step: 521/521, train_loss: 0.152(0.082), train_acc: 93.750(97.106)
12/17 05:06:06 AM [Supernet Training] epoch: 391, train_loss: 0.082, train_acc: 97.106
12/17 05:06:08 AM [Supernet Validation] epoch: 391, val_loss: 0.521, val_acc: 87.390, best_acc: 87.990


12/17 05:06:08 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 001/521, train_loss: 0.040(0.040), train_acc: 97.917(97.917)
12/17 05:06:14 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 101/521, train_loss: 0.027(0.078), train_acc: 98.958(97.236)
12/17 05:06:21 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 201/521, train_loss: 0.063(0.081), train_acc: 96.875(97.030)
12/17 05:06:27 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 301/521, train_loss: 0.060(0.083), train_acc: 98.958(96.979)
12/17 05:06:33 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 401/521, train_loss: 0.047(0.083), train_acc: 98.958(97.028)
12/17 05:06:40 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 501/521, train_loss: 0.030(0.083), train_acc: 98.958(97.039)
12/17 05:06:41 AM [Supernet Training] lr: 0.00871 epoch: 392/600, step: 521/521, train_loss: 0.021(0.083), train_acc: 98.750(97.028)
12/17 05:06:41 AM [Supernet Training] epoch: 392, train_loss: 0.083, train_acc: 97.028
12/17 05:06:42 AM [Supernet Validation] epoch: 392, val_loss: 0.506, val_acc: 87.440, best_acc: 87.990


12/17 05:06:43 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 001/521, train_loss: 0.074(0.074), train_acc: 96.875(96.875)
12/17 05:06:49 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 101/521, train_loss: 0.083(0.082), train_acc: 97.917(97.092)
12/17 05:06:55 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 201/521, train_loss: 0.108(0.082), train_acc: 94.792(97.139)
12/17 05:07:02 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 301/521, train_loss: 0.021(0.082), train_acc: 100.000(97.159)
12/17 05:07:09 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 401/521, train_loss: 0.141(0.082), train_acc: 93.750(97.111)
12/17 05:07:15 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 501/521, train_loss: 0.070(0.082), train_acc: 97.917(97.100)
12/17 05:07:16 AM [Supernet Training] lr: 0.00867 epoch: 393/600, step: 521/521, train_loss: 0.032(0.083), train_acc: 98.750(97.044)
12/17 05:07:16 AM [Supernet Training] epoch: 393, train_loss: 0.083, train_acc: 97.044
12/17 05:07:18 AM [Supernet Validation] epoch: 393, val_loss: 0.531, val_acc: 87.170, best_acc: 87.990


12/17 05:07:18 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 001/521, train_loss: 0.124(0.124), train_acc: 93.750(93.750)
12/17 05:07:24 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 101/521, train_loss: 0.077(0.082), train_acc: 97.917(97.071)
12/17 05:07:31 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 201/521, train_loss: 0.094(0.082), train_acc: 96.875(97.129)
12/17 05:07:37 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 301/521, train_loss: 0.035(0.082), train_acc: 97.917(97.141)
12/17 05:07:43 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 401/521, train_loss: 0.098(0.081), train_acc: 95.833(97.117)
12/17 05:07:50 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 501/521, train_loss: 0.110(0.080), train_acc: 97.917(97.143)
12/17 05:07:51 AM [Supernet Training] lr: 0.00862 epoch: 394/600, step: 521/521, train_loss: 0.062(0.080), train_acc: 97.500(97.152)
12/17 05:07:51 AM [Supernet Training] epoch: 394, train_loss: 0.080, train_acc: 97.152
12/17 05:07:52 AM [Supernet Validation] epoch: 394, val_loss: 0.526, val_acc: 87.570, best_acc: 87.990


12/17 05:07:53 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 001/521, train_loss: 0.051(0.051), train_acc: 98.958(98.958)
12/17 05:07:59 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 101/521, train_loss: 0.036(0.078), train_acc: 100.000(97.360)
12/17 05:08:05 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 201/521, train_loss: 0.045(0.079), train_acc: 98.958(97.227)
12/17 05:08:12 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 301/521, train_loss: 0.066(0.078), train_acc: 97.917(97.273)
12/17 05:08:18 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 401/521, train_loss: 0.072(0.080), train_acc: 97.917(97.210)
12/17 05:08:25 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 501/521, train_loss: 0.075(0.082), train_acc: 95.833(97.156)
12/17 05:08:26 AM [Supernet Training] lr: 0.00858 epoch: 395/600, step: 521/521, train_loss: 0.073(0.082), train_acc: 98.750(97.132)
12/17 05:08:26 AM [Supernet Training] epoch: 395, train_loss: 0.082, train_acc: 97.132
12/17 05:08:27 AM [Supernet Validation] epoch: 395, val_loss: 0.521, val_acc: 87.490, best_acc: 87.990


12/17 05:08:28 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 95.833(95.833)
12/17 05:08:35 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 101/521, train_loss: 0.136(0.085), train_acc: 96.875(97.195)
12/17 05:08:41 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 201/521, train_loss: 0.116(0.081), train_acc: 95.833(97.227)
12/17 05:08:47 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 301/521, train_loss: 0.047(0.083), train_acc: 98.958(97.093)
12/17 05:08:54 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 401/521, train_loss: 0.041(0.083), train_acc: 98.958(97.075)
12/17 05:09:01 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 501/521, train_loss: 0.041(0.082), train_acc: 98.958(97.091)
12/17 05:09:02 AM [Supernet Training] lr: 0.00854 epoch: 396/600, step: 521/521, train_loss: 0.039(0.082), train_acc: 98.750(97.084)
12/17 05:09:02 AM [Supernet Training] epoch: 396, train_loss: 0.082, train_acc: 97.084
12/17 05:09:03 AM [Supernet Validation] epoch: 396, val_loss: 0.516, val_acc: 87.540, best_acc: 87.990


12/17 05:09:04 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 001/521, train_loss: 0.074(0.074), train_acc: 96.875(96.875)
12/17 05:09:10 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 101/521, train_loss: 0.052(0.080), train_acc: 96.875(97.205)
12/17 05:09:17 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 201/521, train_loss: 0.171(0.083), train_acc: 92.708(96.942)
12/17 05:09:23 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 301/521, train_loss: 0.172(0.081), train_acc: 94.792(97.038)
12/17 05:09:29 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 401/521, train_loss: 0.141(0.079), train_acc: 93.750(97.148)
12/17 05:09:36 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 501/521, train_loss: 0.099(0.079), train_acc: 97.917(97.137)
12/17 05:09:37 AM [Supernet Training] lr: 0.00850 epoch: 397/600, step: 521/521, train_loss: 0.064(0.080), train_acc: 98.750(97.142)
12/17 05:09:37 AM [Supernet Training] epoch: 397, train_loss: 0.080, train_acc: 97.142
12/17 05:09:38 AM [Supernet Validation] epoch: 397, val_loss: 0.523, val_acc: 87.810, best_acc: 87.990


12/17 05:09:39 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 001/521, train_loss: 0.076(0.076), train_acc: 96.875(96.875)
12/17 05:09:45 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 101/521, train_loss: 0.121(0.076), train_acc: 97.917(97.401)
12/17 05:09:52 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 201/521, train_loss: 0.038(0.079), train_acc: 98.958(97.186)
12/17 05:09:58 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 301/521, train_loss: 0.119(0.080), train_acc: 96.875(97.190)
12/17 05:10:04 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 401/521, train_loss: 0.074(0.079), train_acc: 95.833(97.233)
12/17 05:10:11 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 501/521, train_loss: 0.055(0.079), train_acc: 97.917(97.176)
12/17 05:10:12 AM [Supernet Training] lr: 0.00846 epoch: 398/600, step: 521/521, train_loss: 0.061(0.079), train_acc: 98.750(97.194)
12/17 05:10:12 AM [Supernet Training] epoch: 398, train_loss: 0.079, train_acc: 97.194
12/17 05:10:13 AM [Supernet Validation] epoch: 398, val_loss: 0.527, val_acc: 87.720, best_acc: 87.990


12/17 05:10:14 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 001/521, train_loss: 0.063(0.063), train_acc: 95.833(95.833)
12/17 05:10:20 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 101/521, train_loss: 0.102(0.079), train_acc: 96.875(97.081)
12/17 05:10:26 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 201/521, train_loss: 0.109(0.076), train_acc: 95.833(97.227)
12/17 05:10:33 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 301/521, train_loss: 0.072(0.079), train_acc: 95.833(97.117)
12/17 05:10:39 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 401/521, train_loss: 0.153(0.080), train_acc: 95.833(97.085)
12/17 05:10:46 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 501/521, train_loss: 0.096(0.081), train_acc: 95.833(97.029)
12/17 05:10:47 AM [Supernet Training] lr: 0.00842 epoch: 399/600, step: 521/521, train_loss: 0.169(0.081), train_acc: 95.000(97.022)
12/17 05:10:47 AM [Supernet Training] epoch: 399, train_loss: 0.081, train_acc: 97.022
12/17 05:10:49 AM [Supernet Validation] epoch: 399, val_loss: 0.514, val_acc: 87.320, best_acc: 87.990


12/17 05:10:49 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 001/521, train_loss: 0.019(0.019), train_acc: 100.000(100.000)
12/17 05:10:55 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 101/521, train_loss: 0.081(0.077), train_acc: 96.875(96.999)
12/17 05:11:02 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 201/521, train_loss: 0.110(0.081), train_acc: 96.875(97.015)
12/17 05:11:08 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 301/521, train_loss: 0.102(0.080), train_acc: 97.917(97.117)
12/17 05:11:14 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 401/521, train_loss: 0.151(0.080), train_acc: 93.750(97.158)
12/17 05:11:21 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 501/521, train_loss: 0.101(0.080), train_acc: 95.833(97.166)
12/17 05:11:22 AM [Supernet Training] lr: 0.00837 epoch: 400/600, step: 521/521, train_loss: 0.148(0.080), train_acc: 93.750(97.180)
12/17 05:11:22 AM [Supernet Training] epoch: 400, train_loss: 0.080, train_acc: 97.180
12/17 05:11:24 AM [Supernet Validation] epoch: 400, val_loss: 0.513, val_acc: 87.710, best_acc: 87.990


12/17 05:11:24 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 97.917(97.917)
12/17 05:11:31 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 101/521, train_loss: 0.065(0.073), train_acc: 97.917(97.308)
12/17 05:11:37 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 201/521, train_loss: 0.035(0.078), train_acc: 98.958(97.196)
12/17 05:11:44 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 301/521, train_loss: 0.072(0.080), train_acc: 97.917(97.045)
12/17 05:11:50 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 401/521, train_loss: 0.032(0.080), train_acc: 98.958(97.088)
12/17 05:11:56 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 501/521, train_loss: 0.040(0.081), train_acc: 100.000(97.039)
12/17 05:11:58 AM [Supernet Training] lr: 0.00833 epoch: 401/600, step: 521/521, train_loss: 0.040(0.081), train_acc: 100.000(97.060)
12/17 05:11:58 AM [Supernet Training] epoch: 401, train_loss: 0.081, train_acc: 97.060
12/17 05:11:59 AM [Supernet Validation] epoch: 401, val_loss: 0.508, val_acc: 87.400, best_acc: 87.990


12/17 05:12:00 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 001/521, train_loss: 0.152(0.152), train_acc: 95.833(95.833)
12/17 05:12:06 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 101/521, train_loss: 0.059(0.077), train_acc: 96.875(97.174)
12/17 05:12:12 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 201/521, train_loss: 0.090(0.080), train_acc: 95.833(97.036)
12/17 05:12:19 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 301/521, train_loss: 0.013(0.080), train_acc: 100.000(97.076)
12/17 05:12:25 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 401/521, train_loss: 0.070(0.080), train_acc: 97.917(97.091)
12/17 05:12:32 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 501/521, train_loss: 0.102(0.080), train_acc: 93.750(97.062)
12/17 05:12:33 AM [Supernet Training] lr: 0.00829 epoch: 402/600, step: 521/521, train_loss: 0.153(0.081), train_acc: 97.500(97.058)
12/17 05:12:33 AM [Supernet Training] epoch: 402, train_loss: 0.081, train_acc: 97.058
12/17 05:12:34 AM [Supernet Validation] epoch: 402, val_loss: 0.525, val_acc: 87.490, best_acc: 87.990


12/17 05:12:35 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 001/521, train_loss: 0.059(0.059), train_acc: 97.917(97.917)
12/17 05:12:41 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 101/521, train_loss: 0.091(0.073), train_acc: 97.917(97.339)
12/17 05:12:48 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 201/521, train_loss: 0.104(0.075), train_acc: 96.875(97.300)
12/17 05:12:54 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 301/521, train_loss: 0.029(0.076), train_acc: 98.958(97.211)
12/17 05:13:00 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 401/521, train_loss: 0.029(0.074), train_acc: 98.958(97.280)
12/17 05:13:07 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 501/521, train_loss: 0.066(0.075), train_acc: 97.917(97.276)
12/17 05:13:08 AM [Supernet Training] lr: 0.00825 epoch: 403/600, step: 521/521, train_loss: 0.076(0.076), train_acc: 97.500(97.252)
12/17 05:13:08 AM [Supernet Training] epoch: 403, train_loss: 0.076, train_acc: 97.252
12/17 05:13:09 AM [Supernet Validation] epoch: 403, val_loss: 0.516, val_acc: 87.130, best_acc: 87.990


12/17 05:13:10 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 001/521, train_loss: 0.028(0.028), train_acc: 98.958(98.958)
12/17 05:13:16 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 101/521, train_loss: 0.046(0.075), train_acc: 98.958(97.401)
12/17 05:13:22 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 201/521, train_loss: 0.157(0.075), train_acc: 93.750(97.373)
12/17 05:13:29 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 301/521, train_loss: 0.102(0.076), train_acc: 96.875(97.349)
12/17 05:13:35 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 401/521, train_loss: 0.091(0.077), train_acc: 94.792(97.298)
12/17 05:13:41 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 501/521, train_loss: 0.179(0.079), train_acc: 93.750(97.222)
12/17 05:13:43 AM [Supernet Training] lr: 0.00821 epoch: 404/600, step: 521/521, train_loss: 0.089(0.079), train_acc: 95.000(97.214)
12/17 05:13:43 AM [Supernet Training] epoch: 404, train_loss: 0.079, train_acc: 97.214
12/17 05:13:44 AM [Supernet Validation] epoch: 404, val_loss: 0.527, val_acc: 87.360, best_acc: 87.990


12/17 05:13:44 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 001/521, train_loss: 0.107(0.107), train_acc: 94.792(94.792)
12/17 05:13:51 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 101/521, train_loss: 0.107(0.084), train_acc: 94.792(97.081)
12/17 05:13:57 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 201/521, train_loss: 0.069(0.079), train_acc: 96.875(97.290)
12/17 05:14:04 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 301/521, train_loss: 0.078(0.079), train_acc: 97.917(97.256)
12/17 05:14:10 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 401/521, train_loss: 0.104(0.079), train_acc: 96.875(97.265)
12/17 05:14:16 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 501/521, train_loss: 0.066(0.077), train_acc: 97.917(97.301)
12/17 05:14:18 AM [Supernet Training] lr: 0.00817 epoch: 405/600, step: 521/521, train_loss: 0.037(0.077), train_acc: 98.750(97.302)
12/17 05:14:18 AM [Supernet Training] epoch: 405, train_loss: 0.077, train_acc: 97.302
12/17 05:14:19 AM [Supernet Validation] epoch: 405, val_loss: 0.532, val_acc: 87.430, best_acc: 87.990


12/17 05:14:20 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 001/521, train_loss: 0.047(0.047), train_acc: 98.958(98.958)
12/17 05:14:26 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 101/521, train_loss: 0.109(0.078), train_acc: 96.875(97.215)
12/17 05:14:32 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 201/521, train_loss: 0.033(0.075), train_acc: 98.958(97.207)
12/17 05:14:39 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 301/521, train_loss: 0.101(0.076), train_acc: 96.875(97.197)
12/17 05:14:45 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 401/521, train_loss: 0.095(0.075), train_acc: 97.917(97.301)
12/17 05:14:52 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 501/521, train_loss: 0.060(0.075), train_acc: 98.958(97.280)
12/17 05:14:53 AM [Supernet Training] lr: 0.00812 epoch: 406/600, step: 521/521, train_loss: 0.125(0.076), train_acc: 95.000(97.270)
12/17 05:14:53 AM [Supernet Training] epoch: 406, train_loss: 0.076, train_acc: 97.270
12/17 05:14:55 AM [Supernet Validation] epoch: 406, val_loss: 0.528, val_acc: 87.230, best_acc: 87.990


12/17 05:14:55 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 96.875(96.875)
12/17 05:15:01 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 101/521, train_loss: 0.065(0.078), train_acc: 97.917(97.360)
12/17 05:15:08 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 201/521, train_loss: 0.023(0.080), train_acc: 100.000(97.186)
12/17 05:15:14 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 301/521, train_loss: 0.035(0.079), train_acc: 97.917(97.193)
12/17 05:15:21 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 401/521, train_loss: 0.070(0.077), train_acc: 95.833(97.233)
12/17 05:15:27 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 501/521, train_loss: 0.092(0.077), train_acc: 95.833(97.272)
12/17 05:15:28 AM [Supernet Training] lr: 0.00808 epoch: 407/600, step: 521/521, train_loss: 0.024(0.076), train_acc: 100.000(97.286)
12/17 05:15:28 AM [Supernet Training] epoch: 407, train_loss: 0.076, train_acc: 97.286
12/17 05:15:30 AM [Supernet Validation] epoch: 407, val_loss: 0.519, val_acc: 87.330, best_acc: 87.990


12/17 05:15:30 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 93.750(93.750)
12/17 05:15:36 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 101/521, train_loss: 0.069(0.076), train_acc: 97.917(97.257)
12/17 05:15:43 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 201/521, train_loss: 0.093(0.075), train_acc: 95.833(97.378)
12/17 05:15:49 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 301/521, train_loss: 0.060(0.076), train_acc: 98.958(97.356)
12/17 05:15:56 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 401/521, train_loss: 0.118(0.076), train_acc: 94.792(97.366)
12/17 05:16:02 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 501/521, train_loss: 0.063(0.077), train_acc: 98.958(97.320)
12/17 05:16:03 AM [Supernet Training] lr: 0.00804 epoch: 408/600, step: 521/521, train_loss: 0.038(0.077), train_acc: 98.750(97.328)
12/17 05:16:03 AM [Supernet Training] epoch: 408, train_loss: 0.077, train_acc: 97.328
12/17 05:16:05 AM [Supernet Validation] epoch: 408, val_loss: 0.525, val_acc: 87.740, best_acc: 87.990


12/17 05:16:05 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 96.875(96.875)
12/17 05:16:12 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 101/521, train_loss: 0.083(0.068), train_acc: 96.875(97.525)
12/17 05:16:18 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 201/521, train_loss: 0.039(0.070), train_acc: 98.958(97.409)
12/17 05:16:25 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 301/521, train_loss: 0.066(0.074), train_acc: 97.917(97.228)
12/17 05:16:32 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 401/521, train_loss: 0.055(0.075), train_acc: 97.917(97.252)
12/17 05:16:38 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 501/521, train_loss: 0.025(0.075), train_acc: 100.000(97.255)
12/17 05:16:39 AM [Supernet Training] lr: 0.00800 epoch: 409/600, step: 521/521, train_loss: 0.084(0.076), train_acc: 97.500(97.242)
12/17 05:16:39 AM [Supernet Training] epoch: 409, train_loss: 0.076, train_acc: 97.242
12/17 05:16:41 AM [Supernet Validation] epoch: 409, val_loss: 0.530, val_acc: 87.540, best_acc: 87.990


12/17 05:16:41 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 98.958(98.958)
12/17 05:16:47 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 101/521, train_loss: 0.076(0.075), train_acc: 95.833(97.277)
12/17 05:16:54 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 201/521, train_loss: 0.090(0.076), train_acc: 95.833(97.222)
12/17 05:17:00 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 301/521, train_loss: 0.078(0.077), train_acc: 95.833(97.190)
12/17 05:17:06 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 401/521, train_loss: 0.055(0.077), train_acc: 98.958(97.218)
12/17 05:17:12 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 501/521, train_loss: 0.047(0.076), train_acc: 98.958(97.255)
12/17 05:17:14 AM [Supernet Training] lr: 0.00796 epoch: 410/600, step: 521/521, train_loss: 0.054(0.076), train_acc: 97.500(97.262)
12/17 05:17:14 AM [Supernet Training] epoch: 410, train_loss: 0.076, train_acc: 97.262
12/17 05:17:15 AM [Supernet Validation] epoch: 410, val_loss: 0.527, val_acc: 87.570, best_acc: 87.990


12/17 05:17:15 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 100.000(100.000)
12/17 05:17:22 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 101/521, train_loss: 0.015(0.076), train_acc: 100.000(97.401)
12/17 05:17:28 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 201/521, train_loss: 0.083(0.077), train_acc: 95.833(97.295)
12/17 05:17:35 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 301/521, train_loss: 0.061(0.074), train_acc: 96.875(97.394)
12/17 05:17:41 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 401/521, train_loss: 0.077(0.074), train_acc: 96.875(97.389)
12/17 05:17:47 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 501/521, train_loss: 0.045(0.074), train_acc: 98.958(97.332)
12/17 05:17:49 AM [Supernet Training] lr: 0.00792 epoch: 411/600, step: 521/521, train_loss: 0.070(0.074), train_acc: 97.500(97.350)
12/17 05:17:49 AM [Supernet Training] epoch: 411, train_loss: 0.074, train_acc: 97.350
12/17 05:17:50 AM [Supernet Validation] epoch: 411, val_loss: 0.512, val_acc: 87.620, best_acc: 87.990


12/17 05:17:51 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 001/521, train_loss: 0.141(0.141), train_acc: 91.667(91.667)
12/17 05:17:57 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 101/521, train_loss: 0.031(0.077), train_acc: 98.958(97.329)
12/17 05:18:03 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 201/521, train_loss: 0.069(0.074), train_acc: 95.833(97.445)
12/17 05:18:10 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 301/521, train_loss: 0.086(0.072), train_acc: 96.875(97.422)
12/17 05:18:16 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 401/521, train_loss: 0.065(0.073), train_acc: 97.917(97.366)
12/17 05:18:22 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 501/521, train_loss: 0.067(0.074), train_acc: 97.917(97.339)
12/17 05:18:24 AM [Supernet Training] lr: 0.00787 epoch: 412/600, step: 521/521, train_loss: 0.119(0.075), train_acc: 95.000(97.316)
12/17 05:18:24 AM [Supernet Training] epoch: 412, train_loss: 0.075, train_acc: 97.316
12/17 05:18:25 AM [Supernet Validation] epoch: 412, val_loss: 0.525, val_acc: 87.370, best_acc: 87.990


12/17 05:18:25 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
12/17 05:18:32 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 101/521, train_loss: 0.043(0.075), train_acc: 98.958(97.288)
12/17 05:18:38 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 201/521, train_loss: 0.024(0.072), train_acc: 100.000(97.481)
12/17 05:18:45 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 301/521, train_loss: 0.088(0.073), train_acc: 96.875(97.398)
12/17 05:18:51 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 401/521, train_loss: 0.078(0.074), train_acc: 95.833(97.335)
12/17 05:18:57 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 501/521, train_loss: 0.053(0.075), train_acc: 97.917(97.276)
12/17 05:18:59 AM [Supernet Training] lr: 0.00783 epoch: 413/600, step: 521/521, train_loss: 0.020(0.075), train_acc: 100.000(97.270)
12/17 05:18:59 AM [Supernet Training] epoch: 413, train_loss: 0.075, train_acc: 97.270
12/17 05:19:00 AM [Supernet Validation] epoch: 413, val_loss: 0.547, val_acc: 87.350, best_acc: 87.990


12/17 05:19:01 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 001/521, train_loss: 0.039(0.039), train_acc: 100.000(100.000)
12/17 05:19:07 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 101/521, train_loss: 0.112(0.078), train_acc: 95.833(97.226)
12/17 05:19:13 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 201/521, train_loss: 0.025(0.074), train_acc: 100.000(97.435)
12/17 05:19:20 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 301/521, train_loss: 0.060(0.072), train_acc: 97.917(97.474)
12/17 05:19:26 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 401/521, train_loss: 0.025(0.073), train_acc: 100.000(97.431)
12/17 05:19:33 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 501/521, train_loss: 0.071(0.074), train_acc: 96.875(97.362)
12/17 05:19:34 AM [Supernet Training] lr: 0.00779 epoch: 414/600, step: 521/521, train_loss: 0.055(0.074), train_acc: 97.500(97.380)
12/17 05:19:34 AM [Supernet Training] epoch: 414, train_loss: 0.074, train_acc: 97.380
12/17 05:19:36 AM [Supernet Validation] epoch: 414, val_loss: 0.530, val_acc: 87.580, best_acc: 87.990


12/17 05:19:36 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 001/521, train_loss: 0.068(0.068), train_acc: 95.833(95.833)
12/17 05:19:42 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 101/521, train_loss: 0.099(0.076), train_acc: 98.958(97.422)
12/17 05:19:49 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 201/521, train_loss: 0.027(0.075), train_acc: 100.000(97.367)
12/17 05:19:55 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 301/521, train_loss: 0.086(0.076), train_acc: 96.875(97.304)
12/17 05:20:02 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 401/521, train_loss: 0.059(0.076), train_acc: 96.875(97.270)
12/17 05:20:08 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 501/521, train_loss: 0.042(0.075), train_acc: 98.958(97.332)
12/17 05:20:09 AM [Supernet Training] lr: 0.00775 epoch: 415/600, step: 521/521, train_loss: 0.032(0.075), train_acc: 100.000(97.316)
12/17 05:20:09 AM [Supernet Training] epoch: 415, train_loss: 0.075, train_acc: 97.316
12/17 05:20:11 AM [Supernet Validation] epoch: 415, val_loss: 0.517, val_acc: 87.730, best_acc: 87.990


12/17 05:20:11 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 001/521, train_loss: 0.083(0.083), train_acc: 95.833(95.833)
12/17 05:20:18 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 101/521, train_loss: 0.109(0.078), train_acc: 95.833(97.422)
12/17 05:20:25 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 201/521, train_loss: 0.091(0.076), train_acc: 97.917(97.373)
12/17 05:20:31 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 301/521, train_loss: 0.135(0.076), train_acc: 95.833(97.363)
12/17 05:20:37 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 401/521, train_loss: 0.046(0.075), train_acc: 98.958(97.382)
12/17 05:20:44 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 501/521, train_loss: 0.044(0.074), train_acc: 97.917(97.418)
12/17 05:20:45 AM [Supernet Training] lr: 0.00771 epoch: 416/600, step: 521/521, train_loss: 0.055(0.074), train_acc: 97.500(97.412)
12/17 05:20:45 AM [Supernet Training] epoch: 416, train_loss: 0.074, train_acc: 97.412
12/17 05:20:47 AM [Supernet Validation] epoch: 416, val_loss: 0.515, val_acc: 87.870, best_acc: 87.990


12/17 05:20:47 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 001/521, train_loss: 0.054(0.054), train_acc: 98.958(98.958)
12/17 05:20:53 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 101/521, train_loss: 0.085(0.067), train_acc: 96.875(97.783)
12/17 05:21:00 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 201/521, train_loss: 0.118(0.067), train_acc: 94.792(97.746)
12/17 05:21:06 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 301/521, train_loss: 0.042(0.067), train_acc: 97.917(97.716)
12/17 05:21:12 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 401/521, train_loss: 0.164(0.069), train_acc: 93.750(97.628)
12/17 05:21:19 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 501/521, train_loss: 0.067(0.070), train_acc: 97.917(97.572)
12/17 05:21:20 AM [Supernet Training] lr: 0.00767 epoch: 417/600, step: 521/521, train_loss: 0.050(0.070), train_acc: 96.250(97.568)
12/17 05:21:20 AM [Supernet Training] epoch: 417, train_loss: 0.070, train_acc: 97.568
12/17 05:21:22 AM [Supernet Validation] epoch: 417, val_loss: 0.537, val_acc: 87.510, best_acc: 87.990


12/17 05:21:22 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 001/521, train_loss: 0.049(0.049), train_acc: 97.917(97.917)
12/17 05:21:29 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 101/521, train_loss: 0.068(0.064), train_acc: 96.875(97.762)
12/17 05:21:35 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 201/521, train_loss: 0.057(0.067), train_acc: 98.958(97.658)
12/17 05:21:41 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 301/521, train_loss: 0.130(0.068), train_acc: 94.792(97.605)
12/17 05:21:48 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 401/521, train_loss: 0.051(0.069), train_acc: 97.917(97.584)
12/17 05:21:54 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 501/521, train_loss: 0.107(0.070), train_acc: 93.750(97.547)
12/17 05:21:55 AM [Supernet Training] lr: 0.00763 epoch: 418/600, step: 521/521, train_loss: 0.026(0.070), train_acc: 98.750(97.564)
12/17 05:21:55 AM [Supernet Training] epoch: 418, train_loss: 0.070, train_acc: 97.564
12/17 05:21:57 AM [Supernet Validation] epoch: 418, val_loss: 0.521, val_acc: 87.380, best_acc: 87.990


12/17 05:21:57 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 001/521, train_loss: 0.123(0.123), train_acc: 94.792(94.792)
12/17 05:22:03 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 101/521, train_loss: 0.030(0.067), train_acc: 100.000(97.741)
12/17 05:22:10 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 201/521, train_loss: 0.079(0.070), train_acc: 97.917(97.590)
12/17 05:22:16 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 301/521, train_loss: 0.116(0.070), train_acc: 95.833(97.546)
12/17 05:22:22 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 401/521, train_loss: 0.100(0.070), train_acc: 97.917(97.537)
12/17 05:22:29 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 501/521, train_loss: 0.052(0.069), train_acc: 97.917(97.561)
12/17 05:22:30 AM [Supernet Training] lr: 0.00758 epoch: 419/600, step: 521/521, train_loss: 0.105(0.069), train_acc: 96.250(97.558)
12/17 05:22:30 AM [Supernet Training] epoch: 419, train_loss: 0.069, train_acc: 97.558
12/17 05:22:32 AM [Supernet Validation] epoch: 419, val_loss: 0.515, val_acc: 87.970, best_acc: 87.990


12/17 05:22:32 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 001/521, train_loss: 0.027(0.027), train_acc: 100.000(100.000)
12/17 05:22:38 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 101/521, train_loss: 0.031(0.073), train_acc: 98.958(97.318)
12/17 05:22:45 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 201/521, train_loss: 0.058(0.072), train_acc: 97.917(97.383)
12/17 05:22:51 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 301/521, train_loss: 0.025(0.071), train_acc: 98.958(97.394)
12/17 05:22:57 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 401/521, train_loss: 0.077(0.072), train_acc: 97.917(97.356)
12/17 05:23:04 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 501/521, train_loss: 0.059(0.072), train_acc: 98.958(97.372)
12/17 05:23:05 AM [Supernet Training] lr: 0.00754 epoch: 420/600, step: 521/521, train_loss: 0.050(0.072), train_acc: 98.750(97.364)
12/17 05:23:05 AM [Supernet Training] epoch: 420, train_loss: 0.072, train_acc: 97.364
12/17 05:23:06 AM [Supernet Validation] epoch: 420, val_loss: 0.516, val_acc: 87.480, best_acc: 87.990


12/17 05:23:07 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 001/521, train_loss: 0.088(0.088), train_acc: 95.833(95.833)
12/17 05:23:13 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 101/521, train_loss: 0.074(0.071), train_acc: 95.833(97.442)
12/17 05:23:19 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 201/521, train_loss: 0.125(0.073), train_acc: 94.792(97.404)
12/17 05:23:26 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 301/521, train_loss: 0.135(0.072), train_acc: 95.833(97.494)
12/17 05:23:32 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 401/521, train_loss: 0.151(0.071), train_acc: 95.833(97.527)
12/17 05:23:39 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 501/521, train_loss: 0.026(0.071), train_acc: 98.958(97.493)
12/17 05:23:40 AM [Supernet Training] lr: 0.00750 epoch: 421/600, step: 521/521, train_loss: 0.050(0.071), train_acc: 98.750(97.514)
12/17 05:23:40 AM [Supernet Training] epoch: 421, train_loss: 0.071, train_acc: 97.514
12/17 05:23:41 AM [Supernet Validation] epoch: 421, val_loss: 0.505, val_acc: 87.940, best_acc: 87.990


12/17 05:23:42 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 001/521, train_loss: 0.088(0.088), train_acc: 95.833(95.833)
12/17 05:23:48 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 101/521, train_loss: 0.071(0.071), train_acc: 96.875(97.525)
12/17 05:23:55 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 201/521, train_loss: 0.043(0.069), train_acc: 97.917(97.492)
12/17 05:24:01 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 301/521, train_loss: 0.115(0.069), train_acc: 94.792(97.557)
12/17 05:24:07 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 401/521, train_loss: 0.078(0.070), train_acc: 96.875(97.535)
12/17 05:24:14 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 501/521, train_loss: 0.071(0.069), train_acc: 96.875(97.555)
12/17 05:24:15 AM [Supernet Training] lr: 0.00746 epoch: 422/600, step: 521/521, train_loss: 0.092(0.069), train_acc: 97.500(97.540)
12/17 05:24:15 AM [Supernet Training] epoch: 422, train_loss: 0.069, train_acc: 97.540
12/17 05:24:16 AM [Supernet Validation] epoch: 422, val_loss: 0.543, val_acc: 87.430, best_acc: 87.990


12/17 05:24:17 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 001/521, train_loss: 0.060(0.060), train_acc: 98.958(98.958)
12/17 05:24:23 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 101/521, train_loss: 0.077(0.057), train_acc: 96.875(98.164)
12/17 05:24:30 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 201/521, train_loss: 0.062(0.064), train_acc: 96.875(97.823)
12/17 05:24:36 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 301/521, train_loss: 0.039(0.066), train_acc: 98.958(97.702)
12/17 05:24:43 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 401/521, train_loss: 0.071(0.068), train_acc: 97.917(97.636)
12/17 05:24:49 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 501/521, train_loss: 0.053(0.068), train_acc: 98.958(97.621)
12/17 05:24:51 AM [Supernet Training] lr: 0.00742 epoch: 423/600, step: 521/521, train_loss: 0.267(0.069), train_acc: 91.250(97.600)
12/17 05:24:51 AM [Supernet Training] epoch: 423, train_loss: 0.069, train_acc: 97.600
12/17 05:24:52 AM [Supernet Validation] epoch: 423, val_loss: 0.534, val_acc: 87.540, best_acc: 87.990


12/17 05:24:53 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 001/521, train_loss: 0.071(0.071), train_acc: 97.917(97.917)
12/17 05:24:59 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 101/521, train_loss: 0.112(0.063), train_acc: 95.833(97.803)
12/17 05:25:05 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 201/521, train_loss: 0.061(0.065), train_acc: 96.875(97.673)
12/17 05:25:12 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 301/521, train_loss: 0.113(0.066), train_acc: 95.833(97.654)
12/17 05:25:18 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 401/521, train_loss: 0.060(0.066), train_acc: 97.917(97.717)
12/17 05:25:24 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 501/521, train_loss: 0.132(0.067), train_acc: 95.833(97.684)
12/17 05:25:26 AM [Supernet Training] lr: 0.00738 epoch: 424/600, step: 521/521, train_loss: 0.020(0.067), train_acc: 100.000(97.658)
12/17 05:25:26 AM [Supernet Training] epoch: 424, train_loss: 0.067, train_acc: 97.658
12/17 05:25:27 AM [Supernet Validation] epoch: 424, val_loss: 0.517, val_acc: 87.760, best_acc: 87.990


12/17 05:25:28 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 001/521, train_loss: 0.047(0.047), train_acc: 97.917(97.917)
12/17 05:25:34 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 101/521, train_loss: 0.050(0.067), train_acc: 98.958(97.638)
12/17 05:25:40 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 201/521, train_loss: 0.082(0.068), train_acc: 96.875(97.621)
12/17 05:25:47 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 301/521, train_loss: 0.044(0.071), train_acc: 98.958(97.519)
12/17 05:25:53 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 401/521, train_loss: 0.091(0.072), train_acc: 98.958(97.467)
12/17 05:25:59 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 501/521, train_loss: 0.113(0.072), train_acc: 95.833(97.463)
12/17 05:26:01 AM [Supernet Training] lr: 0.00733 epoch: 425/600, step: 521/521, train_loss: 0.043(0.072), train_acc: 97.500(97.474)
12/17 05:26:01 AM [Supernet Training] epoch: 425, train_loss: 0.072, train_acc: 97.474
12/17 05:26:02 AM [Supernet Validation] epoch: 425, val_loss: 0.544, val_acc: 87.490, best_acc: 87.990


12/17 05:26:03 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 96.875(96.875)
12/17 05:26:09 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 101/521, train_loss: 0.056(0.068), train_acc: 98.958(97.679)
12/17 05:26:16 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 201/521, train_loss: 0.103(0.070), train_acc: 97.917(97.461)
12/17 05:26:22 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 301/521, train_loss: 0.041(0.069), train_acc: 98.958(97.519)
12/17 05:26:29 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 401/521, train_loss: 0.072(0.068), train_acc: 97.917(97.566)
12/17 05:26:35 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 501/521, train_loss: 0.044(0.068), train_acc: 98.958(97.559)
12/17 05:26:36 AM [Supernet Training] lr: 0.00729 epoch: 426/600, step: 521/521, train_loss: 0.070(0.068), train_acc: 96.250(97.536)
12/17 05:26:36 AM [Supernet Training] epoch: 426, train_loss: 0.068, train_acc: 97.536
12/17 05:26:38 AM [Supernet Validation] epoch: 426, val_loss: 0.526, val_acc: 87.800, best_acc: 87.990


12/17 05:26:38 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 001/521, train_loss: 0.090(0.090), train_acc: 96.875(96.875)
12/17 05:26:45 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 101/521, train_loss: 0.042(0.063), train_acc: 98.958(97.772)
12/17 05:26:51 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 201/521, train_loss: 0.109(0.063), train_acc: 95.833(97.777)
12/17 05:26:58 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 301/521, train_loss: 0.042(0.065), train_acc: 97.917(97.640)
12/17 05:27:04 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 401/521, train_loss: 0.020(0.066), train_acc: 100.000(97.662)
12/17 05:27:11 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 501/521, train_loss: 0.024(0.068), train_acc: 100.000(97.561)
12/17 05:27:12 AM [Supernet Training] lr: 0.00725 epoch: 427/600, step: 521/521, train_loss: 0.024(0.068), train_acc: 100.000(97.554)
12/17 05:27:12 AM [Supernet Training] epoch: 427, train_loss: 0.068, train_acc: 97.554
12/17 05:27:13 AM [Supernet Validation] epoch: 427, val_loss: 0.524, val_acc: 87.540, best_acc: 87.990


12/17 05:27:14 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 98.958(98.958)
12/17 05:27:20 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 101/521, train_loss: 0.027(0.064), train_acc: 98.958(97.896)
12/17 05:27:26 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 201/521, train_loss: 0.078(0.065), train_acc: 96.875(97.787)
12/17 05:27:33 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 301/521, train_loss: 0.096(0.066), train_acc: 96.875(97.712)
12/17 05:27:39 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 401/521, train_loss: 0.116(0.067), train_acc: 94.792(97.644)
12/17 05:27:46 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 501/521, train_loss: 0.062(0.067), train_acc: 96.875(97.586)
12/17 05:27:47 AM [Supernet Training] lr: 0.00721 epoch: 428/600, step: 521/521, train_loss: 0.060(0.067), train_acc: 98.750(97.584)
12/17 05:27:47 AM [Supernet Training] epoch: 428, train_loss: 0.067, train_acc: 97.584
12/17 05:27:48 AM [Supernet Validation] epoch: 428, val_loss: 0.543, val_acc: 87.380, best_acc: 87.990


12/17 05:27:49 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 001/521, train_loss: 0.083(0.083), train_acc: 96.875(96.875)
12/17 05:27:55 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 101/521, train_loss: 0.037(0.069), train_acc: 100.000(97.597)
12/17 05:28:02 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 201/521, train_loss: 0.085(0.065), train_acc: 97.917(97.740)
12/17 05:28:08 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 301/521, train_loss: 0.115(0.067), train_acc: 94.792(97.678)
12/17 05:28:15 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 401/521, train_loss: 0.057(0.067), train_acc: 97.917(97.665)
12/17 05:28:21 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 501/521, train_loss: 0.066(0.067), train_acc: 97.917(97.644)
12/17 05:28:22 AM [Supernet Training] lr: 0.00717 epoch: 429/600, step: 521/521, train_loss: 0.082(0.067), train_acc: 96.250(97.668)
12/17 05:28:22 AM [Supernet Training] epoch: 429, train_loss: 0.067, train_acc: 97.668
12/17 05:28:24 AM [Supernet Validation] epoch: 429, val_loss: 0.537, val_acc: 87.640, best_acc: 87.990


12/17 05:28:24 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 97.917(97.917)
12/17 05:28:31 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 101/521, train_loss: 0.075(0.065), train_acc: 97.917(97.793)
12/17 05:28:37 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 201/521, train_loss: 0.049(0.064), train_acc: 98.958(97.803)
12/17 05:28:44 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 301/521, train_loss: 0.027(0.066), train_acc: 98.958(97.768)
12/17 05:28:51 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 401/521, train_loss: 0.054(0.067), train_acc: 97.917(97.745)
12/17 05:28:57 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 501/521, train_loss: 0.059(0.068), train_acc: 97.917(97.692)
12/17 05:28:58 AM [Supernet Training] lr: 0.00713 epoch: 430/600, step: 521/521, train_loss: 0.081(0.067), train_acc: 96.250(97.712)
12/17 05:28:58 AM [Supernet Training] epoch: 430, train_loss: 0.067, train_acc: 97.712
12/17 05:29:00 AM [Supernet Validation] epoch: 430, val_loss: 0.527, val_acc: 87.810, best_acc: 87.990


12/17 05:29:00 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 001/521, train_loss: 0.129(0.129), train_acc: 94.792(94.792)
12/17 05:29:07 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 101/521, train_loss: 0.030(0.065), train_acc: 98.958(97.566)
12/17 05:29:13 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 201/521, train_loss: 0.085(0.064), train_acc: 96.875(97.699)
12/17 05:29:19 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 301/521, train_loss: 0.058(0.066), train_acc: 96.875(97.595)
12/17 05:29:26 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 401/521, train_loss: 0.032(0.065), train_acc: 100.000(97.574)
12/17 05:29:32 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 501/521, train_loss: 0.043(0.067), train_acc: 98.958(97.574)
12/17 05:29:33 AM [Supernet Training] lr: 0.00708 epoch: 431/600, step: 521/521, train_loss: 0.032(0.067), train_acc: 98.750(97.588)
12/17 05:29:33 AM [Supernet Training] epoch: 431, train_loss: 0.067, train_acc: 97.588
12/17 05:29:35 AM [Supernet Validation] epoch: 431, val_loss: 0.524, val_acc: 87.800, best_acc: 87.990


12/17 05:29:35 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 001/521, train_loss: 0.040(0.040), train_acc: 98.958(98.958)
12/17 05:29:42 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 101/521, train_loss: 0.022(0.065), train_acc: 100.000(97.690)
12/17 05:29:48 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 201/521, train_loss: 0.104(0.067), train_acc: 96.875(97.606)
12/17 05:29:54 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 301/521, train_loss: 0.135(0.068), train_acc: 94.792(97.616)
12/17 05:30:00 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 401/521, train_loss: 0.053(0.067), train_acc: 97.917(97.667)
12/17 05:30:07 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 501/521, train_loss: 0.029(0.068), train_acc: 98.958(97.669)
12/17 05:30:08 AM [Supernet Training] lr: 0.00704 epoch: 432/600, step: 521/521, train_loss: 0.037(0.067), train_acc: 100.000(97.662)
12/17 05:30:08 AM [Supernet Training] epoch: 432, train_loss: 0.067, train_acc: 97.662
12/17 05:30:09 AM [Supernet Validation] epoch: 432, val_loss: 0.516, val_acc: 87.850, best_acc: 87.990


12/17 05:30:10 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 001/521, train_loss: 0.028(0.028), train_acc: 100.000(100.000)
12/17 05:30:16 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 101/521, train_loss: 0.083(0.062), train_acc: 95.833(97.948)
12/17 05:30:22 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 201/521, train_loss: 0.032(0.065), train_acc: 98.958(97.668)
12/17 05:30:29 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 301/521, train_loss: 0.041(0.066), train_acc: 97.917(97.629)
12/17 05:30:35 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 401/521, train_loss: 0.054(0.066), train_acc: 98.958(97.675)
12/17 05:30:41 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 501/521, train_loss: 0.133(0.068), train_acc: 95.833(97.611)
12/17 05:30:43 AM [Supernet Training] lr: 0.00700 epoch: 433/600, step: 521/521, train_loss: 0.050(0.068), train_acc: 98.750(97.596)
12/17 05:30:43 AM [Supernet Training] epoch: 433, train_loss: 0.068, train_acc: 97.596
12/17 05:30:44 AM [Supernet Validation] epoch: 433, val_loss: 0.534, val_acc: 87.680, best_acc: 87.990


12/17 05:30:45 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 001/521, train_loss: 0.179(0.179), train_acc: 93.750(93.750)
12/17 05:30:51 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 101/521, train_loss: 0.023(0.063), train_acc: 100.000(97.669)
12/17 05:30:57 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 201/521, train_loss: 0.019(0.069), train_acc: 98.958(97.549)
12/17 05:31:04 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 301/521, train_loss: 0.048(0.066), train_acc: 98.958(97.647)
12/17 05:31:10 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 401/521, train_loss: 0.036(0.064), train_acc: 98.958(97.678)
12/17 05:31:16 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 501/521, train_loss: 0.070(0.064), train_acc: 96.875(97.684)
12/17 05:31:18 AM [Supernet Training] lr: 0.00696 epoch: 434/600, step: 521/521, train_loss: 0.059(0.064), train_acc: 96.250(97.686)
12/17 05:31:18 AM [Supernet Training] epoch: 434, train_loss: 0.064, train_acc: 97.686
12/17 05:31:19 AM [Supernet Validation] epoch: 434, val_loss: 0.524, val_acc: 87.930, best_acc: 87.990


12/17 05:31:20 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 001/521, train_loss: 0.124(0.124), train_acc: 95.833(95.833)
12/17 05:31:26 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 101/521, train_loss: 0.066(0.067), train_acc: 94.792(97.628)
12/17 05:31:32 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 201/521, train_loss: 0.017(0.066), train_acc: 100.000(97.699)
12/17 05:31:39 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 301/521, train_loss: 0.096(0.066), train_acc: 96.875(97.688)
12/17 05:31:45 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 401/521, train_loss: 0.043(0.067), train_acc: 98.958(97.631)
12/17 05:31:51 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 501/521, train_loss: 0.125(0.068), train_acc: 93.750(97.569)
12/17 05:31:53 AM [Supernet Training] lr: 0.00692 epoch: 435/600, step: 521/521, train_loss: 0.071(0.068), train_acc: 96.250(97.570)
12/17 05:31:53 AM [Supernet Training] epoch: 435, train_loss: 0.068, train_acc: 97.570
12/17 05:31:54 AM [Supernet Validation] epoch: 435, val_loss: 0.534, val_acc: 87.280, best_acc: 87.990


12/17 05:31:54 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 001/521, train_loss: 0.064(0.064), train_acc: 98.958(98.958)
12/17 05:32:01 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 101/521, train_loss: 0.100(0.067), train_acc: 95.833(97.638)
12/17 05:32:07 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 201/521, train_loss: 0.073(0.066), train_acc: 97.917(97.616)
12/17 05:32:14 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 301/521, train_loss: 0.060(0.065), train_acc: 96.875(97.730)
12/17 05:32:20 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 401/521, train_loss: 0.044(0.065), train_acc: 97.917(97.654)
12/17 05:32:26 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 501/521, train_loss: 0.053(0.064), train_acc: 96.875(97.703)
12/17 05:32:27 AM [Supernet Training] lr: 0.00688 epoch: 436/600, step: 521/521, train_loss: 0.019(0.064), train_acc: 98.750(97.694)
12/17 05:32:28 AM [Supernet Training] epoch: 436, train_loss: 0.064, train_acc: 97.694
12/17 05:32:29 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 05:32:29 AM [Supernet Validation] epoch: 436, val_loss: 0.526, val_acc: 88.020, best_acc: 88.020


12/17 05:32:29 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 001/521, train_loss: 0.062(0.062), train_acc: 97.917(97.917)
12/17 05:32:36 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 101/521, train_loss: 0.138(0.063), train_acc: 95.833(97.803)
12/17 05:32:42 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 201/521, train_loss: 0.065(0.065), train_acc: 96.875(97.621)
12/17 05:32:49 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 301/521, train_loss: 0.025(0.065), train_acc: 98.958(97.605)
12/17 05:32:55 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 401/521, train_loss: 0.107(0.064), train_acc: 96.875(97.675)
12/17 05:33:01 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 501/521, train_loss: 0.062(0.064), train_acc: 98.958(97.744)
12/17 05:33:02 AM [Supernet Training] lr: 0.00683 epoch: 437/600, step: 521/521, train_loss: 0.094(0.064), train_acc: 95.000(97.756)
12/17 05:33:02 AM [Supernet Training] epoch: 437, train_loss: 0.064, train_acc: 97.756
12/17 05:33:04 AM [Supernet Validation] epoch: 437, val_loss: 0.535, val_acc: 87.690, best_acc: 88.020


12/17 05:33:04 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 001/521, train_loss: 0.009(0.009), train_acc: 100.000(100.000)
12/17 05:33:11 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 101/521, train_loss: 0.050(0.059), train_acc: 96.875(97.989)
12/17 05:33:17 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 201/521, train_loss: 0.191(0.064), train_acc: 94.792(97.761)
12/17 05:33:24 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 301/521, train_loss: 0.086(0.063), train_acc: 96.875(97.768)
12/17 05:33:30 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 401/521, train_loss: 0.090(0.062), train_acc: 96.875(97.828)
12/17 05:33:36 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 501/521, train_loss: 0.030(0.063), train_acc: 97.917(97.779)
12/17 05:33:38 AM [Supernet Training] lr: 0.00679 epoch: 438/600, step: 521/521, train_loss: 0.062(0.063), train_acc: 97.500(97.790)
12/17 05:33:38 AM [Supernet Training] epoch: 438, train_loss: 0.063, train_acc: 97.790
12/17 05:33:39 AM [Supernet Validation] epoch: 438, val_loss: 0.531, val_acc: 87.970, best_acc: 88.020


12/17 05:33:40 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 98.958(98.958)
12/17 05:33:46 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 101/521, train_loss: 0.036(0.060), train_acc: 98.958(98.040)
12/17 05:33:52 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 201/521, train_loss: 0.053(0.061), train_acc: 98.958(97.979)
12/17 05:33:58 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 301/521, train_loss: 0.041(0.058), train_acc: 98.958(98.003)
12/17 05:34:05 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 401/521, train_loss: 0.085(0.059), train_acc: 96.875(97.966)
12/17 05:34:12 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 501/521, train_loss: 0.131(0.060), train_acc: 96.875(97.919)
12/17 05:34:13 AM [Supernet Training] lr: 0.00675 epoch: 439/600, step: 521/521, train_loss: 0.043(0.060), train_acc: 98.750(97.902)
12/17 05:34:13 AM [Supernet Training] epoch: 439, train_loss: 0.060, train_acc: 97.902
12/17 05:34:15 AM [Supernet Validation] epoch: 439, val_loss: 0.535, val_acc: 87.410, best_acc: 88.020


12/17 05:34:15 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 001/521, train_loss: 0.017(0.017), train_acc: 100.000(100.000)
12/17 05:34:22 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 101/521, train_loss: 0.055(0.059), train_acc: 98.958(97.989)
12/17 05:34:28 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 201/521, train_loss: 0.026(0.061), train_acc: 100.000(97.870)
12/17 05:34:34 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 301/521, train_loss: 0.066(0.061), train_acc: 96.875(97.872)
12/17 05:34:41 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 401/521, train_loss: 0.059(0.061), train_acc: 97.917(97.883)
12/17 05:34:47 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 501/521, train_loss: 0.095(0.062), train_acc: 97.917(97.883)
12/17 05:34:48 AM [Supernet Training] lr: 0.00671 epoch: 440/600, step: 521/521, train_loss: 0.156(0.062), train_acc: 95.000(97.890)
12/17 05:34:48 AM [Supernet Training] epoch: 440, train_loss: 0.062, train_acc: 97.890
12/17 05:34:50 AM [Supernet Validation] epoch: 440, val_loss: 0.525, val_acc: 87.550, best_acc: 88.020


12/17 05:34:50 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 001/521, train_loss: 0.095(0.095), train_acc: 96.875(96.875)
12/17 05:34:56 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 101/521, train_loss: 0.024(0.060), train_acc: 98.958(97.814)
12/17 05:35:03 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 201/521, train_loss: 0.050(0.061), train_acc: 98.958(97.875)
12/17 05:35:09 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 301/521, train_loss: 0.100(0.060), train_acc: 96.875(97.965)
12/17 05:35:15 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 401/521, train_loss: 0.028(0.061), train_acc: 98.958(97.914)
12/17 05:35:22 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 501/521, train_loss: 0.072(0.062), train_acc: 96.875(97.873)
12/17 05:35:23 AM [Supernet Training] lr: 0.00667 epoch: 441/600, step: 521/521, train_loss: 0.064(0.062), train_acc: 97.500(97.888)
12/17 05:35:23 AM [Supernet Training] epoch: 441, train_loss: 0.062, train_acc: 97.888
12/17 05:35:24 AM [Supernet Validation] epoch: 441, val_loss: 0.530, val_acc: 87.910, best_acc: 88.020


12/17 05:35:25 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 001/521, train_loss: 0.097(0.097), train_acc: 97.917(97.917)
12/17 05:35:31 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 101/521, train_loss: 0.053(0.056), train_acc: 96.875(98.071)
12/17 05:35:37 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 201/521, train_loss: 0.070(0.055), train_acc: 95.833(98.119)
12/17 05:35:44 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 301/521, train_loss: 0.015(0.057), train_acc: 100.000(98.010)
12/17 05:35:50 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 401/521, train_loss: 0.091(0.060), train_acc: 97.917(97.854)
12/17 05:35:56 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 501/521, train_loss: 0.094(0.061), train_acc: 96.875(97.788)
12/17 05:35:58 AM [Supernet Training] lr: 0.00663 epoch: 442/600, step: 521/521, train_loss: 0.034(0.061), train_acc: 97.500(97.808)
12/17 05:35:58 AM [Supernet Training] epoch: 442, train_loss: 0.061, train_acc: 97.808
12/17 05:35:59 AM [Supernet Validation] epoch: 442, val_loss: 0.550, val_acc: 87.900, best_acc: 88.020


12/17 05:35:59 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 001/521, train_loss: 0.062(0.062), train_acc: 96.875(96.875)
12/17 05:36:06 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 101/521, train_loss: 0.027(0.065), train_acc: 98.958(97.576)
12/17 05:36:12 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 201/521, train_loss: 0.050(0.062), train_acc: 98.958(97.756)
12/17 05:36:18 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 301/521, train_loss: 0.042(0.063), train_acc: 98.958(97.764)
12/17 05:36:25 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 401/521, train_loss: 0.024(0.061), train_acc: 100.000(97.834)
12/17 05:36:31 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 501/521, train_loss: 0.062(0.061), train_acc: 97.917(97.858)
12/17 05:36:32 AM [Supernet Training] lr: 0.00658 epoch: 443/600, step: 521/521, train_loss: 0.066(0.061), train_acc: 97.500(97.856)
12/17 05:36:32 AM [Supernet Training] epoch: 443, train_loss: 0.061, train_acc: 97.856
12/17 05:36:34 AM [Supernet Validation] epoch: 443, val_loss: 0.526, val_acc: 87.790, best_acc: 88.020


12/17 05:36:34 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 001/521, train_loss: 0.064(0.064), train_acc: 97.917(97.917)
12/17 05:36:41 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 101/521, train_loss: 0.051(0.060), train_acc: 97.917(97.979)
12/17 05:36:47 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 201/521, train_loss: 0.063(0.062), train_acc: 96.875(97.844)
12/17 05:36:53 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 301/521, train_loss: 0.036(0.061), train_acc: 97.917(97.879)
12/17 05:37:00 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 401/521, train_loss: 0.056(0.061), train_acc: 97.917(97.885)
12/17 05:37:06 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 501/521, train_loss: 0.126(0.061), train_acc: 96.875(97.873)
12/17 05:37:08 AM [Supernet Training] lr: 0.00654 epoch: 444/600, step: 521/521, train_loss: 0.047(0.061), train_acc: 97.500(97.880)
12/17 05:37:08 AM [Supernet Training] epoch: 444, train_loss: 0.061, train_acc: 97.880
12/17 05:37:09 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 05:37:09 AM [Supernet Validation] epoch: 444, val_loss: 0.534, val_acc: 88.070, best_acc: 88.070


12/17 05:37:10 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 97.917(97.917)
12/17 05:37:16 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 101/521, train_loss: 0.060(0.054), train_acc: 98.958(98.164)
12/17 05:37:23 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 201/521, train_loss: 0.038(0.058), train_acc: 98.958(97.937)
12/17 05:37:29 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 301/521, train_loss: 0.045(0.059), train_acc: 97.917(97.910)
12/17 05:37:35 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 401/521, train_loss: 0.098(0.059), train_acc: 95.833(97.953)
12/17 05:37:42 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 501/521, train_loss: 0.029(0.060), train_acc: 98.958(97.937)
12/17 05:37:43 AM [Supernet Training] lr: 0.00650 epoch: 445/600, step: 521/521, train_loss: 0.045(0.060), train_acc: 98.750(97.924)
12/17 05:37:43 AM [Supernet Training] epoch: 445, train_loss: 0.060, train_acc: 97.924
12/17 05:37:45 AM [Supernet Validation] epoch: 445, val_loss: 0.530, val_acc: 87.580, best_acc: 88.070


12/17 05:37:45 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 001/521, train_loss: 0.066(0.066), train_acc: 96.875(96.875)
12/17 05:37:51 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 101/521, train_loss: 0.038(0.057), train_acc: 98.958(97.999)
12/17 05:37:58 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 201/521, train_loss: 0.058(0.056), train_acc: 96.875(98.041)
12/17 05:38:04 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 301/521, train_loss: 0.082(0.057), train_acc: 97.917(98.017)
12/17 05:38:10 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 401/521, train_loss: 0.073(0.058), train_acc: 97.917(97.979)
12/17 05:38:17 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 501/521, train_loss: 0.116(0.059), train_acc: 94.792(97.950)
12/17 05:38:18 AM [Supernet Training] lr: 0.00646 epoch: 446/600, step: 521/521, train_loss: 0.070(0.059), train_acc: 97.500(97.944)
12/17 05:38:18 AM [Supernet Training] epoch: 446, train_loss: 0.059, train_acc: 97.944
12/17 05:38:20 AM [Supernet Validation] epoch: 446, val_loss: 0.528, val_acc: 87.620, best_acc: 88.070


12/17 05:38:20 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
12/17 05:38:26 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 101/521, train_loss: 0.198(0.061), train_acc: 93.750(97.927)
12/17 05:38:33 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 201/521, train_loss: 0.105(0.063), train_acc: 95.833(97.823)
12/17 05:38:39 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 301/521, train_loss: 0.051(0.062), train_acc: 98.958(97.892)
12/17 05:38:46 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 401/521, train_loss: 0.052(0.062), train_acc: 96.875(97.885)
12/17 05:38:52 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 501/521, train_loss: 0.128(0.063), train_acc: 93.750(97.815)
12/17 05:38:53 AM [Supernet Training] lr: 0.00642 epoch: 447/600, step: 521/521, train_loss: 0.093(0.064), train_acc: 96.250(97.792)
12/17 05:38:53 AM [Supernet Training] epoch: 447, train_loss: 0.064, train_acc: 97.792
12/17 05:38:55 AM [Supernet Validation] epoch: 447, val_loss: 0.534, val_acc: 87.660, best_acc: 88.070


12/17 05:38:55 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 001/521, train_loss: 0.124(0.124), train_acc: 96.875(96.875)
12/17 05:39:01 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 101/521, train_loss: 0.033(0.068), train_acc: 100.000(97.618)
12/17 05:39:08 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 201/521, train_loss: 0.031(0.060), train_acc: 98.958(97.875)
12/17 05:39:14 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 301/521, train_loss: 0.016(0.060), train_acc: 100.000(97.927)
12/17 05:39:20 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 401/521, train_loss: 0.055(0.060), train_acc: 97.917(97.904)
12/17 05:39:27 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 501/521, train_loss: 0.095(0.060), train_acc: 96.875(97.871)
12/17 05:39:28 AM [Supernet Training] lr: 0.00638 epoch: 448/600, step: 521/521, train_loss: 0.011(0.060), train_acc: 100.000(97.870)
12/17 05:39:28 AM [Supernet Training] epoch: 448, train_loss: 0.060, train_acc: 97.870
12/17 05:39:29 AM [Supernet Validation] epoch: 448, val_loss: 0.552, val_acc: 87.770, best_acc: 88.070


12/17 05:39:30 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 001/521, train_loss: 0.019(0.019), train_acc: 98.958(98.958)
12/17 05:39:36 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 101/521, train_loss: 0.032(0.060), train_acc: 100.000(97.948)
12/17 05:39:42 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 201/521, train_loss: 0.064(0.059), train_acc: 98.958(98.041)
12/17 05:39:49 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 301/521, train_loss: 0.056(0.057), train_acc: 98.958(98.024)
12/17 05:39:55 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 401/521, train_loss: 0.029(0.058), train_acc: 98.958(97.984)
12/17 05:40:01 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 501/521, train_loss: 0.045(0.059), train_acc: 98.958(97.967)
12/17 05:40:03 AM [Supernet Training] lr: 0.00633 epoch: 449/600, step: 521/521, train_loss: 0.066(0.059), train_acc: 96.250(97.954)
12/17 05:40:03 AM [Supernet Training] epoch: 449, train_loss: 0.059, train_acc: 97.954
12/17 05:40:04 AM [Supernet Validation] epoch: 449, val_loss: 0.534, val_acc: 87.850, best_acc: 88.070


12/17 05:40:05 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
12/17 05:40:11 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 101/521, train_loss: 0.048(0.056), train_acc: 97.917(98.040)
12/17 05:40:17 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 201/521, train_loss: 0.025(0.057), train_acc: 100.000(98.015)
12/17 05:40:24 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 301/521, train_loss: 0.029(0.057), train_acc: 98.958(98.003)
12/17 05:40:30 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 401/521, train_loss: 0.080(0.057), train_acc: 96.875(98.021)
12/17 05:40:36 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 501/521, train_loss: 0.030(0.057), train_acc: 97.917(97.969)
12/17 05:40:38 AM [Supernet Training] lr: 0.00629 epoch: 450/600, step: 521/521, train_loss: 0.162(0.057), train_acc: 92.500(97.964)
12/17 05:40:38 AM [Supernet Training] epoch: 450, train_loss: 0.057, train_acc: 97.964
12/17 05:40:39 AM [Supernet Validation] epoch: 450, val_loss: 0.539, val_acc: 87.510, best_acc: 88.070


12/17 05:40:40 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 001/521, train_loss: 0.045(0.045), train_acc: 98.958(98.958)
12/17 05:40:46 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 101/521, train_loss: 0.064(0.055), train_acc: 98.958(98.195)
12/17 05:40:52 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 201/521, train_loss: 0.071(0.057), train_acc: 97.917(98.114)
12/17 05:40:59 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 301/521, train_loss: 0.140(0.057), train_acc: 95.833(98.114)
12/17 05:41:05 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 401/521, train_loss: 0.082(0.058), train_acc: 96.875(98.049)
12/17 05:41:11 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 501/521, train_loss: 0.027(0.058), train_acc: 100.000(98.027)
12/17 05:41:12 AM [Supernet Training] lr: 0.00625 epoch: 451/600, step: 521/521, train_loss: 0.042(0.058), train_acc: 98.750(98.026)
12/17 05:41:12 AM [Supernet Training] epoch: 451, train_loss: 0.058, train_acc: 98.026
12/17 05:41:14 AM [Supernet Validation] epoch: 451, val_loss: 0.546, val_acc: 87.740, best_acc: 88.070


12/17 05:41:14 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 001/521, train_loss: 0.103(0.103), train_acc: 97.917(97.917)
12/17 05:41:21 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 101/521, train_loss: 0.090(0.058), train_acc: 95.833(97.989)
12/17 05:41:27 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 201/521, train_loss: 0.077(0.057), train_acc: 95.833(98.020)
12/17 05:41:33 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 301/521, train_loss: 0.023(0.057), train_acc: 100.000(98.038)
12/17 05:41:40 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 401/521, train_loss: 0.101(0.057), train_acc: 96.875(98.062)
12/17 05:41:46 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 501/521, train_loss: 0.096(0.057), train_acc: 95.833(98.039)
12/17 05:41:48 AM [Supernet Training] lr: 0.00621 epoch: 452/600, step: 521/521, train_loss: 0.050(0.057), train_acc: 97.500(98.050)
12/17 05:41:48 AM [Supernet Training] epoch: 452, train_loss: 0.057, train_acc: 98.050
12/17 05:41:49 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 05:41:49 AM [Supernet Validation] epoch: 452, val_loss: 0.530, val_acc: 88.240, best_acc: 88.240


12/17 05:41:50 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 98.958(98.958)
12/17 05:41:56 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 101/521, train_loss: 0.063(0.056), train_acc: 97.917(98.020)
12/17 05:42:02 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 201/521, train_loss: 0.016(0.057), train_acc: 100.000(97.927)
12/17 05:42:09 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 301/521, train_loss: 0.047(0.060), train_acc: 98.958(97.823)
12/17 05:42:15 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 401/521, train_loss: 0.090(0.061), train_acc: 96.875(97.795)
12/17 05:42:21 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 501/521, train_loss: 0.058(0.060), train_acc: 98.958(97.827)
12/17 05:42:22 AM [Supernet Training] lr: 0.00617 epoch: 453/600, step: 521/521, train_loss: 0.068(0.060), train_acc: 98.750(97.830)
12/17 05:42:22 AM [Supernet Training] epoch: 453, train_loss: 0.060, train_acc: 97.830
12/17 05:42:24 AM [Supernet Validation] epoch: 453, val_loss: 0.537, val_acc: 87.860, best_acc: 88.240


12/17 05:42:24 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 98.958(98.958)
12/17 05:42:31 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 101/521, train_loss: 0.023(0.058), train_acc: 98.958(97.958)
12/17 05:42:37 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 201/521, train_loss: 0.064(0.060), train_acc: 96.875(97.886)
12/17 05:42:43 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 301/521, train_loss: 0.032(0.058), train_acc: 98.958(97.982)
12/17 05:42:50 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 401/521, train_loss: 0.051(0.058), train_acc: 98.958(97.963)
12/17 05:42:56 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 501/521, train_loss: 0.082(0.058), train_acc: 95.833(98.014)
12/17 05:42:57 AM [Supernet Training] lr: 0.00613 epoch: 454/600, step: 521/521, train_loss: 0.018(0.057), train_acc: 98.750(98.034)
12/17 05:42:57 AM [Supernet Training] epoch: 454, train_loss: 0.057, train_acc: 98.034
12/17 05:42:59 AM [Supernet Validation] epoch: 454, val_loss: 0.539, val_acc: 87.960, best_acc: 88.240


12/17 05:42:59 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 97.917(97.917)
12/17 05:43:05 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 101/521, train_loss: 0.022(0.058), train_acc: 98.958(97.979)
12/17 05:43:12 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 201/521, train_loss: 0.115(0.059), train_acc: 95.833(97.968)
12/17 05:43:18 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 301/521, train_loss: 0.064(0.057), train_acc: 96.875(98.048)
12/17 05:43:25 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 401/521, train_loss: 0.024(0.057), train_acc: 98.958(98.005)
12/17 05:43:31 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 501/521, train_loss: 0.049(0.056), train_acc: 98.958(98.056)
12/17 05:43:32 AM [Supernet Training] lr: 0.00608 epoch: 455/600, step: 521/521, train_loss: 0.048(0.056), train_acc: 97.500(98.048)
12/17 05:43:32 AM [Supernet Training] epoch: 455, train_loss: 0.056, train_acc: 98.048
12/17 05:43:34 AM [Supernet Validation] epoch: 455, val_loss: 0.532, val_acc: 87.970, best_acc: 88.240


12/17 05:43:34 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 97.917(97.917)
12/17 05:43:41 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 101/521, train_loss: 0.038(0.057), train_acc: 98.958(98.082)
12/17 05:43:47 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 201/521, train_loss: 0.081(0.058), train_acc: 97.917(97.994)
12/17 05:43:53 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 301/521, train_loss: 0.022(0.057), train_acc: 100.000(98.045)
12/17 05:44:00 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 401/521, train_loss: 0.044(0.058), train_acc: 97.917(98.000)
12/17 05:44:06 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 501/521, train_loss: 0.090(0.058), train_acc: 95.833(98.012)
12/17 05:44:07 AM [Supernet Training] lr: 0.00604 epoch: 456/600, step: 521/521, train_loss: 0.099(0.059), train_acc: 96.250(97.978)
12/17 05:44:07 AM [Supernet Training] epoch: 456, train_loss: 0.059, train_acc: 97.978
12/17 05:44:09 AM [Supernet Validation] epoch: 456, val_loss: 0.535, val_acc: 88.090, best_acc: 88.240


12/17 05:44:09 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 001/521, train_loss: 0.060(0.060), train_acc: 97.917(97.917)
12/17 05:44:15 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 101/521, train_loss: 0.076(0.056), train_acc: 96.875(98.030)
12/17 05:44:22 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 201/521, train_loss: 0.099(0.056), train_acc: 94.792(97.974)
12/17 05:44:28 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 301/521, train_loss: 0.018(0.055), train_acc: 100.000(98.048)
12/17 05:44:34 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 401/521, train_loss: 0.055(0.054), train_acc: 97.917(98.117)
12/17 05:44:40 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 501/521, train_loss: 0.075(0.055), train_acc: 97.917(98.060)
12/17 05:44:42 AM [Supernet Training] lr: 0.00600 epoch: 457/600, step: 521/521, train_loss: 0.105(0.055), train_acc: 95.000(98.064)
12/17 05:44:42 AM [Supernet Training] epoch: 457, train_loss: 0.055, train_acc: 98.064
12/17 05:44:43 AM [Supernet Validation] epoch: 457, val_loss: 0.534, val_acc: 87.550, best_acc: 88.240


12/17 05:44:44 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 001/521, train_loss: 0.060(0.060), train_acc: 98.958(98.958)
12/17 05:44:50 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 101/521, train_loss: 0.037(0.055), train_acc: 98.958(97.855)
12/17 05:44:57 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 201/521, train_loss: 0.037(0.055), train_acc: 98.958(97.932)
12/17 05:45:03 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 301/521, train_loss: 0.070(0.054), train_acc: 97.917(98.010)
12/17 05:45:09 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 401/521, train_loss: 0.016(0.054), train_acc: 100.000(98.015)
12/17 05:45:16 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 501/521, train_loss: 0.028(0.055), train_acc: 98.958(97.998)
12/17 05:45:17 AM [Supernet Training] lr: 0.00596 epoch: 458/600, step: 521/521, train_loss: 0.049(0.056), train_acc: 98.750(97.982)
12/17 05:45:17 AM [Supernet Training] epoch: 458, train_loss: 0.056, train_acc: 97.982
12/17 05:45:19 AM [Supernet Validation] epoch: 458, val_loss: 0.542, val_acc: 87.520, best_acc: 88.240


12/17 05:45:19 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
12/17 05:45:25 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 101/521, train_loss: 0.043(0.059), train_acc: 97.917(97.927)
12/17 05:45:31 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 201/521, train_loss: 0.074(0.056), train_acc: 96.875(98.005)
12/17 05:45:38 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 301/521, train_loss: 0.061(0.054), train_acc: 98.958(98.090)
12/17 05:45:44 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 401/521, train_loss: 0.043(0.054), train_acc: 97.917(98.122)
12/17 05:45:50 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 501/521, train_loss: 0.122(0.055), train_acc: 95.833(98.083)
12/17 05:45:52 AM [Supernet Training] lr: 0.00592 epoch: 459/600, step: 521/521, train_loss: 0.051(0.055), train_acc: 98.750(98.070)
12/17 05:45:52 AM [Supernet Training] epoch: 459, train_loss: 0.055, train_acc: 98.070
12/17 05:45:53 AM [Supernet Validation] epoch: 459, val_loss: 0.534, val_acc: 88.020, best_acc: 88.240


12/17 05:45:54 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 001/521, train_loss: 0.018(0.018), train_acc: 100.000(100.000)
12/17 05:46:00 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 101/521, train_loss: 0.119(0.053), train_acc: 93.750(98.185)
12/17 05:46:07 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 201/521, train_loss: 0.054(0.054), train_acc: 96.875(98.150)
12/17 05:46:13 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 301/521, train_loss: 0.022(0.056), train_acc: 100.000(98.097)
12/17 05:46:19 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 401/521, train_loss: 0.043(0.055), train_acc: 97.917(98.070)
12/17 05:46:26 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 501/521, train_loss: 0.037(0.056), train_acc: 98.958(98.048)
12/17 05:46:27 AM [Supernet Training] lr: 0.00588 epoch: 460/600, step: 521/521, train_loss: 0.117(0.056), train_acc: 95.000(98.036)
12/17 05:46:27 AM [Supernet Training] epoch: 460, train_loss: 0.056, train_acc: 98.036
12/17 05:46:28 AM [Supernet Validation] epoch: 460, val_loss: 0.539, val_acc: 88.080, best_acc: 88.240


12/17 05:46:29 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 001/521, train_loss: 0.093(0.093), train_acc: 95.833(95.833)
12/17 05:46:35 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 101/521, train_loss: 0.044(0.061), train_acc: 98.958(97.948)
12/17 05:46:41 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 201/521, train_loss: 0.056(0.057), train_acc: 96.875(98.103)
12/17 05:46:48 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 301/521, train_loss: 0.053(0.057), train_acc: 97.917(98.072)
12/17 05:46:54 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 401/521, train_loss: 0.040(0.056), train_acc: 98.958(98.080)
12/17 05:47:01 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 501/521, train_loss: 0.052(0.055), train_acc: 97.917(98.100)
12/17 05:47:02 AM [Supernet Training] lr: 0.00583 epoch: 461/600, step: 521/521, train_loss: 0.052(0.055), train_acc: 98.750(98.120)
12/17 05:47:02 AM [Supernet Training] epoch: 461, train_loss: 0.055, train_acc: 98.120
12/17 05:47:04 AM [Supernet Validation] epoch: 461, val_loss: 0.538, val_acc: 87.870, best_acc: 88.240


12/17 05:47:04 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 001/521, train_loss: 0.064(0.064), train_acc: 98.958(98.958)
12/17 05:47:10 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 101/521, train_loss: 0.058(0.059), train_acc: 96.875(97.937)
12/17 05:47:17 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 201/521, train_loss: 0.025(0.056), train_acc: 100.000(98.041)
12/17 05:47:23 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 301/521, train_loss: 0.038(0.056), train_acc: 98.958(97.986)
12/17 05:47:30 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 401/521, train_loss: 0.023(0.056), train_acc: 98.958(98.013)
12/17 05:47:36 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 501/521, train_loss: 0.125(0.055), train_acc: 94.792(98.041)
12/17 05:47:37 AM [Supernet Training] lr: 0.00579 epoch: 462/600, step: 521/521, train_loss: 0.119(0.055), train_acc: 95.000(98.058)
12/17 05:47:37 AM [Supernet Training] epoch: 462, train_loss: 0.055, train_acc: 98.058
12/17 05:47:39 AM [Supernet Validation] epoch: 462, val_loss: 0.537, val_acc: 87.590, best_acc: 88.240


12/17 05:47:39 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 001/521, train_loss: 0.020(0.020), train_acc: 100.000(100.000)
12/17 05:47:46 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 101/521, train_loss: 0.054(0.050), train_acc: 97.917(98.247)
12/17 05:47:53 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 201/521, train_loss: 0.038(0.052), train_acc: 98.958(98.197)
12/17 05:47:59 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 301/521, train_loss: 0.053(0.053), train_acc: 98.958(98.183)
12/17 05:48:05 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 401/521, train_loss: 0.061(0.053), train_acc: 96.875(98.148)
12/17 05:48:12 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 501/521, train_loss: 0.030(0.053), train_acc: 98.958(98.145)
12/17 05:48:13 AM [Supernet Training] lr: 0.00575 epoch: 463/600, step: 521/521, train_loss: 0.043(0.053), train_acc: 97.500(98.144)
12/17 05:48:13 AM [Supernet Training] epoch: 463, train_loss: 0.053, train_acc: 98.144
12/17 05:48:15 AM [Supernet Validation] epoch: 463, val_loss: 0.528, val_acc: 87.850, best_acc: 88.240


12/17 05:48:15 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 001/521, train_loss: 0.029(0.029), train_acc: 98.958(98.958)
12/17 05:48:21 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 101/521, train_loss: 0.048(0.049), train_acc: 97.917(98.257)
12/17 05:48:28 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 201/521, train_loss: 0.033(0.053), train_acc: 98.958(98.077)
12/17 05:48:34 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 301/521, train_loss: 0.034(0.054), train_acc: 100.000(98.100)
12/17 05:48:40 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 401/521, train_loss: 0.019(0.056), train_acc: 100.000(98.075)
12/17 05:48:47 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 501/521, train_loss: 0.015(0.055), train_acc: 100.000(98.081)
12/17 05:48:48 AM [Supernet Training] lr: 0.00571 epoch: 464/600, step: 521/521, train_loss: 0.042(0.055), train_acc: 98.750(98.098)
12/17 05:48:48 AM [Supernet Training] epoch: 464, train_loss: 0.055, train_acc: 98.098
12/17 05:48:50 AM [Supernet Validation] epoch: 464, val_loss: 0.530, val_acc: 87.950, best_acc: 88.240


12/17 05:48:50 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 001/521, train_loss: 0.058(0.058), train_acc: 98.958(98.958)
12/17 05:48:56 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 101/521, train_loss: 0.067(0.050), train_acc: 97.917(98.257)
12/17 05:49:02 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 201/521, train_loss: 0.046(0.052), train_acc: 97.917(98.197)
12/17 05:49:09 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 301/521, train_loss: 0.036(0.052), train_acc: 97.917(98.207)
12/17 05:49:15 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 401/521, train_loss: 0.024(0.052), train_acc: 98.958(98.202)
12/17 05:49:21 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 501/521, train_loss: 0.193(0.053), train_acc: 91.667(98.158)
12/17 05:49:23 AM [Supernet Training] lr: 0.00567 epoch: 465/600, step: 521/521, train_loss: 0.083(0.053), train_acc: 97.500(98.148)
12/17 05:49:23 AM [Supernet Training] epoch: 465, train_loss: 0.053, train_acc: 98.148
12/17 05:49:24 AM [Supernet Validation] epoch: 465, val_loss: 0.543, val_acc: 87.630, best_acc: 88.240


12/17 05:49:25 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 001/521, train_loss: 0.112(0.112), train_acc: 95.833(95.833)
12/17 05:49:31 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 101/521, train_loss: 0.064(0.060), train_acc: 96.875(97.958)
12/17 05:49:37 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 201/521, train_loss: 0.032(0.056), train_acc: 100.000(98.036)
12/17 05:49:43 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 301/521, train_loss: 0.037(0.056), train_acc: 100.000(98.010)
12/17 05:49:50 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 401/521, train_loss: 0.072(0.056), train_acc: 97.917(98.057)
12/17 05:49:57 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 501/521, train_loss: 0.052(0.055), train_acc: 96.875(98.060)
12/17 05:49:58 AM [Supernet Training] lr: 0.00562 epoch: 466/600, step: 521/521, train_loss: 0.075(0.056), train_acc: 96.250(98.034)
12/17 05:49:58 AM [Supernet Training] epoch: 466, train_loss: 0.056, train_acc: 98.034
12/17 05:49:59 AM [Supernet Validation] epoch: 466, val_loss: 0.554, val_acc: 87.730, best_acc: 88.240


12/17 05:50:00 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 001/521, train_loss: 0.013(0.013), train_acc: 100.000(100.000)
12/17 05:50:06 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 101/521, train_loss: 0.042(0.054), train_acc: 98.958(98.133)
12/17 05:50:13 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 201/521, train_loss: 0.085(0.057), train_acc: 95.833(98.051)
12/17 05:50:19 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 301/521, train_loss: 0.108(0.056), train_acc: 96.875(98.038)
12/17 05:50:25 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 401/521, train_loss: 0.080(0.056), train_acc: 96.875(98.031)
12/17 05:50:32 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 501/521, train_loss: 0.055(0.055), train_acc: 96.875(98.037)
12/17 05:50:33 AM [Supernet Training] lr: 0.00558 epoch: 467/600, step: 521/521, train_loss: 0.072(0.055), train_acc: 97.500(98.058)
12/17 05:50:33 AM [Supernet Training] epoch: 467, train_loss: 0.055, train_acc: 98.058
12/17 05:50:34 AM [Supernet Validation] epoch: 467, val_loss: 0.527, val_acc: 88.190, best_acc: 88.240


12/17 05:50:35 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 001/521, train_loss: 0.068(0.068), train_acc: 96.875(96.875)
12/17 05:50:41 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 101/521, train_loss: 0.036(0.053), train_acc: 98.958(98.195)
12/17 05:50:47 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 201/521, train_loss: 0.020(0.052), train_acc: 100.000(98.171)
12/17 05:50:53 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 301/521, train_loss: 0.037(0.051), train_acc: 98.958(98.249)
12/17 05:51:00 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 401/521, train_loss: 0.046(0.053), train_acc: 97.917(98.202)
12/17 05:51:06 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 501/521, train_loss: 0.035(0.053), train_acc: 97.917(98.170)
12/17 05:51:07 AM [Supernet Training] lr: 0.00554 epoch: 468/600, step: 521/521, train_loss: 0.070(0.053), train_acc: 98.750(98.188)
12/17 05:51:07 AM [Supernet Training] epoch: 468, train_loss: 0.053, train_acc: 98.188
12/17 05:51:09 AM [Supernet Validation] epoch: 468, val_loss: 0.528, val_acc: 87.990, best_acc: 88.240


12/17 05:51:09 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 96.875(96.875)
12/17 05:51:15 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 101/521, train_loss: 0.052(0.053), train_acc: 97.917(98.144)
12/17 05:51:22 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 201/521, train_loss: 0.082(0.052), train_acc: 96.875(98.243)
12/17 05:51:28 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 301/521, train_loss: 0.090(0.053), train_acc: 96.875(98.187)
12/17 05:51:34 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 401/521, train_loss: 0.039(0.053), train_acc: 98.958(98.187)
12/17 05:51:41 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 501/521, train_loss: 0.031(0.052), train_acc: 98.958(98.202)
12/17 05:51:42 AM [Supernet Training] lr: 0.00550 epoch: 469/600, step: 521/521, train_loss: 0.049(0.053), train_acc: 98.750(98.184)
12/17 05:51:42 AM [Supernet Training] epoch: 469, train_loss: 0.053, train_acc: 98.184
12/17 05:51:43 AM [Supernet Validation] epoch: 469, val_loss: 0.519, val_acc: 88.040, best_acc: 88.240


12/17 05:51:44 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 001/521, train_loss: 0.047(0.047), train_acc: 96.875(96.875)
12/17 05:51:50 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 101/521, train_loss: 0.031(0.053), train_acc: 97.917(98.175)
12/17 05:51:56 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 201/521, train_loss: 0.023(0.057), train_acc: 100.000(98.062)
12/17 05:52:02 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 301/521, train_loss: 0.051(0.057), train_acc: 97.917(98.024)
12/17 05:52:09 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 401/521, train_loss: 0.052(0.055), train_acc: 97.917(98.054)
12/17 05:52:15 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 501/521, train_loss: 0.032(0.055), train_acc: 98.958(98.068)
12/17 05:52:16 AM [Supernet Training] lr: 0.00546 epoch: 470/600, step: 521/521, train_loss: 0.089(0.056), train_acc: 97.500(98.072)
12/17 05:52:17 AM [Supernet Training] epoch: 470, train_loss: 0.056, train_acc: 98.072
12/17 05:52:18 AM [Supernet Validation] epoch: 470, val_loss: 0.529, val_acc: 87.980, best_acc: 88.240


12/17 05:52:18 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 001/521, train_loss: 0.086(0.086), train_acc: 94.792(94.792)
12/17 05:52:25 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 101/521, train_loss: 0.063(0.049), train_acc: 96.875(98.329)
12/17 05:52:31 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 201/521, train_loss: 0.022(0.049), train_acc: 98.958(98.373)
12/17 05:52:37 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 301/521, train_loss: 0.070(0.050), train_acc: 96.875(98.273)
12/17 05:52:43 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 401/521, train_loss: 0.024(0.050), train_acc: 100.000(98.239)
12/17 05:52:50 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 501/521, train_loss: 0.072(0.051), train_acc: 97.917(98.174)
12/17 05:52:51 AM [Supernet Training] lr: 0.00542 epoch: 471/600, step: 521/521, train_loss: 0.028(0.051), train_acc: 98.750(98.180)
12/17 05:52:51 AM [Supernet Training] epoch: 471, train_loss: 0.051, train_acc: 98.180
12/17 05:52:52 AM [Supernet Validation] epoch: 471, val_loss: 0.563, val_acc: 87.440, best_acc: 88.240


12/17 05:52:53 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 001/521, train_loss: 0.035(0.035), train_acc: 98.958(98.958)
12/17 05:52:59 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 101/521, train_loss: 0.064(0.048), train_acc: 97.917(98.319)
12/17 05:53:05 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 201/521, train_loss: 0.025(0.051), train_acc: 98.958(98.243)
12/17 05:53:12 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 301/521, train_loss: 0.029(0.052), train_acc: 100.000(98.131)
12/17 05:53:18 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 401/521, train_loss: 0.084(0.053), train_acc: 96.875(98.106)
12/17 05:53:24 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 501/521, train_loss: 0.051(0.053), train_acc: 97.917(98.114)
12/17 05:53:25 AM [Supernet Training] lr: 0.00537 epoch: 472/600, step: 521/521, train_loss: 0.027(0.053), train_acc: 100.000(98.134)
12/17 05:53:26 AM [Supernet Training] epoch: 472, train_loss: 0.053, train_acc: 98.134
12/17 05:53:27 AM [Supernet Validation] epoch: 472, val_loss: 0.539, val_acc: 87.770, best_acc: 88.240


12/17 05:53:27 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 001/521, train_loss: 0.076(0.076), train_acc: 98.958(98.958)
12/17 05:53:34 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 101/521, train_loss: 0.055(0.048), train_acc: 97.917(98.360)
12/17 05:53:40 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 201/521, train_loss: 0.061(0.050), train_acc: 97.917(98.311)
12/17 05:53:46 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 301/521, train_loss: 0.045(0.050), train_acc: 98.958(98.228)
12/17 05:53:53 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 401/521, train_loss: 0.104(0.051), train_acc: 95.833(98.223)
12/17 05:53:59 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 501/521, train_loss: 0.083(0.051), train_acc: 95.833(98.237)
12/17 05:54:01 AM [Supernet Training] lr: 0.00533 epoch: 473/600, step: 521/521, train_loss: 0.018(0.051), train_acc: 100.000(98.242)
12/17 05:54:01 AM [Supernet Training] epoch: 473, train_loss: 0.051, train_acc: 98.242
12/17 05:54:02 AM [Supernet Validation] epoch: 473, val_loss: 0.534, val_acc: 87.920, best_acc: 88.240


12/17 05:54:03 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 001/521, train_loss: 0.086(0.086), train_acc: 96.875(96.875)
12/17 05:54:09 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 101/521, train_loss: 0.059(0.053), train_acc: 98.958(98.133)
12/17 05:54:16 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 201/521, train_loss: 0.029(0.055), train_acc: 98.958(98.103)
12/17 05:54:22 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 301/521, train_loss: 0.150(0.052), train_acc: 95.833(98.194)
12/17 05:54:28 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 401/521, train_loss: 0.055(0.051), train_acc: 97.917(98.254)
12/17 05:54:35 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 501/521, train_loss: 0.040(0.052), train_acc: 98.958(98.224)
12/17 05:54:36 AM [Supernet Training] lr: 0.00529 epoch: 474/600, step: 521/521, train_loss: 0.051(0.052), train_acc: 98.750(98.232)
12/17 05:54:36 AM [Supernet Training] epoch: 474, train_loss: 0.052, train_acc: 98.232
12/17 05:54:37 AM [Supernet Validation] epoch: 474, val_loss: 0.551, val_acc: 87.880, best_acc: 88.240


12/17 05:54:38 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 001/521, train_loss: 0.021(0.021), train_acc: 98.958(98.958)
12/17 05:54:44 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 101/521, train_loss: 0.034(0.049), train_acc: 97.917(98.236)
12/17 05:54:50 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 201/521, train_loss: 0.030(0.049), train_acc: 98.958(98.300)
12/17 05:54:57 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 301/521, train_loss: 0.091(0.050), train_acc: 96.875(98.301)
12/17 05:55:03 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 401/521, train_loss: 0.052(0.050), train_acc: 96.875(98.314)
12/17 05:55:10 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 501/521, train_loss: 0.093(0.051), train_acc: 96.875(98.262)
12/17 05:55:11 AM [Supernet Training] lr: 0.00525 epoch: 475/600, step: 521/521, train_loss: 0.058(0.051), train_acc: 97.500(98.268)
12/17 05:55:11 AM [Supernet Training] epoch: 475, train_loss: 0.051, train_acc: 98.268
12/17 05:55:13 AM [Supernet Validation] epoch: 475, val_loss: 0.534, val_acc: 88.220, best_acc: 88.240


12/17 05:55:13 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 001/521, train_loss: 0.046(0.046), train_acc: 98.958(98.958)
12/17 05:55:19 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 101/521, train_loss: 0.041(0.046), train_acc: 98.958(98.432)
12/17 05:55:26 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 201/521, train_loss: 0.048(0.050), train_acc: 97.917(98.212)
12/17 05:55:32 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 301/521, train_loss: 0.014(0.051), train_acc: 100.000(98.162)
12/17 05:55:39 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 401/521, train_loss: 0.053(0.052), train_acc: 95.833(98.163)
12/17 05:55:45 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 501/521, train_loss: 0.038(0.052), train_acc: 97.917(98.141)
12/17 05:55:47 AM [Supernet Training] lr: 0.00521 epoch: 476/600, step: 521/521, train_loss: 0.018(0.052), train_acc: 100.000(98.146)
12/17 05:55:47 AM [Supernet Training] epoch: 476, train_loss: 0.052, train_acc: 98.146
12/17 05:55:48 AM [Supernet Validation] epoch: 476, val_loss: 0.551, val_acc: 87.670, best_acc: 88.240


12/17 05:55:49 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 001/521, train_loss: 0.076(0.076), train_acc: 96.875(96.875)
12/17 05:55:55 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 101/521, train_loss: 0.076(0.053), train_acc: 97.917(98.298)
12/17 05:56:02 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 201/521, train_loss: 0.030(0.053), train_acc: 98.958(98.228)
12/17 05:56:08 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 301/521, train_loss: 0.048(0.052), train_acc: 97.917(98.242)
12/17 05:56:14 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 401/521, train_loss: 0.028(0.052), train_acc: 98.958(98.195)
12/17 05:56:21 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 501/521, train_loss: 0.058(0.052), train_acc: 97.917(98.199)
12/17 05:56:22 AM [Supernet Training] lr: 0.00517 epoch: 477/600, step: 521/521, train_loss: 0.111(0.052), train_acc: 97.500(98.202)
12/17 05:56:22 AM [Supernet Training] epoch: 477, train_loss: 0.052, train_acc: 98.202
12/17 05:56:24 AM [Supernet Validation] epoch: 477, val_loss: 0.529, val_acc: 88.030, best_acc: 88.240


12/17 05:56:24 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 001/521, train_loss: 0.043(0.043), train_acc: 98.958(98.958)
12/17 05:56:30 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 101/521, train_loss: 0.046(0.051), train_acc: 98.958(98.309)
12/17 05:56:37 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 201/521, train_loss: 0.054(0.051), train_acc: 97.917(98.264)
12/17 05:56:43 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 301/521, train_loss: 0.047(0.052), train_acc: 97.917(98.183)
12/17 05:56:50 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 401/521, train_loss: 0.024(0.050), train_acc: 98.958(98.291)
12/17 05:56:56 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 501/521, train_loss: 0.026(0.051), train_acc: 98.958(98.237)
12/17 05:56:57 AM [Supernet Training] lr: 0.00512 epoch: 478/600, step: 521/521, train_loss: 0.034(0.051), train_acc: 100.000(98.222)
12/17 05:56:57 AM [Supernet Training] epoch: 478, train_loss: 0.051, train_acc: 98.222
12/17 05:56:59 AM [Supernet Validation] epoch: 478, val_loss: 0.545, val_acc: 87.840, best_acc: 88.240


12/17 05:56:59 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 001/521, train_loss: 0.027(0.027), train_acc: 98.958(98.958)
12/17 05:57:05 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 101/521, train_loss: 0.090(0.047), train_acc: 96.875(98.443)
12/17 05:57:12 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 201/521, train_loss: 0.107(0.052), train_acc: 95.833(98.254)
12/17 05:57:18 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 301/521, train_loss: 0.039(0.051), train_acc: 97.917(98.232)
12/17 05:57:24 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 401/521, train_loss: 0.103(0.051), train_acc: 95.833(98.247)
12/17 05:57:31 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 501/521, train_loss: 0.094(0.051), train_acc: 96.875(98.233)
12/17 05:57:32 AM [Supernet Training] lr: 0.00508 epoch: 479/600, step: 521/521, train_loss: 0.019(0.052), train_acc: 98.750(98.212)
12/17 05:57:32 AM [Supernet Training] epoch: 479, train_loss: 0.052, train_acc: 98.212
12/17 05:57:34 AM [Supernet Validation] epoch: 479, val_loss: 0.533, val_acc: 87.770, best_acc: 88.240


12/17 05:57:34 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 96.875(96.875)
12/17 05:57:40 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 101/521, train_loss: 0.023(0.048), train_acc: 98.958(98.443)
12/17 05:57:47 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 201/521, train_loss: 0.063(0.050), train_acc: 97.917(98.269)
12/17 05:57:53 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 301/521, train_loss: 0.037(0.052), train_acc: 97.917(98.207)
12/17 05:58:00 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 401/521, train_loss: 0.082(0.053), train_acc: 95.833(98.176)
12/17 05:58:06 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 501/521, train_loss: 0.088(0.051), train_acc: 96.875(98.243)
12/17 05:58:07 AM [Supernet Training] lr: 0.00504 epoch: 480/600, step: 521/521, train_loss: 0.048(0.052), train_acc: 98.750(98.226)
12/17 05:58:07 AM [Supernet Training] epoch: 480, train_loss: 0.052, train_acc: 98.226
12/17 05:58:09 AM [Supernet Validation] epoch: 480, val_loss: 0.533, val_acc: 87.960, best_acc: 88.240


12/17 05:58:10 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 97.917(97.917)
12/17 05:58:16 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 101/521, train_loss: 0.046(0.045), train_acc: 98.958(98.453)
12/17 05:58:22 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 201/521, train_loss: 0.057(0.046), train_acc: 96.875(98.399)
12/17 05:58:29 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 301/521, train_loss: 0.063(0.046), train_acc: 97.917(98.387)
12/17 05:58:35 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 401/521, train_loss: 0.093(0.048), train_acc: 95.833(98.309)
12/17 05:58:42 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 501/521, train_loss: 0.020(0.047), train_acc: 98.958(98.335)
12/17 05:58:43 AM [Supernet Training] lr: 0.00500 epoch: 481/600, step: 521/521, train_loss: 0.058(0.048), train_acc: 98.750(98.322)
12/17 05:58:43 AM [Supernet Training] epoch: 481, train_loss: 0.048, train_acc: 98.322
12/17 05:58:44 AM [Supernet Validation] epoch: 481, val_loss: 0.544, val_acc: 87.630, best_acc: 88.240


12/17 05:58:45 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 001/521, train_loss: 0.109(0.109), train_acc: 94.792(94.792)
12/17 05:58:51 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 101/521, train_loss: 0.064(0.051), train_acc: 95.833(98.278)
12/17 05:58:58 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 201/521, train_loss: 0.025(0.049), train_acc: 98.958(98.316)
12/17 05:59:04 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 301/521, train_loss: 0.047(0.049), train_acc: 95.833(98.328)
12/17 05:59:11 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 401/521, train_loss: 0.061(0.049), train_acc: 98.958(98.340)
12/17 05:59:18 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 501/521, train_loss: 0.091(0.050), train_acc: 95.833(98.297)
12/17 05:59:19 AM [Supernet Training] lr: 0.00496 epoch: 482/600, step: 521/521, train_loss: 0.015(0.050), train_acc: 98.750(98.308)
12/17 05:59:19 AM [Supernet Training] epoch: 482, train_loss: 0.050, train_acc: 98.308
12/17 05:59:20 AM [Supernet Validation] epoch: 482, val_loss: 0.544, val_acc: 88.060, best_acc: 88.240


12/17 05:59:21 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 97.917(97.917)
12/17 05:59:27 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 101/521, train_loss: 0.038(0.046), train_acc: 98.958(98.484)
12/17 05:59:34 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 201/521, train_loss: 0.032(0.048), train_acc: 98.958(98.388)
12/17 05:59:40 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 301/521, train_loss: 0.018(0.049), train_acc: 98.958(98.401)
12/17 05:59:46 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 401/521, train_loss: 0.058(0.049), train_acc: 98.958(98.374)
12/17 05:59:53 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 501/521, train_loss: 0.084(0.049), train_acc: 96.875(98.370)
12/17 05:59:54 AM [Supernet Training] lr: 0.00492 epoch: 483/600, step: 521/521, train_loss: 0.112(0.050), train_acc: 96.250(98.338)
12/17 05:59:54 AM [Supernet Training] epoch: 483, train_loss: 0.050, train_acc: 98.338
12/17 05:59:56 AM [Supernet Validation] epoch: 483, val_loss: 0.542, val_acc: 87.710, best_acc: 88.240


12/17 05:59:56 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 98.958(98.958)
12/17 06:00:03 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 101/521, train_loss: 0.134(0.053), train_acc: 94.792(98.226)
12/17 06:00:09 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 201/521, train_loss: 0.041(0.051), train_acc: 98.958(98.300)
12/17 06:00:16 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 301/521, train_loss: 0.042(0.051), train_acc: 96.875(98.297)
12/17 06:00:22 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 401/521, train_loss: 0.028(0.052), train_acc: 100.000(98.236)
12/17 06:00:28 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 501/521, train_loss: 0.038(0.052), train_acc: 98.958(98.224)
12/17 06:00:29 AM [Supernet Training] lr: 0.00487 epoch: 484/600, step: 521/521, train_loss: 0.064(0.052), train_acc: 96.250(98.226)
12/17 06:00:29 AM [Supernet Training] epoch: 484, train_loss: 0.052, train_acc: 98.226
12/17 06:00:31 AM [Supernet Validation] epoch: 484, val_loss: 0.547, val_acc: 87.940, best_acc: 88.240


12/17 06:00:31 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 001/521, train_loss: 0.024(0.024), train_acc: 100.000(100.000)
12/17 06:00:38 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 101/521, train_loss: 0.030(0.047), train_acc: 100.000(98.432)
12/17 06:00:44 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 201/521, train_loss: 0.044(0.048), train_acc: 97.917(98.404)
12/17 06:00:51 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 301/521, train_loss: 0.080(0.049), train_acc: 96.875(98.377)
12/17 06:00:57 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 401/521, train_loss: 0.081(0.048), train_acc: 97.917(98.369)
12/17 06:01:03 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 501/521, train_loss: 0.028(0.050), train_acc: 100.000(98.312)
12/17 06:01:04 AM [Supernet Training] lr: 0.00483 epoch: 485/600, step: 521/521, train_loss: 0.028(0.050), train_acc: 100.000(98.314)
12/17 06:01:04 AM [Supernet Training] epoch: 485, train_loss: 0.050, train_acc: 98.314
12/17 06:01:06 AM [Supernet Validation] epoch: 485, val_loss: 0.543, val_acc: 88.070, best_acc: 88.240


12/17 06:01:06 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 001/521, train_loss: 0.075(0.075), train_acc: 97.917(97.917)
12/17 06:01:13 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 101/521, train_loss: 0.017(0.048), train_acc: 100.000(98.401)
12/17 06:01:19 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 201/521, train_loss: 0.034(0.049), train_acc: 98.958(98.316)
12/17 06:01:26 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 301/521, train_loss: 0.031(0.050), train_acc: 98.958(98.239)
12/17 06:01:32 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 401/521, train_loss: 0.061(0.049), train_acc: 95.833(98.247)
12/17 06:01:39 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 501/521, train_loss: 0.031(0.049), train_acc: 98.958(98.287)
12/17 06:01:40 AM [Supernet Training] lr: 0.00479 epoch: 486/600, step: 521/521, train_loss: 0.217(0.050), train_acc: 93.750(98.276)
12/17 06:01:40 AM [Supernet Training] epoch: 486, train_loss: 0.050, train_acc: 98.276
12/17 06:01:42 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 06:01:42 AM [Supernet Validation] epoch: 486, val_loss: 0.529, val_acc: 88.350, best_acc: 88.350


12/17 06:01:42 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 96.875(96.875)
12/17 06:01:49 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 101/521, train_loss: 0.033(0.045), train_acc: 98.958(98.422)
12/17 06:01:55 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 201/521, train_loss: 0.021(0.046), train_acc: 98.958(98.378)
12/17 06:02:02 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 301/521, train_loss: 0.088(0.049), train_acc: 96.875(98.277)
12/17 06:02:08 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 401/521, train_loss: 0.038(0.048), train_acc: 98.958(98.309)
12/17 06:02:15 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 501/521, train_loss: 0.045(0.049), train_acc: 98.958(98.268)
12/17 06:02:16 AM [Supernet Training] lr: 0.00475 epoch: 487/600, step: 521/521, train_loss: 0.031(0.049), train_acc: 98.750(98.258)
12/17 06:02:16 AM [Supernet Training] epoch: 487, train_loss: 0.049, train_acc: 98.258
12/17 06:02:17 AM [Supernet Validation] epoch: 487, val_loss: 0.551, val_acc: 87.950, best_acc: 88.350


12/17 06:02:18 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 98.958(98.958)
12/17 06:02:24 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 101/521, train_loss: 0.104(0.047), train_acc: 94.792(98.381)
12/17 06:02:30 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 201/521, train_loss: 0.057(0.046), train_acc: 97.917(98.414)
12/17 06:02:37 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 301/521, train_loss: 0.064(0.047), train_acc: 96.875(98.380)
12/17 06:02:43 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 401/521, train_loss: 0.153(0.046), train_acc: 92.708(98.431)
12/17 06:02:49 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 501/521, train_loss: 0.058(0.046), train_acc: 96.875(98.430)
12/17 06:02:51 AM [Supernet Training] lr: 0.00471 epoch: 488/600, step: 521/521, train_loss: 0.090(0.047), train_acc: 96.250(98.406)
12/17 06:02:51 AM [Supernet Training] epoch: 488, train_loss: 0.047, train_acc: 98.406
12/17 06:02:52 AM [Supernet Validation] epoch: 488, val_loss: 0.540, val_acc: 88.110, best_acc: 88.350


12/17 06:02:53 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 001/521, train_loss: 0.078(0.078), train_acc: 96.875(96.875)
12/17 06:02:59 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 101/521, train_loss: 0.054(0.048), train_acc: 97.917(98.185)
12/17 06:03:05 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 201/521, train_loss: 0.053(0.044), train_acc: 97.917(98.393)
12/17 06:03:12 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 301/521, train_loss: 0.091(0.046), train_acc: 95.833(98.363)
12/17 06:03:19 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 401/521, train_loss: 0.027(0.046), train_acc: 97.917(98.335)
12/17 06:03:26 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 501/521, train_loss: 0.044(0.046), train_acc: 98.958(98.382)
12/17 06:03:27 AM [Supernet Training] lr: 0.00467 epoch: 489/600, step: 521/521, train_loss: 0.010(0.046), train_acc: 100.000(98.382)
12/17 06:03:27 AM [Supernet Training] epoch: 489, train_loss: 0.046, train_acc: 98.382
12/17 06:03:29 AM [Supernet Validation] epoch: 489, val_loss: 0.537, val_acc: 88.030, best_acc: 88.350


12/17 06:03:29 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 001/521, train_loss: 0.038(0.038), train_acc: 98.958(98.958)
12/17 06:03:36 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 101/521, train_loss: 0.018(0.048), train_acc: 100.000(98.381)
12/17 06:03:42 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 201/521, train_loss: 0.064(0.048), train_acc: 97.917(98.378)
12/17 06:03:48 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 301/521, train_loss: 0.018(0.047), train_acc: 98.958(98.408)
12/17 06:03:55 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 401/521, train_loss: 0.104(0.047), train_acc: 96.875(98.415)
12/17 06:04:01 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 501/521, train_loss: 0.046(0.047), train_acc: 96.875(98.424)
12/17 06:04:02 AM [Supernet Training] lr: 0.00463 epoch: 490/600, step: 521/521, train_loss: 0.027(0.047), train_acc: 100.000(98.422)
12/17 06:04:02 AM [Supernet Training] epoch: 490, train_loss: 0.047, train_acc: 98.422
12/17 06:04:04 AM [Supernet Validation] epoch: 490, val_loss: 0.545, val_acc: 88.260, best_acc: 88.350


12/17 06:04:04 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 100.000(100.000)
12/17 06:04:11 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 101/521, train_loss: 0.029(0.044), train_acc: 98.958(98.556)
12/17 06:04:17 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 201/521, train_loss: 0.025(0.049), train_acc: 98.958(98.336)
12/17 06:04:23 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 301/521, train_loss: 0.094(0.048), train_acc: 98.958(98.373)
12/17 06:04:30 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 401/521, train_loss: 0.007(0.048), train_acc: 100.000(98.337)
12/17 06:04:36 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 501/521, train_loss: 0.081(0.048), train_acc: 96.875(98.320)
12/17 06:04:37 AM [Supernet Training] lr: 0.00458 epoch: 491/600, step: 521/521, train_loss: 0.091(0.049), train_acc: 95.000(98.316)
12/17 06:04:37 AM [Supernet Training] epoch: 491, train_loss: 0.049, train_acc: 98.316
12/17 06:04:39 AM [Supernet Validation] epoch: 491, val_loss: 0.556, val_acc: 87.960, best_acc: 88.350


12/17 06:04:39 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 97.917(97.917)
12/17 06:04:46 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 101/521, train_loss: 0.038(0.044), train_acc: 97.917(98.494)
12/17 06:04:52 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 201/521, train_loss: 0.041(0.045), train_acc: 98.958(98.476)
12/17 06:04:59 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 301/521, train_loss: 0.010(0.044), train_acc: 100.000(98.547)
12/17 06:05:05 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 401/521, train_loss: 0.109(0.045), train_acc: 95.833(98.504)
12/17 06:05:11 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 501/521, train_loss: 0.039(0.045), train_acc: 98.958(98.459)
12/17 06:05:13 AM [Supernet Training] lr: 0.00454 epoch: 492/600, step: 521/521, train_loss: 0.063(0.046), train_acc: 97.500(98.450)
12/17 06:05:13 AM [Supernet Training] epoch: 492, train_loss: 0.046, train_acc: 98.450
12/17 06:05:14 AM [Supernet Validation] epoch: 492, val_loss: 0.539, val_acc: 87.990, best_acc: 88.350


12/17 06:05:14 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 001/521, train_loss: 0.064(0.064), train_acc: 97.917(97.917)
12/17 06:05:21 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 101/521, train_loss: 0.022(0.043), train_acc: 100.000(98.546)
12/17 06:05:27 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 201/521, train_loss: 0.036(0.044), train_acc: 98.958(98.461)
12/17 06:05:34 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 301/521, train_loss: 0.028(0.045), train_acc: 100.000(98.481)
12/17 06:05:40 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 401/521, train_loss: 0.035(0.045), train_acc: 98.958(98.447)
12/17 06:05:46 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 501/521, train_loss: 0.089(0.046), train_acc: 97.917(98.439)
12/17 06:05:47 AM [Supernet Training] lr: 0.00450 epoch: 493/600, step: 521/521, train_loss: 0.082(0.045), train_acc: 96.250(98.434)
12/17 06:05:48 AM [Supernet Training] epoch: 493, train_loss: 0.045, train_acc: 98.434
12/17 06:05:49 AM [Supernet Validation] epoch: 493, val_loss: 0.532, val_acc: 87.810, best_acc: 88.350


12/17 06:05:49 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 001/521, train_loss: 0.039(0.039), train_acc: 98.958(98.958)
12/17 06:05:56 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 101/521, train_loss: 0.062(0.046), train_acc: 96.875(98.432)
12/17 06:06:02 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 201/521, train_loss: 0.020(0.045), train_acc: 100.000(98.533)
12/17 06:06:08 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 301/521, train_loss: 0.028(0.046), train_acc: 100.000(98.522)
12/17 06:06:15 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 401/521, train_loss: 0.078(0.047), train_acc: 97.917(98.467)
12/17 06:06:21 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 501/521, train_loss: 0.032(0.047), train_acc: 100.000(98.466)
12/17 06:06:22 AM [Supernet Training] lr: 0.00446 epoch: 494/600, step: 521/521, train_loss: 0.072(0.047), train_acc: 98.750(98.444)
12/17 06:06:22 AM [Supernet Training] epoch: 494, train_loss: 0.047, train_acc: 98.444
12/17 06:06:24 AM [Supernet Validation] epoch: 494, val_loss: 0.548, val_acc: 88.160, best_acc: 88.350


12/17 06:06:24 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 001/521, train_loss: 0.054(0.054), train_acc: 97.917(97.917)
12/17 06:06:31 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 101/521, train_loss: 0.026(0.048), train_acc: 98.958(98.443)
12/17 06:06:37 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 201/521, train_loss: 0.014(0.046), train_acc: 98.958(98.507)
12/17 06:06:43 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 301/521, train_loss: 0.064(0.045), train_acc: 97.917(98.460)
12/17 06:06:50 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 401/521, train_loss: 0.032(0.047), train_acc: 98.958(98.400)
12/17 06:06:56 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 501/521, train_loss: 0.038(0.046), train_acc: 98.958(98.407)
12/17 06:06:57 AM [Supernet Training] lr: 0.00442 epoch: 495/600, step: 521/521, train_loss: 0.026(0.046), train_acc: 100.000(98.426)
12/17 06:06:57 AM [Supernet Training] epoch: 495, train_loss: 0.046, train_acc: 98.426
12/17 06:06:59 AM [Supernet Validation] epoch: 495, val_loss: 0.545, val_acc: 88.000, best_acc: 88.350


12/17 06:06:59 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 96.875(96.875)
12/17 06:07:05 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 101/521, train_loss: 0.027(0.044), train_acc: 98.958(98.381)
12/17 06:07:12 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 201/521, train_loss: 0.044(0.044), train_acc: 97.917(98.419)
12/17 06:07:18 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 301/521, train_loss: 0.009(0.044), train_acc: 100.000(98.474)
12/17 06:07:25 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 401/521, train_loss: 0.074(0.045), train_acc: 97.917(98.480)
12/17 06:07:31 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 501/521, train_loss: 0.068(0.045), train_acc: 96.875(98.501)
12/17 06:07:32 AM [Supernet Training] lr: 0.00438 epoch: 496/600, step: 521/521, train_loss: 0.042(0.045), train_acc: 100.000(98.496)
12/17 06:07:32 AM [Supernet Training] epoch: 496, train_loss: 0.045, train_acc: 98.496
12/17 06:07:34 AM [Supernet Validation] epoch: 496, val_loss: 0.547, val_acc: 87.960, best_acc: 88.350


12/17 06:07:34 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 001/521, train_loss: 0.091(0.091), train_acc: 96.875(96.875)
12/17 06:07:40 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 101/521, train_loss: 0.042(0.043), train_acc: 98.958(98.566)
12/17 06:07:47 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 201/521, train_loss: 0.019(0.044), train_acc: 100.000(98.507)
12/17 06:07:53 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 301/521, train_loss: 0.053(0.044), train_acc: 97.917(98.450)
12/17 06:07:59 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 401/521, train_loss: 0.050(0.045), train_acc: 97.917(98.434)
12/17 06:08:05 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 501/521, train_loss: 0.050(0.046), train_acc: 98.958(98.424)
12/17 06:08:07 AM [Supernet Training] lr: 0.00433 epoch: 497/600, step: 521/521, train_loss: 0.048(0.045), train_acc: 97.500(98.440)
12/17 06:08:07 AM [Supernet Training] epoch: 497, train_loss: 0.045, train_acc: 98.440
12/17 06:08:08 AM [Supernet Validation] epoch: 497, val_loss: 0.539, val_acc: 87.950, best_acc: 88.350


12/17 06:08:09 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 001/521, train_loss: 0.020(0.020), train_acc: 100.000(100.000)
12/17 06:08:15 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 101/521, train_loss: 0.046(0.047), train_acc: 97.917(98.340)
12/17 06:08:22 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 201/521, train_loss: 0.143(0.049), train_acc: 94.792(98.207)
12/17 06:08:28 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 301/521, train_loss: 0.036(0.048), train_acc: 100.000(98.277)
12/17 06:08:34 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 401/521, train_loss: 0.051(0.047), train_acc: 97.917(98.335)
12/17 06:08:41 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 501/521, train_loss: 0.141(0.047), train_acc: 95.833(98.380)
12/17 06:08:42 AM [Supernet Training] lr: 0.00429 epoch: 498/600, step: 521/521, train_loss: 0.043(0.047), train_acc: 97.500(98.384)
12/17 06:08:42 AM [Supernet Training] epoch: 498, train_loss: 0.047, train_acc: 98.384
12/17 06:08:44 AM [Supernet Validation] epoch: 498, val_loss: 0.535, val_acc: 87.800, best_acc: 88.350


12/17 06:08:44 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 001/521, train_loss: 0.011(0.011), train_acc: 100.000(100.000)
12/17 06:08:50 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 101/521, train_loss: 0.061(0.044), train_acc: 97.917(98.484)
12/17 06:08:56 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 201/521, train_loss: 0.019(0.045), train_acc: 100.000(98.399)
12/17 06:09:03 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 301/521, train_loss: 0.030(0.045), train_acc: 97.917(98.432)
12/17 06:09:09 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 401/521, train_loss: 0.019(0.044), train_acc: 100.000(98.470)
12/17 06:09:16 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 501/521, train_loss: 0.047(0.044), train_acc: 95.833(98.455)
12/17 06:09:17 AM [Supernet Training] lr: 0.00425 epoch: 499/600, step: 521/521, train_loss: 0.103(0.045), train_acc: 97.500(98.440)
12/17 06:09:17 AM [Supernet Training] epoch: 499, train_loss: 0.045, train_acc: 98.440
12/17 06:09:18 AM [Supernet Validation] epoch: 499, val_loss: 0.541, val_acc: 87.910, best_acc: 88.350


12/17 06:09:19 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 98.958(98.958)
12/17 06:09:25 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 101/521, train_loss: 0.052(0.044), train_acc: 97.917(98.546)
12/17 06:09:32 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 201/521, train_loss: 0.035(0.044), train_acc: 98.958(98.554)
12/17 06:09:39 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 301/521, train_loss: 0.049(0.044), train_acc: 98.958(98.550)
12/17 06:09:45 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 401/521, train_loss: 0.027(0.045), train_acc: 100.000(98.488)
12/17 06:09:52 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 501/521, train_loss: 0.044(0.045), train_acc: 97.917(98.455)
12/17 06:09:53 AM [Supernet Training] lr: 0.00421 epoch: 500/600, step: 521/521, train_loss: 0.029(0.046), train_acc: 98.750(98.442)
12/17 06:09:53 AM [Supernet Training] epoch: 500, train_loss: 0.046, train_acc: 98.442
12/17 06:09:55 AM [Supernet Validation] epoch: 500, val_loss: 0.539, val_acc: 88.270, best_acc: 88.350


12/17 06:09:55 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 001/521, train_loss: 0.050(0.050), train_acc: 96.875(96.875)
12/17 06:10:01 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 101/521, train_loss: 0.040(0.047), train_acc: 98.958(98.309)
12/17 06:10:08 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 201/521, train_loss: 0.060(0.046), train_acc: 97.917(98.368)
12/17 06:10:14 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 301/521, train_loss: 0.033(0.046), train_acc: 100.000(98.415)
12/17 06:10:20 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 401/521, train_loss: 0.045(0.046), train_acc: 97.917(98.449)
12/17 06:10:27 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 501/521, train_loss: 0.018(0.046), train_acc: 100.000(98.443)
12/17 06:10:28 AM [Supernet Training] lr: 0.00417 epoch: 501/600, step: 521/521, train_loss: 0.062(0.046), train_acc: 97.500(98.436)
12/17 06:10:28 AM [Supernet Training] epoch: 501, train_loss: 0.046, train_acc: 98.436
12/17 06:10:30 AM [Supernet Validation] epoch: 501, val_loss: 0.560, val_acc: 87.650, best_acc: 88.350


12/17 06:10:30 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 001/521, train_loss: 0.018(0.018), train_acc: 100.000(100.000)
12/17 06:10:36 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 101/521, train_loss: 0.006(0.043), train_acc: 100.000(98.649)
12/17 06:10:43 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 201/521, train_loss: 0.029(0.043), train_acc: 98.958(98.539)
12/17 06:10:49 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 301/521, train_loss: 0.027(0.043), train_acc: 100.000(98.526)
12/17 06:10:55 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 401/521, train_loss: 0.019(0.043), train_acc: 100.000(98.525)
12/17 06:11:02 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 501/521, train_loss: 0.030(0.044), train_acc: 98.958(98.505)
12/17 06:11:03 AM [Supernet Training] lr: 0.00413 epoch: 502/600, step: 521/521, train_loss: 0.121(0.044), train_acc: 96.250(98.510)
12/17 06:11:03 AM [Supernet Training] epoch: 502, train_loss: 0.044, train_acc: 98.510
12/17 06:11:04 AM [Supernet Validation] epoch: 502, val_loss: 0.548, val_acc: 87.930, best_acc: 88.350


12/17 06:11:05 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 001/521, train_loss: 0.019(0.019), train_acc: 100.000(100.000)
12/17 06:11:11 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 101/521, train_loss: 0.053(0.042), train_acc: 97.917(98.566)
12/17 06:11:18 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 201/521, train_loss: 0.076(0.041), train_acc: 95.833(98.549)
12/17 06:11:24 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 301/521, train_loss: 0.014(0.042), train_acc: 98.958(98.460)
12/17 06:11:31 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 401/521, train_loss: 0.052(0.043), train_acc: 97.917(98.436)
12/17 06:11:37 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 501/521, train_loss: 0.064(0.044), train_acc: 96.875(98.403)
12/17 06:11:38 AM [Supernet Training] lr: 0.00408 epoch: 503/600, step: 521/521, train_loss: 0.042(0.044), train_acc: 96.250(98.402)
12/17 06:11:39 AM [Supernet Training] epoch: 503, train_loss: 0.044, train_acc: 98.402
12/17 06:11:40 AM [Supernet Validation] epoch: 503, val_loss: 0.539, val_acc: 87.980, best_acc: 88.350


12/17 06:11:40 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 001/521, train_loss: 0.028(0.028), train_acc: 98.958(98.958)
12/17 06:11:47 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 101/521, train_loss: 0.050(0.043), train_acc: 98.958(98.443)
12/17 06:11:53 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 201/521, train_loss: 0.030(0.043), train_acc: 98.958(98.502)
12/17 06:11:59 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 301/521, train_loss: 0.068(0.045), train_acc: 97.917(98.446)
12/17 06:12:06 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 401/521, train_loss: 0.019(0.045), train_acc: 100.000(98.441)
12/17 06:12:12 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 501/521, train_loss: 0.016(0.045), train_acc: 100.000(98.436)
12/17 06:12:14 AM [Supernet Training] lr: 0.00404 epoch: 504/600, step: 521/521, train_loss: 0.112(0.045), train_acc: 93.750(98.428)
12/17 06:12:14 AM [Supernet Training] epoch: 504, train_loss: 0.045, train_acc: 98.428
12/17 06:12:15 AM [Supernet Validation] epoch: 504, val_loss: 0.544, val_acc: 87.740, best_acc: 88.350


12/17 06:12:16 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 001/521, train_loss: 0.022(0.022), train_acc: 98.958(98.958)
12/17 06:12:22 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 101/521, train_loss: 0.047(0.046), train_acc: 98.958(98.484)
12/17 06:12:28 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 201/521, train_loss: 0.012(0.047), train_acc: 100.000(98.404)
12/17 06:12:35 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 301/521, train_loss: 0.088(0.045), train_acc: 96.875(98.474)
12/17 06:12:41 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 401/521, train_loss: 0.046(0.046), train_acc: 98.958(98.449)
12/17 06:12:48 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 501/521, train_loss: 0.078(0.046), train_acc: 98.958(98.455)
12/17 06:12:49 AM [Supernet Training] lr: 0.00400 epoch: 505/600, step: 521/521, train_loss: 0.027(0.046), train_acc: 100.000(98.464)
12/17 06:12:49 AM [Supernet Training] epoch: 505, train_loss: 0.046, train_acc: 98.464
12/17 06:12:51 AM [Supernet Validation] epoch: 505, val_loss: 0.548, val_acc: 87.930, best_acc: 88.350


12/17 06:12:51 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 001/521, train_loss: 0.019(0.019), train_acc: 97.917(97.917)
12/17 06:12:57 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 101/521, train_loss: 0.045(0.045), train_acc: 97.917(98.401)
12/17 06:13:04 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 201/521, train_loss: 0.030(0.044), train_acc: 98.958(98.440)
12/17 06:13:10 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 301/521, train_loss: 0.012(0.043), train_acc: 100.000(98.553)
12/17 06:13:17 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 401/521, train_loss: 0.025(0.043), train_acc: 100.000(98.556)
12/17 06:13:23 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 501/521, train_loss: 0.024(0.043), train_acc: 100.000(98.561)
12/17 06:13:24 AM [Supernet Training] lr: 0.00396 epoch: 506/600, step: 521/521, train_loss: 0.050(0.043), train_acc: 98.750(98.576)
12/17 06:13:24 AM [Supernet Training] epoch: 506, train_loss: 0.043, train_acc: 98.576
12/17 06:13:26 AM [Supernet Validation] epoch: 506, val_loss: 0.534, val_acc: 88.170, best_acc: 88.350


12/17 06:13:26 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 001/521, train_loss: 0.040(0.040), train_acc: 98.958(98.958)
12/17 06:13:33 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 101/521, train_loss: 0.104(0.045), train_acc: 95.833(98.505)
12/17 06:13:39 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 201/521, train_loss: 0.054(0.045), train_acc: 98.958(98.440)
12/17 06:13:46 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 301/521, train_loss: 0.032(0.045), train_acc: 100.000(98.477)
12/17 06:13:52 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 401/521, train_loss: 0.020(0.045), train_acc: 98.958(98.488)
12/17 06:13:58 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 501/521, train_loss: 0.088(0.045), train_acc: 96.875(98.486)
12/17 06:13:59 AM [Supernet Training] lr: 0.00392 epoch: 507/600, step: 521/521, train_loss: 0.029(0.045), train_acc: 97.500(98.494)
12/17 06:13:59 AM [Supernet Training] epoch: 507, train_loss: 0.045, train_acc: 98.494
12/17 06:14:01 AM [Supernet Validation] epoch: 507, val_loss: 0.533, val_acc: 87.770, best_acc: 88.350


12/17 06:14:01 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 001/521, train_loss: 0.105(0.105), train_acc: 95.833(95.833)
12/17 06:14:08 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 101/521, train_loss: 0.050(0.044), train_acc: 98.958(98.505)
12/17 06:14:14 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 201/521, train_loss: 0.044(0.042), train_acc: 98.958(98.596)
12/17 06:14:21 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 301/521, train_loss: 0.037(0.045), train_acc: 98.958(98.505)
12/17 06:14:27 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 401/521, train_loss: 0.017(0.044), train_acc: 100.000(98.566)
12/17 06:14:33 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 501/521, train_loss: 0.055(0.043), train_acc: 96.875(98.559)
12/17 06:14:34 AM [Supernet Training] lr: 0.00388 epoch: 508/600, step: 521/521, train_loss: 0.045(0.043), train_acc: 100.000(98.556)
12/17 06:14:34 AM [Supernet Training] epoch: 508, train_loss: 0.043, train_acc: 98.556
12/17 06:14:36 AM [Supernet Validation] epoch: 508, val_loss: 0.534, val_acc: 88.230, best_acc: 88.350


12/17 06:14:36 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 001/521, train_loss: 0.013(0.013), train_acc: 100.000(100.000)
12/17 06:14:43 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 101/521, train_loss: 0.060(0.041), train_acc: 97.917(98.639)
12/17 06:14:49 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 201/521, train_loss: 0.062(0.043), train_acc: 96.875(98.544)
12/17 06:14:55 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 301/521, train_loss: 0.019(0.044), train_acc: 100.000(98.519)
12/17 06:15:02 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 401/521, train_loss: 0.017(0.043), train_acc: 100.000(98.517)
12/17 06:15:08 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 501/521, train_loss: 0.007(0.044), train_acc: 100.000(98.509)
12/17 06:15:09 AM [Supernet Training] lr: 0.00383 epoch: 509/600, step: 521/521, train_loss: 0.020(0.044), train_acc: 98.750(98.520)
12/17 06:15:09 AM [Supernet Training] epoch: 509, train_loss: 0.044, train_acc: 98.520
12/17 06:15:11 AM [Supernet Validation] epoch: 509, val_loss: 0.524, val_acc: 88.300, best_acc: 88.350


12/17 06:15:11 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 95.833(95.833)
12/17 06:15:18 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 101/521, train_loss: 0.031(0.043), train_acc: 97.917(98.587)
12/17 06:15:24 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 201/521, train_loss: 0.009(0.044), train_acc: 100.000(98.513)
12/17 06:15:30 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 301/521, train_loss: 0.044(0.043), train_acc: 98.958(98.564)
12/17 06:15:37 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 401/521, train_loss: 0.032(0.043), train_acc: 98.958(98.550)
12/17 06:15:43 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 501/521, train_loss: 0.045(0.043), train_acc: 98.958(98.565)
12/17 06:15:44 AM [Supernet Training] lr: 0.00379 epoch: 510/600, step: 521/521, train_loss: 0.025(0.043), train_acc: 100.000(98.564)
12/17 06:15:44 AM [Supernet Training] epoch: 510, train_loss: 0.043, train_acc: 98.564
12/17 06:15:46 AM [Supernet Validation] epoch: 510, val_loss: 0.535, val_acc: 88.300, best_acc: 88.350


12/17 06:15:46 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 001/521, train_loss: 0.024(0.024), train_acc: 98.958(98.958)
12/17 06:15:52 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 101/521, train_loss: 0.037(0.045), train_acc: 98.958(98.597)
12/17 06:15:59 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 201/521, train_loss: 0.050(0.044), train_acc: 98.958(98.611)
12/17 06:16:05 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 301/521, train_loss: 0.062(0.043), train_acc: 96.875(98.630)
12/17 06:16:11 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 401/521, train_loss: 0.044(0.043), train_acc: 97.917(98.631)
12/17 06:16:18 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 501/521, train_loss: 0.028(0.042), train_acc: 97.917(98.657)
12/17 06:16:19 AM [Supernet Training] lr: 0.00375 epoch: 511/600, step: 521/521, train_loss: 0.023(0.042), train_acc: 100.000(98.664)
12/17 06:16:19 AM [Supernet Training] epoch: 511, train_loss: 0.042, train_acc: 98.664
12/17 06:16:20 AM [Supernet Validation] epoch: 511, val_loss: 0.547, val_acc: 88.010, best_acc: 88.350


12/17 06:16:21 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 001/521, train_loss: 0.028(0.028), train_acc: 100.000(100.000)
12/17 06:16:27 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 101/521, train_loss: 0.010(0.050), train_acc: 100.000(98.267)
12/17 06:16:33 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 201/521, train_loss: 0.045(0.046), train_acc: 98.958(98.425)
12/17 06:16:40 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 301/521, train_loss: 0.071(0.046), train_acc: 96.875(98.415)
12/17 06:16:46 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 401/521, train_loss: 0.069(0.045), train_acc: 96.875(98.397)
12/17 06:16:52 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 501/521, train_loss: 0.012(0.044), train_acc: 100.000(98.426)
12/17 06:16:53 AM [Supernet Training] lr: 0.00371 epoch: 512/600, step: 521/521, train_loss: 0.088(0.044), train_acc: 97.500(98.444)
12/17 06:16:53 AM [Supernet Training] epoch: 512, train_loss: 0.044, train_acc: 98.444
12/17 06:16:55 AM [Supernet Validation] epoch: 512, val_loss: 0.553, val_acc: 88.140, best_acc: 88.350


12/17 06:16:55 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 100.000(100.000)
12/17 06:17:02 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 101/521, train_loss: 0.063(0.041), train_acc: 96.875(98.659)
12/17 06:17:08 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 201/521, train_loss: 0.030(0.040), train_acc: 98.958(98.689)
12/17 06:17:14 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 301/521, train_loss: 0.053(0.041), train_acc: 96.875(98.630)
12/17 06:17:20 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 401/521, train_loss: 0.089(0.041), train_acc: 96.875(98.644)
12/17 06:17:27 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 501/521, train_loss: 0.013(0.041), train_acc: 100.000(98.634)
12/17 06:17:28 AM [Supernet Training] lr: 0.00367 epoch: 513/600, step: 521/521, train_loss: 0.079(0.041), train_acc: 97.500(98.624)
12/17 06:17:28 AM [Supernet Training] epoch: 513, train_loss: 0.041, train_acc: 98.624
12/17 06:17:30 AM [Supernet Validation] epoch: 513, val_loss: 0.541, val_acc: 88.260, best_acc: 88.350


12/17 06:17:30 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 001/521, train_loss: 0.009(0.009), train_acc: 100.000(100.000)
12/17 06:17:36 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 101/521, train_loss: 0.025(0.044), train_acc: 98.958(98.515)
12/17 06:17:43 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 201/521, train_loss: 0.042(0.043), train_acc: 98.958(98.482)
12/17 06:17:49 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 301/521, train_loss: 0.048(0.043), train_acc: 97.917(98.505)
12/17 06:17:55 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 401/521, train_loss: 0.032(0.044), train_acc: 97.917(98.493)
12/17 06:18:01 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 501/521, train_loss: 0.020(0.043), train_acc: 98.958(98.538)
12/17 06:18:03 AM [Supernet Training] lr: 0.00363 epoch: 514/600, step: 521/521, train_loss: 0.070(0.043), train_acc: 97.500(98.536)
12/17 06:18:03 AM [Supernet Training] epoch: 514, train_loss: 0.043, train_acc: 98.536
12/17 06:18:04 AM [Supernet Validation] epoch: 514, val_loss: 0.544, val_acc: 88.190, best_acc: 88.350


12/17 06:18:05 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 97.917(97.917)
12/17 06:18:11 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 101/521, train_loss: 0.022(0.038), train_acc: 100.000(98.886)
12/17 06:18:17 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 201/521, train_loss: 0.055(0.040), train_acc: 98.958(98.710)
12/17 06:18:23 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 301/521, train_loss: 0.035(0.041), train_acc: 98.958(98.668)
12/17 06:18:30 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 401/521, train_loss: 0.016(0.040), train_acc: 100.000(98.665)
12/17 06:18:36 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 501/521, train_loss: 0.060(0.040), train_acc: 97.917(98.661)
12/17 06:18:37 AM [Supernet Training] lr: 0.00358 epoch: 515/600, step: 521/521, train_loss: 0.036(0.040), train_acc: 97.500(98.658)
12/17 06:18:37 AM [Supernet Training] epoch: 515, train_loss: 0.040, train_acc: 98.658
12/17 06:18:39 AM [Supernet Validation] epoch: 515, val_loss: 0.544, val_acc: 88.080, best_acc: 88.350


12/17 06:18:39 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 001/521, train_loss: 0.008(0.008), train_acc: 100.000(100.000)
12/17 06:18:46 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 101/521, train_loss: 0.038(0.040), train_acc: 97.917(98.659)
12/17 06:18:52 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 201/521, train_loss: 0.063(0.043), train_acc: 97.917(98.559)
12/17 06:18:58 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 301/521, train_loss: 0.065(0.043), train_acc: 97.917(98.536)
12/17 06:19:04 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 401/521, train_loss: 0.020(0.043), train_acc: 100.000(98.517)
12/17 06:19:11 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 501/521, train_loss: 0.115(0.043), train_acc: 97.917(98.553)
12/17 06:19:12 AM [Supernet Training] lr: 0.00354 epoch: 516/600, step: 521/521, train_loss: 0.063(0.042), train_acc: 98.750(98.558)
12/17 06:19:12 AM [Supernet Training] epoch: 516, train_loss: 0.042, train_acc: 98.558
12/17 06:19:13 AM [Supernet Validation] epoch: 516, val_loss: 0.552, val_acc: 88.280, best_acc: 88.350


12/17 06:19:14 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 001/521, train_loss: 0.096(0.096), train_acc: 96.875(96.875)
12/17 06:19:20 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 101/521, train_loss: 0.026(0.041), train_acc: 98.958(98.680)
12/17 06:19:27 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 201/521, train_loss: 0.094(0.041), train_acc: 96.875(98.570)
12/17 06:19:33 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 301/521, train_loss: 0.046(0.041), train_acc: 97.917(98.640)
12/17 06:19:39 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 401/521, train_loss: 0.016(0.041), train_acc: 100.000(98.618)
12/17 06:19:46 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 501/521, train_loss: 0.014(0.041), train_acc: 100.000(98.615)
12/17 06:19:47 AM [Supernet Training] lr: 0.00350 epoch: 517/600, step: 521/521, train_loss: 0.016(0.041), train_acc: 100.000(98.632)
12/17 06:19:47 AM [Supernet Training] epoch: 517, train_loss: 0.041, train_acc: 98.632
12/17 06:19:48 AM [Supernet Validation] epoch: 517, val_loss: 0.533, val_acc: 88.290, best_acc: 88.350


12/17 06:19:49 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 001/521, train_loss: 0.048(0.048), train_acc: 98.958(98.958)
12/17 06:19:55 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 101/521, train_loss: 0.017(0.043), train_acc: 100.000(98.422)
12/17 06:20:02 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 201/521, train_loss: 0.060(0.043), train_acc: 96.875(98.425)
12/17 06:20:08 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 301/521, train_loss: 0.047(0.042), train_acc: 98.958(98.505)
12/17 06:20:14 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 401/521, train_loss: 0.079(0.043), train_acc: 96.875(98.519)
12/17 06:20:21 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 501/521, train_loss: 0.061(0.042), train_acc: 97.917(98.526)
12/17 06:20:22 AM [Supernet Training] lr: 0.00346 epoch: 518/600, step: 521/521, train_loss: 0.026(0.042), train_acc: 100.000(98.532)
12/17 06:20:22 AM [Supernet Training] epoch: 518, train_loss: 0.042, train_acc: 98.532
12/17 06:20:24 AM [Supernet Validation] epoch: 518, val_loss: 0.536, val_acc: 87.930, best_acc: 88.350


12/17 06:20:24 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 001/521, train_loss: 0.018(0.018), train_acc: 100.000(100.000)
12/17 06:20:30 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 101/521, train_loss: 0.029(0.039), train_acc: 98.958(98.866)
12/17 06:20:37 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 201/521, train_loss: 0.011(0.039), train_acc: 100.000(98.767)
12/17 06:20:43 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 301/521, train_loss: 0.055(0.039), train_acc: 98.958(98.737)
12/17 06:20:49 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 401/521, train_loss: 0.030(0.040), train_acc: 98.958(98.683)
12/17 06:20:55 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 501/521, train_loss: 0.039(0.041), train_acc: 98.958(98.626)
12/17 06:20:57 AM [Supernet Training] lr: 0.00342 epoch: 519/600, step: 521/521, train_loss: 0.074(0.041), train_acc: 96.250(98.630)
12/17 06:20:57 AM [Supernet Training] epoch: 519, train_loss: 0.041, train_acc: 98.630
12/17 06:20:58 AM [Supernet Validation] epoch: 519, val_loss: 0.553, val_acc: 87.810, best_acc: 88.350


12/17 06:20:59 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 001/521, train_loss: 0.059(0.059), train_acc: 97.917(97.917)
12/17 06:21:05 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 101/521, train_loss: 0.055(0.042), train_acc: 97.917(98.690)
12/17 06:21:11 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 201/521, train_loss: 0.033(0.040), train_acc: 98.958(98.673)
12/17 06:21:18 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 301/521, train_loss: 0.042(0.041), train_acc: 96.875(98.612)
12/17 06:21:24 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 401/521, train_loss: 0.028(0.041), train_acc: 98.958(98.615)
12/17 06:21:31 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 501/521, train_loss: 0.052(0.041), train_acc: 96.875(98.597)
12/17 06:21:32 AM [Supernet Training] lr: 0.00338 epoch: 520/600, step: 521/521, train_loss: 0.094(0.041), train_acc: 97.500(98.596)
12/17 06:21:32 AM [Supernet Training] epoch: 520, train_loss: 0.041, train_acc: 98.596
12/17 06:21:33 AM [Supernet Validation] epoch: 520, val_loss: 0.553, val_acc: 88.250, best_acc: 88.350


12/17 06:21:34 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 001/521, train_loss: 0.117(0.117), train_acc: 93.750(93.750)
12/17 06:21:40 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 101/521, train_loss: 0.073(0.038), train_acc: 97.917(98.773)
12/17 06:21:46 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 201/521, train_loss: 0.050(0.038), train_acc: 98.958(98.772)
12/17 06:21:53 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 301/521, train_loss: 0.043(0.038), train_acc: 97.917(98.740)
12/17 06:21:59 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 401/521, train_loss: 0.066(0.038), train_acc: 97.917(98.756)
12/17 06:22:05 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 501/521, train_loss: 0.015(0.038), train_acc: 98.958(98.763)
12/17 06:22:07 AM [Supernet Training] lr: 0.00333 epoch: 521/600, step: 521/521, train_loss: 0.017(0.038), train_acc: 100.000(98.760)
12/17 06:22:07 AM [Supernet Training] epoch: 521, train_loss: 0.038, train_acc: 98.760
12/17 06:22:08 AM [Supernet Validation] epoch: 521, val_loss: 0.547, val_acc: 88.080, best_acc: 88.350


12/17 06:22:08 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 98.958(98.958)
12/17 06:22:15 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 101/521, train_loss: 0.045(0.038), train_acc: 98.958(98.680)
12/17 06:22:21 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 201/521, train_loss: 0.074(0.040), train_acc: 97.917(98.570)
12/17 06:22:28 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 301/521, train_loss: 0.014(0.040), train_acc: 100.000(98.609)
12/17 06:22:34 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 401/521, train_loss: 0.034(0.041), train_acc: 100.000(98.579)
12/17 06:22:41 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 501/521, train_loss: 0.023(0.041), train_acc: 98.958(98.609)
12/17 06:22:42 AM [Supernet Training] lr: 0.00329 epoch: 522/600, step: 521/521, train_loss: 0.037(0.041), train_acc: 98.750(98.614)
12/17 06:22:42 AM [Supernet Training] epoch: 522, train_loss: 0.041, train_acc: 98.614
12/17 06:22:43 AM [Supernet Validation] epoch: 522, val_loss: 0.548, val_acc: 88.050, best_acc: 88.350


12/17 06:22:44 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 97.917(97.917)
12/17 06:22:50 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 101/521, train_loss: 0.019(0.039), train_acc: 98.958(98.711)
12/17 06:22:57 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 201/521, train_loss: 0.042(0.039), train_acc: 97.917(98.710)
12/17 06:23:03 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 301/521, train_loss: 0.041(0.040), train_acc: 97.917(98.664)
12/17 06:23:10 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 401/521, train_loss: 0.059(0.041), train_acc: 96.875(98.621)
12/17 06:23:16 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 501/521, train_loss: 0.031(0.041), train_acc: 98.958(98.628)
12/17 06:23:17 AM [Supernet Training] lr: 0.00325 epoch: 523/600, step: 521/521, train_loss: 0.029(0.040), train_acc: 100.000(98.640)
12/17 06:23:17 AM [Supernet Training] epoch: 523, train_loss: 0.040, train_acc: 98.640
12/17 06:23:19 AM [Supernet Validation] epoch: 523, val_loss: 0.530, val_acc: 88.190, best_acc: 88.350


12/17 06:23:19 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 001/521, train_loss: 0.059(0.059), train_acc: 97.917(97.917)
12/17 06:23:26 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 101/521, train_loss: 0.024(0.038), train_acc: 97.917(98.721)
12/17 06:23:32 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 201/521, train_loss: 0.010(0.040), train_acc: 100.000(98.653)
12/17 06:23:38 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 301/521, train_loss: 0.071(0.042), train_acc: 96.875(98.588)
12/17 06:23:45 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 401/521, train_loss: 0.079(0.042), train_acc: 96.875(98.592)
12/17 06:23:51 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 501/521, train_loss: 0.013(0.042), train_acc: 100.000(98.580)
12/17 06:23:52 AM [Supernet Training] lr: 0.00321 epoch: 524/600, step: 521/521, train_loss: 0.077(0.042), train_acc: 97.500(98.578)
12/17 06:23:52 AM [Supernet Training] epoch: 524, train_loss: 0.042, train_acc: 98.578
12/17 06:23:54 AM [Supernet Validation] epoch: 524, val_loss: 0.537, val_acc: 88.250, best_acc: 88.350


12/17 06:23:54 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 001/521, train_loss: 0.088(0.088), train_acc: 96.875(96.875)
12/17 06:24:00 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 101/521, train_loss: 0.009(0.037), train_acc: 100.000(98.783)
12/17 06:24:07 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 201/521, train_loss: 0.019(0.038), train_acc: 100.000(98.772)
12/17 06:24:13 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 301/521, train_loss: 0.075(0.037), train_acc: 98.958(98.785)
12/17 06:24:19 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 401/521, train_loss: 0.038(0.038), train_acc: 98.958(98.751)
12/17 06:24:26 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 501/521, train_loss: 0.017(0.038), train_acc: 100.000(98.761)
12/17 06:24:27 AM [Supernet Training] lr: 0.00317 epoch: 525/600, step: 521/521, train_loss: 0.029(0.038), train_acc: 98.750(98.760)
12/17 06:24:27 AM [Supernet Training] epoch: 525, train_loss: 0.038, train_acc: 98.760
12/17 06:24:29 AM [Supernet Validation] epoch: 525, val_loss: 0.549, val_acc: 88.210, best_acc: 88.350


12/17 06:24:29 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 001/521, train_loss: 0.027(0.027), train_acc: 98.958(98.958)
12/17 06:24:35 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 101/521, train_loss: 0.024(0.038), train_acc: 100.000(98.680)
12/17 06:24:42 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 201/521, train_loss: 0.024(0.038), train_acc: 100.000(98.808)
12/17 06:24:48 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 301/521, train_loss: 0.014(0.039), train_acc: 100.000(98.702)
12/17 06:24:54 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 401/521, train_loss: 0.082(0.039), train_acc: 96.875(98.722)
12/17 06:25:01 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 501/521, train_loss: 0.017(0.040), train_acc: 100.000(98.725)
12/17 06:25:02 AM [Supernet Training] lr: 0.00313 epoch: 526/600, step: 521/521, train_loss: 0.018(0.039), train_acc: 100.000(98.730)
12/17 06:25:02 AM [Supernet Training] epoch: 526, train_loss: 0.039, train_acc: 98.730
12/17 06:25:04 AM [Supernet Validation] epoch: 526, val_loss: 0.544, val_acc: 88.160, best_acc: 88.350


12/17 06:25:04 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
12/17 06:25:10 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 101/521, train_loss: 0.134(0.035), train_acc: 95.833(98.886)
12/17 06:25:17 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 201/521, train_loss: 0.019(0.038), train_acc: 98.958(98.730)
12/17 06:25:23 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 301/521, train_loss: 0.009(0.040), train_acc: 100.000(98.681)
12/17 06:25:29 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 401/521, train_loss: 0.028(0.039), train_acc: 100.000(98.701)
12/17 06:25:36 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 501/521, train_loss: 0.011(0.039), train_acc: 100.000(98.705)
12/17 06:25:37 AM [Supernet Training] lr: 0.00308 epoch: 527/600, step: 521/521, train_loss: 0.019(0.039), train_acc: 100.000(98.722)
12/17 06:25:37 AM [Supernet Training] epoch: 527, train_loss: 0.039, train_acc: 98.722
12/17 06:25:39 AM [Supernet Validation] epoch: 527, val_loss: 0.538, val_acc: 88.270, best_acc: 88.350


12/17 06:25:39 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 001/521, train_loss: 0.049(0.049), train_acc: 98.958(98.958)
12/17 06:25:45 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 101/521, train_loss: 0.102(0.040), train_acc: 97.917(98.690)
12/17 06:25:52 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 201/521, train_loss: 0.044(0.042), train_acc: 97.917(98.476)
12/17 06:25:58 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 301/521, train_loss: 0.048(0.041), train_acc: 98.958(98.560)
12/17 06:26:04 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 401/521, train_loss: 0.019(0.041), train_acc: 100.000(98.597)
12/17 06:26:11 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 501/521, train_loss: 0.029(0.040), train_acc: 98.958(98.592)
12/17 06:26:12 AM [Supernet Training] lr: 0.00304 epoch: 528/600, step: 521/521, train_loss: 0.041(0.040), train_acc: 97.500(98.586)
12/17 06:26:12 AM [Supernet Training] epoch: 528, train_loss: 0.040, train_acc: 98.586
12/17 06:26:14 AM [Supernet Validation] epoch: 528, val_loss: 0.543, val_acc: 88.080, best_acc: 88.350


12/17 06:26:14 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 94.792(94.792)
12/17 06:26:20 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 101/521, train_loss: 0.011(0.041), train_acc: 100.000(98.546)
12/17 06:26:27 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 201/521, train_loss: 0.031(0.038), train_acc: 97.917(98.699)
12/17 06:26:33 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 301/521, train_loss: 0.023(0.038), train_acc: 98.958(98.716)
12/17 06:26:39 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 401/521, train_loss: 0.017(0.038), train_acc: 100.000(98.727)
12/17 06:26:46 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 501/521, train_loss: 0.017(0.039), train_acc: 100.000(98.707)
12/17 06:26:47 AM [Supernet Training] lr: 0.00300 epoch: 529/600, step: 521/521, train_loss: 0.041(0.039), train_acc: 100.000(98.716)
12/17 06:26:47 AM [Supernet Training] epoch: 529, train_loss: 0.039, train_acc: 98.716
12/17 06:26:48 AM [Supernet Validation] epoch: 529, val_loss: 0.555, val_acc: 88.320, best_acc: 88.350


12/17 06:26:49 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 98.958(98.958)
12/17 06:26:55 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 101/521, train_loss: 0.019(0.040), train_acc: 100.000(98.700)
12/17 06:27:01 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 201/521, train_loss: 0.026(0.039), train_acc: 98.958(98.704)
12/17 06:27:08 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 301/521, train_loss: 0.016(0.039), train_acc: 100.000(98.688)
12/17 06:27:14 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 401/521, train_loss: 0.044(0.039), train_acc: 97.917(98.680)
12/17 06:27:20 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 501/521, train_loss: 0.063(0.038), train_acc: 98.958(98.711)
12/17 06:27:22 AM [Supernet Training] lr: 0.00296 epoch: 530/600, step: 521/521, train_loss: 0.069(0.038), train_acc: 96.250(98.706)
12/17 06:27:22 AM [Supernet Training] epoch: 530, train_loss: 0.038, train_acc: 98.706
12/17 06:27:23 AM [Supernet Validation] epoch: 530, val_loss: 0.540, val_acc: 88.000, best_acc: 88.350


12/17 06:27:24 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 001/521, train_loss: 0.047(0.047), train_acc: 98.958(98.958)
12/17 06:27:30 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 101/521, train_loss: 0.062(0.041), train_acc: 97.917(98.742)
12/17 06:27:36 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 201/521, train_loss: 0.049(0.041), train_acc: 98.958(98.720)
12/17 06:27:43 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 301/521, train_loss: 0.048(0.040), train_acc: 98.958(98.709)
12/17 06:27:49 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 401/521, train_loss: 0.015(0.041), train_acc: 100.000(98.673)
12/17 06:27:55 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 501/521, train_loss: 0.021(0.040), train_acc: 98.958(98.680)
12/17 06:27:56 AM [Supernet Training] lr: 0.00292 epoch: 531/600, step: 521/521, train_loss: 0.046(0.040), train_acc: 98.750(98.668)
12/17 06:27:56 AM [Supernet Training] epoch: 531, train_loss: 0.040, train_acc: 98.668
12/17 06:27:58 AM [Supernet Validation] epoch: 531, val_loss: 0.541, val_acc: 88.230, best_acc: 88.350


12/17 06:27:58 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 001/521, train_loss: 0.010(0.010), train_acc: 100.000(100.000)
12/17 06:28:05 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 101/521, train_loss: 0.052(0.040), train_acc: 97.917(98.680)
12/17 06:28:11 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 201/521, train_loss: 0.012(0.039), train_acc: 100.000(98.704)
12/17 06:28:17 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 301/521, train_loss: 0.025(0.038), train_acc: 98.958(98.730)
12/17 06:28:24 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 401/521, train_loss: 0.025(0.038), train_acc: 98.958(98.675)
12/17 06:28:30 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 501/521, train_loss: 0.048(0.039), train_acc: 98.958(98.644)
12/17 06:28:31 AM [Supernet Training] lr: 0.00287 epoch: 532/600, step: 521/521, train_loss: 0.024(0.039), train_acc: 98.750(98.636)
12/17 06:28:31 AM [Supernet Training] epoch: 532, train_loss: 0.039, train_acc: 98.636
12/17 06:28:33 AM [Supernet Validation] epoch: 532, val_loss: 0.548, val_acc: 88.110, best_acc: 88.350


12/17 06:28:33 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
12/17 06:28:39 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 101/521, train_loss: 0.018(0.042), train_acc: 100.000(98.546)
12/17 06:28:46 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 201/521, train_loss: 0.035(0.039), train_acc: 98.958(98.621)
12/17 06:28:52 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 301/521, train_loss: 0.053(0.039), train_acc: 98.958(98.636)
12/17 06:28:58 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 401/521, train_loss: 0.061(0.039), train_acc: 96.875(98.618)
12/17 06:29:05 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 501/521, train_loss: 0.041(0.039), train_acc: 97.917(98.634)
12/17 06:29:06 AM [Supernet Training] lr: 0.00283 epoch: 533/600, step: 521/521, train_loss: 0.034(0.039), train_acc: 100.000(98.656)
12/17 06:29:06 AM [Supernet Training] epoch: 533, train_loss: 0.039, train_acc: 98.656
12/17 06:29:07 AM [Supernet Validation] epoch: 533, val_loss: 0.554, val_acc: 88.320, best_acc: 88.350


12/17 06:29:08 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 98.958(98.958)
12/17 06:29:14 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 101/521, train_loss: 0.019(0.039), train_acc: 100.000(98.700)
12/17 06:29:20 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 201/521, train_loss: 0.027(0.038), train_acc: 100.000(98.658)
12/17 06:29:27 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 301/521, train_loss: 0.021(0.039), train_acc: 100.000(98.630)
12/17 06:29:33 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 401/521, train_loss: 0.102(0.038), train_acc: 93.750(98.667)
12/17 06:29:39 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 501/521, train_loss: 0.072(0.038), train_acc: 96.875(98.671)
12/17 06:29:41 AM [Supernet Training] lr: 0.00279 epoch: 534/600, step: 521/521, train_loss: 0.071(0.038), train_acc: 96.250(98.682)
12/17 06:29:41 AM [Supernet Training] epoch: 534, train_loss: 0.038, train_acc: 98.682
12/17 06:29:42 AM [Supernet Validation] epoch: 534, val_loss: 0.543, val_acc: 88.160, best_acc: 88.350


12/17 06:29:43 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 001/521, train_loss: 0.033(0.033), train_acc: 98.958(98.958)
12/17 06:29:49 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 101/521, train_loss: 0.041(0.040), train_acc: 98.958(98.639)
12/17 06:29:55 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 201/521, train_loss: 0.022(0.039), train_acc: 100.000(98.694)
12/17 06:30:01 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 301/521, train_loss: 0.032(0.037), train_acc: 98.958(98.768)
12/17 06:30:08 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 401/521, train_loss: 0.046(0.037), train_acc: 97.917(98.805)
12/17 06:30:14 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 501/521, train_loss: 0.040(0.037), train_acc: 98.958(98.807)
12/17 06:30:15 AM [Supernet Training] lr: 0.00275 epoch: 535/600, step: 521/521, train_loss: 0.024(0.037), train_acc: 98.750(98.802)
12/17 06:30:16 AM [Supernet Training] epoch: 535, train_loss: 0.037, train_acc: 98.802
12/17 06:30:17 AM [Supernet Validation] epoch: 535, val_loss: 0.541, val_acc: 88.290, best_acc: 88.350


12/17 06:30:17 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 001/521, train_loss: 0.040(0.040), train_acc: 98.958(98.958)
12/17 06:30:24 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 101/521, train_loss: 0.023(0.038), train_acc: 100.000(98.721)
12/17 06:30:30 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 201/521, train_loss: 0.016(0.037), train_acc: 100.000(98.741)
12/17 06:30:36 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 301/521, train_loss: 0.042(0.038), train_acc: 98.958(98.706)
12/17 06:30:43 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 401/521, train_loss: 0.083(0.039), train_acc: 96.875(98.706)
12/17 06:30:49 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 501/521, train_loss: 0.026(0.039), train_acc: 100.000(98.690)
12/17 06:30:50 AM [Supernet Training] lr: 0.00271 epoch: 536/600, step: 521/521, train_loss: 0.011(0.039), train_acc: 100.000(98.706)
12/17 06:30:50 AM [Supernet Training] epoch: 536, train_loss: 0.039, train_acc: 98.706
12/17 06:30:52 AM [Supernet Validation] epoch: 536, val_loss: 0.552, val_acc: 87.790, best_acc: 88.350


12/17 06:30:52 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 001/521, train_loss: 0.014(0.014), train_acc: 100.000(100.000)
12/17 06:30:58 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 101/521, train_loss: 0.039(0.039), train_acc: 97.917(98.649)
12/17 06:31:05 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 201/521, train_loss: 0.051(0.038), train_acc: 97.917(98.725)
12/17 06:31:11 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 301/521, train_loss: 0.034(0.038), train_acc: 98.958(98.751)
12/17 06:31:18 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 401/521, train_loss: 0.042(0.038), train_acc: 97.917(98.771)
12/17 06:31:24 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 501/521, train_loss: 0.043(0.038), train_acc: 98.958(98.775)
12/17 06:31:25 AM [Supernet Training] lr: 0.00267 epoch: 537/600, step: 521/521, train_loss: 0.038(0.038), train_acc: 100.000(98.760)
12/17 06:31:25 AM [Supernet Training] epoch: 537, train_loss: 0.038, train_acc: 98.760
12/17 06:31:27 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 06:31:27 AM [Supernet Validation] epoch: 537, val_loss: 0.536, val_acc: 88.570, best_acc: 88.570


12/17 06:31:27 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 98.958(98.958)
12/17 06:31:34 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 101/521, train_loss: 0.012(0.033), train_acc: 100.000(99.020)
12/17 06:31:40 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 201/521, train_loss: 0.009(0.033), train_acc: 100.000(98.917)
12/17 06:31:46 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 301/521, train_loss: 0.073(0.034), train_acc: 97.917(98.913)
12/17 06:31:53 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 401/521, train_loss: 0.047(0.036), train_acc: 98.958(98.841)
12/17 06:31:59 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 501/521, train_loss: 0.043(0.036), train_acc: 98.958(98.863)
12/17 06:32:00 AM [Supernet Training] lr: 0.00262 epoch: 538/600, step: 521/521, train_loss: 0.064(0.036), train_acc: 97.500(98.842)
12/17 06:32:00 AM [Supernet Training] epoch: 538, train_loss: 0.036, train_acc: 98.842
12/17 06:32:02 AM [Supernet Validation] epoch: 538, val_loss: 0.558, val_acc: 87.890, best_acc: 88.570


12/17 06:32:02 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 98.958(98.958)
12/17 06:32:08 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 101/521, train_loss: 0.021(0.035), train_acc: 100.000(98.649)
12/17 06:32:15 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 201/521, train_loss: 0.037(0.036), train_acc: 98.958(98.710)
12/17 06:32:21 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 301/521, train_loss: 0.066(0.038), train_acc: 95.833(98.675)
12/17 06:32:28 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 401/521, train_loss: 0.043(0.037), train_acc: 97.917(98.699)
12/17 06:32:34 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 501/521, train_loss: 0.065(0.037), train_acc: 98.958(98.694)
12/17 06:32:35 AM [Supernet Training] lr: 0.00258 epoch: 539/600, step: 521/521, train_loss: 0.046(0.037), train_acc: 97.500(98.686)
12/17 06:32:35 AM [Supernet Training] epoch: 539, train_loss: 0.037, train_acc: 98.686
12/17 06:32:37 AM [Supernet Validation] epoch: 539, val_loss: 0.547, val_acc: 88.000, best_acc: 88.570


12/17 06:32:38 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 97.917(97.917)
12/17 06:32:44 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 101/521, train_loss: 0.035(0.037), train_acc: 98.958(98.731)
12/17 06:32:50 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 201/521, train_loss: 0.038(0.036), train_acc: 98.958(98.730)
12/17 06:32:56 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 301/521, train_loss: 0.035(0.036), train_acc: 100.000(98.778)
12/17 06:33:03 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 401/521, train_loss: 0.025(0.037), train_acc: 98.958(98.743)
12/17 06:33:09 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 501/521, train_loss: 0.031(0.038), train_acc: 100.000(98.717)
12/17 06:33:10 AM [Supernet Training] lr: 0.00254 epoch: 540/600, step: 521/521, train_loss: 0.061(0.037), train_acc: 97.500(98.730)
12/17 06:33:10 AM [Supernet Training] epoch: 540, train_loss: 0.037, train_acc: 98.730
12/17 06:33:12 AM [Supernet Validation] epoch: 540, val_loss: 0.545, val_acc: 88.170, best_acc: 88.570


12/17 06:33:12 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 001/521, train_loss: 0.019(0.019), train_acc: 98.958(98.958)
12/17 06:33:19 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 101/521, train_loss: 0.030(0.035), train_acc: 100.000(98.773)
12/17 06:33:26 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 201/521, train_loss: 0.035(0.036), train_acc: 98.958(98.761)
12/17 06:33:32 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 301/521, train_loss: 0.026(0.036), train_acc: 98.958(98.740)
12/17 06:33:39 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 401/521, train_loss: 0.047(0.038), train_acc: 98.958(98.732)
12/17 06:33:45 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 501/521, train_loss: 0.018(0.037), train_acc: 98.958(98.763)
12/17 06:33:46 AM [Supernet Training] lr: 0.00250 epoch: 541/600, step: 521/521, train_loss: 0.061(0.037), train_acc: 97.500(98.758)
12/17 06:33:46 AM [Supernet Training] epoch: 541, train_loss: 0.037, train_acc: 98.758
12/17 06:33:48 AM [Supernet Validation] epoch: 541, val_loss: 0.534, val_acc: 88.400, best_acc: 88.570


12/17 06:33:49 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 001/521, train_loss: 0.026(0.026), train_acc: 100.000(100.000)
12/17 06:33:55 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 101/521, train_loss: 0.036(0.034), train_acc: 98.958(98.876)
12/17 06:34:01 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 201/521, train_loss: 0.044(0.036), train_acc: 97.917(98.746)
12/17 06:34:08 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 301/521, train_loss: 0.019(0.036), train_acc: 100.000(98.754)
12/17 06:34:14 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 401/521, train_loss: 0.018(0.037), train_acc: 98.958(98.719)
12/17 06:34:20 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 501/521, train_loss: 0.014(0.037), train_acc: 100.000(98.728)
12/17 06:34:22 AM [Supernet Training] lr: 0.00246 epoch: 542/600, step: 521/521, train_loss: 0.028(0.037), train_acc: 98.750(98.738)
12/17 06:34:22 AM [Supernet Training] epoch: 542, train_loss: 0.037, train_acc: 98.738
12/17 06:34:23 AM [Supernet Validation] epoch: 542, val_loss: 0.541, val_acc: 88.240, best_acc: 88.570


12/17 06:34:24 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 001/521, train_loss: 0.063(0.063), train_acc: 96.875(96.875)
12/17 06:34:30 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 101/521, train_loss: 0.010(0.037), train_acc: 100.000(98.731)
12/17 06:34:36 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 201/521, train_loss: 0.076(0.039), train_acc: 97.917(98.637)
12/17 06:34:43 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 301/521, train_loss: 0.019(0.038), train_acc: 98.958(98.681)
12/17 06:34:49 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 401/521, train_loss: 0.022(0.038), train_acc: 100.000(98.699)
12/17 06:34:55 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 501/521, train_loss: 0.028(0.038), train_acc: 100.000(98.694)
12/17 06:34:57 AM [Supernet Training] lr: 0.00242 epoch: 543/600, step: 521/521, train_loss: 0.023(0.038), train_acc: 100.000(98.704)
12/17 06:34:57 AM [Supernet Training] epoch: 543, train_loss: 0.038, train_acc: 98.704
12/17 06:34:58 AM [Supernet Validation] epoch: 543, val_loss: 0.547, val_acc: 88.180, best_acc: 88.570


12/17 06:34:59 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 001/521, train_loss: 0.027(0.027), train_acc: 100.000(100.000)
12/17 06:35:05 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 101/521, train_loss: 0.014(0.039), train_acc: 100.000(98.752)
12/17 06:35:11 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 201/521, train_loss: 0.044(0.037), train_acc: 97.917(98.813)
12/17 06:35:18 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 301/521, train_loss: 0.027(0.037), train_acc: 98.958(98.775)
12/17 06:35:24 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 401/521, train_loss: 0.008(0.037), train_acc: 100.000(98.740)
12/17 06:35:30 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 501/521, train_loss: 0.054(0.037), train_acc: 97.917(98.765)
12/17 06:35:32 AM [Supernet Training] lr: 0.00237 epoch: 544/600, step: 521/521, train_loss: 0.085(0.037), train_acc: 98.750(98.770)
12/17 06:35:32 AM [Supernet Training] epoch: 544, train_loss: 0.037, train_acc: 98.770
12/17 06:35:33 AM [Supernet Validation] epoch: 544, val_loss: 0.543, val_acc: 88.420, best_acc: 88.570


12/17 06:35:34 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 97.917(97.917)
12/17 06:35:40 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 101/521, train_loss: 0.019(0.037), train_acc: 100.000(98.721)
12/17 06:35:46 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 201/521, train_loss: 0.055(0.038), train_acc: 97.917(98.720)
12/17 06:35:53 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 301/521, train_loss: 0.081(0.039), train_acc: 95.833(98.675)
12/17 06:35:59 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 401/521, train_loss: 0.051(0.039), train_acc: 96.875(98.680)
12/17 06:36:06 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 501/521, train_loss: 0.019(0.038), train_acc: 100.000(98.705)
12/17 06:36:07 AM [Supernet Training] lr: 0.00233 epoch: 545/600, step: 521/521, train_loss: 0.018(0.038), train_acc: 98.750(98.710)
12/17 06:36:07 AM [Supernet Training] epoch: 545, train_loss: 0.038, train_acc: 98.710
12/17 06:36:09 AM [Supernet Validation] epoch: 545, val_loss: 0.540, val_acc: 87.970, best_acc: 88.570


12/17 06:36:09 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 98.958(98.958)
12/17 06:36:15 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 101/521, train_loss: 0.068(0.038), train_acc: 96.875(98.804)
12/17 06:36:22 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 201/521, train_loss: 0.025(0.036), train_acc: 98.958(98.824)
12/17 06:36:28 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 301/521, train_loss: 0.023(0.035), train_acc: 98.958(98.889)
12/17 06:36:35 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 401/521, train_loss: 0.034(0.035), train_acc: 98.958(98.891)
12/17 06:36:41 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 501/521, train_loss: 0.030(0.036), train_acc: 98.958(98.888)
12/17 06:36:42 AM [Supernet Training] lr: 0.00229 epoch: 546/600, step: 521/521, train_loss: 0.061(0.036), train_acc: 97.500(98.870)
12/17 06:36:42 AM [Supernet Training] epoch: 546, train_loss: 0.036, train_acc: 98.870
12/17 06:36:44 AM [Supernet Validation] epoch: 546, val_loss: 0.556, val_acc: 87.740, best_acc: 88.570


12/17 06:36:44 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 001/521, train_loss: 0.014(0.014), train_acc: 100.000(100.000)
12/17 06:36:51 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 101/521, train_loss: 0.049(0.034), train_acc: 95.833(98.876)
12/17 06:36:57 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 201/521, train_loss: 0.011(0.036), train_acc: 100.000(98.777)
12/17 06:37:04 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 301/521, train_loss: 0.114(0.037), train_acc: 94.792(98.768)
12/17 06:37:10 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 401/521, train_loss: 0.021(0.037), train_acc: 98.958(98.795)
12/17 06:37:16 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 501/521, train_loss: 0.074(0.036), train_acc: 98.958(98.838)
12/17 06:37:17 AM [Supernet Training] lr: 0.00225 epoch: 547/600, step: 521/521, train_loss: 0.052(0.036), train_acc: 97.500(98.836)
12/17 06:37:17 AM [Supernet Training] epoch: 547, train_loss: 0.036, train_acc: 98.836
12/17 06:37:19 AM [Supernet Validation] epoch: 547, val_loss: 0.539, val_acc: 88.320, best_acc: 88.570


12/17 06:37:20 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 001/521, train_loss: 0.009(0.009), train_acc: 100.000(100.000)
12/17 06:37:26 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 101/521, train_loss: 0.030(0.034), train_acc: 98.958(98.896)
12/17 06:37:32 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 201/521, train_loss: 0.076(0.036), train_acc: 96.875(98.767)
12/17 06:37:39 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 301/521, train_loss: 0.026(0.036), train_acc: 98.958(98.799)
12/17 06:37:45 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 401/521, train_loss: 0.029(0.035), train_acc: 98.958(98.813)
12/17 06:37:51 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 501/521, train_loss: 0.021(0.036), train_acc: 100.000(98.804)
12/17 06:37:52 AM [Supernet Training] lr: 0.00221 epoch: 548/600, step: 521/521, train_loss: 0.051(0.036), train_acc: 97.500(98.816)
12/17 06:37:52 AM [Supernet Training] epoch: 548, train_loss: 0.036, train_acc: 98.816
12/17 06:37:54 AM [Supernet Validation] epoch: 548, val_loss: 0.547, val_acc: 87.900, best_acc: 88.570


12/17 06:37:54 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 001/521, train_loss: 0.106(0.106), train_acc: 97.917(97.917)
12/17 06:38:01 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 101/521, train_loss: 0.043(0.036), train_acc: 97.917(98.886)
12/17 06:38:07 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 201/521, train_loss: 0.035(0.037), train_acc: 100.000(98.803)
12/17 06:38:13 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 301/521, train_loss: 0.076(0.037), train_acc: 96.875(98.806)
12/17 06:38:20 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 401/521, train_loss: 0.046(0.036), train_acc: 97.917(98.810)
12/17 06:38:26 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 501/521, train_loss: 0.015(0.036), train_acc: 100.000(98.821)
12/17 06:38:27 AM [Supernet Training] lr: 0.00217 epoch: 549/600, step: 521/521, train_loss: 0.020(0.036), train_acc: 100.000(98.820)
12/17 06:38:27 AM [Supernet Training] epoch: 549, train_loss: 0.036, train_acc: 98.820
12/17 06:38:29 AM [Supernet Validation] epoch: 549, val_loss: 0.541, val_acc: 87.990, best_acc: 88.570


12/17 06:38:29 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 100.000(100.000)
12/17 06:38:35 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 101/521, train_loss: 0.031(0.034), train_acc: 98.958(98.948)
12/17 06:38:42 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 201/521, train_loss: 0.051(0.037), train_acc: 97.917(98.881)
12/17 06:38:48 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 301/521, train_loss: 0.036(0.037), train_acc: 98.958(98.806)
12/17 06:38:54 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 401/521, train_loss: 0.039(0.036), train_acc: 97.917(98.826)
12/17 06:39:01 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 501/521, train_loss: 0.022(0.036), train_acc: 98.958(98.819)
12/17 06:39:02 AM [Supernet Training] lr: 0.00212 epoch: 550/600, step: 521/521, train_loss: 0.014(0.037), train_acc: 100.000(98.814)
12/17 06:39:02 AM [Supernet Training] epoch: 550, train_loss: 0.037, train_acc: 98.814
12/17 06:39:04 AM [Supernet Validation] epoch: 550, val_loss: 0.537, val_acc: 88.450, best_acc: 88.570


12/17 06:39:04 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 001/521, train_loss: 0.029(0.029), train_acc: 100.000(100.000)
12/17 06:39:10 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 101/521, train_loss: 0.016(0.037), train_acc: 100.000(98.793)
12/17 06:39:17 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 201/521, train_loss: 0.042(0.037), train_acc: 97.917(98.741)
12/17 06:39:23 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 301/521, train_loss: 0.017(0.037), train_acc: 100.000(98.761)
12/17 06:39:29 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 401/521, train_loss: 0.032(0.036), train_acc: 98.958(98.810)
12/17 06:39:36 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 501/521, train_loss: 0.022(0.036), train_acc: 98.958(98.829)
12/17 06:39:37 AM [Supernet Training] lr: 0.00208 epoch: 551/600, step: 521/521, train_loss: 0.028(0.036), train_acc: 98.750(98.828)
12/17 06:39:37 AM [Supernet Training] epoch: 551, train_loss: 0.036, train_acc: 98.828
12/17 06:39:38 AM [Supernet Validation] epoch: 551, val_loss: 0.535, val_acc: 88.240, best_acc: 88.570


12/17 06:39:39 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 001/521, train_loss: 0.026(0.026), train_acc: 100.000(100.000)
12/17 06:39:45 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 101/521, train_loss: 0.050(0.033), train_acc: 98.958(98.948)
12/17 06:39:52 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 201/521, train_loss: 0.066(0.033), train_acc: 96.875(98.953)
12/17 06:39:58 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 301/521, train_loss: 0.012(0.033), train_acc: 100.000(98.948)
12/17 06:40:04 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 401/521, train_loss: 0.024(0.034), train_acc: 100.000(98.927)
12/17 06:40:10 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 501/521, train_loss: 0.049(0.034), train_acc: 97.917(98.931)
12/17 06:40:12 AM [Supernet Training] lr: 0.00204 epoch: 552/600, step: 521/521, train_loss: 0.066(0.034), train_acc: 96.250(98.924)
12/17 06:40:12 AM [Supernet Training] epoch: 552, train_loss: 0.034, train_acc: 98.924
12/17 06:40:13 AM [Supernet Validation] epoch: 552, val_loss: 0.546, val_acc: 88.190, best_acc: 88.570


12/17 06:40:14 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 001/521, train_loss: 0.008(0.008), train_acc: 100.000(100.000)
12/17 06:40:20 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 101/521, train_loss: 0.026(0.036), train_acc: 97.917(98.845)
12/17 06:40:26 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 201/521, train_loss: 0.018(0.036), train_acc: 100.000(98.844)
12/17 06:40:33 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 301/521, train_loss: 0.047(0.036), train_acc: 98.958(98.858)
12/17 06:40:39 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 401/521, train_loss: 0.047(0.036), train_acc: 96.875(98.808)
12/17 06:40:45 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 501/521, train_loss: 0.039(0.036), train_acc: 97.917(98.788)
12/17 06:40:47 AM [Supernet Training] lr: 0.00200 epoch: 553/600, step: 521/521, train_loss: 0.013(0.036), train_acc: 100.000(98.770)
12/17 06:40:47 AM [Supernet Training] epoch: 553, train_loss: 0.036, train_acc: 98.770
12/17 06:40:48 AM [Supernet Validation] epoch: 553, val_loss: 0.534, val_acc: 88.230, best_acc: 88.570


12/17 06:40:49 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 001/521, train_loss: 0.024(0.024), train_acc: 100.000(100.000)
12/17 06:40:55 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 101/521, train_loss: 0.035(0.030), train_acc: 96.875(99.051)
12/17 06:41:01 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 201/521, train_loss: 0.005(0.031), train_acc: 100.000(99.010)
12/17 06:41:08 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 301/521, train_loss: 0.055(0.032), train_acc: 97.917(98.962)
12/17 06:41:14 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 401/521, train_loss: 0.031(0.034), train_acc: 98.958(98.958)
12/17 06:41:20 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 501/521, train_loss: 0.027(0.034), train_acc: 98.958(98.942)
12/17 06:41:22 AM [Supernet Training] lr: 0.00196 epoch: 554/600, step: 521/521, train_loss: 0.045(0.034), train_acc: 97.500(98.946)
12/17 06:41:22 AM [Supernet Training] epoch: 554, train_loss: 0.034, train_acc: 98.946
12/17 06:41:23 AM [Supernet Validation] epoch: 554, val_loss: 0.539, val_acc: 88.310, best_acc: 88.570


12/17 06:41:24 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 97.917(97.917)
12/17 06:41:30 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 101/521, train_loss: 0.051(0.032), train_acc: 98.958(98.969)
12/17 06:41:36 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 201/521, train_loss: 0.010(0.036), train_acc: 100.000(98.917)
12/17 06:41:43 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 301/521, train_loss: 0.008(0.036), train_acc: 100.000(98.903)
12/17 06:41:49 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 401/521, train_loss: 0.027(0.035), train_acc: 98.958(98.899)
12/17 06:41:55 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 501/521, train_loss: 0.040(0.035), train_acc: 98.958(98.896)
12/17 06:41:56 AM [Supernet Training] lr: 0.00192 epoch: 555/600, step: 521/521, train_loss: 0.032(0.035), train_acc: 97.500(98.890)
12/17 06:41:56 AM [Supernet Training] epoch: 555, train_loss: 0.035, train_acc: 98.890
12/17 06:41:58 AM [Supernet Validation] epoch: 555, val_loss: 0.532, val_acc: 88.290, best_acc: 88.570


12/17 06:41:58 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 001/521, train_loss: 0.030(0.030), train_acc: 100.000(100.000)
12/17 06:42:05 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 101/521, train_loss: 0.070(0.040), train_acc: 97.917(98.597)
12/17 06:42:11 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 201/521, train_loss: 0.030(0.038), train_acc: 98.958(98.689)
12/17 06:42:18 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 301/521, train_loss: 0.025(0.037), train_acc: 98.958(98.716)
12/17 06:42:24 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 401/521, train_loss: 0.009(0.037), train_acc: 100.000(98.722)
12/17 06:42:30 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 501/521, train_loss: 0.028(0.036), train_acc: 100.000(98.765)
12/17 06:42:31 AM [Supernet Training] lr: 0.00187 epoch: 556/600, step: 521/521, train_loss: 0.031(0.036), train_acc: 98.750(98.772)
12/17 06:42:31 AM [Supernet Training] epoch: 556, train_loss: 0.036, train_acc: 98.772
12/17 06:42:33 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 06:42:33 AM [Supernet Validation] epoch: 556, val_loss: 0.538, val_acc: 88.640, best_acc: 88.640


12/17 06:42:34 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 100.000(100.000)
12/17 06:42:40 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 101/521, train_loss: 0.036(0.037), train_acc: 98.958(98.690)
12/17 06:42:46 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 201/521, train_loss: 0.025(0.037), train_acc: 98.958(98.735)
12/17 06:42:52 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 301/521, train_loss: 0.019(0.036), train_acc: 100.000(98.744)
12/17 06:42:59 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 401/521, train_loss: 0.052(0.036), train_acc: 98.958(98.761)
12/17 06:43:05 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 501/521, train_loss: 0.044(0.036), train_acc: 98.958(98.777)
12/17 06:43:06 AM [Supernet Training] lr: 0.00183 epoch: 557/600, step: 521/521, train_loss: 0.028(0.035), train_acc: 100.000(98.786)
12/17 06:43:07 AM [Supernet Training] epoch: 557, train_loss: 0.035, train_acc: 98.786
12/17 06:43:08 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 06:43:08 AM [Supernet Validation] epoch: 557, val_loss: 0.538, val_acc: 88.650, best_acc: 88.650


12/17 06:43:08 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 97.917(97.917)
12/17 06:43:15 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 101/521, train_loss: 0.027(0.035), train_acc: 98.958(98.855)
12/17 06:43:21 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 201/521, train_loss: 0.113(0.034), train_acc: 96.875(98.938)
12/17 06:43:28 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 301/521, train_loss: 0.046(0.034), train_acc: 98.958(98.927)
12/17 06:43:34 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 401/521, train_loss: 0.015(0.034), train_acc: 98.958(98.912)
12/17 06:43:40 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 501/521, train_loss: 0.053(0.034), train_acc: 97.917(98.890)
12/17 06:43:41 AM [Supernet Training] lr: 0.00179 epoch: 558/600, step: 521/521, train_loss: 0.007(0.034), train_acc: 100.000(98.870)
12/17 06:43:41 AM [Supernet Training] epoch: 558, train_loss: 0.034, train_acc: 98.870
12/17 06:43:43 AM [Supernet Validation] epoch: 558, val_loss: 0.537, val_acc: 88.510, best_acc: 88.650


12/17 06:43:43 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 001/521, train_loss: 0.024(0.024), train_acc: 98.958(98.958)
12/17 06:43:50 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 101/521, train_loss: 0.052(0.034), train_acc: 96.875(98.938)
12/17 06:43:56 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 201/521, train_loss: 0.036(0.034), train_acc: 98.958(98.912)
12/17 06:44:02 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 301/521, train_loss: 0.066(0.035), train_acc: 97.917(98.886)
12/17 06:44:09 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 401/521, train_loss: 0.017(0.035), train_acc: 100.000(98.875)
12/17 06:44:15 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 501/521, train_loss: 0.032(0.034), train_acc: 97.917(98.892)
12/17 06:44:16 AM [Supernet Training] lr: 0.00175 epoch: 559/600, step: 521/521, train_loss: 0.058(0.035), train_acc: 96.250(98.848)
12/17 06:44:16 AM [Supernet Training] epoch: 559, train_loss: 0.035, train_acc: 98.848
12/17 06:44:18 AM [Supernet Validation] epoch: 559, val_loss: 0.538, val_acc: 88.560, best_acc: 88.650


12/17 06:44:19 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 001/521, train_loss: 0.012(0.012), train_acc: 100.000(100.000)
12/17 06:44:25 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 101/521, train_loss: 0.013(0.036), train_acc: 100.000(98.793)
12/17 06:44:31 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 201/521, train_loss: 0.061(0.034), train_acc: 98.958(98.818)
12/17 06:44:38 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 301/521, train_loss: 0.023(0.034), train_acc: 98.958(98.858)
12/17 06:44:44 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 401/521, train_loss: 0.009(0.034), train_acc: 100.000(98.831)
12/17 06:44:50 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 501/521, train_loss: 0.029(0.033), train_acc: 97.917(98.861)
12/17 06:44:51 AM [Supernet Training] lr: 0.00171 epoch: 560/600, step: 521/521, train_loss: 0.029(0.033), train_acc: 100.000(98.874)
12/17 06:44:51 AM [Supernet Training] epoch: 560, train_loss: 0.033, train_acc: 98.874
12/17 06:44:53 AM [Supernet Validation] epoch: 560, val_loss: 0.540, val_acc: 88.340, best_acc: 88.650


12/17 06:44:53 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 001/521, train_loss: 0.102(0.102), train_acc: 95.833(95.833)
12/17 06:45:00 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 101/521, train_loss: 0.031(0.034), train_acc: 98.958(98.927)
12/17 06:45:06 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 201/521, train_loss: 0.049(0.033), train_acc: 98.958(98.974)
12/17 06:45:12 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 301/521, train_loss: 0.069(0.032), train_acc: 98.958(98.993)
12/17 06:45:19 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 401/521, train_loss: 0.007(0.033), train_acc: 100.000(98.974)
12/17 06:45:25 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 501/521, train_loss: 0.019(0.033), train_acc: 98.958(98.948)
12/17 06:45:26 AM [Supernet Training] lr: 0.00167 epoch: 561/600, step: 521/521, train_loss: 0.027(0.033), train_acc: 98.750(98.932)
12/17 06:45:27 AM [Supernet Training] epoch: 561, train_loss: 0.033, train_acc: 98.932
12/17 06:45:28 AM [Supernet Validation] epoch: 561, val_loss: 0.552, val_acc: 87.990, best_acc: 88.650


12/17 06:45:29 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 001/521, train_loss: 0.014(0.014), train_acc: 100.000(100.000)
12/17 06:45:35 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 101/521, train_loss: 0.051(0.032), train_acc: 98.958(98.917)
12/17 06:45:41 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 201/521, train_loss: 0.017(0.033), train_acc: 100.000(98.917)
12/17 06:45:48 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 301/521, train_loss: 0.029(0.033), train_acc: 100.000(98.924)
12/17 06:45:54 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 401/521, train_loss: 0.055(0.033), train_acc: 97.917(98.932)
12/17 06:46:00 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 501/521, train_loss: 0.024(0.033), train_acc: 98.958(98.913)
12/17 06:46:02 AM [Supernet Training] lr: 0.00162 epoch: 562/600, step: 521/521, train_loss: 0.050(0.033), train_acc: 98.750(98.916)
12/17 06:46:02 AM [Supernet Training] epoch: 562, train_loss: 0.033, train_acc: 98.916
12/17 06:46:03 AM [Supernet Validation] epoch: 562, val_loss: 0.552, val_acc: 88.250, best_acc: 88.650


12/17 06:46:04 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 001/521, train_loss: 0.054(0.054), train_acc: 97.917(97.917)
12/17 06:46:10 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 101/521, train_loss: 0.006(0.036), train_acc: 100.000(98.835)
12/17 06:46:17 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 201/521, train_loss: 0.012(0.035), train_acc: 100.000(98.818)
12/17 06:46:23 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 301/521, train_loss: 0.030(0.034), train_acc: 100.000(98.872)
12/17 06:46:30 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 401/521, train_loss: 0.023(0.034), train_acc: 100.000(98.849)
12/17 06:46:36 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 501/521, train_loss: 0.021(0.034), train_acc: 100.000(98.859)
12/17 06:46:38 AM [Supernet Training] lr: 0.00158 epoch: 563/600, step: 521/521, train_loss: 0.014(0.034), train_acc: 100.000(98.870)
12/17 06:46:38 AM [Supernet Training] epoch: 563, train_loss: 0.034, train_acc: 98.870
12/17 06:46:39 AM [Supernet Validation] epoch: 563, val_loss: 0.541, val_acc: 87.980, best_acc: 88.650


12/17 06:46:40 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 001/521, train_loss: 0.005(0.005), train_acc: 100.000(100.000)
12/17 06:46:46 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 101/521, train_loss: 0.021(0.034), train_acc: 98.958(98.886)
12/17 06:46:52 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 201/521, train_loss: 0.017(0.033), train_acc: 100.000(98.943)
12/17 06:46:59 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 301/521, train_loss: 0.040(0.032), train_acc: 97.917(98.941)
12/17 06:47:05 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 401/521, train_loss: 0.073(0.033), train_acc: 96.875(98.917)
12/17 06:47:12 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 501/521, train_loss: 0.024(0.032), train_acc: 98.958(98.925)
12/17 06:47:13 AM [Supernet Training] lr: 0.00154 epoch: 564/600, step: 521/521, train_loss: 0.021(0.033), train_acc: 98.750(98.912)
12/17 06:47:13 AM [Supernet Training] epoch: 564, train_loss: 0.033, train_acc: 98.912
12/17 06:47:14 AM [Supernet Validation] epoch: 564, val_loss: 0.548, val_acc: 88.310, best_acc: 88.650


12/17 06:47:15 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 001/521, train_loss: 0.061(0.061), train_acc: 98.958(98.958)
12/17 06:47:21 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 101/521, train_loss: 0.045(0.034), train_acc: 98.958(98.866)
12/17 06:47:27 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 201/521, train_loss: 0.028(0.033), train_acc: 98.958(98.938)
12/17 06:47:34 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 301/521, train_loss: 0.024(0.034), train_acc: 98.958(98.872)
12/17 06:47:40 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 401/521, train_loss: 0.068(0.034), train_acc: 96.875(98.880)
12/17 06:47:46 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 501/521, train_loss: 0.049(0.034), train_acc: 97.917(98.867)
12/17 06:47:48 AM [Supernet Training] lr: 0.00150 epoch: 565/600, step: 521/521, train_loss: 0.059(0.034), train_acc: 98.750(98.878)
12/17 06:47:48 AM [Supernet Training] epoch: 565, train_loss: 0.034, train_acc: 98.878
12/17 06:47:49 AM [Supernet Validation] epoch: 565, val_loss: 0.533, val_acc: 88.490, best_acc: 88.650


12/17 06:47:50 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 98.958(98.958)
12/17 06:47:56 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 101/521, train_loss: 0.035(0.035), train_acc: 98.958(98.793)
12/17 06:48:02 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 201/521, train_loss: 0.060(0.035), train_acc: 97.917(98.834)
12/17 06:48:08 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 301/521, train_loss: 0.069(0.034), train_acc: 97.917(98.882)
12/17 06:48:15 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 401/521, train_loss: 0.016(0.034), train_acc: 100.000(98.886)
12/17 06:48:21 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 501/521, train_loss: 0.038(0.034), train_acc: 98.958(98.892)
12/17 06:48:22 AM [Supernet Training] lr: 0.00146 epoch: 566/600, step: 521/521, train_loss: 0.025(0.034), train_acc: 98.750(98.886)
12/17 06:48:22 AM [Supernet Training] epoch: 566, train_loss: 0.034, train_acc: 98.886
12/17 06:48:24 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 06:48:24 AM [Supernet Validation] epoch: 566, val_loss: 0.528, val_acc: 88.850, best_acc: 88.850


12/17 06:48:25 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 98.958(98.958)
12/17 06:48:31 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 101/521, train_loss: 0.087(0.033), train_acc: 96.875(98.938)
12/17 06:48:37 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 201/521, train_loss: 0.013(0.033), train_acc: 98.958(98.948)
12/17 06:48:44 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 301/521, train_loss: 0.025(0.033), train_acc: 98.958(98.931)
12/17 06:48:50 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 401/521, train_loss: 0.028(0.033), train_acc: 100.000(98.935)
12/17 06:48:56 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 501/521, train_loss: 0.007(0.034), train_acc: 100.000(98.913)
12/17 06:48:58 AM [Supernet Training] lr: 0.00142 epoch: 567/600, step: 521/521, train_loss: 0.050(0.034), train_acc: 97.500(98.922)
12/17 06:48:58 AM [Supernet Training] epoch: 567, train_loss: 0.034, train_acc: 98.922
12/17 06:48:59 AM [Supernet Validation] epoch: 567, val_loss: 0.558, val_acc: 88.150, best_acc: 88.850


12/17 06:49:00 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 001/521, train_loss: 0.026(0.026), train_acc: 98.958(98.958)
12/17 06:49:06 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 101/521, train_loss: 0.020(0.034), train_acc: 98.958(98.835)
12/17 06:49:12 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 201/521, train_loss: 0.072(0.034), train_acc: 95.833(98.855)
12/17 06:49:19 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 301/521, train_loss: 0.039(0.034), train_acc: 98.958(98.823)
12/17 06:49:25 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 401/521, train_loss: 0.020(0.034), train_acc: 98.958(98.862)
12/17 06:49:31 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 501/521, train_loss: 0.019(0.034), train_acc: 100.000(98.881)
12/17 06:49:32 AM [Supernet Training] lr: 0.00138 epoch: 568/600, step: 521/521, train_loss: 0.095(0.034), train_acc: 97.500(98.880)
12/17 06:49:32 AM [Supernet Training] epoch: 568, train_loss: 0.034, train_acc: 98.880
12/17 06:49:34 AM [Supernet Validation] epoch: 568, val_loss: 0.553, val_acc: 88.280, best_acc: 88.850


12/17 06:49:34 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 001/521, train_loss: 0.029(0.029), train_acc: 98.958(98.958)
12/17 06:49:41 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 101/521, train_loss: 0.013(0.032), train_acc: 98.958(99.020)
12/17 06:49:47 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 201/521, train_loss: 0.009(0.033), train_acc: 100.000(98.948)
12/17 06:49:54 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 301/521, train_loss: 0.011(0.033), train_acc: 100.000(98.969)
12/17 06:50:00 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 401/521, train_loss: 0.014(0.033), train_acc: 100.000(98.935)
12/17 06:50:07 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 501/521, train_loss: 0.026(0.033), train_acc: 98.958(98.942)
12/17 06:50:08 AM [Supernet Training] lr: 0.00133 epoch: 569/600, step: 521/521, train_loss: 0.037(0.033), train_acc: 98.750(98.950)
12/17 06:50:08 AM [Supernet Training] epoch: 569, train_loss: 0.033, train_acc: 98.950
12/17 06:50:09 AM [Supernet Validation] epoch: 569, val_loss: 0.553, val_acc: 88.090, best_acc: 88.850


12/17 06:50:10 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 001/521, train_loss: 0.102(0.102), train_acc: 95.833(95.833)
12/17 06:50:16 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 101/521, train_loss: 0.035(0.033), train_acc: 98.958(99.072)
12/17 06:50:23 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 201/521, train_loss: 0.009(0.033), train_acc: 100.000(98.953)
12/17 06:50:29 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 301/521, train_loss: 0.028(0.033), train_acc: 97.917(98.941)
12/17 06:50:35 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 401/521, train_loss: 0.067(0.033), train_acc: 97.917(98.930)
12/17 06:50:42 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 501/521, train_loss: 0.009(0.033), train_acc: 100.000(98.942)
12/17 06:50:43 AM [Supernet Training] lr: 0.00129 epoch: 570/600, step: 521/521, train_loss: 0.092(0.033), train_acc: 96.250(98.918)
12/17 06:50:43 AM [Supernet Training] epoch: 570, train_loss: 0.033, train_acc: 98.918
12/17 06:50:44 AM [Supernet Validation] epoch: 570, val_loss: 0.529, val_acc: 88.670, best_acc: 88.850


12/17 06:50:45 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 001/521, train_loss: 0.028(0.028), train_acc: 100.000(100.000)
12/17 06:50:51 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 101/521, train_loss: 0.017(0.035), train_acc: 100.000(98.845)
12/17 06:50:57 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 201/521, train_loss: 0.022(0.036), train_acc: 98.958(98.792)
12/17 06:51:04 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 301/521, train_loss: 0.019(0.036), train_acc: 98.958(98.765)
12/17 06:51:10 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 401/521, train_loss: 0.055(0.036), train_acc: 97.917(98.787)
12/17 06:51:16 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 501/521, train_loss: 0.035(0.035), train_acc: 98.958(98.859)
12/17 06:51:18 AM [Supernet Training] lr: 0.00125 epoch: 571/600, step: 521/521, train_loss: 0.102(0.035), train_acc: 96.250(98.858)
12/17 06:51:18 AM [Supernet Training] epoch: 571, train_loss: 0.035, train_acc: 98.858
12/17 06:51:19 AM [Supernet Validation] epoch: 571, val_loss: 0.548, val_acc: 88.370, best_acc: 88.850


12/17 06:51:20 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 001/521, train_loss: 0.014(0.014), train_acc: 100.000(100.000)
12/17 06:51:26 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 101/521, train_loss: 0.035(0.035), train_acc: 98.958(98.814)
12/17 06:51:32 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 201/521, train_loss: 0.006(0.034), train_acc: 100.000(98.844)
12/17 06:51:39 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 301/521, train_loss: 0.024(0.033), train_acc: 98.958(98.924)
12/17 06:51:45 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 401/521, train_loss: 0.051(0.032), train_acc: 98.958(98.956)
12/17 06:51:52 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 501/521, train_loss: 0.014(0.033), train_acc: 100.000(98.933)
12/17 06:51:53 AM [Supernet Training] lr: 0.00121 epoch: 572/600, step: 521/521, train_loss: 0.034(0.033), train_acc: 98.750(98.930)
12/17 06:51:53 AM [Supernet Training] epoch: 572, train_loss: 0.033, train_acc: 98.930
12/17 06:51:54 AM [Supernet Validation] epoch: 572, val_loss: 0.541, val_acc: 88.400, best_acc: 88.850


12/17 06:51:55 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 001/521, train_loss: 0.059(0.059), train_acc: 97.917(97.917)
12/17 06:52:01 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 101/521, train_loss: 0.030(0.030), train_acc: 98.958(98.927)
12/17 06:52:07 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 201/521, train_loss: 0.047(0.030), train_acc: 97.917(98.958)
12/17 06:52:14 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 301/521, train_loss: 0.024(0.031), train_acc: 100.000(98.924)
12/17 06:52:20 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 401/521, train_loss: 0.021(0.031), train_acc: 98.958(98.935)
12/17 06:52:27 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 501/521, train_loss: 0.023(0.032), train_acc: 98.958(98.915)
12/17 06:52:28 AM [Supernet Training] lr: 0.00117 epoch: 573/600, step: 521/521, train_loss: 0.010(0.032), train_acc: 100.000(98.912)
12/17 06:52:28 AM [Supernet Training] epoch: 573, train_loss: 0.032, train_acc: 98.912
12/17 06:52:30 AM [Supernet Validation] epoch: 573, val_loss: 0.554, val_acc: 88.360, best_acc: 88.850


12/17 06:52:30 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 98.958(98.958)
12/17 06:52:37 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 101/521, train_loss: 0.033(0.032), train_acc: 98.958(98.989)
12/17 06:52:43 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 201/521, train_loss: 0.071(0.034), train_acc: 96.875(98.870)
12/17 06:52:49 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 301/521, train_loss: 0.040(0.033), train_acc: 97.917(98.927)
12/17 06:52:56 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 401/521, train_loss: 0.017(0.034), train_acc: 100.000(98.896)
12/17 06:53:02 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 501/521, train_loss: 0.038(0.033), train_acc: 98.958(98.927)
12/17 06:53:03 AM [Supernet Training] lr: 0.00113 epoch: 574/600, step: 521/521, train_loss: 0.015(0.034), train_acc: 100.000(98.924)
12/17 06:53:03 AM [Supernet Training] epoch: 574, train_loss: 0.034, train_acc: 98.924
12/17 06:53:05 AM [Supernet Validation] epoch: 574, val_loss: 0.536, val_acc: 88.590, best_acc: 88.850


12/17 06:53:05 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 97.917(97.917)
12/17 06:53:12 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 101/521, train_loss: 0.053(0.031), train_acc: 97.917(99.061)
12/17 06:53:18 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 201/521, train_loss: 0.046(0.033), train_acc: 96.875(98.938)
12/17 06:53:25 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 301/521, train_loss: 0.009(0.033), train_acc: 100.000(98.944)
12/17 06:53:31 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 401/521, train_loss: 0.092(0.032), train_acc: 96.875(98.951)
12/17 06:53:37 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 501/521, train_loss: 0.022(0.033), train_acc: 100.000(98.927)
12/17 06:53:39 AM [Supernet Training] lr: 0.00108 epoch: 575/600, step: 521/521, train_loss: 0.052(0.033), train_acc: 98.750(98.942)
12/17 06:53:39 AM [Supernet Training] epoch: 575, train_loss: 0.033, train_acc: 98.942
12/17 06:53:40 AM [Supernet Validation] epoch: 575, val_loss: 0.560, val_acc: 88.480, best_acc: 88.850


12/17 06:53:41 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 001/521, train_loss: 0.029(0.029), train_acc: 97.917(97.917)
12/17 06:53:47 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 101/521, train_loss: 0.027(0.033), train_acc: 98.958(98.979)
12/17 06:53:53 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 201/521, train_loss: 0.051(0.033), train_acc: 98.958(98.979)
12/17 06:54:00 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 301/521, train_loss: 0.069(0.033), train_acc: 97.917(98.941)
12/17 06:54:06 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 401/521, train_loss: 0.004(0.033), train_acc: 100.000(98.899)
12/17 06:54:12 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 501/521, train_loss: 0.018(0.032), train_acc: 100.000(98.938)
12/17 06:54:14 AM [Supernet Training] lr: 0.00104 epoch: 576/600, step: 521/521, train_loss: 0.029(0.032), train_acc: 98.750(98.936)
12/17 06:54:14 AM [Supernet Training] epoch: 576, train_loss: 0.032, train_acc: 98.936
12/17 06:54:15 AM [Supernet Validation] epoch: 576, val_loss: 0.544, val_acc: 88.190, best_acc: 88.850


12/17 06:54:16 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 98.958(98.958)
12/17 06:54:22 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 101/521, train_loss: 0.030(0.037), train_acc: 100.000(98.618)
12/17 06:54:28 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 201/521, train_loss: 0.014(0.036), train_acc: 98.958(98.751)
12/17 06:54:35 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 301/521, train_loss: 0.040(0.034), train_acc: 98.958(98.841)
12/17 06:54:41 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 401/521, train_loss: 0.042(0.033), train_acc: 98.958(98.901)
12/17 06:54:47 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 501/521, train_loss: 0.042(0.033), train_acc: 97.917(98.906)
12/17 06:54:49 AM [Supernet Training] lr: 0.00100 epoch: 577/600, step: 521/521, train_loss: 0.016(0.032), train_acc: 100.000(98.912)
12/17 06:54:49 AM [Supernet Training] epoch: 577, train_loss: 0.032, train_acc: 98.912
12/17 06:54:50 AM [Supernet Validation] epoch: 577, val_loss: 0.529, val_acc: 88.530, best_acc: 88.850


12/17 06:54:50 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 001/521, train_loss: 0.037(0.037), train_acc: 98.958(98.958)
12/17 06:54:57 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 101/521, train_loss: 0.014(0.035), train_acc: 100.000(98.783)
12/17 06:55:03 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 201/521, train_loss: 0.032(0.033), train_acc: 97.917(98.891)
12/17 06:55:09 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 301/521, train_loss: 0.031(0.033), train_acc: 97.917(98.903)
12/17 06:55:15 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 401/521, train_loss: 0.055(0.034), train_acc: 97.917(98.873)
12/17 06:55:22 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 501/521, train_loss: 0.121(0.033), train_acc: 95.833(98.896)
12/17 06:55:23 AM [Supernet Training] lr: 0.00096 epoch: 578/600, step: 521/521, train_loss: 0.021(0.033), train_acc: 100.000(98.902)
12/17 06:55:23 AM [Supernet Training] epoch: 578, train_loss: 0.033, train_acc: 98.902
12/17 06:55:24 AM [Supernet Validation] epoch: 578, val_loss: 0.553, val_acc: 88.330, best_acc: 88.850


12/17 06:55:25 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 100.000(100.000)
12/17 06:55:31 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 101/521, train_loss: 0.046(0.033), train_acc: 97.917(98.886)
12/17 06:55:37 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 201/521, train_loss: 0.031(0.033), train_acc: 98.958(98.922)
12/17 06:55:44 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 301/521, train_loss: 0.020(0.033), train_acc: 98.958(98.868)
12/17 06:55:50 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 401/521, train_loss: 0.015(0.034), train_acc: 100.000(98.857)
12/17 06:55:56 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 501/521, train_loss: 0.013(0.034), train_acc: 98.958(98.871)
12/17 06:55:57 AM [Supernet Training] lr: 0.00092 epoch: 579/600, step: 521/521, train_loss: 0.045(0.034), train_acc: 98.750(98.848)
12/17 06:55:57 AM [Supernet Training] epoch: 579, train_loss: 0.034, train_acc: 98.848
12/17 06:55:59 AM [Supernet Validation] epoch: 579, val_loss: 0.548, val_acc: 88.360, best_acc: 88.850


12/17 06:55:59 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
12/17 06:56:06 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 101/521, train_loss: 0.016(0.033), train_acc: 100.000(98.855)
12/17 06:56:12 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 201/521, train_loss: 0.022(0.035), train_acc: 100.000(98.803)
12/17 06:56:18 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 301/521, train_loss: 0.094(0.034), train_acc: 95.833(98.837)
12/17 06:56:24 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 401/521, train_loss: 0.022(0.034), train_acc: 100.000(98.847)
12/17 06:56:31 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 501/521, train_loss: 0.100(0.034), train_acc: 94.792(98.827)
12/17 06:56:32 AM [Supernet Training] lr: 0.00088 epoch: 580/600, step: 521/521, train_loss: 0.029(0.034), train_acc: 98.750(98.840)
12/17 06:56:32 AM [Supernet Training] epoch: 580, train_loss: 0.034, train_acc: 98.840
12/17 06:56:33 AM [Supernet Validation] epoch: 580, val_loss: 0.534, val_acc: 88.550, best_acc: 88.850


12/17 06:56:34 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 001/521, train_loss: 0.017(0.017), train_acc: 100.000(100.000)
12/17 06:56:40 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 101/521, train_loss: 0.036(0.034), train_acc: 98.958(98.866)
12/17 06:56:47 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 201/521, train_loss: 0.068(0.033), train_acc: 95.833(98.886)
12/17 06:56:53 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 301/521, train_loss: 0.040(0.032), train_acc: 100.000(98.976)
12/17 06:56:59 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 401/521, train_loss: 0.025(0.032), train_acc: 100.000(98.971)
12/17 06:57:06 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 501/521, train_loss: 0.031(0.033), train_acc: 98.958(98.931)
12/17 06:57:07 AM [Supernet Training] lr: 0.00083 epoch: 581/600, step: 521/521, train_loss: 0.052(0.033), train_acc: 98.750(98.932)
12/17 06:57:07 AM [Supernet Training] epoch: 581, train_loss: 0.033, train_acc: 98.932
12/17 06:57:08 AM [Supernet Validation] epoch: 581, val_loss: 0.541, val_acc: 88.440, best_acc: 88.850


12/17 06:57:09 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 001/521, train_loss: 0.015(0.015), train_acc: 100.000(100.000)
12/17 06:57:15 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 101/521, train_loss: 0.017(0.032), train_acc: 98.958(99.010)
12/17 06:57:21 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 201/521, train_loss: 0.022(0.031), train_acc: 98.958(99.046)
12/17 06:57:28 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 301/521, train_loss: 0.051(0.031), train_acc: 97.917(99.007)
12/17 06:57:34 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 401/521, train_loss: 0.024(0.031), train_acc: 98.958(99.028)
12/17 06:57:41 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 501/521, train_loss: 0.007(0.031), train_acc: 100.000(99.029)
12/17 06:57:42 AM [Supernet Training] lr: 0.00079 epoch: 582/600, step: 521/521, train_loss: 0.036(0.031), train_acc: 98.750(99.036)
12/17 06:57:42 AM [Supernet Training] epoch: 582, train_loss: 0.031, train_acc: 99.036
12/17 06:57:43 AM [Supernet Validation] epoch: 582, val_loss: 0.542, val_acc: 88.290, best_acc: 88.850


12/17 06:57:44 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 001/521, train_loss: 0.026(0.026), train_acc: 98.958(98.958)
12/17 06:57:50 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 101/521, train_loss: 0.032(0.035), train_acc: 98.958(98.824)
12/17 06:57:57 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 201/521, train_loss: 0.004(0.034), train_acc: 100.000(98.886)
12/17 06:58:03 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 301/521, train_loss: 0.021(0.033), train_acc: 98.958(98.941)
12/17 06:58:09 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 401/521, train_loss: 0.028(0.032), train_acc: 98.958(98.938)
12/17 06:58:16 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 501/521, train_loss: 0.027(0.032), train_acc: 98.958(98.987)
12/17 06:58:17 AM [Supernet Training] lr: 0.00075 epoch: 583/600, step: 521/521, train_loss: 0.054(0.032), train_acc: 97.500(98.974)
12/17 06:58:17 AM [Supernet Training] epoch: 583, train_loss: 0.032, train_acc: 98.974
12/17 06:58:18 AM [Supernet Validation] epoch: 583, val_loss: 0.538, val_acc: 88.410, best_acc: 88.850


12/17 06:58:19 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 001/521, train_loss: 0.037(0.037), train_acc: 97.917(97.917)
12/17 06:58:25 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 101/521, train_loss: 0.014(0.033), train_acc: 98.958(98.917)
12/17 06:58:32 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 201/521, train_loss: 0.026(0.032), train_acc: 98.958(98.912)
12/17 06:58:38 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 301/521, train_loss: 0.040(0.032), train_acc: 98.958(98.893)
12/17 06:58:44 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 401/521, train_loss: 0.017(0.032), train_acc: 100.000(98.922)
12/17 06:58:51 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 501/521, train_loss: 0.032(0.032), train_acc: 98.958(98.938)
12/17 06:58:52 AM [Supernet Training] lr: 0.00071 epoch: 584/600, step: 521/521, train_loss: 0.021(0.032), train_acc: 100.000(98.946)
12/17 06:58:52 AM [Supernet Training] epoch: 584, train_loss: 0.032, train_acc: 98.946
12/17 06:58:54 AM [Supernet Validation] epoch: 584, val_loss: 0.524, val_acc: 88.490, best_acc: 88.850


12/17 06:58:54 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 001/521, train_loss: 0.018(0.018), train_acc: 98.958(98.958)
12/17 06:59:00 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 101/521, train_loss: 0.045(0.032), train_acc: 98.958(98.907)
12/17 06:59:07 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 201/521, train_loss: 0.060(0.033), train_acc: 96.875(98.917)
12/17 06:59:13 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 301/521, train_loss: 0.082(0.032), train_acc: 96.875(98.969)
12/17 06:59:20 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 401/521, train_loss: 0.010(0.032), train_acc: 100.000(98.977)
12/17 06:59:26 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 501/521, train_loss: 0.027(0.032), train_acc: 98.958(98.925)
12/17 06:59:27 AM [Supernet Training] lr: 0.00067 epoch: 585/600, step: 521/521, train_loss: 0.033(0.033), train_acc: 97.500(98.916)
12/17 06:59:27 AM [Supernet Training] epoch: 585, train_loss: 0.033, train_acc: 98.916
12/17 06:59:29 AM [Supernet Validation] epoch: 585, val_loss: 0.537, val_acc: 88.310, best_acc: 88.850


12/17 06:59:29 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 98.958(98.958)
12/17 06:59:36 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 101/521, train_loss: 0.059(0.032), train_acc: 96.875(98.907)
12/17 06:59:42 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 201/521, train_loss: 0.041(0.031), train_acc: 97.917(98.938)
12/17 06:59:48 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 301/521, train_loss: 0.030(0.032), train_acc: 98.958(98.903)
12/17 06:59:55 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 401/521, train_loss: 0.082(0.032), train_acc: 97.917(98.940)
12/17 07:00:01 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 501/521, train_loss: 0.022(0.032), train_acc: 98.958(98.975)
12/17 07:00:02 AM [Supernet Training] lr: 0.00063 epoch: 586/600, step: 521/521, train_loss: 0.035(0.031), train_acc: 98.750(98.978)
12/17 07:00:02 AM [Supernet Training] epoch: 586, train_loss: 0.031, train_acc: 98.978
12/17 07:00:04 AM [Supernet Validation] epoch: 586, val_loss: 0.539, val_acc: 88.690, best_acc: 88.850


12/17 07:00:04 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 001/521, train_loss: 0.042(0.042), train_acc: 96.875(96.875)
12/17 07:00:11 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 101/521, train_loss: 0.036(0.033), train_acc: 97.917(98.855)
12/17 07:00:17 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 201/521, train_loss: 0.050(0.033), train_acc: 96.875(98.865)
12/17 07:00:23 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 301/521, train_loss: 0.042(0.033), train_acc: 96.875(98.941)
12/17 07:00:30 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 401/521, train_loss: 0.005(0.033), train_acc: 100.000(98.932)
12/17 07:00:36 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 501/521, train_loss: 0.019(0.032), train_acc: 100.000(98.956)
12/17 07:00:37 AM [Supernet Training] lr: 0.00058 epoch: 587/600, step: 521/521, train_loss: 0.013(0.032), train_acc: 100.000(98.958)
12/17 07:00:37 AM [Supernet Training] epoch: 587, train_loss: 0.032, train_acc: 98.958
12/17 07:00:39 AM [Supernet Validation] epoch: 587, val_loss: 0.534, val_acc: 88.300, best_acc: 88.850


12/17 07:00:39 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 001/521, train_loss: 0.073(0.073), train_acc: 96.875(96.875)
12/17 07:00:46 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 101/521, train_loss: 0.012(0.035), train_acc: 100.000(98.793)
12/17 07:00:52 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 201/521, train_loss: 0.041(0.032), train_acc: 96.875(98.932)
12/17 07:00:58 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 301/521, train_loss: 0.021(0.032), train_acc: 100.000(98.927)
12/17 07:01:05 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 401/521, train_loss: 0.006(0.032), train_acc: 100.000(98.958)
12/17 07:01:11 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 501/521, train_loss: 0.019(0.032), train_acc: 98.958(98.921)
12/17 07:01:13 AM [Supernet Training] lr: 0.00054 epoch: 588/600, step: 521/521, train_loss: 0.015(0.032), train_acc: 100.000(98.928)
12/17 07:01:13 AM [Supernet Training] epoch: 588, train_loss: 0.032, train_acc: 98.928
12/17 07:01:14 AM [Supernet Validation] epoch: 588, val_loss: 0.540, val_acc: 88.550, best_acc: 88.850


12/17 07:01:15 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 001/521, train_loss: 0.015(0.015), train_acc: 100.000(100.000)
12/17 07:01:21 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 101/521, train_loss: 0.010(0.033), train_acc: 100.000(98.969)
12/17 07:01:27 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 201/521, train_loss: 0.013(0.031), train_acc: 100.000(99.005)
12/17 07:01:34 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 301/521, train_loss: 0.026(0.032), train_acc: 98.958(98.993)
12/17 07:01:40 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 401/521, train_loss: 0.037(0.031), train_acc: 98.958(99.008)
12/17 07:01:46 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 501/521, train_loss: 0.037(0.032), train_acc: 97.917(98.967)
12/17 07:01:48 AM [Supernet Training] lr: 0.00050 epoch: 589/600, step: 521/521, train_loss: 0.027(0.032), train_acc: 98.750(98.960)
12/17 07:01:48 AM [Supernet Training] epoch: 589, train_loss: 0.032, train_acc: 98.960
12/17 07:01:49 AM [Supernet Validation] epoch: 589, val_loss: 0.537, val_acc: 88.130, best_acc: 88.850


12/17 07:01:50 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 001/521, train_loss: 0.040(0.040), train_acc: 98.958(98.958)
12/17 07:01:56 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 101/521, train_loss: 0.028(0.032), train_acc: 98.958(98.927)
12/17 07:02:03 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 201/521, train_loss: 0.021(0.032), train_acc: 98.958(98.995)
12/17 07:02:09 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 301/521, train_loss: 0.048(0.032), train_acc: 96.875(99.014)
12/17 07:02:16 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 401/521, train_loss: 0.033(0.032), train_acc: 98.958(98.997)
12/17 07:02:22 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 501/521, train_loss: 0.064(0.033), train_acc: 95.833(98.940)
12/17 07:02:23 AM [Supernet Training] lr: 0.00046 epoch: 590/600, step: 521/521, train_loss: 0.025(0.033), train_acc: 98.750(98.924)
12/17 07:02:23 AM [Supernet Training] epoch: 590, train_loss: 0.033, train_acc: 98.924
12/17 07:02:25 AM [Supernet Validation] epoch: 590, val_loss: 0.541, val_acc: 88.440, best_acc: 88.850


12/17 07:02:26 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 001/521, train_loss: 0.048(0.048), train_acc: 96.875(96.875)
12/17 07:02:32 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 101/521, train_loss: 0.011(0.027), train_acc: 100.000(99.154)
12/17 07:02:38 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 201/521, train_loss: 0.030(0.029), train_acc: 98.958(99.067)
12/17 07:02:45 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 301/521, train_loss: 0.013(0.030), train_acc: 100.000(99.028)
12/17 07:02:51 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 401/521, train_loss: 0.044(0.031), train_acc: 98.958(98.964)
12/17 07:02:58 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 501/521, train_loss: 0.037(0.031), train_acc: 96.875(98.973)
12/17 07:02:59 AM [Supernet Training] lr: 0.00042 epoch: 591/600, step: 521/521, train_loss: 0.012(0.031), train_acc: 100.000(98.972)
12/17 07:02:59 AM [Supernet Training] epoch: 591, train_loss: 0.031, train_acc: 98.972
12/17 07:03:00 AM [Supernet Validation] epoch: 591, val_loss: 0.518, val_acc: 88.660, best_acc: 88.850


12/17 07:03:01 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 001/521, train_loss: 0.071(0.071), train_acc: 98.958(98.958)
12/17 07:03:07 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 101/521, train_loss: 0.024(0.033), train_acc: 98.958(98.958)
12/17 07:03:14 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 201/521, train_loss: 0.059(0.034), train_acc: 97.917(98.839)
12/17 07:03:20 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 301/521, train_loss: 0.047(0.033), train_acc: 98.958(98.913)
12/17 07:03:27 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 401/521, train_loss: 0.040(0.033), train_acc: 97.917(98.904)
12/17 07:03:33 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 501/521, train_loss: 0.047(0.033), train_acc: 98.958(98.927)
12/17 07:03:34 AM [Supernet Training] lr: 0.00038 epoch: 592/600, step: 521/521, train_loss: 0.035(0.033), train_acc: 98.750(98.932)
12/17 07:03:34 AM [Supernet Training] epoch: 592, train_loss: 0.033, train_acc: 98.932
12/17 07:03:36 AM [Supernet Validation] epoch: 592, val_loss: 0.536, val_acc: 88.480, best_acc: 88.850


12/17 07:03:36 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 001/521, train_loss: 0.046(0.046), train_acc: 97.917(97.917)
12/17 07:03:43 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 101/521, train_loss: 0.048(0.032), train_acc: 97.917(99.020)
12/17 07:03:49 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 201/521, train_loss: 0.059(0.032), train_acc: 97.917(98.995)
12/17 07:03:56 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 301/521, train_loss: 0.040(0.033), train_acc: 98.958(98.958)
12/17 07:04:03 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 401/521, train_loss: 0.006(0.033), train_acc: 100.000(98.984)
12/17 07:04:09 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 501/521, train_loss: 0.060(0.032), train_acc: 96.875(98.979)
12/17 07:04:10 AM [Supernet Training] lr: 0.00033 epoch: 593/600, step: 521/521, train_loss: 0.017(0.032), train_acc: 100.000(98.982)
12/17 07:04:10 AM [Supernet Training] epoch: 593, train_loss: 0.032, train_acc: 98.982
12/17 07:04:12 AM [Supernet Validation] epoch: 593, val_loss: 0.553, val_acc: 88.650, best_acc: 88.850


12/17 07:04:12 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 98.958(98.958)
12/17 07:04:19 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 101/521, train_loss: 0.019(0.031), train_acc: 100.000(98.989)
12/17 07:04:25 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 201/521, train_loss: 0.046(0.032), train_acc: 98.958(98.938)
12/17 07:04:32 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 301/521, train_loss: 0.016(0.032), train_acc: 98.958(98.944)
12/17 07:04:38 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 401/521, train_loss: 0.059(0.032), train_acc: 97.917(98.979)
12/17 07:04:44 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 501/521, train_loss: 0.052(0.032), train_acc: 98.958(98.971)
12/17 07:04:45 AM [Supernet Training] lr: 0.00029 epoch: 594/600, step: 521/521, train_loss: 0.032(0.032), train_acc: 98.750(98.972)
12/17 07:04:45 AM [Supernet Training] epoch: 594, train_loss: 0.032, train_acc: 98.972
12/17 07:04:47 AM [Supernet Validation] epoch: 594, val_loss: 0.551, val_acc: 88.360, best_acc: 88.850


12/17 07:04:47 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 97.917(97.917)
12/17 07:04:54 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 101/521, train_loss: 0.061(0.033), train_acc: 96.875(98.835)
12/17 07:05:00 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 201/521, train_loss: 0.049(0.032), train_acc: 98.958(98.958)
12/17 07:05:06 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 301/521, train_loss: 0.057(0.032), train_acc: 97.917(98.948)
12/17 07:05:13 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 401/521, train_loss: 0.049(0.032), train_acc: 96.875(98.940)
12/17 07:05:19 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 501/521, train_loss: 0.084(0.032), train_acc: 97.917(98.948)
12/17 07:05:20 AM [Supernet Training] lr: 0.00025 epoch: 595/600, step: 521/521, train_loss: 0.032(0.031), train_acc: 98.750(98.962)
12/17 07:05:20 AM [Supernet Training] epoch: 595, train_loss: 0.031, train_acc: 98.962
12/17 07:05:22 AM [Supernet Validation] epoch: 595, val_loss: 0.537, val_acc: 88.610, best_acc: 88.850


12/17 07:05:22 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 100.000(100.000)
12/17 07:05:29 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 101/521, train_loss: 0.034(0.030), train_acc: 97.917(99.000)
12/17 07:05:35 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 201/521, train_loss: 0.021(0.031), train_acc: 98.958(99.000)
12/17 07:05:41 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 301/521, train_loss: 0.009(0.032), train_acc: 100.000(98.969)
12/17 07:05:47 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 401/521, train_loss: 0.027(0.033), train_acc: 98.958(98.940)
12/17 07:05:54 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 501/521, train_loss: 0.077(0.032), train_acc: 95.833(98.965)
12/17 07:05:55 AM [Supernet Training] lr: 0.00021 epoch: 596/600, step: 521/521, train_loss: 0.003(0.032), train_acc: 100.000(98.958)
12/17 07:05:55 AM [Supernet Training] epoch: 596, train_loss: 0.032, train_acc: 98.958
12/17 07:05:57 AM [Supernet Validation] epoch: 596, val_loss: 0.537, val_acc: 88.640, best_acc: 88.850


12/17 07:05:57 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 001/521, train_loss: 0.013(0.013), train_acc: 100.000(100.000)
12/17 07:06:03 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 101/521, train_loss: 0.028(0.028), train_acc: 98.958(99.123)
12/17 07:06:10 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 201/521, train_loss: 0.018(0.030), train_acc: 100.000(98.958)
12/17 07:06:16 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 301/521, train_loss: 0.040(0.031), train_acc: 98.958(98.983)
12/17 07:06:23 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 401/521, train_loss: 0.031(0.031), train_acc: 98.958(98.974)
12/17 07:06:29 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 501/521, train_loss: 0.016(0.031), train_acc: 98.958(98.975)
12/17 07:06:30 AM [Supernet Training] lr: 0.00017 epoch: 597/600, step: 521/521, train_loss: 0.012(0.031), train_acc: 100.000(98.956)
12/17 07:06:30 AM [Supernet Training] epoch: 597, train_loss: 0.031, train_acc: 98.956
12/17 07:06:32 AM [Supernet Validation] epoch: 597, val_loss: 0.545, val_acc: 88.250, best_acc: 88.850


12/17 07:06:32 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 001/521, train_loss: 0.026(0.026), train_acc: 100.000(100.000)
12/17 07:06:39 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 101/521, train_loss: 0.038(0.031), train_acc: 98.958(99.031)
12/17 07:06:45 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 201/521, train_loss: 0.076(0.031), train_acc: 98.958(99.046)
12/17 07:06:51 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 301/521, train_loss: 0.023(0.030), train_acc: 98.958(99.055)
12/17 07:06:58 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 401/521, train_loss: 0.022(0.030), train_acc: 100.000(99.052)
12/17 07:07:04 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 501/521, train_loss: 0.018(0.030), train_acc: 100.000(99.054)
12/17 07:07:05 AM [Supernet Training] lr: 0.00013 epoch: 598/600, step: 521/521, train_loss: 0.007(0.031), train_acc: 100.000(99.044)
12/17 07:07:05 AM [Supernet Training] epoch: 598, train_loss: 0.031, train_acc: 99.044
12/17 07:07:07 AM [Supernet Validation] epoch: 598, val_loss: 0.531, val_acc: 88.460, best_acc: 88.850


12/17 07:07:07 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 001/521, train_loss: 0.027(0.027), train_acc: 97.917(97.917)
12/17 07:07:14 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 101/521, train_loss: 0.020(0.033), train_acc: 98.958(98.845)
12/17 07:07:20 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 201/521, train_loss: 0.056(0.032), train_acc: 97.917(98.896)
12/17 07:07:27 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 301/521, train_loss: 0.054(0.033), train_acc: 98.958(98.931)
12/17 07:07:33 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 401/521, train_loss: 0.018(0.033), train_acc: 100.000(98.951)
12/17 07:07:40 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 501/521, train_loss: 0.035(0.033), train_acc: 97.917(98.958)
12/17 07:07:41 AM [Supernet Training] lr: 0.00008 epoch: 599/600, step: 521/521, train_loss: 0.019(0.033), train_acc: 100.000(98.950)
12/17 07:07:41 AM [Supernet Training] epoch: 599, train_loss: 0.033, train_acc: 98.950
12/17 07:07:42 AM [Supernet Validation] epoch: 599, val_loss: 0.549, val_acc: 88.500, best_acc: 88.850


12/17 07:07:43 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 001/521, train_loss: 0.013(0.013), train_acc: 100.000(100.000)
12/17 07:07:49 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 101/521, train_loss: 0.036(0.034), train_acc: 97.917(98.896)
12/17 07:07:56 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 201/521, train_loss: 0.006(0.032), train_acc: 100.000(99.036)
12/17 07:08:02 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 301/521, train_loss: 0.027(0.032), train_acc: 98.958(99.000)
12/17 07:08:08 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 401/521, train_loss: 0.010(0.032), train_acc: 100.000(98.984)
12/17 07:08:15 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 501/521, train_loss: 0.022(0.032), train_acc: 100.000(98.983)
12/17 07:08:16 AM [Supernet Training] lr: 0.00004 epoch: 600/600, step: 521/521, train_loss: 0.027(0.032), train_acc: 98.750(98.990)
12/17 07:08:16 AM [Supernet Training] epoch: 600, train_loss: 0.032, train_acc: 98.990
12/17 07:08:18 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
12/17 07:08:18 AM [Supernet Validation] epoch: 600, val_loss: 0.530, val_acc: 88.860, best_acc: 88.860


12/17 07:08:18 AM Elapsed time: 5h 51min 1s
