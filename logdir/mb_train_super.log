01/18 06:03:26 PM Namespace(exp_name='spos_c10_train_supernet', layers=16, num_choices=3, batch_size=96, epochs=600, learning_rate=0.025, momentum=0.9, weight_decay=0.0003, print_freq=100, val_interval=5, ckpt_dir='./checkpoints/', seed=0, data_root='./dataset/', classes=10, dataset='cifar10', cutout=False, cutout_length=16, auto_aug=False, resize=False, device=device(type='cuda'))
01/18 06:03:27 PM SinglePath_OneShot(
  (stem): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (2): ReLU6(inplace=True)
    (3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (5): ReLU6(inplace=True)
  )
  (choice_block): ModuleList(
    (0): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (3): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (4): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (5): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (6): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (7): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (8): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (9): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (10): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (11): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (12): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (13): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (14): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (15): ModuleList(
      (0): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): Choice_Block_mb(
        (cb_main): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
          (3): Conv2d(960, 960, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=960, bias=False)
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (5): ReLU6(inplace=True)
          (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (last_conv): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (global_pooling): AdaptiveAvgPool2d(output_size=1)
  (classifier): Linear(in_features=1280, out_features=10, bias=False)
)
01/18 06:03:28 PM 

01/18 06:03:30 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 001/521, train_loss: 2.323(2.323), train_acc: 5.208(5.208)
01/18 06:03:43 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 101/521, train_loss: 2.560(2.566), train_acc: 10.417(9.973)
01/18 06:03:56 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 201/521, train_loss: 2.317(2.472), train_acc: 12.500(10.676)
01/18 06:04:09 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 301/521, train_loss: 2.244(2.413), train_acc: 17.708(11.476)
01/18 06:04:22 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 401/521, train_loss: 2.078(2.363), train_acc: 15.625(12.627)
01/18 06:04:34 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 501/521, train_loss: 2.105(2.323), train_acc: 18.750(13.666)
01/18 06:04:37 PM [Supernet Training] lr: 0.02500 epoch: 001/600, step: 521/521, train_loss: 2.185(2.314), train_acc: 11.250(13.850)
01/18 06:04:37 PM [Supernet Training] epoch: 001, train_loss: 2.314, train_acc: 13.850
01/18 06:04:41 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:04:41 PM [Supernet Validation] epoch: 001, val_loss: 2.370, val_acc: 15.580, best_acc: 15.580
01/18 06:04:41 PM 

01/18 06:04:41 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 001/521, train_loss: 2.285(2.285), train_acc: 12.500(12.500)
01/18 06:04:54 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 101/521, train_loss: 2.162(2.125), train_acc: 13.542(17.781)
01/18 06:05:07 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 201/521, train_loss: 1.857(2.089), train_acc: 29.167(19.367)
01/18 06:05:20 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 301/521, train_loss: 2.041(2.061), train_acc: 25.000(20.107)
01/18 06:05:32 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 401/521, train_loss: 1.892(2.039), train_acc: 21.875(20.753)
01/18 06:05:45 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 501/521, train_loss: 2.094(2.024), train_acc: 19.792(21.197)
01/18 06:05:48 PM [Supernet Training] lr: 0.02500 epoch: 002/600, step: 521/521, train_loss: 2.037(2.020), train_acc: 20.000(21.298)
01/18 06:05:48 PM [Supernet Training] epoch: 002, train_loss: 2.020, train_acc: 21.298
01/18 06:05:52 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:05:52 PM [Supernet Validation] epoch: 002, val_loss: 2.059, val_acc: 19.760, best_acc: 19.760
01/18 06:05:52 PM 

01/18 06:05:52 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 001/521, train_loss: 1.907(1.907), train_acc: 26.042(26.042)
01/18 06:06:05 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 101/521, train_loss: 2.005(1.952), train_acc: 21.875(23.360)
01/18 06:06:18 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 201/521, train_loss: 1.987(1.937), train_acc: 21.875(23.741)
01/18 06:06:30 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 301/521, train_loss: 1.939(1.924), train_acc: 16.667(24.052)
01/18 06:06:43 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 401/521, train_loss: 1.896(1.914), train_acc: 27.083(24.395)
01/18 06:06:56 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 501/521, train_loss: 2.028(1.905), train_acc: 28.125(24.819)
01/18 06:06:58 PM [Supernet Training] lr: 0.02500 epoch: 003/600, step: 521/521, train_loss: 1.815(1.903), train_acc: 30.000(24.906)
01/18 06:06:59 PM [Supernet Training] epoch: 003, train_loss: 1.903, train_acc: 24.906
01/18 06:07:02 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:07:02 PM [Supernet Validation] epoch: 003, val_loss: 1.917, val_acc: 25.900, best_acc: 25.900
01/18 06:07:02 PM 

01/18 06:07:03 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 001/521, train_loss: 1.722(1.722), train_acc: 33.333(33.333)
01/18 06:07:15 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 101/521, train_loss: 1.779(1.831), train_acc: 30.208(28.146)
01/18 06:07:28 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 201/521, train_loss: 1.916(1.827), train_acc: 19.792(28.141)
01/18 06:07:41 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 301/521, train_loss: 1.900(1.817), train_acc: 22.917(28.350)
01/18 06:07:54 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 401/521, train_loss: 1.809(1.810), train_acc: 30.208(28.852)
01/18 06:08:06 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 501/521, train_loss: 1.688(1.803), train_acc: 35.417(29.535)
01/18 06:08:09 PM [Supernet Training] lr: 0.02500 epoch: 004/600, step: 521/521, train_loss: 1.775(1.803), train_acc: 31.250(29.602)
01/18 06:08:09 PM [Supernet Training] epoch: 004, train_loss: 1.803, train_acc: 29.602
01/18 06:08:13 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:08:13 PM [Supernet Validation] epoch: 004, val_loss: 1.853, val_acc: 29.970, best_acc: 29.970
01/18 06:08:13 PM 

01/18 06:08:13 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 001/521, train_loss: 1.702(1.702), train_acc: 41.667(41.667)
01/18 06:08:26 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 101/521, train_loss: 1.773(1.738), train_acc: 33.333(32.993)
01/18 06:08:39 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 201/521, train_loss: 1.772(1.730), train_acc: 31.250(33.292)
01/18 06:08:51 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 301/521, train_loss: 1.772(1.727), train_acc: 31.250(33.572)
01/18 06:09:04 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 401/521, train_loss: 1.777(1.722), train_acc: 30.208(33.741)
01/18 06:09:17 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 501/521, train_loss: 1.670(1.717), train_acc: 36.458(33.851)
01/18 06:09:20 PM [Supernet Training] lr: 0.02500 epoch: 005/600, step: 521/521, train_loss: 1.536(1.717), train_acc: 45.000(33.848)
01/18 06:09:20 PM [Supernet Training] epoch: 005, train_loss: 1.717, train_acc: 33.848
01/18 06:09:23 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:09:23 PM [Supernet Validation] epoch: 005, val_loss: 1.751, val_acc: 33.440, best_acc: 33.440
01/18 06:09:23 PM 

01/18 06:09:24 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 001/521, train_loss: 1.435(1.435), train_acc: 44.792(44.792)
01/18 06:09:37 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 101/521, train_loss: 1.501(1.663), train_acc: 41.667(36.159)
01/18 06:09:49 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 201/521, train_loss: 1.547(1.667), train_acc: 44.792(35.857)
01/18 06:10:02 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 301/521, train_loss: 1.617(1.669), train_acc: 37.500(35.811)
01/18 06:10:15 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 401/521, train_loss: 1.684(1.662), train_acc: 35.417(35.931)
01/18 06:10:28 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 501/521, train_loss: 1.715(1.655), train_acc: 35.417(36.346)
01/18 06:10:30 PM [Supernet Training] lr: 0.02500 epoch: 006/600, step: 521/521, train_loss: 1.456(1.652), train_acc: 45.000(36.484)
01/18 06:10:30 PM [Supernet Training] epoch: 006, train_loss: 1.652, train_acc: 36.484
01/18 06:10:34 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:10:34 PM [Supernet Validation] epoch: 006, val_loss: 1.699, val_acc: 34.930, best_acc: 34.930
01/18 06:10:34 PM 

01/18 06:10:35 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 001/521, train_loss: 1.614(1.614), train_acc: 37.500(37.500)
01/18 06:10:47 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 101/521, train_loss: 1.532(1.620), train_acc: 36.458(37.304)
01/18 06:11:00 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 201/521, train_loss: 1.724(1.606), train_acc: 37.500(38.220)
01/18 06:11:13 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 301/521, train_loss: 1.471(1.600), train_acc: 37.500(38.524)
01/18 06:11:26 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 401/521, train_loss: 1.555(1.594), train_acc: 39.583(38.861)
01/18 06:11:38 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 501/521, train_loss: 1.619(1.588), train_acc: 36.458(39.039)
01/18 06:11:41 PM [Supernet Training] lr: 0.02499 epoch: 007/600, step: 521/521, train_loss: 1.840(1.587), train_acc: 27.500(39.084)
01/18 06:11:41 PM [Supernet Training] epoch: 007, train_loss: 1.587, train_acc: 39.084
01/18 06:11:45 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:11:45 PM [Supernet Validation] epoch: 007, val_loss: 1.612, val_acc: 39.170, best_acc: 39.170
01/18 06:11:45 PM 

01/18 06:11:45 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 001/521, train_loss: 1.673(1.673), train_acc: 38.542(38.542)
01/18 06:11:58 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 101/521, train_loss: 1.554(1.531), train_acc: 40.625(41.966)
01/18 06:12:11 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 201/521, train_loss: 1.669(1.524), train_acc: 29.167(42.330)
01/18 06:12:23 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 301/521, train_loss: 1.570(1.521), train_acc: 41.667(42.497)
01/18 06:12:36 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 401/521, train_loss: 1.554(1.521), train_acc: 34.375(42.550)
01/18 06:12:49 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 501/521, train_loss: 1.621(1.508), train_acc: 43.750(43.093)
01/18 06:12:51 PM [Supernet Training] lr: 0.02499 epoch: 008/600, step: 521/521, train_loss: 1.393(1.505), train_acc: 47.500(43.196)
01/18 06:12:52 PM [Supernet Training] epoch: 008, train_loss: 1.505, train_acc: 43.196
01/18 06:12:55 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:12:55 PM [Supernet Validation] epoch: 008, val_loss: 1.556, val_acc: 42.500, best_acc: 42.500
01/18 06:12:55 PM 

01/18 06:12:56 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 001/521, train_loss: 1.520(1.520), train_acc: 37.500(37.500)
01/18 06:13:08 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 101/521, train_loss: 1.395(1.434), train_acc: 46.875(46.132)
01/18 06:13:21 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 201/521, train_loss: 1.545(1.439), train_acc: 40.625(46.010)
01/18 06:13:34 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 301/521, train_loss: 1.552(1.442), train_acc: 43.750(46.038)
01/18 06:13:47 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 401/521, train_loss: 1.560(1.434), train_acc: 43.750(46.379)
01/18 06:14:00 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 501/521, train_loss: 1.409(1.425), train_acc: 48.958(46.723)
01/18 06:14:02 PM [Supernet Training] lr: 0.02499 epoch: 009/600, step: 521/521, train_loss: 1.310(1.426), train_acc: 53.750(46.780)
01/18 06:14:02 PM [Supernet Training] epoch: 009, train_loss: 1.426, train_acc: 46.780
01/18 06:14:06 PM [Supernet Validation] epoch: 009, val_loss: 1.583, val_acc: 42.070, best_acc: 42.500
01/18 06:14:06 PM 

01/18 06:14:06 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 001/521, train_loss: 1.370(1.370), train_acc: 45.833(45.833)
01/18 06:14:19 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 101/521, train_loss: 1.329(1.412), train_acc: 54.167(47.195)
01/18 06:14:32 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 201/521, train_loss: 1.563(1.394), train_acc: 46.875(48.129)
01/18 06:14:44 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 301/521, train_loss: 1.514(1.385), train_acc: 42.708(48.540)
01/18 06:14:57 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 401/521, train_loss: 1.151(1.381), train_acc: 57.292(48.870)
01/18 06:15:10 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 501/521, train_loss: 1.294(1.373), train_acc: 51.042(49.216)
01/18 06:15:13 PM [Supernet Training] lr: 0.02499 epoch: 010/600, step: 521/521, train_loss: 1.342(1.372), train_acc: 52.500(49.274)
01/18 06:15:13 PM [Supernet Training] epoch: 010, train_loss: 1.372, train_acc: 49.274
01/18 06:15:17 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:15:17 PM [Supernet Validation] epoch: 010, val_loss: 1.472, val_acc: 46.920, best_acc: 46.920
01/18 06:15:17 PM 

01/18 06:15:17 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 001/521, train_loss: 1.483(1.483), train_acc: 47.917(47.917)
01/18 06:15:30 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 101/521, train_loss: 1.424(1.323), train_acc: 45.833(51.609)
01/18 06:15:43 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 201/521, train_loss: 1.400(1.328), train_acc: 51.042(51.368)
01/18 06:15:56 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 301/521, train_loss: 1.406(1.320), train_acc: 45.833(51.810)
01/18 06:16:08 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 401/521, train_loss: 1.279(1.315), train_acc: 47.917(51.873)
01/18 06:16:21 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 501/521, train_loss: 1.194(1.311), train_acc: 62.500(52.133)
01/18 06:16:24 PM [Supernet Training] lr: 0.02498 epoch: 011/600, step: 521/521, train_loss: 1.380(1.310), train_acc: 52.500(52.120)
01/18 06:16:24 PM [Supernet Training] epoch: 011, train_loss: 1.310, train_acc: 52.120
01/18 06:16:28 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:16:28 PM [Supernet Validation] epoch: 011, val_loss: 1.385, val_acc: 49.530, best_acc: 49.530
01/18 06:16:28 PM 

01/18 06:16:28 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 001/521, train_loss: 1.385(1.385), train_acc: 47.917(47.917)
01/18 06:16:41 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 101/521, train_loss: 1.398(1.259), train_acc: 45.833(53.568)
01/18 06:16:53 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 201/521, train_loss: 1.238(1.267), train_acc: 59.375(53.560)
01/18 06:17:06 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 301/521, train_loss: 1.355(1.267), train_acc: 53.125(53.679)
01/18 06:17:19 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 401/521, train_loss: 1.414(1.267), train_acc: 42.708(53.803)
01/18 06:17:32 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 501/521, train_loss: 1.331(1.265), train_acc: 47.917(53.836)
01/18 06:17:34 PM [Supernet Training] lr: 0.02498 epoch: 012/600, step: 521/521, train_loss: 1.195(1.265), train_acc: 51.250(53.864)
01/18 06:17:34 PM [Supernet Training] epoch: 012, train_loss: 1.265, train_acc: 53.864
01/18 06:17:38 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:17:38 PM [Supernet Validation] epoch: 012, val_loss: 1.311, val_acc: 52.980, best_acc: 52.980
01/18 06:17:38 PM 

01/18 06:17:38 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 001/521, train_loss: 1.330(1.330), train_acc: 51.042(51.042)
01/18 06:17:51 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 101/521, train_loss: 1.289(1.246), train_acc: 56.250(54.259)
01/18 06:18:04 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 201/521, train_loss: 1.229(1.237), train_acc: 61.458(55.001)
01/18 06:18:17 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 301/521, train_loss: 1.491(1.236), train_acc: 42.708(55.153)
01/18 06:18:29 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 401/521, train_loss: 1.440(1.232), train_acc: 51.042(55.328)
01/18 06:18:42 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 501/521, train_loss: 1.303(1.225), train_acc: 59.375(55.537)
01/18 06:18:45 PM [Supernet Training] lr: 0.02498 epoch: 013/600, step: 521/521, train_loss: 1.522(1.225), train_acc: 46.250(55.546)
01/18 06:18:45 PM [Supernet Training] epoch: 013, train_loss: 1.225, train_acc: 55.546
01/18 06:18:49 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:18:49 PM [Supernet Validation] epoch: 013, val_loss: 1.300, val_acc: 53.070, best_acc: 53.070
01/18 06:18:49 PM 

01/18 06:18:49 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 001/521, train_loss: 1.274(1.274), train_acc: 58.333(58.333)
01/18 06:19:02 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 101/521, train_loss: 1.318(1.209), train_acc: 51.042(56.539)
01/18 06:19:15 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 201/521, train_loss: 1.383(1.201), train_acc: 53.125(56.924)
01/18 06:19:27 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 301/521, train_loss: 1.079(1.198), train_acc: 64.583(57.053)
01/18 06:19:40 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 401/521, train_loss: 1.239(1.190), train_acc: 54.167(57.149)
01/18 06:19:53 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 501/521, train_loss: 1.167(1.183), train_acc: 63.542(57.240)
01/18 06:19:55 PM [Supernet Training] lr: 0.02497 epoch: 014/600, step: 521/521, train_loss: 1.010(1.182), train_acc: 66.250(57.304)
01/18 06:19:55 PM [Supernet Training] epoch: 014, train_loss: 1.182, train_acc: 57.304
01/18 06:19:59 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:19:59 PM [Supernet Validation] epoch: 014, val_loss: 1.243, val_acc: 55.470, best_acc: 55.470
01/18 06:19:59 PM 

01/18 06:19:59 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 001/521, train_loss: 1.057(1.057), train_acc: 62.500(62.500)
01/18 06:20:12 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 101/521, train_loss: 0.931(1.160), train_acc: 70.833(57.838)
01/18 06:20:25 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 201/521, train_loss: 1.218(1.156), train_acc: 59.375(58.292)
01/18 06:20:38 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 301/521, train_loss: 1.225(1.154), train_acc: 54.167(58.143)
01/18 06:20:51 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 401/521, train_loss: 1.326(1.158), train_acc: 53.125(58.097)
01/18 06:21:03 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 501/521, train_loss: 1.037(1.155), train_acc: 60.417(58.142)
01/18 06:21:06 PM [Supernet Training] lr: 0.02497 epoch: 015/600, step: 521/521, train_loss: 1.220(1.154), train_acc: 53.750(58.162)
01/18 06:21:06 PM [Supernet Training] epoch: 015, train_loss: 1.154, train_acc: 58.162
01/18 06:21:10 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:21:10 PM [Supernet Validation] epoch: 015, val_loss: 1.234, val_acc: 55.650, best_acc: 55.650
01/18 06:21:10 PM 

01/18 06:21:11 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 001/521, train_loss: 1.037(1.037), train_acc: 59.375(59.375)
01/18 06:21:23 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 101/521, train_loss: 0.977(1.125), train_acc: 66.667(58.674)
01/18 06:21:36 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 201/521, train_loss: 1.145(1.145), train_acc: 53.125(58.484)
01/18 06:21:49 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 301/521, train_loss: 0.997(1.137), train_acc: 66.667(58.984)
01/18 06:22:02 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 401/521, train_loss: 0.988(1.136), train_acc: 60.417(58.861)
01/18 06:22:15 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 501/521, train_loss: 1.182(1.127), train_acc: 52.083(59.250)
01/18 06:22:17 PM [Supernet Training] lr: 0.02496 epoch: 016/600, step: 521/521, train_loss: 1.213(1.125), train_acc: 60.000(59.308)
01/18 06:22:17 PM [Supernet Training] epoch: 016, train_loss: 1.125, train_acc: 59.308
01/18 06:22:22 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:22:22 PM [Supernet Validation] epoch: 016, val_loss: 1.197, val_acc: 57.660, best_acc: 57.660
01/18 06:22:22 PM 

01/18 06:22:22 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 001/521, train_loss: 1.211(1.211), train_acc: 57.292(57.292)
01/18 06:22:35 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 101/521, train_loss: 0.985(1.087), train_acc: 62.500(60.870)
01/18 06:22:48 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 201/521, train_loss: 1.109(1.100), train_acc: 54.167(60.360)
01/18 06:23:00 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 301/521, train_loss: 1.120(1.099), train_acc: 64.583(60.489)
01/18 06:23:13 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 401/521, train_loss: 1.207(1.100), train_acc: 56.250(60.461)
01/18 06:23:26 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 501/521, train_loss: 1.022(1.099), train_acc: 57.292(60.506)
01/18 06:23:29 PM [Supernet Training] lr: 0.02496 epoch: 017/600, step: 521/521, train_loss: 1.055(1.098), train_acc: 57.500(60.484)
01/18 06:23:29 PM [Supernet Training] epoch: 017, train_loss: 1.098, train_acc: 60.484
01/18 06:23:32 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:23:32 PM [Supernet Validation] epoch: 017, val_loss: 1.145, val_acc: 59.450, best_acc: 59.450
01/18 06:23:32 PM 

01/18 06:23:33 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 001/521, train_loss: 0.939(0.939), train_acc: 66.667(66.667)
01/18 06:23:45 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 101/521, train_loss: 1.139(1.077), train_acc: 54.167(61.128)
01/18 06:23:58 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 201/521, train_loss: 1.194(1.077), train_acc: 59.375(61.303)
01/18 06:24:11 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 301/521, train_loss: 1.070(1.074), train_acc: 60.417(61.310)
01/18 06:24:24 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 401/521, train_loss: 1.080(1.076), train_acc: 61.458(61.360)
01/18 06:24:37 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 501/521, train_loss: 1.107(1.079), train_acc: 58.333(61.190)
01/18 06:24:39 PM [Supernet Training] lr: 0.02495 epoch: 018/600, step: 521/521, train_loss: 1.211(1.080), train_acc: 56.250(61.128)
01/18 06:24:39 PM [Supernet Training] epoch: 018, train_loss: 1.080, train_acc: 61.128
01/18 06:24:43 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:24:43 PM [Supernet Validation] epoch: 018, val_loss: 1.119, val_acc: 60.560, best_acc: 60.560
01/18 06:24:43 PM 

01/18 06:24:43 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 001/521, train_loss: 0.945(0.945), train_acc: 68.750(68.750)
01/18 06:24:56 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 101/521, train_loss: 1.105(1.059), train_acc: 64.583(61.479)
01/18 06:25:09 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 201/521, train_loss: 0.925(1.060), train_acc: 65.625(61.464)
01/18 06:25:22 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 301/521, train_loss: 1.159(1.065), train_acc: 66.667(61.500)
01/18 06:25:35 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 401/521, train_loss: 0.908(1.061), train_acc: 64.583(61.666)
01/18 06:25:47 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 501/521, train_loss: 0.939(1.057), train_acc: 65.625(61.820)
01/18 06:25:50 PM [Supernet Training] lr: 0.02494 epoch: 019/600, step: 521/521, train_loss: 0.975(1.056), train_acc: 60.000(61.820)
01/18 06:25:50 PM [Supernet Training] epoch: 019, train_loss: 1.056, train_acc: 61.820
01/18 06:25:53 PM [Supernet Validation] epoch: 019, val_loss: 1.119, val_acc: 60.320, best_acc: 60.560
01/18 06:25:53 PM 

01/18 06:25:54 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 001/521, train_loss: 0.957(0.957), train_acc: 67.708(67.708)
01/18 06:26:07 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 101/521, train_loss: 0.926(1.018), train_acc: 68.750(63.800)
01/18 06:26:19 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 201/521, train_loss: 1.009(1.028), train_acc: 62.500(63.158)
01/18 06:26:32 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 301/521, train_loss: 0.934(1.029), train_acc: 61.458(63.126)
01/18 06:26:45 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 401/521, train_loss: 0.903(1.028), train_acc: 68.750(63.212)
01/18 06:26:58 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 501/521, train_loss: 0.997(1.025), train_acc: 69.792(63.240)
01/18 06:27:00 PM [Supernet Training] lr: 0.02494 epoch: 020/600, step: 521/521, train_loss: 0.896(1.024), train_acc: 66.250(63.244)
01/18 06:27:00 PM [Supernet Training] epoch: 020, train_loss: 1.024, train_acc: 63.244
01/18 06:27:04 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:27:04 PM [Supernet Validation] epoch: 020, val_loss: 1.096, val_acc: 61.510, best_acc: 61.510
01/18 06:27:04 PM 

01/18 06:27:05 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 001/521, train_loss: 1.191(1.191), train_acc: 59.375(59.375)
01/18 06:27:18 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 101/521, train_loss: 1.124(1.024), train_acc: 62.500(63.005)
01/18 06:27:30 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 201/521, train_loss: 1.177(1.020), train_acc: 57.292(63.122)
01/18 06:27:43 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 301/521, train_loss: 0.767(1.015), train_acc: 66.667(63.282)
01/18 06:27:56 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 401/521, train_loss: 0.958(1.011), train_acc: 66.667(63.378)
01/18 06:28:09 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 501/521, train_loss: 1.016(1.010), train_acc: 65.625(63.463)
01/18 06:28:11 PM [Supernet Training] lr: 0.02493 epoch: 021/600, step: 521/521, train_loss: 0.936(1.010), train_acc: 65.000(63.474)
01/18 06:28:11 PM [Supernet Training] epoch: 021, train_loss: 1.010, train_acc: 63.474
01/18 06:28:15 PM [Supernet Validation] epoch: 021, val_loss: 1.126, val_acc: 61.130, best_acc: 61.510
01/18 06:28:15 PM 

01/18 06:28:15 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 001/521, train_loss: 0.918(0.918), train_acc: 67.708(67.708)
01/18 06:28:28 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 101/521, train_loss: 1.014(0.994), train_acc: 66.667(64.470)
01/18 06:28:41 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 201/521, train_loss: 1.154(1.001), train_acc: 61.458(64.039)
01/18 06:28:54 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 301/521, train_loss: 0.858(0.997), train_acc: 72.917(64.185)
01/18 06:29:06 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 401/521, train_loss: 1.049(0.993), train_acc: 64.583(64.204)
01/18 06:29:19 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 501/521, train_loss: 0.925(0.993), train_acc: 63.542(64.236)
01/18 06:29:22 PM [Supernet Training] lr: 0.02492 epoch: 022/600, step: 521/521, train_loss: 0.769(0.993), train_acc: 72.500(64.186)
01/18 06:29:22 PM [Supernet Training] epoch: 022, train_loss: 0.993, train_acc: 64.186
01/18 06:29:25 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:29:25 PM [Supernet Validation] epoch: 022, val_loss: 1.075, val_acc: 62.060, best_acc: 62.060
01/18 06:29:25 PM 

01/18 06:29:26 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 001/521, train_loss: 1.039(1.039), train_acc: 66.667(66.667)
01/18 06:29:39 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 101/521, train_loss: 1.268(0.998), train_acc: 56.250(63.861)
01/18 06:29:51 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 201/521, train_loss: 0.999(0.988), train_acc: 60.417(64.158)
01/18 06:30:04 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 301/521, train_loss: 0.901(0.975), train_acc: 69.792(64.774)
01/18 06:30:17 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 401/521, train_loss: 1.246(0.967), train_acc: 64.583(65.178)
01/18 06:30:30 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 501/521, train_loss: 0.891(0.964), train_acc: 70.833(65.274)
01/18 06:30:32 PM [Supernet Training] lr: 0.02492 epoch: 023/600, step: 521/521, train_loss: 0.849(0.966), train_acc: 65.000(65.218)
01/18 06:30:32 PM [Supernet Training] epoch: 023, train_loss: 0.966, train_acc: 65.218
01/18 06:30:36 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:30:36 PM [Supernet Validation] epoch: 023, val_loss: 1.047, val_acc: 63.020, best_acc: 63.020
01/18 06:30:36 PM 

01/18 06:30:36 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 001/521, train_loss: 1.162(1.162), train_acc: 55.208(55.208)
01/18 06:30:49 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 101/521, train_loss: 0.862(0.951), train_acc: 69.792(65.842)
01/18 06:31:02 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 201/521, train_loss: 0.920(0.951), train_acc: 64.583(65.972)
01/18 06:31:15 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 301/521, train_loss: 1.042(0.954), train_acc: 64.583(65.705)
01/18 06:31:28 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 401/521, train_loss: 1.086(0.952), train_acc: 65.625(65.794)
01/18 06:31:40 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 501/521, train_loss: 0.986(0.951), train_acc: 63.542(65.856)
01/18 06:31:43 PM [Supernet Training] lr: 0.02491 epoch: 024/600, step: 521/521, train_loss: 0.978(0.951), train_acc: 68.750(65.884)
01/18 06:31:43 PM [Supernet Training] epoch: 024, train_loss: 0.951, train_acc: 65.884
01/18 06:31:47 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:31:47 PM [Supernet Validation] epoch: 024, val_loss: 1.037, val_acc: 64.130, best_acc: 64.130
01/18 06:31:47 PM 

01/18 06:31:47 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 001/521, train_loss: 1.074(1.074), train_acc: 60.417(60.417)
01/18 06:32:00 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 101/521, train_loss: 0.950(0.949), train_acc: 62.500(66.285)
01/18 06:32:12 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 201/521, train_loss: 0.941(0.939), train_acc: 69.792(66.672)
01/18 06:32:25 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 301/521, train_loss: 1.109(0.936), train_acc: 60.417(66.601)
01/18 06:32:38 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 401/521, train_loss: 0.975(0.931), train_acc: 63.542(66.687)
01/18 06:32:51 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 501/521, train_loss: 0.876(0.931), train_acc: 64.583(66.646)
01/18 06:32:53 PM [Supernet Training] lr: 0.02490 epoch: 025/600, step: 521/521, train_loss: 0.789(0.931), train_acc: 77.500(66.634)
01/18 06:32:53 PM [Supernet Training] epoch: 025, train_loss: 0.931, train_acc: 66.634
01/18 06:32:57 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:32:57 PM [Supernet Validation] epoch: 025, val_loss: 1.002, val_acc: 64.880, best_acc: 64.880
01/18 06:32:57 PM 

01/18 06:32:58 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 001/521, train_loss: 0.898(0.898), train_acc: 69.792(69.792)
01/18 06:33:10 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 101/521, train_loss: 1.007(0.920), train_acc: 61.458(67.358)
01/18 06:33:23 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 201/521, train_loss: 0.823(0.914), train_acc: 69.792(67.434)
01/18 06:33:36 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 301/521, train_loss: 0.963(0.919), train_acc: 68.750(67.279)
01/18 06:33:49 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 401/521, train_loss: 1.062(0.921), train_acc: 62.500(67.082)
01/18 06:34:01 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 501/521, train_loss: 0.885(0.917), train_acc: 71.875(67.193)
01/18 06:34:04 PM [Supernet Training] lr: 0.02489 epoch: 026/600, step: 521/521, train_loss: 1.089(0.917), train_acc: 63.750(67.184)
01/18 06:34:04 PM [Supernet Training] epoch: 026, train_loss: 0.917, train_acc: 67.184
01/18 06:34:08 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:34:08 PM [Supernet Validation] epoch: 026, val_loss: 1.001, val_acc: 64.900, best_acc: 64.900
01/18 06:34:08 PM 

01/18 06:34:08 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 001/521, train_loss: 0.940(0.940), train_acc: 62.500(62.500)
01/18 06:34:21 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 101/521, train_loss: 0.822(0.911), train_acc: 68.750(67.543)
01/18 06:34:34 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 201/521, train_loss: 0.974(0.904), train_acc: 65.625(67.869)
01/18 06:34:46 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 301/521, train_loss: 0.935(0.901), train_acc: 68.750(68.086)
01/18 06:34:59 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 401/521, train_loss: 0.848(0.904), train_acc: 68.750(67.872)
01/18 06:35:12 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 501/521, train_loss: 0.688(0.896), train_acc: 79.167(68.334)
01/18 06:35:15 PM [Supernet Training] lr: 0.02488 epoch: 027/600, step: 521/521, train_loss: 0.920(0.897), train_acc: 70.000(68.296)
01/18 06:35:15 PM [Supernet Training] epoch: 027, train_loss: 0.897, train_acc: 68.296
01/18 06:35:18 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:35:18 PM [Supernet Validation] epoch: 027, val_loss: 0.968, val_acc: 65.480, best_acc: 65.480
01/18 06:35:18 PM 

01/18 06:35:19 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 001/521, train_loss: 1.056(1.056), train_acc: 63.542(63.542)
01/18 06:35:31 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 101/521, train_loss: 0.859(0.889), train_acc: 70.833(68.368)
01/18 06:35:44 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 201/521, train_loss: 0.845(0.888), train_acc: 71.875(68.563)
01/18 06:35:57 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 301/521, train_loss: 0.808(0.881), train_acc: 71.875(68.788)
01/18 06:36:10 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 401/521, train_loss: 0.716(0.883), train_acc: 76.042(68.703)
01/18 06:36:22 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 501/521, train_loss: 0.808(0.881), train_acc: 72.917(68.750)
01/18 06:36:25 PM [Supernet Training] lr: 0.02488 epoch: 028/600, step: 521/521, train_loss: 0.823(0.882), train_acc: 73.750(68.674)
01/18 06:36:25 PM [Supernet Training] epoch: 028, train_loss: 0.882, train_acc: 68.674
01/18 06:36:29 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:36:29 PM [Supernet Validation] epoch: 028, val_loss: 0.975, val_acc: 66.850, best_acc: 66.850
01/18 06:36:29 PM 

01/18 06:36:29 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 001/521, train_loss: 0.922(0.922), train_acc: 64.583(64.583)
01/18 06:36:42 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 101/521, train_loss: 0.984(0.875), train_acc: 65.625(69.245)
01/18 06:36:55 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 201/521, train_loss: 0.969(0.876), train_acc: 67.708(69.170)
01/18 06:37:08 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 301/521, train_loss: 1.058(0.868), train_acc: 61.458(69.549)
01/18 06:37:20 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 401/521, train_loss: 0.951(0.865), train_acc: 69.792(69.732)
01/18 06:37:33 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 501/521, train_loss: 0.827(0.867), train_acc: 72.917(69.567)
01/18 06:37:36 PM [Supernet Training] lr: 0.02487 epoch: 029/600, step: 521/521, train_loss: 1.067(0.867), train_acc: 60.000(69.556)
01/18 06:37:36 PM [Supernet Training] epoch: 029, train_loss: 0.867, train_acc: 69.556
01/18 06:37:39 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:37:39 PM [Supernet Validation] epoch: 029, val_loss: 0.946, val_acc: 67.650, best_acc: 67.650
01/18 06:37:39 PM 

01/18 06:37:40 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 001/521, train_loss: 0.878(0.878), train_acc: 70.833(70.833)
01/18 06:37:53 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 101/521, train_loss: 1.031(0.865), train_acc: 65.625(69.472)
01/18 06:38:05 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 201/521, train_loss: 0.946(0.861), train_acc: 62.500(69.579)
01/18 06:38:18 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 301/521, train_loss: 0.781(0.856), train_acc: 72.917(69.722)
01/18 06:38:31 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 401/521, train_loss: 1.134(0.857), train_acc: 70.833(69.690)
01/18 06:38:44 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 501/521, train_loss: 0.743(0.858), train_acc: 76.042(69.648)
01/18 06:38:46 PM [Supernet Training] lr: 0.02486 epoch: 030/600, step: 521/521, train_loss: 0.786(0.858), train_acc: 72.500(69.624)
01/18 06:38:46 PM [Supernet Training] epoch: 030, train_loss: 0.858, train_acc: 69.624
01/18 06:38:50 PM [Supernet Validation] epoch: 030, val_loss: 0.926, val_acc: 67.450, best_acc: 67.650
01/18 06:38:50 PM 

01/18 06:38:50 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 001/521, train_loss: 0.873(0.873), train_acc: 67.708(67.708)
01/18 06:39:03 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 101/521, train_loss: 0.825(0.854), train_acc: 68.750(70.111)
01/18 06:39:16 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 201/521, train_loss: 0.826(0.854), train_acc: 66.667(69.766)
01/18 06:39:29 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 301/521, train_loss: 0.794(0.848), train_acc: 73.958(70.134)
01/18 06:39:41 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 401/521, train_loss: 0.750(0.846), train_acc: 76.042(70.171)
01/18 06:39:54 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 501/521, train_loss: 1.039(0.847), train_acc: 63.542(70.112)
01/18 06:39:57 PM [Supernet Training] lr: 0.02485 epoch: 031/600, step: 521/521, train_loss: 0.910(0.846), train_acc: 63.750(70.160)
01/18 06:39:57 PM [Supernet Training] epoch: 031, train_loss: 0.846, train_acc: 70.160
01/18 06:40:00 PM [Supernet Validation] epoch: 031, val_loss: 0.996, val_acc: 66.510, best_acc: 67.650
01/18 06:40:00 PM 

01/18 06:40:01 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 001/521, train_loss: 0.856(0.856), train_acc: 62.500(62.500)
01/18 06:40:14 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 101/521, train_loss: 0.677(0.819), train_acc: 73.958(70.555)
01/18 06:40:26 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 201/521, train_loss: 0.851(0.823), train_acc: 66.667(70.507)
01/18 06:40:39 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 301/521, train_loss: 0.833(0.827), train_acc: 68.750(70.383)
01/18 06:40:52 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 401/521, train_loss: 0.841(0.829), train_acc: 72.917(70.488)
01/18 06:41:05 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 501/521, train_loss: 0.762(0.829), train_acc: 69.792(70.561)
01/18 06:41:07 PM [Supernet Training] lr: 0.02484 epoch: 032/600, step: 521/521, train_loss: 0.705(0.829), train_acc: 75.000(70.590)
01/18 06:41:07 PM [Supernet Training] epoch: 032, train_loss: 0.829, train_acc: 70.590
01/18 06:41:11 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:41:11 PM [Supernet Validation] epoch: 032, val_loss: 0.864, val_acc: 69.990, best_acc: 69.990
01/18 06:41:11 PM 

01/18 06:41:11 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 001/521, train_loss: 0.967(0.967), train_acc: 60.417(60.417)
01/18 06:41:24 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 101/521, train_loss: 0.863(0.819), train_acc: 71.875(71.029)
01/18 06:41:37 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 201/521, train_loss: 0.708(0.815), train_acc: 75.000(71.279)
01/18 06:41:50 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 301/521, train_loss: 0.756(0.819), train_acc: 77.083(70.989)
01/18 06:42:03 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 401/521, train_loss: 0.711(0.817), train_acc: 76.042(70.971)
01/18 06:42:15 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 501/521, train_loss: 0.928(0.817), train_acc: 70.833(71.070)
01/18 06:42:18 PM [Supernet Training] lr: 0.02482 epoch: 033/600, step: 521/521, train_loss: 0.841(0.815), train_acc: 72.500(71.146)
01/18 06:42:18 PM [Supernet Training] epoch: 033, train_loss: 0.815, train_acc: 71.146
01/18 06:42:21 PM [Supernet Validation] epoch: 033, val_loss: 0.932, val_acc: 68.820, best_acc: 69.990
01/18 06:42:21 PM 

01/18 06:42:22 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 001/521, train_loss: 0.923(0.923), train_acc: 66.667(66.667)
01/18 06:42:35 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 101/521, train_loss: 0.751(0.803), train_acc: 75.000(71.380)
01/18 06:42:47 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 201/521, train_loss: 0.756(0.805), train_acc: 73.958(71.455)
01/18 06:43:00 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 301/521, train_loss: 0.908(0.813), train_acc: 72.917(71.370)
01/18 06:43:13 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 401/521, train_loss: 0.740(0.805), train_acc: 77.083(71.696)
01/18 06:43:26 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 501/521, train_loss: 0.737(0.805), train_acc: 77.083(71.740)
01/18 06:43:28 PM [Supernet Training] lr: 0.02481 epoch: 034/600, step: 521/521, train_loss: 1.041(0.804), train_acc: 66.250(71.728)
01/18 06:43:28 PM [Supernet Training] epoch: 034, train_loss: 0.804, train_acc: 71.728
01/18 06:43:32 PM [Supernet Validation] epoch: 034, val_loss: 0.890, val_acc: 69.080, best_acc: 69.990
01/18 06:43:32 PM 

01/18 06:43:32 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 001/521, train_loss: 1.025(1.025), train_acc: 65.625(65.625)
01/18 06:43:45 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 101/521, train_loss: 0.648(0.823), train_acc: 78.125(71.040)
01/18 06:43:58 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 201/521, train_loss: 0.817(0.800), train_acc: 73.958(71.870)
01/18 06:44:11 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 301/521, train_loss: 0.751(0.802), train_acc: 69.792(71.636)
01/18 06:44:24 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 401/521, train_loss: 0.810(0.806), train_acc: 68.750(71.462)
01/18 06:44:36 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 501/521, train_loss: 0.863(0.802), train_acc: 65.625(71.767)
01/18 06:44:39 PM [Supernet Training] lr: 0.02480 epoch: 035/600, step: 521/521, train_loss: 0.763(0.801), train_acc: 76.250(71.808)
01/18 06:44:39 PM [Supernet Training] epoch: 035, train_loss: 0.801, train_acc: 71.808
01/18 06:44:42 PM [Supernet Validation] epoch: 035, val_loss: 0.913, val_acc: 69.480, best_acc: 69.990
01/18 06:44:42 PM 

01/18 06:44:43 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 001/521, train_loss: 0.651(0.651), train_acc: 79.167(79.167)
01/18 06:44:56 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 101/521, train_loss: 0.605(0.771), train_acc: 71.875(73.236)
01/18 06:45:08 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 201/521, train_loss: 0.932(0.784), train_acc: 65.625(72.569)
01/18 06:45:21 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 301/521, train_loss: 0.945(0.790), train_acc: 62.500(72.443)
01/18 06:45:34 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 401/521, train_loss: 0.723(0.788), train_acc: 76.042(72.395)
01/18 06:45:47 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 501/521, train_loss: 0.864(0.785), train_acc: 71.875(72.430)
01/18 06:45:49 PM [Supernet Training] lr: 0.02479 epoch: 036/600, step: 521/521, train_loss: 0.957(0.786), train_acc: 67.500(72.400)
01/18 06:45:49 PM [Supernet Training] epoch: 036, train_loss: 0.786, train_acc: 72.400
01/18 06:45:53 PM [Supernet Validation] epoch: 036, val_loss: 0.877, val_acc: 69.770, best_acc: 69.990
01/18 06:45:53 PM 

01/18 06:45:53 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 001/521, train_loss: 0.611(0.611), train_acc: 78.125(78.125)
01/18 06:46:06 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 101/521, train_loss: 0.633(0.765), train_acc: 79.167(73.030)
01/18 06:46:19 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 201/521, train_loss: 0.718(0.770), train_acc: 71.875(72.678)
01/18 06:46:31 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 301/521, train_loss: 0.853(0.769), train_acc: 63.542(72.903)
01/18 06:46:44 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 401/521, train_loss: 0.929(0.772), train_acc: 71.875(72.717)
01/18 06:46:57 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 501/521, train_loss: 0.911(0.774), train_acc: 68.750(72.725)
01/18 06:47:00 PM [Supernet Training] lr: 0.02478 epoch: 037/600, step: 521/521, train_loss: 0.701(0.772), train_acc: 77.500(72.818)
01/18 06:47:00 PM [Supernet Training] epoch: 037, train_loss: 0.772, train_acc: 72.818
01/18 06:47:03 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:47:03 PM [Supernet Validation] epoch: 037, val_loss: 0.818, val_acc: 71.950, best_acc: 71.950
01/18 06:47:03 PM 

01/18 06:47:04 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 001/521, train_loss: 0.754(0.754), train_acc: 70.833(70.833)
01/18 06:47:16 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 101/521, train_loss: 1.085(0.756), train_acc: 66.667(72.968)
01/18 06:47:29 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 201/521, train_loss: 0.849(0.760), train_acc: 70.833(73.093)
01/18 06:47:42 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 301/521, train_loss: 0.880(0.759), train_acc: 70.833(73.176)
01/18 06:47:55 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 401/521, train_loss: 0.721(0.763), train_acc: 71.875(73.065)
01/18 06:48:07 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 501/521, train_loss: 0.697(0.764), train_acc: 71.875(73.002)
01/18 06:48:10 PM [Supernet Training] lr: 0.02477 epoch: 038/600, step: 521/521, train_loss: 0.876(0.764), train_acc: 71.250(72.978)
01/18 06:48:10 PM [Supernet Training] epoch: 038, train_loss: 0.764, train_acc: 72.978
01/18 06:48:14 PM [Supernet Validation] epoch: 038, val_loss: 0.848, val_acc: 71.150, best_acc: 71.950
01/18 06:48:14 PM 

01/18 06:48:14 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 001/521, train_loss: 0.816(0.816), train_acc: 73.958(73.958)
01/18 06:48:27 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 101/521, train_loss: 0.808(0.755), train_acc: 73.958(74.226)
01/18 06:48:40 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 201/521, train_loss: 0.726(0.755), train_acc: 75.000(73.642)
01/18 06:48:52 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 301/521, train_loss: 0.685(0.757), train_acc: 78.125(73.553)
01/18 06:49:05 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 401/521, train_loss: 0.926(0.755), train_acc: 71.875(73.602)
01/18 06:49:18 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 501/521, train_loss: 0.766(0.754), train_acc: 68.750(73.634)
01/18 06:49:21 PM [Supernet Training] lr: 0.02475 epoch: 039/600, step: 521/521, train_loss: 0.598(0.755), train_acc: 81.250(73.574)
01/18 06:49:21 PM [Supernet Training] epoch: 039, train_loss: 0.755, train_acc: 73.574
01/18 06:49:24 PM [Supernet Validation] epoch: 039, val_loss: 0.811, val_acc: 71.830, best_acc: 71.950
01/18 06:49:24 PM 

01/18 06:49:25 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 001/521, train_loss: 0.704(0.704), train_acc: 73.958(73.958)
01/18 06:49:37 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 101/521, train_loss: 0.805(0.736), train_acc: 69.792(74.216)
01/18 06:49:50 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 201/521, train_loss: 0.722(0.743), train_acc: 68.750(74.078)
01/18 06:50:03 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 301/521, train_loss: 0.798(0.744), train_acc: 70.833(74.034)
01/18 06:50:16 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 401/521, train_loss: 0.728(0.742), train_acc: 69.792(74.109)
01/18 06:50:29 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 501/521, train_loss: 0.677(0.744), train_acc: 75.000(74.062)
01/18 06:50:31 PM [Supernet Training] lr: 0.02474 epoch: 040/600, step: 521/521, train_loss: 0.852(0.745), train_acc: 67.500(74.002)
01/18 06:50:31 PM [Supernet Training] epoch: 040, train_loss: 0.745, train_acc: 74.002
01/18 06:50:35 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:50:35 PM [Supernet Validation] epoch: 040, val_loss: 0.809, val_acc: 72.220, best_acc: 72.220
01/18 06:50:35 PM 

01/18 06:50:35 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 001/521, train_loss: 0.669(0.669), train_acc: 67.708(67.708)
01/18 06:50:48 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 101/521, train_loss: 0.738(0.726), train_acc: 75.000(74.464)
01/18 06:51:01 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 201/521, train_loss: 0.797(0.737), train_acc: 69.792(73.917)
01/18 06:51:14 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 301/521, train_loss: 0.725(0.732), train_acc: 79.167(74.163)
01/18 06:51:26 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 401/521, train_loss: 0.821(0.740), train_acc: 66.667(74.054)
01/18 06:51:39 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 501/521, train_loss: 0.645(0.736), train_acc: 78.125(74.112)
01/18 06:51:42 PM [Supernet Training] lr: 0.02473 epoch: 041/600, step: 521/521, train_loss: 0.999(0.737), train_acc: 63.750(74.090)
01/18 06:51:42 PM [Supernet Training] epoch: 041, train_loss: 0.737, train_acc: 74.090
01/18 06:51:45 PM [Supernet Validation] epoch: 041, val_loss: 0.810, val_acc: 72.210, best_acc: 72.220
01/18 06:51:45 PM 

01/18 06:51:46 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 001/521, train_loss: 0.673(0.673), train_acc: 73.958(73.958)
01/18 06:51:59 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 101/521, train_loss: 0.799(0.724), train_acc: 72.917(74.794)
01/18 06:52:11 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 201/521, train_loss: 0.708(0.718), train_acc: 73.958(75.031)
01/18 06:52:24 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 301/521, train_loss: 0.825(0.726), train_acc: 70.833(74.571)
01/18 06:52:37 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 401/521, train_loss: 0.617(0.724), train_acc: 78.125(74.571)
01/18 06:52:50 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 501/521, train_loss: 0.840(0.723), train_acc: 71.875(74.534)
01/18 06:52:52 PM [Supernet Training] lr: 0.02471 epoch: 042/600, step: 521/521, train_loss: 0.735(0.723), train_acc: 71.250(74.496)
01/18 06:52:52 PM [Supernet Training] epoch: 042, train_loss: 0.723, train_acc: 74.496
01/18 06:52:56 PM [Supernet Validation] epoch: 042, val_loss: 0.829, val_acc: 72.070, best_acc: 72.220
01/18 06:52:56 PM 

01/18 06:52:56 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 001/521, train_loss: 0.573(0.573), train_acc: 80.208(80.208)
01/18 06:53:09 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 101/521, train_loss: 0.708(0.701), train_acc: 76.042(75.299)
01/18 06:53:22 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 201/521, train_loss: 0.738(0.707), train_acc: 71.875(74.870)
01/18 06:53:35 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 301/521, train_loss: 0.732(0.709), train_acc: 80.208(75.052)
01/18 06:53:47 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 401/521, train_loss: 0.704(0.715), train_acc: 77.083(74.813)
01/18 06:54:00 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 501/521, train_loss: 0.629(0.715), train_acc: 76.042(74.888)
01/18 06:54:03 PM [Supernet Training] lr: 0.02470 epoch: 043/600, step: 521/521, train_loss: 0.677(0.715), train_acc: 80.000(74.866)
01/18 06:54:03 PM [Supernet Training] epoch: 043, train_loss: 0.715, train_acc: 74.866
01/18 06:54:06 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:54:06 PM [Supernet Validation] epoch: 043, val_loss: 0.778, val_acc: 72.810, best_acc: 72.810
01/18 06:54:06 PM 

01/18 06:54:07 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 001/521, train_loss: 0.563(0.563), train_acc: 82.292(82.292)
01/18 06:54:20 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 101/521, train_loss: 0.848(0.716), train_acc: 68.750(75.248)
01/18 06:54:32 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 201/521, train_loss: 0.708(0.709), train_acc: 73.958(75.306)
01/18 06:54:45 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 301/521, train_loss: 0.589(0.703), train_acc: 79.167(75.388)
01/18 06:54:58 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 401/521, train_loss: 0.559(0.705), train_acc: 81.250(75.332)
01/18 06:55:11 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 501/521, train_loss: 0.623(0.709), train_acc: 73.958(75.204)
01/18 06:55:13 PM [Supernet Training] lr: 0.02468 epoch: 044/600, step: 521/521, train_loss: 0.560(0.708), train_acc: 82.500(75.228)
01/18 06:55:13 PM [Supernet Training] epoch: 044, train_loss: 0.708, train_acc: 75.228
01/18 06:55:17 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:55:17 PM [Supernet Validation] epoch: 044, val_loss: 0.785, val_acc: 72.910, best_acc: 72.910
01/18 06:55:17 PM 

01/18 06:55:17 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 001/521, train_loss: 0.656(0.656), train_acc: 79.167(79.167)
01/18 06:55:30 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 101/521, train_loss: 0.500(0.690), train_acc: 81.250(76.165)
01/18 06:55:43 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 201/521, train_loss: 0.882(0.701), train_acc: 68.750(75.586)
01/18 06:55:56 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 301/521, train_loss: 0.584(0.699), train_acc: 81.250(75.623)
01/18 06:56:09 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 401/521, train_loss: 0.647(0.702), train_acc: 83.333(75.499)
01/18 06:56:21 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 501/521, train_loss: 0.595(0.706), train_acc: 75.000(75.397)
01/18 06:56:24 PM [Supernet Training] lr: 0.02467 epoch: 045/600, step: 521/521, train_loss: 0.628(0.705), train_acc: 76.250(75.472)
01/18 06:56:24 PM [Supernet Training] epoch: 045, train_loss: 0.705, train_acc: 75.472
01/18 06:56:28 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 06:56:28 PM [Supernet Validation] epoch: 045, val_loss: 0.762, val_acc: 74.120, best_acc: 74.120
01/18 06:56:28 PM 

01/18 06:56:28 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 001/521, train_loss: 0.616(0.616), train_acc: 80.208(80.208)
01/18 06:56:41 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 101/521, train_loss: 0.448(0.690), train_acc: 84.375(76.372)
01/18 06:56:54 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 201/521, train_loss: 0.492(0.694), train_acc: 83.333(75.959)
01/18 06:57:06 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 301/521, train_loss: 0.653(0.694), train_acc: 76.042(75.893)
01/18 06:57:19 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 401/521, train_loss: 0.938(0.688), train_acc: 69.792(76.062)
01/18 06:57:32 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 501/521, train_loss: 0.784(0.691), train_acc: 73.958(76.048)
01/18 06:57:35 PM [Supernet Training] lr: 0.02465 epoch: 046/600, step: 521/521, train_loss: 0.520(0.690), train_acc: 77.500(76.070)
01/18 06:57:35 PM [Supernet Training] epoch: 046, train_loss: 0.690, train_acc: 76.070
01/18 06:57:38 PM [Supernet Validation] epoch: 046, val_loss: 0.787, val_acc: 73.910, best_acc: 74.120
01/18 06:57:38 PM 

01/18 06:57:39 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 001/521, train_loss: 0.747(0.747), train_acc: 72.917(72.917)
01/18 06:57:51 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 101/521, train_loss: 0.670(0.676), train_acc: 75.000(76.475)
01/18 06:58:04 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 201/521, train_loss: 0.582(0.683), train_acc: 77.083(76.275)
01/18 06:58:17 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 301/521, train_loss: 0.653(0.683), train_acc: 79.167(76.121)
01/18 06:58:30 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 401/521, train_loss: 0.891(0.681), train_acc: 65.625(76.143)
01/18 06:58:42 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 501/521, train_loss: 0.845(0.683), train_acc: 70.833(76.017)
01/18 06:58:45 PM [Supernet Training] lr: 0.02464 epoch: 047/600, step: 521/521, train_loss: 0.707(0.684), train_acc: 70.000(75.990)
01/18 06:58:45 PM [Supernet Training] epoch: 047, train_loss: 0.684, train_acc: 75.990
01/18 06:58:49 PM [Supernet Validation] epoch: 047, val_loss: 0.777, val_acc: 73.260, best_acc: 74.120
01/18 06:58:49 PM 

01/18 06:58:49 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 001/521, train_loss: 0.651(0.651), train_acc: 75.000(75.000)
01/18 06:59:02 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 101/521, train_loss: 0.616(0.677), train_acc: 76.042(76.795)
01/18 06:59:15 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 201/521, train_loss: 0.958(0.679), train_acc: 68.750(76.472)
01/18 06:59:27 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 301/521, train_loss: 0.522(0.670), train_acc: 82.292(76.668)
01/18 06:59:40 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 401/521, train_loss: 0.789(0.671), train_acc: 76.042(76.624)
01/18 06:59:53 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 501/521, train_loss: 0.718(0.672), train_acc: 72.917(76.570)
01/18 06:59:56 PM [Supernet Training] lr: 0.02462 epoch: 048/600, step: 521/521, train_loss: 0.704(0.673), train_acc: 78.750(76.538)
01/18 06:59:56 PM [Supernet Training] epoch: 048, train_loss: 0.673, train_acc: 76.538
01/18 06:59:59 PM [Supernet Validation] epoch: 048, val_loss: 0.750, val_acc: 73.960, best_acc: 74.120
01/18 06:59:59 PM 

01/18 07:00:00 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 001/521, train_loss: 0.466(0.466), train_acc: 84.375(84.375)
01/18 07:00:12 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 101/521, train_loss: 0.751(0.662), train_acc: 70.833(76.918)
01/18 07:00:25 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 201/521, train_loss: 0.774(0.669), train_acc: 78.125(76.689)
01/18 07:00:38 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 301/521, train_loss: 0.746(0.670), train_acc: 73.958(76.640)
01/18 07:00:51 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 401/521, train_loss: 0.518(0.665), train_acc: 82.292(76.834)
01/18 07:01:03 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 501/521, train_loss: 0.879(0.663), train_acc: 69.792(76.869)
01/18 07:01:06 PM [Supernet Training] lr: 0.02461 epoch: 049/600, step: 521/521, train_loss: 0.564(0.663), train_acc: 80.000(76.892)
01/18 07:01:06 PM [Supernet Training] epoch: 049, train_loss: 0.663, train_acc: 76.892
01/18 07:01:10 PM [Supernet Validation] epoch: 049, val_loss: 0.771, val_acc: 73.300, best_acc: 74.120
01/18 07:01:10 PM 

01/18 07:01:10 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 001/521, train_loss: 0.472(0.472), train_acc: 79.167(79.167)
01/18 07:01:23 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 101/521, train_loss: 0.745(0.639), train_acc: 69.792(77.186)
01/18 07:01:36 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 201/521, train_loss: 0.464(0.649), train_acc: 85.417(77.120)
01/18 07:01:48 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 301/521, train_loss: 0.550(0.649), train_acc: 82.292(77.301)
01/18 07:02:01 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 401/521, train_loss: 0.622(0.650), train_acc: 84.375(77.273)
01/18 07:02:14 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 501/521, train_loss: 0.626(0.656), train_acc: 76.042(77.075)
01/18 07:02:16 PM [Supernet Training] lr: 0.02459 epoch: 050/600, step: 521/521, train_loss: 0.434(0.656), train_acc: 87.500(77.034)
01/18 07:02:16 PM [Supernet Training] epoch: 050, train_loss: 0.656, train_acc: 77.034
01/18 07:02:20 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:02:20 PM [Supernet Validation] epoch: 050, val_loss: 0.777, val_acc: 74.300, best_acc: 74.300
01/18 07:02:20 PM 

01/18 07:02:20 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 001/521, train_loss: 0.689(0.689), train_acc: 76.042(76.042)
01/18 07:02:33 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 101/521, train_loss: 0.764(0.645), train_acc: 72.917(77.248)
01/18 07:02:46 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 201/521, train_loss: 0.859(0.645), train_acc: 71.875(77.446)
01/18 07:02:59 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 301/521, train_loss: 0.689(0.656), train_acc: 80.208(76.997)
01/18 07:03:12 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 401/521, train_loss: 0.756(0.655), train_acc: 71.875(77.050)
01/18 07:03:24 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 501/521, train_loss: 0.721(0.660), train_acc: 72.917(76.915)
01/18 07:03:27 PM [Supernet Training] lr: 0.02457 epoch: 051/600, step: 521/521, train_loss: 0.728(0.660), train_acc: 70.000(76.914)
01/18 07:03:27 PM [Supernet Training] epoch: 051, train_loss: 0.660, train_acc: 76.914
01/18 07:03:31 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:03:31 PM [Supernet Validation] epoch: 051, val_loss: 0.726, val_acc: 75.770, best_acc: 75.770
01/18 07:03:31 PM 

01/18 07:03:31 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 001/521, train_loss: 0.535(0.535), train_acc: 83.333(83.333)
01/18 07:03:44 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 101/521, train_loss: 0.550(0.647), train_acc: 81.250(77.362)
01/18 07:03:57 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 201/521, train_loss: 0.537(0.650), train_acc: 83.333(77.177)
01/18 07:04:09 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 301/521, train_loss: 0.618(0.648), train_acc: 80.208(77.364)
01/18 07:04:22 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 401/521, train_loss: 0.686(0.649), train_acc: 81.250(77.361)
01/18 07:04:35 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 501/521, train_loss: 0.651(0.645), train_acc: 71.875(77.489)
01/18 07:04:38 PM [Supernet Training] lr: 0.02456 epoch: 052/600, step: 521/521, train_loss: 0.572(0.644), train_acc: 78.750(77.492)
01/18 07:04:38 PM [Supernet Training] epoch: 052, train_loss: 0.644, train_acc: 77.492
01/18 07:04:41 PM [Supernet Validation] epoch: 052, val_loss: 0.739, val_acc: 75.280, best_acc: 75.770
01/18 07:04:41 PM 

01/18 07:04:42 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 001/521, train_loss: 0.522(0.522), train_acc: 84.375(84.375)
01/18 07:04:54 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 101/521, train_loss: 0.660(0.640), train_acc: 71.875(77.486)
01/18 07:05:07 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 201/521, train_loss: 0.569(0.638), train_acc: 81.250(77.477)
01/18 07:05:20 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 301/521, train_loss: 0.581(0.638), train_acc: 81.250(77.613)
01/18 07:05:33 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 401/521, train_loss: 0.440(0.640), train_acc: 85.417(77.566)
01/18 07:05:46 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 501/521, train_loss: 0.517(0.642), train_acc: 81.250(77.478)
01/18 07:05:48 PM [Supernet Training] lr: 0.02454 epoch: 053/600, step: 521/521, train_loss: 0.790(0.642), train_acc: 72.500(77.472)
01/18 07:05:48 PM [Supernet Training] epoch: 053, train_loss: 0.642, train_acc: 77.472
01/18 07:05:52 PM [Supernet Validation] epoch: 053, val_loss: 0.730, val_acc: 75.310, best_acc: 75.770
01/18 07:05:52 PM 

01/18 07:05:52 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 001/521, train_loss: 0.577(0.577), train_acc: 81.250(81.250)
01/18 07:06:05 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 101/521, train_loss: 0.716(0.617), train_acc: 75.000(78.764)
01/18 07:06:18 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 201/521, train_loss: 0.812(0.629), train_acc: 71.875(78.249)
01/18 07:06:30 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 301/521, train_loss: 0.622(0.636), train_acc: 76.042(77.838)
01/18 07:06:43 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 401/521, train_loss: 0.452(0.636), train_acc: 85.417(77.735)
01/18 07:06:56 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 501/521, train_loss: 0.619(0.633), train_acc: 77.083(77.859)
01/18 07:06:59 PM [Supernet Training] lr: 0.02452 epoch: 054/600, step: 521/521, train_loss: 0.839(0.633), train_acc: 68.750(77.820)
01/18 07:06:59 PM [Supernet Training] epoch: 054, train_loss: 0.633, train_acc: 77.820
01/18 07:07:02 PM [Supernet Validation] epoch: 054, val_loss: 0.765, val_acc: 74.350, best_acc: 75.770
01/18 07:07:02 PM 

01/18 07:07:03 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 001/521, train_loss: 0.781(0.781), train_acc: 76.042(76.042)
01/18 07:07:15 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 101/521, train_loss: 0.498(0.613), train_acc: 82.292(78.362)
01/18 07:07:28 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 201/521, train_loss: 0.647(0.620), train_acc: 79.167(78.146)
01/18 07:07:41 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 301/521, train_loss: 0.473(0.615), train_acc: 81.250(78.336)
01/18 07:07:54 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 401/521, train_loss: 0.617(0.621), train_acc: 75.000(78.128)
01/18 07:08:06 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 501/521, train_loss: 0.638(0.626), train_acc: 76.042(77.919)
01/18 07:08:09 PM [Supernet Training] lr: 0.02450 epoch: 055/600, step: 521/521, train_loss: 0.582(0.626), train_acc: 80.000(77.892)
01/18 07:08:09 PM [Supernet Training] epoch: 055, train_loss: 0.626, train_acc: 77.892
01/18 07:08:12 PM [Supernet Validation] epoch: 055, val_loss: 0.740, val_acc: 74.760, best_acc: 75.770
01/18 07:08:12 PM 

01/18 07:08:13 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 001/521, train_loss: 0.657(0.657), train_acc: 73.958(73.958)
01/18 07:08:26 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 101/521, train_loss: 0.572(0.617), train_acc: 77.083(78.094)
01/18 07:08:38 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 201/521, train_loss: 0.883(0.629), train_acc: 71.875(77.368)
01/18 07:08:51 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 301/521, train_loss: 0.538(0.617), train_acc: 80.208(78.066)
01/18 07:09:04 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 401/521, train_loss: 0.581(0.616), train_acc: 82.292(78.187)
01/18 07:09:17 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 501/521, train_loss: 0.726(0.618), train_acc: 73.958(78.250)
01/18 07:09:19 PM [Supernet Training] lr: 0.02449 epoch: 056/600, step: 521/521, train_loss: 0.878(0.618), train_acc: 70.000(78.226)
01/18 07:09:20 PM [Supernet Training] epoch: 056, train_loss: 0.618, train_acc: 78.226
01/18 07:09:23 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:09:23 PM [Supernet Validation] epoch: 056, val_loss: 0.705, val_acc: 76.180, best_acc: 76.180
01/18 07:09:23 PM 

01/18 07:09:24 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 001/521, train_loss: 0.564(0.564), train_acc: 82.292(82.292)
01/18 07:09:36 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 101/521, train_loss: 0.668(0.611), train_acc: 76.042(78.703)
01/18 07:09:49 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 201/521, train_loss: 0.810(0.614), train_acc: 70.833(78.431)
01/18 07:10:02 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 301/521, train_loss: 0.652(0.615), train_acc: 76.042(78.568)
01/18 07:10:15 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 401/521, train_loss: 0.607(0.615), train_acc: 76.042(78.403)
01/18 07:10:27 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 501/521, train_loss: 0.817(0.614), train_acc: 75.000(78.497)
01/18 07:10:30 PM [Supernet Training] lr: 0.02447 epoch: 057/600, step: 521/521, train_loss: 0.703(0.613), train_acc: 71.250(78.536)
01/18 07:10:30 PM [Supernet Training] epoch: 057, train_loss: 0.613, train_acc: 78.536
01/18 07:10:34 PM [Supernet Validation] epoch: 057, val_loss: 0.718, val_acc: 75.520, best_acc: 76.180
01/18 07:10:34 PM 

01/18 07:10:34 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 001/521, train_loss: 0.477(0.477), train_acc: 83.333(83.333)
01/18 07:10:47 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 101/521, train_loss: 0.852(0.606), train_acc: 72.917(79.012)
01/18 07:11:00 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 201/521, train_loss: 0.710(0.614), train_acc: 71.875(78.566)
01/18 07:11:12 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 301/521, train_loss: 0.525(0.609), train_acc: 79.167(78.720)
01/18 07:11:25 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 401/521, train_loss: 0.702(0.604), train_acc: 71.875(78.837)
01/18 07:11:38 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 501/521, train_loss: 0.616(0.603), train_acc: 78.125(78.859)
01/18 07:11:40 PM [Supernet Training] lr: 0.02445 epoch: 058/600, step: 521/521, train_loss: 0.585(0.602), train_acc: 77.500(78.888)
01/18 07:11:41 PM [Supernet Training] epoch: 058, train_loss: 0.602, train_acc: 78.888
01/18 07:11:44 PM [Supernet Validation] epoch: 058, val_loss: 0.688, val_acc: 76.170, best_acc: 76.180
01/18 07:11:44 PM 

01/18 07:11:45 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 001/521, train_loss: 0.484(0.484), train_acc: 83.333(83.333)
01/18 07:11:57 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 101/521, train_loss: 0.743(0.593), train_acc: 75.000(79.064)
01/18 07:12:10 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 201/521, train_loss: 0.546(0.599), train_acc: 81.250(79.011)
01/18 07:12:23 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 301/521, train_loss: 0.677(0.605), train_acc: 80.208(78.810)
01/18 07:12:36 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 401/521, train_loss: 0.477(0.598), train_acc: 80.208(79.110)
01/18 07:12:49 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 501/521, train_loss: 0.556(0.600), train_acc: 79.167(79.059)
01/18 07:12:51 PM [Supernet Training] lr: 0.02443 epoch: 059/600, step: 521/521, train_loss: 0.587(0.600), train_acc: 81.250(79.070)
01/18 07:12:51 PM [Supernet Training] epoch: 059, train_loss: 0.600, train_acc: 79.070
01/18 07:12:55 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:12:55 PM [Supernet Validation] epoch: 059, val_loss: 0.691, val_acc: 76.600, best_acc: 76.600
01/18 07:12:55 PM 

01/18 07:12:55 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 001/521, train_loss: 0.423(0.423), train_acc: 86.458(86.458)
01/18 07:13:08 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 101/521, train_loss: 0.616(0.585), train_acc: 75.000(79.466)
01/18 07:13:21 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 201/521, train_loss: 0.518(0.589), train_acc: 84.375(79.068)
01/18 07:13:34 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 301/521, train_loss: 0.526(0.592), train_acc: 81.250(79.049)
01/18 07:13:47 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 401/521, train_loss: 0.655(0.597), train_acc: 80.208(78.972)
01/18 07:13:59 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 501/521, train_loss: 0.550(0.596), train_acc: 82.292(79.069)
01/18 07:14:02 PM [Supernet Training] lr: 0.02441 epoch: 060/600, step: 521/521, train_loss: 0.504(0.597), train_acc: 83.750(79.048)
01/18 07:14:02 PM [Supernet Training] epoch: 060, train_loss: 0.597, train_acc: 79.048
01/18 07:14:05 PM [Supernet Validation] epoch: 060, val_loss: 0.694, val_acc: 76.390, best_acc: 76.600
01/18 07:14:05 PM 

01/18 07:14:06 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 001/521, train_loss: 0.657(0.657), train_acc: 80.208(80.208)
01/18 07:14:19 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 101/521, train_loss: 0.411(0.567), train_acc: 83.333(80.064)
01/18 07:14:31 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 201/521, train_loss: 0.584(0.578), train_acc: 81.250(79.628)
01/18 07:14:44 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 301/521, train_loss: 0.720(0.589), train_acc: 73.958(79.174)
01/18 07:14:57 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 401/521, train_loss: 0.665(0.590), train_acc: 73.958(79.128)
01/18 07:15:10 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 501/521, train_loss: 0.564(0.591), train_acc: 82.292(79.142)
01/18 07:15:12 PM [Supernet Training] lr: 0.02439 epoch: 061/600, step: 521/521, train_loss: 0.642(0.592), train_acc: 77.500(79.146)
01/18 07:15:12 PM [Supernet Training] epoch: 061, train_loss: 0.592, train_acc: 79.146
01/18 07:15:16 PM [Supernet Validation] epoch: 061, val_loss: 0.681, val_acc: 76.310, best_acc: 76.600
01/18 07:15:16 PM 

01/18 07:15:16 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 001/521, train_loss: 0.536(0.536), train_acc: 82.292(82.292)
01/18 07:15:29 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 101/521, train_loss: 0.538(0.583), train_acc: 83.333(80.074)
01/18 07:15:42 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 201/521, train_loss: 0.544(0.584), train_acc: 81.250(79.737)
01/18 07:15:55 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 301/521, train_loss: 0.497(0.580), train_acc: 80.208(79.893)
01/18 07:16:08 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 401/521, train_loss: 0.529(0.584), train_acc: 84.375(79.715)
01/18 07:16:20 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 501/521, train_loss: 0.602(0.589), train_acc: 79.167(79.512)
01/18 07:16:23 PM [Supernet Training] lr: 0.02437 epoch: 062/600, step: 521/521, train_loss: 0.727(0.587), train_acc: 73.750(79.556)
01/18 07:16:23 PM [Supernet Training] epoch: 062, train_loss: 0.587, train_acc: 79.556
01/18 07:16:27 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:16:27 PM [Supernet Validation] epoch: 062, val_loss: 0.672, val_acc: 77.200, best_acc: 77.200
01/18 07:16:27 PM 

01/18 07:16:27 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 001/521, train_loss: 0.501(0.501), train_acc: 79.167(79.167)
01/18 07:16:40 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 101/521, train_loss: 0.688(0.579), train_acc: 79.167(79.785)
01/18 07:16:53 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 201/521, train_loss: 0.798(0.570), train_acc: 70.833(79.980)
01/18 07:17:06 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 301/521, train_loss: 0.551(0.577), train_acc: 79.167(79.866)
01/18 07:17:18 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 401/521, train_loss: 0.576(0.577), train_acc: 80.208(79.865)
01/18 07:17:31 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 501/521, train_loss: 0.565(0.580), train_acc: 77.083(79.641)
01/18 07:17:34 PM [Supernet Training] lr: 0.02435 epoch: 063/600, step: 521/521, train_loss: 0.477(0.580), train_acc: 83.750(79.674)
01/18 07:17:34 PM [Supernet Training] epoch: 063, train_loss: 0.580, train_acc: 79.674
01/18 07:17:37 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:17:37 PM [Supernet Validation] epoch: 063, val_loss: 0.659, val_acc: 77.730, best_acc: 77.730
01/18 07:17:37 PM 

01/18 07:17:38 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 001/521, train_loss: 0.482(0.482), train_acc: 83.333(83.333)
01/18 07:17:51 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 101/521, train_loss: 0.555(0.557), train_acc: 81.250(80.208)
01/18 07:18:03 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 201/521, train_loss: 0.567(0.564), train_acc: 84.375(80.063)
01/18 07:18:16 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 301/521, train_loss: 0.660(0.570), train_acc: 75.000(79.824)
01/18 07:18:29 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 401/521, train_loss: 0.443(0.566), train_acc: 84.375(80.021)
01/18 07:18:42 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 501/521, train_loss: 0.530(0.567), train_acc: 82.292(80.063)
01/18 07:18:44 PM [Supernet Training] lr: 0.02433 epoch: 064/600, step: 521/521, train_loss: 0.448(0.568), train_acc: 88.750(80.052)
01/18 07:18:44 PM [Supernet Training] epoch: 064, train_loss: 0.568, train_acc: 80.052
01/18 07:18:48 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:18:48 PM [Supernet Validation] epoch: 064, val_loss: 0.634, val_acc: 78.500, best_acc: 78.500
01/18 07:18:48 PM 

01/18 07:18:48 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 001/521, train_loss: 0.681(0.681), train_acc: 76.042(76.042)
01/18 07:19:01 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 101/521, train_loss: 0.521(0.559), train_acc: 85.417(80.363)
01/18 07:19:14 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 201/521, train_loss: 0.616(0.560), train_acc: 77.083(80.141)
01/18 07:19:27 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 301/521, train_loss: 0.556(0.560), train_acc: 82.292(80.118)
01/18 07:19:40 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 401/521, train_loss: 0.567(0.557), train_acc: 80.208(80.395)
01/18 07:19:52 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 501/521, train_loss: 0.659(0.567), train_acc: 73.958(80.117)
01/18 07:19:55 PM [Supernet Training] lr: 0.02430 epoch: 065/600, step: 521/521, train_loss: 0.510(0.566), train_acc: 85.000(80.140)
01/18 07:19:55 PM [Supernet Training] epoch: 065, train_loss: 0.566, train_acc: 80.140
01/18 07:19:59 PM [Supernet Validation] epoch: 065, val_loss: 0.671, val_acc: 77.600, best_acc: 78.500
01/18 07:19:59 PM 

01/18 07:19:59 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 001/521, train_loss: 0.550(0.550), train_acc: 78.125(78.125)
01/18 07:20:12 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 101/521, train_loss: 0.707(0.569), train_acc: 79.167(79.858)
01/18 07:20:25 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 201/521, train_loss: 0.446(0.582), train_acc: 83.333(79.436)
01/18 07:20:38 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 301/521, train_loss: 0.454(0.569), train_acc: 84.375(79.786)
01/18 07:20:50 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 401/521, train_loss: 0.632(0.569), train_acc: 78.125(79.847)
01/18 07:21:03 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 501/521, train_loss: 0.539(0.564), train_acc: 77.083(80.069)
01/18 07:21:06 PM [Supernet Training] lr: 0.02428 epoch: 066/600, step: 521/521, train_loss: 0.551(0.564), train_acc: 81.250(80.108)
01/18 07:21:06 PM [Supernet Training] epoch: 066, train_loss: 0.564, train_acc: 80.108
01/18 07:21:09 PM [Supernet Validation] epoch: 066, val_loss: 0.669, val_acc: 77.190, best_acc: 78.500
01/18 07:21:09 PM 

01/18 07:21:10 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 001/521, train_loss: 0.603(0.603), train_acc: 81.250(81.250)
01/18 07:21:23 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 101/521, train_loss: 0.448(0.539), train_acc: 84.375(81.415)
01/18 07:21:35 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 201/521, train_loss: 0.567(0.554), train_acc: 78.125(80.685)
01/18 07:21:48 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 301/521, train_loss: 0.400(0.558), train_acc: 87.500(80.523)
01/18 07:22:01 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 401/521, train_loss: 0.648(0.557), train_acc: 75.000(80.608)
01/18 07:22:14 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 501/521, train_loss: 0.614(0.555), train_acc: 80.208(80.641)
01/18 07:22:16 PM [Supernet Training] lr: 0.02426 epoch: 067/600, step: 521/521, train_loss: 0.557(0.556), train_acc: 77.500(80.634)
01/18 07:22:16 PM [Supernet Training] epoch: 067, train_loss: 0.556, train_acc: 80.634
01/18 07:22:20 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:22:20 PM [Supernet Validation] epoch: 067, val_loss: 0.634, val_acc: 78.950, best_acc: 78.950
01/18 07:22:20 PM 

01/18 07:22:20 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 001/521, train_loss: 0.671(0.671), train_acc: 69.792(69.792)
01/18 07:22:33 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 101/521, train_loss: 0.485(0.548), train_acc: 85.417(80.941)
01/18 07:22:46 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 201/521, train_loss: 0.550(0.555), train_acc: 78.125(80.514)
01/18 07:22:59 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 301/521, train_loss: 0.373(0.553), train_acc: 89.583(80.696)
01/18 07:23:11 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 401/521, train_loss: 0.385(0.552), train_acc: 87.500(80.801)
01/18 07:23:24 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 501/521, train_loss: 0.473(0.552), train_acc: 83.333(80.749)
01/18 07:23:27 PM [Supernet Training] lr: 0.02424 epoch: 068/600, step: 521/521, train_loss: 0.526(0.552), train_acc: 81.250(80.738)
01/18 07:23:27 PM [Supernet Training] epoch: 068, train_loss: 0.552, train_acc: 80.738
01/18 07:23:30 PM [Supernet Validation] epoch: 068, val_loss: 0.655, val_acc: 78.230, best_acc: 78.950
01/18 07:23:30 PM 

01/18 07:23:31 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 001/521, train_loss: 0.509(0.509), train_acc: 79.167(79.167)
01/18 07:23:44 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 101/521, train_loss: 0.429(0.539), train_acc: 80.208(80.930)
01/18 07:23:56 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 201/521, train_loss: 0.659(0.535), train_acc: 77.083(81.172)
01/18 07:24:09 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 301/521, train_loss: 0.617(0.546), train_acc: 79.167(80.866)
01/18 07:24:22 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 401/521, train_loss: 0.592(0.549), train_acc: 80.208(80.684)
01/18 07:24:35 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 501/521, train_loss: 0.408(0.550), train_acc: 83.333(80.624)
01/18 07:24:37 PM [Supernet Training] lr: 0.02422 epoch: 069/600, step: 521/521, train_loss: 0.619(0.550), train_acc: 78.750(80.642)
01/18 07:24:37 PM [Supernet Training] epoch: 069, train_loss: 0.550, train_acc: 80.642
01/18 07:24:41 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:24:41 PM [Supernet Validation] epoch: 069, val_loss: 0.621, val_acc: 79.330, best_acc: 79.330
01/18 07:24:41 PM 

01/18 07:24:41 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 001/521, train_loss: 0.576(0.576), train_acc: 78.125(78.125)
01/18 07:24:54 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 101/521, train_loss: 0.762(0.551), train_acc: 71.875(80.177)
01/18 07:25:07 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 201/521, train_loss: 0.467(0.540), train_acc: 84.375(80.908)
01/18 07:25:20 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 301/521, train_loss: 0.481(0.536), train_acc: 85.417(81.250)
01/18 07:25:33 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 401/521, train_loss: 0.647(0.536), train_acc: 76.042(81.250)
01/18 07:25:46 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 501/521, train_loss: 0.739(0.540), train_acc: 72.917(81.086)
01/18 07:25:48 PM [Supernet Training] lr: 0.02419 epoch: 070/600, step: 521/521, train_loss: 0.442(0.540), train_acc: 87.500(81.056)
01/18 07:25:48 PM [Supernet Training] epoch: 070, train_loss: 0.540, train_acc: 81.056
01/18 07:25:52 PM [Supernet Validation] epoch: 070, val_loss: 0.660, val_acc: 77.660, best_acc: 79.330
01/18 07:25:52 PM 

01/18 07:25:52 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 001/521, train_loss: 0.648(0.648), train_acc: 73.958(73.958)
01/18 07:26:05 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 101/521, train_loss: 0.585(0.536), train_acc: 82.292(81.116)
01/18 07:26:18 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 201/521, train_loss: 0.545(0.540), train_acc: 81.250(81.131)
01/18 07:26:30 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 301/521, train_loss: 0.421(0.535), train_acc: 85.417(81.385)
01/18 07:26:43 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 401/521, train_loss: 0.436(0.538), train_acc: 86.458(81.198)
01/18 07:26:56 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 501/521, train_loss: 0.435(0.538), train_acc: 86.458(81.146)
01/18 07:26:59 PM [Supernet Training] lr: 0.02417 epoch: 071/600, step: 521/521, train_loss: 0.349(0.539), train_acc: 88.750(81.122)
01/18 07:26:59 PM [Supernet Training] epoch: 071, train_loss: 0.539, train_acc: 81.122
01/18 07:27:02 PM [Supernet Validation] epoch: 071, val_loss: 0.648, val_acc: 77.820, best_acc: 79.330
01/18 07:27:02 PM 

01/18 07:27:03 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 001/521, train_loss: 0.491(0.491), train_acc: 84.375(84.375)
01/18 07:27:15 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 101/521, train_loss: 0.642(0.528), train_acc: 76.042(81.415)
01/18 07:27:28 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 201/521, train_loss: 0.565(0.542), train_acc: 78.125(81.001)
01/18 07:27:41 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 301/521, train_loss: 0.558(0.537), train_acc: 80.208(81.240)
01/18 07:27:54 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 401/521, train_loss: 0.422(0.536), train_acc: 86.458(81.346)
01/18 07:28:07 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 501/521, train_loss: 0.542(0.533), train_acc: 81.250(81.381)
01/18 07:28:09 PM [Supernet Training] lr: 0.02415 epoch: 072/600, step: 521/521, train_loss: 0.575(0.533), train_acc: 76.250(81.454)
01/18 07:28:09 PM [Supernet Training] epoch: 072, train_loss: 0.533, train_acc: 81.454
01/18 07:28:13 PM [Supernet Validation] epoch: 072, val_loss: 0.625, val_acc: 79.100, best_acc: 79.330
01/18 07:28:13 PM 

01/18 07:28:13 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 001/521, train_loss: 0.552(0.552), train_acc: 82.292(82.292)
01/18 07:28:26 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 101/521, train_loss: 0.465(0.541), train_acc: 84.375(81.033)
01/18 07:28:39 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 201/521, train_loss: 0.458(0.535), train_acc: 82.292(81.198)
01/18 07:28:52 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 301/521, train_loss: 0.691(0.530), train_acc: 72.917(81.378)
01/18 07:29:04 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 401/521, train_loss: 0.558(0.535), train_acc: 81.250(81.229)
01/18 07:29:17 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 501/521, train_loss: 0.731(0.532), train_acc: 70.833(81.339)
01/18 07:29:20 PM [Supernet Training] lr: 0.02412 epoch: 073/600, step: 521/521, train_loss: 0.690(0.533), train_acc: 78.750(81.286)
01/18 07:29:20 PM [Supernet Training] epoch: 073, train_loss: 0.533, train_acc: 81.286
01/18 07:29:23 PM [Supernet Validation] epoch: 073, val_loss: 0.628, val_acc: 79.170, best_acc: 79.330
01/18 07:29:23 PM 

01/18 07:29:24 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 001/521, train_loss: 0.589(0.589), train_acc: 76.042(76.042)
01/18 07:29:36 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 101/521, train_loss: 0.455(0.536), train_acc: 81.250(81.002)
01/18 07:29:49 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 201/521, train_loss: 0.702(0.524), train_acc: 82.292(81.711)
01/18 07:30:02 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 301/521, train_loss: 0.531(0.530), train_acc: 82.292(81.593)
01/18 07:30:15 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 401/521, train_loss: 0.611(0.528), train_acc: 82.292(81.710)
01/18 07:30:27 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 501/521, train_loss: 0.479(0.526), train_acc: 85.417(81.685)
01/18 07:30:30 PM [Supernet Training] lr: 0.02410 epoch: 074/600, step: 521/521, train_loss: 0.592(0.526), train_acc: 78.750(81.716)
01/18 07:30:30 PM [Supernet Training] epoch: 074, train_loss: 0.526, train_acc: 81.716
01/18 07:30:34 PM [Supernet Validation] epoch: 074, val_loss: 0.633, val_acc: 79.320, best_acc: 79.330
01/18 07:30:34 PM 

01/18 07:30:34 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 001/521, train_loss: 0.629(0.629), train_acc: 78.125(78.125)
01/18 07:30:47 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 101/521, train_loss: 0.504(0.519), train_acc: 86.458(81.745)
01/18 07:30:59 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 201/521, train_loss: 0.478(0.524), train_acc: 81.250(81.680)
01/18 07:31:12 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 301/521, train_loss: 0.445(0.525), train_acc: 87.500(81.658)
01/18 07:31:25 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 401/521, train_loss: 0.571(0.524), train_acc: 79.167(81.718)
01/18 07:31:38 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 501/521, train_loss: 0.521(0.525), train_acc: 84.375(81.685)
01/18 07:31:41 PM [Supernet Training] lr: 0.02407 epoch: 075/600, step: 521/521, train_loss: 0.467(0.525), train_acc: 85.000(81.692)
01/18 07:31:41 PM [Supernet Training] epoch: 075, train_loss: 0.525, train_acc: 81.692
01/18 07:31:44 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:31:44 PM [Supernet Validation] epoch: 075, val_loss: 0.623, val_acc: 79.350, best_acc: 79.350
01/18 07:31:44 PM 

01/18 07:31:45 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 001/521, train_loss: 0.387(0.387), train_acc: 89.583(89.583)
01/18 07:31:57 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 101/521, train_loss: 0.521(0.506), train_acc: 78.125(81.817)
01/18 07:32:10 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 201/521, train_loss: 0.537(0.513), train_acc: 80.208(81.773)
01/18 07:32:23 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 301/521, train_loss: 0.529(0.512), train_acc: 80.208(81.970)
01/18 07:32:36 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 401/521, train_loss: 0.498(0.514), train_acc: 83.333(81.847)
01/18 07:32:49 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 501/521, train_loss: 0.383(0.515), train_acc: 88.542(81.959)
01/18 07:32:51 PM [Supernet Training] lr: 0.02405 epoch: 076/600, step: 521/521, train_loss: 0.562(0.515), train_acc: 75.000(81.924)
01/18 07:32:51 PM [Supernet Training] epoch: 076, train_loss: 0.515, train_acc: 81.924
01/18 07:32:55 PM [Supernet Validation] epoch: 076, val_loss: 0.616, val_acc: 79.250, best_acc: 79.350
01/18 07:32:55 PM 

01/18 07:32:55 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 001/521, train_loss: 0.489(0.489), train_acc: 85.417(85.417)
01/18 07:33:08 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 101/521, train_loss: 0.493(0.516), train_acc: 81.250(81.982)
01/18 07:33:21 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 201/521, train_loss: 0.651(0.517), train_acc: 77.083(81.742)
01/18 07:33:34 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 301/521, train_loss: 0.446(0.517), train_acc: 84.375(81.845)
01/18 07:33:46 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 401/521, train_loss: 0.485(0.515), train_acc: 82.292(81.941)
01/18 07:33:59 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 501/521, train_loss: 0.341(0.514), train_acc: 89.583(81.919)
01/18 07:34:02 PM [Supernet Training] lr: 0.02402 epoch: 077/600, step: 521/521, train_loss: 0.579(0.515), train_acc: 80.000(81.872)
01/18 07:34:02 PM [Supernet Training] epoch: 077, train_loss: 0.515, train_acc: 81.872
01/18 07:34:05 PM [Supernet Validation] epoch: 077, val_loss: 0.610, val_acc: 79.220, best_acc: 79.350
01/18 07:34:05 PM 

01/18 07:34:06 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 001/521, train_loss: 0.452(0.452), train_acc: 83.333(83.333)
01/18 07:34:19 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 101/521, train_loss: 0.445(0.494), train_acc: 85.417(82.653)
01/18 07:34:31 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 201/521, train_loss: 0.666(0.501), train_acc: 76.042(82.292)
01/18 07:34:44 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 301/521, train_loss: 0.601(0.507), train_acc: 79.167(82.267)
01/18 07:34:57 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 401/521, train_loss: 0.429(0.507), train_acc: 84.375(82.349)
01/18 07:35:09 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 501/521, train_loss: 0.464(0.512), train_acc: 83.333(82.109)
01/18 07:35:12 PM [Supernet Training] lr: 0.02400 epoch: 078/600, step: 521/521, train_loss: 0.521(0.512), train_acc: 81.250(82.138)
01/18 07:35:12 PM [Supernet Training] epoch: 078, train_loss: 0.512, train_acc: 82.138
01/18 07:35:16 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:35:16 PM [Supernet Validation] epoch: 078, val_loss: 0.610, val_acc: 79.590, best_acc: 79.590
01/18 07:35:16 PM 

01/18 07:35:16 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 001/521, train_loss: 0.515(0.515), train_acc: 82.292(82.292)
01/18 07:35:29 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 101/521, train_loss: 0.459(0.514), train_acc: 83.333(82.065)
01/18 07:35:42 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 201/521, train_loss: 0.670(0.506), train_acc: 71.875(82.338)
01/18 07:35:55 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 301/521, train_loss: 0.423(0.506), train_acc: 82.292(82.288)
01/18 07:36:07 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 401/521, train_loss: 0.428(0.509), train_acc: 84.375(82.232)
01/18 07:36:20 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 501/521, train_loss: 0.488(0.505), train_acc: 84.375(82.342)
01/18 07:36:23 PM [Supernet Training] lr: 0.02397 epoch: 079/600, step: 521/521, train_loss: 0.295(0.506), train_acc: 90.000(82.344)
01/18 07:36:23 PM [Supernet Training] epoch: 079, train_loss: 0.506, train_acc: 82.344
01/18 07:36:27 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:36:27 PM [Supernet Validation] epoch: 079, val_loss: 0.582, val_acc: 80.270, best_acc: 80.270
01/18 07:36:27 PM 

01/18 07:36:27 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 001/521, train_loss: 0.703(0.703), train_acc: 70.833(70.833)
01/18 07:36:40 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 101/521, train_loss: 0.375(0.518), train_acc: 83.333(82.065)
01/18 07:36:52 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 201/521, train_loss: 0.621(0.507), train_acc: 79.167(82.411)
01/18 07:37:05 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 301/521, train_loss: 0.474(0.511), train_acc: 85.417(82.267)
01/18 07:37:18 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 401/521, train_loss: 0.455(0.506), train_acc: 82.292(82.486)
01/18 07:37:31 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 501/521, train_loss: 0.471(0.503), train_acc: 80.208(82.518)
01/18 07:37:33 PM [Supernet Training] lr: 0.02395 epoch: 080/600, step: 521/521, train_loss: 0.401(0.503), train_acc: 82.500(82.524)
01/18 07:37:33 PM [Supernet Training] epoch: 080, train_loss: 0.503, train_acc: 82.524
01/18 07:37:37 PM [Supernet Validation] epoch: 080, val_loss: 0.636, val_acc: 79.200, best_acc: 80.270
01/18 07:37:37 PM 

01/18 07:37:37 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 001/521, train_loss: 0.652(0.652), train_acc: 77.083(77.083)
01/18 07:37:50 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 101/521, train_loss: 0.590(0.496), train_acc: 80.208(83.034)
01/18 07:38:03 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 201/521, train_loss: 0.484(0.498), train_acc: 86.458(82.727)
01/18 07:38:16 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 301/521, train_loss: 0.395(0.506), train_acc: 83.333(82.472)
01/18 07:38:28 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 401/521, train_loss: 0.595(0.503), train_acc: 79.167(82.528)
01/18 07:38:41 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 501/521, train_loss: 0.301(0.499), train_acc: 89.583(82.641)
01/18 07:38:44 PM [Supernet Training] lr: 0.02392 epoch: 081/600, step: 521/521, train_loss: 0.413(0.499), train_acc: 81.250(82.632)
01/18 07:38:44 PM [Supernet Training] epoch: 081, train_loss: 0.499, train_acc: 82.632
01/18 07:38:47 PM [Supernet Validation] epoch: 081, val_loss: 0.583, val_acc: 80.190, best_acc: 80.270
01/18 07:38:47 PM 

01/18 07:38:48 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 001/521, train_loss: 0.516(0.516), train_acc: 80.208(80.208)
01/18 07:39:01 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 101/521, train_loss: 0.574(0.494), train_acc: 80.208(82.312)
01/18 07:39:13 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 201/521, train_loss: 0.476(0.490), train_acc: 82.292(82.675)
01/18 07:39:26 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 301/521, train_loss: 0.605(0.494), train_acc: 77.083(82.527)
01/18 07:39:39 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 401/521, train_loss: 0.516(0.496), train_acc: 86.458(82.429)
01/18 07:39:52 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 501/521, train_loss: 0.415(0.492), train_acc: 86.458(82.504)
01/18 07:39:54 PM [Supernet Training] lr: 0.02389 epoch: 082/600, step: 521/521, train_loss: 0.422(0.493), train_acc: 83.750(82.500)
01/18 07:39:54 PM [Supernet Training] epoch: 082, train_loss: 0.493, train_acc: 82.500
01/18 07:39:58 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:39:58 PM [Supernet Validation] epoch: 082, val_loss: 0.589, val_acc: 80.280, best_acc: 80.280
01/18 07:39:58 PM 

01/18 07:39:58 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 001/521, train_loss: 0.547(0.547), train_acc: 81.250(81.250)
01/18 07:40:11 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 101/521, train_loss: 0.441(0.487), train_acc: 90.625(82.921)
01/18 07:40:24 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 201/521, train_loss: 0.646(0.487), train_acc: 79.167(83.085)
01/18 07:40:37 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 301/521, train_loss: 0.501(0.486), train_acc: 82.292(83.050)
01/18 07:40:49 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 401/521, train_loss: 0.460(0.487), train_acc: 83.333(83.120)
01/18 07:41:02 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 501/521, train_loss: 0.627(0.488), train_acc: 77.083(82.940)
01/18 07:41:04 PM [Supernet Training] lr: 0.02387 epoch: 083/600, step: 521/521, train_loss: 0.557(0.488), train_acc: 85.000(82.952)
01/18 07:41:05 PM [Supernet Training] epoch: 083, train_loss: 0.488, train_acc: 82.952
01/18 07:41:08 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:41:08 PM [Supernet Validation] epoch: 083, val_loss: 0.586, val_acc: 80.730, best_acc: 80.730
01/18 07:41:08 PM 

01/18 07:41:09 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 001/521, train_loss: 0.443(0.443), train_acc: 82.292(82.292)
01/18 07:41:21 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 101/521, train_loss: 0.477(0.471), train_acc: 86.458(83.457)
01/18 07:41:34 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 201/521, train_loss: 0.504(0.473), train_acc: 85.417(83.427)
01/18 07:41:47 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 301/521, train_loss: 0.602(0.476), train_acc: 83.333(83.461)
01/18 07:42:00 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 401/521, train_loss: 0.372(0.477), train_acc: 85.417(83.313)
01/18 07:42:13 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 501/521, train_loss: 0.499(0.482), train_acc: 78.125(83.155)
01/18 07:42:15 PM [Supernet Training] lr: 0.02384 epoch: 084/600, step: 521/521, train_loss: 0.391(0.485), train_acc: 86.250(83.058)
01/18 07:42:15 PM [Supernet Training] epoch: 084, train_loss: 0.485, train_acc: 83.058
01/18 07:42:19 PM [Supernet Validation] epoch: 084, val_loss: 0.590, val_acc: 80.270, best_acc: 80.730
01/18 07:42:19 PM 

01/18 07:42:19 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 001/521, train_loss: 0.507(0.507), train_acc: 82.292(82.292)
01/18 07:42:32 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 101/521, train_loss: 0.344(0.479), train_acc: 87.500(83.323)
01/18 07:42:45 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 201/521, train_loss: 0.426(0.480), train_acc: 84.375(83.318)
01/18 07:42:58 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 301/521, train_loss: 0.528(0.485), train_acc: 83.333(83.101)
01/18 07:43:10 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 401/521, train_loss: 0.416(0.484), train_acc: 85.417(83.053)
01/18 07:43:23 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 501/521, train_loss: 0.416(0.482), train_acc: 86.458(83.142)
01/18 07:43:26 PM [Supernet Training] lr: 0.02381 epoch: 085/600, step: 521/521, train_loss: 0.532(0.482), train_acc: 86.250(83.130)
01/18 07:43:26 PM [Supernet Training] epoch: 085, train_loss: 0.482, train_acc: 83.130
01/18 07:43:29 PM [Supernet Validation] epoch: 085, val_loss: 0.589, val_acc: 80.380, best_acc: 80.730
01/18 07:43:29 PM 

01/18 07:43:30 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 001/521, train_loss: 0.692(0.692), train_acc: 80.208(80.208)
01/18 07:43:43 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 101/521, train_loss: 0.570(0.479), train_acc: 81.250(83.241)
01/18 07:43:55 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 201/521, train_loss: 0.541(0.479), train_acc: 82.292(83.416)
01/18 07:44:08 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 301/521, train_loss: 0.511(0.474), train_acc: 81.250(83.565)
01/18 07:44:21 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 401/521, train_loss: 0.621(0.479), train_acc: 77.083(83.284)
01/18 07:44:34 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 501/521, train_loss: 0.304(0.482), train_acc: 86.458(83.196)
01/18 07:44:36 PM [Supernet Training] lr: 0.02378 epoch: 086/600, step: 521/521, train_loss: 0.476(0.481), train_acc: 81.250(83.242)
01/18 07:44:36 PM [Supernet Training] epoch: 086, train_loss: 0.481, train_acc: 83.242
01/18 07:44:40 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:44:40 PM [Supernet Validation] epoch: 086, val_loss: 0.564, val_acc: 81.110, best_acc: 81.110
01/18 07:44:40 PM 

01/18 07:44:40 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 001/521, train_loss: 0.570(0.570), train_acc: 76.042(76.042)
01/18 07:44:53 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 101/521, train_loss: 0.515(0.468), train_acc: 84.375(83.550)
01/18 07:45:06 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 201/521, train_loss: 0.599(0.473), train_acc: 81.250(83.510)
01/18 07:45:18 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 301/521, train_loss: 0.352(0.477), train_acc: 88.542(83.358)
01/18 07:45:31 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 401/521, train_loss: 0.370(0.479), train_acc: 86.458(83.274)
01/18 07:45:44 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 501/521, train_loss: 0.574(0.479), train_acc: 85.417(83.269)
01/18 07:45:47 PM [Supernet Training] lr: 0.02375 epoch: 087/600, step: 521/521, train_loss: 0.599(0.479), train_acc: 81.250(83.268)
01/18 07:45:47 PM [Supernet Training] epoch: 087, train_loss: 0.479, train_acc: 83.268
01/18 07:45:50 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:45:50 PM [Supernet Validation] epoch: 087, val_loss: 0.579, val_acc: 81.260, best_acc: 81.260
01/18 07:45:50 PM 

01/18 07:45:51 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 001/521, train_loss: 0.587(0.587), train_acc: 76.042(76.042)
01/18 07:46:04 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 101/521, train_loss: 0.504(0.466), train_acc: 82.292(83.880)
01/18 07:46:16 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 201/521, train_loss: 0.401(0.472), train_acc: 85.417(83.582)
01/18 07:46:29 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 301/521, train_loss: 0.415(0.472), train_acc: 82.292(83.486)
01/18 07:46:42 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 401/521, train_loss: 0.371(0.471), train_acc: 86.458(83.629)
01/18 07:46:55 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 501/521, train_loss: 0.542(0.472), train_acc: 80.208(83.566)
01/18 07:46:57 PM [Supernet Training] lr: 0.02373 epoch: 088/600, step: 521/521, train_loss: 0.409(0.473), train_acc: 88.750(83.520)
01/18 07:46:57 PM [Supernet Training] epoch: 088, train_loss: 0.473, train_acc: 83.520
01/18 07:47:01 PM [Supernet Validation] epoch: 088, val_loss: 0.573, val_acc: 80.760, best_acc: 81.260
01/18 07:47:01 PM 

01/18 07:47:01 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 001/521, train_loss: 0.369(0.369), train_acc: 86.458(86.458)
01/18 07:47:14 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 101/521, train_loss: 0.450(0.466), train_acc: 83.333(83.942)
01/18 07:47:27 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 201/521, train_loss: 0.311(0.471), train_acc: 91.667(83.888)
01/18 07:47:40 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 301/521, train_loss: 0.419(0.474), train_acc: 86.458(83.690)
01/18 07:47:52 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 401/521, train_loss: 0.393(0.472), train_acc: 84.375(83.642)
01/18 07:48:05 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 501/521, train_loss: 0.466(0.470), train_acc: 88.542(83.633)
01/18 07:48:08 PM [Supernet Training] lr: 0.02370 epoch: 089/600, step: 521/521, train_loss: 0.372(0.470), train_acc: 86.250(83.600)
01/18 07:48:08 PM [Supernet Training] epoch: 089, train_loss: 0.470, train_acc: 83.600
01/18 07:48:11 PM [Supernet Validation] epoch: 089, val_loss: 0.602, val_acc: 80.120, best_acc: 81.260
01/18 07:48:11 PM 

01/18 07:48:12 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 001/521, train_loss: 0.526(0.526), train_acc: 79.167(79.167)
01/18 07:48:25 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 101/521, train_loss: 0.552(0.474), train_acc: 79.167(83.271)
01/18 07:48:37 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 201/521, train_loss: 0.468(0.464), train_acc: 84.375(83.831)
01/18 07:48:50 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 301/521, train_loss: 0.461(0.465), train_acc: 86.458(83.877)
01/18 07:49:03 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 401/521, train_loss: 0.376(0.468), train_acc: 89.583(83.778)
01/18 07:49:16 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 501/521, train_loss: 0.338(0.468), train_acc: 89.583(83.797)
01/18 07:49:18 PM [Supernet Training] lr: 0.02367 epoch: 090/600, step: 521/521, train_loss: 0.527(0.468), train_acc: 83.750(83.776)
01/18 07:49:18 PM [Supernet Training] epoch: 090, train_loss: 0.468, train_acc: 83.776
01/18 07:49:22 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 07:49:22 PM [Supernet Validation] epoch: 090, val_loss: 0.543, val_acc: 82.130, best_acc: 82.130
01/18 07:49:22 PM 

01/18 07:49:23 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 001/521, train_loss: 0.336(0.336), train_acc: 90.625(90.625)
01/18 07:49:35 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 101/521, train_loss: 0.500(0.448), train_acc: 85.417(84.334)
01/18 07:49:48 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 201/521, train_loss: 0.461(0.459), train_acc: 83.333(83.955)
01/18 07:50:01 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 301/521, train_loss: 0.528(0.461), train_acc: 82.292(83.818)
01/18 07:50:14 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 401/521, train_loss: 0.437(0.461), train_acc: 85.417(83.788)
01/18 07:50:26 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 501/521, train_loss: 0.372(0.463), train_acc: 87.500(83.737)
01/18 07:50:29 PM [Supernet Training] lr: 0.02364 epoch: 091/600, step: 521/521, train_loss: 0.578(0.462), train_acc: 76.250(83.770)
01/18 07:50:29 PM [Supernet Training] epoch: 091, train_loss: 0.462, train_acc: 83.770
01/18 07:50:33 PM [Supernet Validation] epoch: 091, val_loss: 0.564, val_acc: 81.290, best_acc: 82.130
01/18 07:50:33 PM 

01/18 07:50:33 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 001/521, train_loss: 0.420(0.420), train_acc: 82.292(82.292)
01/18 07:50:46 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 101/521, train_loss: 0.717(0.455), train_acc: 80.208(84.499)
01/18 07:50:59 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 201/521, train_loss: 0.464(0.463), train_acc: 84.375(84.168)
01/18 07:51:11 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 301/521, train_loss: 0.404(0.465), train_acc: 84.375(84.036)
01/18 07:51:24 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 401/521, train_loss: 0.579(0.460), train_acc: 80.208(84.128)
01/18 07:51:37 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 501/521, train_loss: 0.462(0.463), train_acc: 80.208(83.961)
01/18 07:51:39 PM [Supernet Training] lr: 0.02361 epoch: 092/600, step: 521/521, train_loss: 0.350(0.464), train_acc: 88.750(83.968)
01/18 07:51:40 PM [Supernet Training] epoch: 092, train_loss: 0.464, train_acc: 83.968
01/18 07:51:43 PM [Supernet Validation] epoch: 092, val_loss: 0.566, val_acc: 80.560, best_acc: 82.130
01/18 07:51:43 PM 

01/18 07:51:43 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 001/521, train_loss: 0.292(0.292), train_acc: 89.583(89.583)
01/18 07:51:56 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 101/521, train_loss: 0.396(0.454), train_acc: 87.500(84.179)
01/18 07:52:09 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 201/521, train_loss: 0.487(0.450), train_acc: 82.292(84.266)
01/18 07:52:22 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 301/521, train_loss: 0.312(0.455), train_acc: 87.500(84.091)
01/18 07:52:35 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 401/521, train_loss: 0.448(0.454), train_acc: 85.417(84.162)
01/18 07:52:47 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 501/521, train_loss: 0.460(0.454), train_acc: 83.333(84.207)
01/18 07:52:50 PM [Supernet Training] lr: 0.02358 epoch: 093/600, step: 521/521, train_loss: 0.397(0.454), train_acc: 86.250(84.202)
01/18 07:52:50 PM [Supernet Training] epoch: 093, train_loss: 0.454, train_acc: 84.202
01/18 07:52:54 PM [Supernet Validation] epoch: 093, val_loss: 0.541, val_acc: 81.870, best_acc: 82.130
01/18 07:52:54 PM 

01/18 07:52:54 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 001/521, train_loss: 0.363(0.363), train_acc: 88.542(88.542)
01/18 07:53:07 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 101/521, train_loss: 0.583(0.450), train_acc: 81.250(84.602)
01/18 07:53:20 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 201/521, train_loss: 0.517(0.453), train_acc: 83.333(84.209)
01/18 07:53:32 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 301/521, train_loss: 0.430(0.454), train_acc: 83.333(84.243)
01/18 07:53:45 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 401/521, train_loss: 0.380(0.452), train_acc: 85.417(84.326)
01/18 07:53:58 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 501/521, train_loss: 0.533(0.454), train_acc: 82.292(84.192)
01/18 07:54:01 PM [Supernet Training] lr: 0.02355 epoch: 094/600, step: 521/521, train_loss: 0.724(0.454), train_acc: 68.750(84.166)
01/18 07:54:01 PM [Supernet Training] epoch: 094, train_loss: 0.454, train_acc: 84.166
01/18 07:54:04 PM [Supernet Validation] epoch: 094, val_loss: 0.582, val_acc: 80.880, best_acc: 82.130
01/18 07:54:04 PM 

01/18 07:54:05 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 001/521, train_loss: 0.524(0.524), train_acc: 80.208(80.208)
01/18 07:54:17 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 101/521, train_loss: 0.425(0.454), train_acc: 85.417(84.220)
01/18 07:54:30 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 201/521, train_loss: 0.340(0.452), train_acc: 89.583(84.515)
01/18 07:54:43 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 301/521, train_loss: 0.331(0.447), train_acc: 87.500(84.468)
01/18 07:54:56 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 401/521, train_loss: 0.560(0.454), train_acc: 79.167(84.256)
01/18 07:55:08 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 501/521, train_loss: 0.674(0.453), train_acc: 78.125(84.225)
01/18 07:55:11 PM [Supernet Training] lr: 0.02352 epoch: 095/600, step: 521/521, train_loss: 0.580(0.453), train_acc: 76.250(84.232)
01/18 07:55:11 PM [Supernet Training] epoch: 095, train_loss: 0.453, train_acc: 84.232
01/18 07:55:15 PM [Supernet Validation] epoch: 095, val_loss: 0.548, val_acc: 81.570, best_acc: 82.130
01/18 07:55:15 PM 

01/18 07:55:15 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 001/521, train_loss: 0.358(0.358), train_acc: 88.542(88.542)
01/18 07:55:28 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 101/521, train_loss: 0.538(0.441), train_acc: 79.167(84.509)
01/18 07:55:41 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 201/521, train_loss: 0.437(0.445), train_acc: 88.542(84.510)
01/18 07:55:53 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 301/521, train_loss: 0.270(0.445), train_acc: 90.625(84.385)
01/18 07:56:06 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 401/521, train_loss: 0.447(0.449), train_acc: 82.292(84.276)
01/18 07:56:19 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 501/521, train_loss: 0.661(0.450), train_acc: 80.208(84.227)
01/18 07:56:21 PM [Supernet Training] lr: 0.02349 epoch: 096/600, step: 521/521, train_loss: 0.354(0.450), train_acc: 87.500(84.206)
01/18 07:56:21 PM [Supernet Training] epoch: 096, train_loss: 0.450, train_acc: 84.206
01/18 07:56:25 PM [Supernet Validation] epoch: 096, val_loss: 0.564, val_acc: 80.940, best_acc: 82.130
01/18 07:56:25 PM 

01/18 07:56:25 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 001/521, train_loss: 0.401(0.401), train_acc: 87.500(87.500)
01/18 07:56:38 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 101/521, train_loss: 0.455(0.431), train_acc: 82.292(84.891)
01/18 07:56:51 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 201/521, train_loss: 0.538(0.441), train_acc: 82.292(84.530)
01/18 07:57:04 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 301/521, train_loss: 0.441(0.441), train_acc: 82.292(84.496)
01/18 07:57:17 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 401/521, train_loss: 0.461(0.446), train_acc: 85.417(84.320)
01/18 07:57:29 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 501/521, train_loss: 0.572(0.445), train_acc: 80.208(84.402)
01/18 07:57:32 PM [Supernet Training] lr: 0.02345 epoch: 097/600, step: 521/521, train_loss: 0.437(0.444), train_acc: 87.500(84.430)
01/18 07:57:32 PM [Supernet Training] epoch: 097, train_loss: 0.444, train_acc: 84.430
01/18 07:57:36 PM [Supernet Validation] epoch: 097, val_loss: 0.548, val_acc: 81.560, best_acc: 82.130
01/18 07:57:36 PM 

01/18 07:57:36 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 001/521, train_loss: 0.475(0.475), train_acc: 81.250(81.250)
01/18 07:57:49 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 101/521, train_loss: 0.626(0.448), train_acc: 81.250(84.189)
01/18 07:58:02 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 201/521, train_loss: 0.369(0.446), train_acc: 86.458(84.396)
01/18 07:58:14 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 301/521, train_loss: 0.449(0.449), train_acc: 84.375(84.292)
01/18 07:58:27 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 401/521, train_loss: 0.495(0.446), train_acc: 80.208(84.354)
01/18 07:58:40 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 501/521, train_loss: 0.370(0.446), train_acc: 89.583(84.300)
01/18 07:58:42 PM [Supernet Training] lr: 0.02342 epoch: 098/600, step: 521/521, train_loss: 0.451(0.445), train_acc: 86.250(84.352)
01/18 07:58:42 PM [Supernet Training] epoch: 098, train_loss: 0.445, train_acc: 84.352
01/18 07:58:46 PM [Supernet Validation] epoch: 098, val_loss: 0.536, val_acc: 82.030, best_acc: 82.130
01/18 07:58:46 PM 

01/18 07:58:46 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 001/521, train_loss: 0.407(0.407), train_acc: 82.292(82.292)
01/18 07:58:59 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 101/521, train_loss: 0.489(0.428), train_acc: 82.292(84.798)
01/18 07:59:12 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 201/521, train_loss: 0.418(0.434), train_acc: 85.417(84.914)
01/18 07:59:25 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 301/521, train_loss: 0.296(0.436), train_acc: 89.583(84.835)
01/18 07:59:38 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 401/521, train_loss: 0.382(0.435), train_acc: 85.417(84.822)
01/18 07:59:51 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 501/521, train_loss: 0.315(0.438), train_acc: 92.708(84.652)
01/18 07:59:53 PM [Supernet Training] lr: 0.02339 epoch: 099/600, step: 521/521, train_loss: 0.533(0.438), train_acc: 81.250(84.634)
01/18 07:59:53 PM [Supernet Training] epoch: 099, train_loss: 0.438, train_acc: 84.634
01/18 07:59:57 PM [Supernet Validation] epoch: 099, val_loss: 0.550, val_acc: 81.560, best_acc: 82.130
01/18 07:59:57 PM 

01/18 07:59:57 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 001/521, train_loss: 0.453(0.453), train_acc: 86.458(86.458)
01/18 08:00:10 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 101/521, train_loss: 0.490(0.437), train_acc: 80.208(84.767)
01/18 08:00:23 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 201/521, train_loss: 0.469(0.443), train_acc: 82.292(84.479)
01/18 08:00:35 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 301/521, train_loss: 0.470(0.443), train_acc: 82.292(84.462)
01/18 08:00:48 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 401/521, train_loss: 0.487(0.441), train_acc: 85.417(84.614)
01/18 08:01:01 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 501/521, train_loss: 0.502(0.440), train_acc: 83.333(84.633)
01/18 08:01:03 PM [Supernet Training] lr: 0.02336 epoch: 100/600, step: 521/521, train_loss: 0.502(0.441), train_acc: 82.500(84.582)
01/18 08:01:04 PM [Supernet Training] epoch: 100, train_loss: 0.441, train_acc: 84.582
01/18 08:01:07 PM [Supernet Validation] epoch: 100, val_loss: 0.534, val_acc: 82.130, best_acc: 82.130
01/18 08:01:07 PM 

01/18 08:01:07 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 001/521, train_loss: 0.513(0.513), train_acc: 75.000(75.000)
01/18 08:01:20 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 101/521, train_loss: 0.551(0.429), train_acc: 83.333(84.870)
01/18 08:01:33 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 201/521, train_loss: 0.467(0.434), train_acc: 85.417(84.898)
01/18 08:01:46 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 301/521, train_loss: 0.443(0.435), train_acc: 86.458(84.773)
01/18 08:01:59 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 401/521, train_loss: 0.418(0.435), train_acc: 83.333(84.796)
01/18 08:02:11 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 501/521, train_loss: 0.401(0.438), train_acc: 85.417(84.753)
01/18 08:02:14 PM [Supernet Training] lr: 0.02333 epoch: 101/600, step: 521/521, train_loss: 0.477(0.438), train_acc: 86.250(84.730)
01/18 08:02:14 PM [Supernet Training] epoch: 101, train_loss: 0.438, train_acc: 84.730
01/18 08:02:18 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:02:18 PM [Supernet Validation] epoch: 101, val_loss: 0.533, val_acc: 82.140, best_acc: 82.140
01/18 08:02:18 PM 

01/18 08:02:18 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 001/521, train_loss: 0.597(0.597), train_acc: 82.292(82.292)
01/18 08:02:31 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 101/521, train_loss: 0.194(0.423), train_acc: 94.792(85.561)
01/18 08:02:44 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 201/521, train_loss: 0.500(0.429), train_acc: 83.333(85.220)
01/18 08:02:56 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 301/521, train_loss: 0.406(0.429), train_acc: 85.417(85.261)
01/18 08:03:09 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 401/521, train_loss: 0.407(0.429), train_acc: 88.542(85.188)
01/18 08:03:22 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 501/521, train_loss: 0.464(0.428), train_acc: 84.375(85.244)
01/18 08:03:24 PM [Supernet Training] lr: 0.02329 epoch: 102/600, step: 521/521, train_loss: 0.323(0.428), train_acc: 90.000(85.208)
01/18 08:03:25 PM [Supernet Training] epoch: 102, train_loss: 0.428, train_acc: 85.208
01/18 08:03:28 PM [Supernet Validation] epoch: 102, val_loss: 0.541, val_acc: 81.640, best_acc: 82.140
01/18 08:03:28 PM 

01/18 08:03:28 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 001/521, train_loss: 0.608(0.608), train_acc: 82.292(82.292)
01/18 08:03:41 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 101/521, train_loss: 0.676(0.437), train_acc: 69.792(84.592)
01/18 08:03:54 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 201/521, train_loss: 0.408(0.424), train_acc: 85.417(84.935)
01/18 08:04:07 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 301/521, train_loss: 0.604(0.425), train_acc: 81.250(84.974)
01/18 08:04:20 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 401/521, train_loss: 0.601(0.429), train_acc: 80.208(84.866)
01/18 08:04:33 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 501/521, train_loss: 0.244(0.430), train_acc: 92.708(84.874)
01/18 08:04:35 PM [Supernet Training] lr: 0.02326 epoch: 103/600, step: 521/521, train_loss: 0.500(0.430), train_acc: 83.750(84.874)
01/18 08:04:35 PM [Supernet Training] epoch: 103, train_loss: 0.430, train_acc: 84.874
01/18 08:04:39 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:04:39 PM [Supernet Validation] epoch: 103, val_loss: 0.524, val_acc: 82.300, best_acc: 82.300
01/18 08:04:39 PM 

01/18 08:04:39 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 001/521, train_loss: 0.385(0.385), train_acc: 86.458(86.458)
01/18 08:04:52 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 101/521, train_loss: 0.310(0.405), train_acc: 88.542(85.953)
01/18 08:05:05 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 201/521, train_loss: 0.601(0.419), train_acc: 80.208(85.525)
01/18 08:05:18 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 301/521, train_loss: 0.327(0.421), train_acc: 88.542(85.403)
01/18 08:05:30 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 401/521, train_loss: 0.390(0.427), train_acc: 87.500(85.115)
01/18 08:05:43 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 501/521, train_loss: 0.545(0.428), train_acc: 81.250(85.096)
01/18 08:05:46 PM [Supernet Training] lr: 0.02323 epoch: 104/600, step: 521/521, train_loss: 0.406(0.427), train_acc: 86.250(85.112)
01/18 08:05:46 PM [Supernet Training] epoch: 104, train_loss: 0.427, train_acc: 85.112
01/18 08:05:49 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:05:49 PM [Supernet Validation] epoch: 104, val_loss: 0.521, val_acc: 82.880, best_acc: 82.880
01/18 08:05:49 PM 

01/18 08:05:50 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 001/521, train_loss: 0.444(0.444), train_acc: 84.375(84.375)
01/18 08:06:03 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 101/521, train_loss: 0.285(0.420), train_acc: 89.583(85.716)
01/18 08:06:15 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 201/521, train_loss: 0.424(0.418), train_acc: 86.458(85.386)
01/18 08:06:28 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 301/521, train_loss: 0.310(0.422), train_acc: 88.542(85.230)
01/18 08:06:41 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 401/521, train_loss: 0.404(0.427), train_acc: 91.667(85.084)
01/18 08:06:54 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 501/521, train_loss: 0.373(0.425), train_acc: 89.583(85.190)
01/18 08:06:57 PM [Supernet Training] lr: 0.02319 epoch: 105/600, step: 521/521, train_loss: 0.691(0.426), train_acc: 80.000(85.122)
01/18 08:06:57 PM [Supernet Training] epoch: 105, train_loss: 0.426, train_acc: 85.122
01/18 08:07:00 PM [Supernet Validation] epoch: 105, val_loss: 0.538, val_acc: 82.220, best_acc: 82.880
01/18 08:07:00 PM 

01/18 08:07:01 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 001/521, train_loss: 0.341(0.341), train_acc: 86.458(86.458)
01/18 08:07:13 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 101/521, train_loss: 0.362(0.401), train_acc: 88.542(85.809)
01/18 08:07:26 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 201/521, train_loss: 0.567(0.412), train_acc: 82.292(85.567)
01/18 08:07:39 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 301/521, train_loss: 0.311(0.413), train_acc: 87.500(85.631)
01/18 08:07:52 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 401/521, train_loss: 0.590(0.419), train_acc: 80.208(85.339)
01/18 08:08:04 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 501/521, train_loss: 0.251(0.421), train_acc: 90.625(85.259)
01/18 08:08:07 PM [Supernet Training] lr: 0.02316 epoch: 106/600, step: 521/521, train_loss: 0.430(0.422), train_acc: 87.500(85.198)
01/18 08:08:07 PM [Supernet Training] epoch: 106, train_loss: 0.422, train_acc: 85.198
01/18 08:08:11 PM [Supernet Validation] epoch: 106, val_loss: 0.546, val_acc: 81.940, best_acc: 82.880
01/18 08:08:11 PM 

01/18 08:08:11 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 001/521, train_loss: 0.455(0.455), train_acc: 84.375(84.375)
01/18 08:08:24 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 101/521, train_loss: 0.458(0.425), train_acc: 78.125(85.097)
01/18 08:08:37 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 201/521, train_loss: 0.249(0.416), train_acc: 91.667(85.329)
01/18 08:08:49 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 301/521, train_loss: 0.521(0.420), train_acc: 82.292(85.271)
01/18 08:09:02 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 401/521, train_loss: 0.575(0.418), train_acc: 83.333(85.344)
01/18 08:09:15 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 501/521, train_loss: 0.521(0.414), train_acc: 86.458(85.479)
01/18 08:09:18 PM [Supernet Training] lr: 0.02312 epoch: 107/600, step: 521/521, train_loss: 0.485(0.415), train_acc: 85.000(85.416)
01/18 08:09:18 PM [Supernet Training] epoch: 107, train_loss: 0.415, train_acc: 85.416
01/18 08:09:21 PM [Supernet Validation] epoch: 107, val_loss: 0.521, val_acc: 82.650, best_acc: 82.880
01/18 08:09:21 PM 

01/18 08:09:22 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 001/521, train_loss: 0.320(0.320), train_acc: 92.708(92.708)
01/18 08:09:34 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 101/521, train_loss: 0.489(0.402), train_acc: 83.333(85.943)
01/18 08:09:47 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 201/521, train_loss: 0.492(0.408), train_acc: 85.417(85.702)
01/18 08:10:00 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 301/521, train_loss: 0.371(0.415), train_acc: 87.500(85.451)
01/18 08:10:13 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 401/521, train_loss: 0.392(0.414), train_acc: 88.542(85.466)
01/18 08:10:26 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 501/521, train_loss: 0.387(0.414), train_acc: 83.333(85.454)
01/18 08:10:28 PM [Supernet Training] lr: 0.02309 epoch: 108/600, step: 521/521, train_loss: 0.265(0.415), train_acc: 92.500(85.420)
01/18 08:10:28 PM [Supernet Training] epoch: 108, train_loss: 0.415, train_acc: 85.420
01/18 08:10:32 PM [Supernet Validation] epoch: 108, val_loss: 0.521, val_acc: 82.390, best_acc: 82.880
01/18 08:10:32 PM 

01/18 08:10:32 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 001/521, train_loss: 0.355(0.355), train_acc: 90.625(90.625)
01/18 08:10:45 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 101/521, train_loss: 0.355(0.412), train_acc: 89.583(85.582)
01/18 08:10:58 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 201/521, train_loss: 0.508(0.411), train_acc: 85.417(85.453)
01/18 08:11:11 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 301/521, train_loss: 0.343(0.408), train_acc: 86.458(85.683)
01/18 08:11:23 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 401/521, train_loss: 0.435(0.413), train_acc: 84.375(85.526)
01/18 08:11:36 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 501/521, train_loss: 0.425(0.415), train_acc: 84.375(85.535)
01/18 08:11:39 PM [Supernet Training] lr: 0.02305 epoch: 109/600, step: 521/521, train_loss: 0.379(0.415), train_acc: 88.750(85.554)
01/18 08:11:39 PM [Supernet Training] epoch: 109, train_loss: 0.415, train_acc: 85.554
01/18 08:11:42 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:11:42 PM [Supernet Validation] epoch: 109, val_loss: 0.502, val_acc: 83.280, best_acc: 83.280
01/18 08:11:42 PM 

01/18 08:11:43 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 001/521, train_loss: 0.349(0.349), train_acc: 89.583(89.583)
01/18 08:11:55 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 101/521, train_loss: 0.348(0.390), train_acc: 85.417(86.541)
01/18 08:12:08 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 201/521, train_loss: 0.542(0.397), train_acc: 83.333(86.272)
01/18 08:12:21 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 301/521, train_loss: 0.298(0.400), train_acc: 89.583(85.977)
01/18 08:12:34 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 401/521, train_loss: 0.394(0.406), train_acc: 86.458(85.741)
01/18 08:12:47 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 501/521, train_loss: 0.542(0.411), train_acc: 83.333(85.593)
01/18 08:12:49 PM [Supernet Training] lr: 0.02302 epoch: 110/600, step: 521/521, train_loss: 0.396(0.412), train_acc: 85.000(85.572)
01/18 08:12:49 PM [Supernet Training] epoch: 110, train_loss: 0.412, train_acc: 85.572
01/18 08:12:53 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:12:53 PM [Supernet Validation] epoch: 110, val_loss: 0.506, val_acc: 83.510, best_acc: 83.510
01/18 08:12:53 PM 

01/18 08:12:53 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 001/521, train_loss: 0.504(0.504), train_acc: 80.208(80.208)
01/18 08:13:06 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 101/521, train_loss: 0.383(0.403), train_acc: 86.458(86.438)
01/18 08:13:19 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 201/521, train_loss: 0.520(0.404), train_acc: 84.375(86.111)
01/18 08:13:32 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 301/521, train_loss: 0.358(0.412), train_acc: 86.458(85.815)
01/18 08:13:45 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 401/521, train_loss: 0.536(0.407), train_acc: 77.083(85.918)
01/18 08:13:57 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 501/521, train_loss: 0.472(0.409), train_acc: 83.333(85.797)
01/18 08:14:00 PM [Supernet Training] lr: 0.02298 epoch: 111/600, step: 521/521, train_loss: 0.446(0.409), train_acc: 88.750(85.848)
01/18 08:14:00 PM [Supernet Training] epoch: 111, train_loss: 0.409, train_acc: 85.848
01/18 08:14:04 PM [Supernet Validation] epoch: 111, val_loss: 0.510, val_acc: 82.700, best_acc: 83.510
01/18 08:14:04 PM 

01/18 08:14:04 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 001/521, train_loss: 0.417(0.417), train_acc: 83.333(83.333)
01/18 08:14:17 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 101/521, train_loss: 0.325(0.392), train_acc: 89.583(86.015)
01/18 08:14:30 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 201/521, train_loss: 0.284(0.399), train_acc: 93.750(85.826)
01/18 08:14:42 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 301/521, train_loss: 0.476(0.408), train_acc: 83.333(85.621)
01/18 08:14:55 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 401/521, train_loss: 0.411(0.405), train_acc: 84.375(85.643)
01/18 08:15:08 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 501/521, train_loss: 0.394(0.407), train_acc: 82.292(85.604)
01/18 08:15:11 PM [Supernet Training] lr: 0.02295 epoch: 112/600, step: 521/521, train_loss: 0.333(0.407), train_acc: 88.750(85.654)
01/18 08:15:11 PM [Supernet Training] epoch: 112, train_loss: 0.407, train_acc: 85.654
01/18 08:15:14 PM [Supernet Validation] epoch: 112, val_loss: 0.520, val_acc: 82.590, best_acc: 83.510
01/18 08:15:14 PM 

01/18 08:15:15 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 001/521, train_loss: 0.308(0.308), train_acc: 86.458(86.458)
01/18 08:15:27 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 101/521, train_loss: 0.311(0.399), train_acc: 89.583(86.221)
01/18 08:15:40 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 201/521, train_loss: 0.435(0.410), train_acc: 83.333(85.748)
01/18 08:15:53 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 301/521, train_loss: 0.344(0.408), train_acc: 87.500(85.801)
01/18 08:16:06 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 401/521, train_loss: 0.244(0.409), train_acc: 92.708(85.721)
01/18 08:16:18 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 501/521, train_loss: 0.249(0.407), train_acc: 93.750(85.760)
01/18 08:16:21 PM [Supernet Training] lr: 0.02291 epoch: 113/600, step: 521/521, train_loss: 0.248(0.407), train_acc: 92.500(85.778)
01/18 08:16:21 PM [Supernet Training] epoch: 113, train_loss: 0.407, train_acc: 85.778
01/18 08:16:25 PM [Supernet Validation] epoch: 113, val_loss: 0.519, val_acc: 83.000, best_acc: 83.510
01/18 08:16:25 PM 

01/18 08:16:25 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 001/521, train_loss: 0.240(0.240), train_acc: 91.667(91.667)
01/18 08:16:38 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 101/521, train_loss: 0.458(0.403), train_acc: 83.333(85.685)
01/18 08:16:51 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 201/521, train_loss: 0.580(0.402), train_acc: 80.208(85.904)
01/18 08:17:03 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 301/521, train_loss: 0.620(0.401), train_acc: 79.167(85.808)
01/18 08:17:16 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 401/521, train_loss: 0.408(0.403), train_acc: 87.500(85.874)
01/18 08:17:29 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 501/521, train_loss: 0.624(0.405), train_acc: 78.125(85.866)
01/18 08:17:32 PM [Supernet Training] lr: 0.02288 epoch: 114/600, step: 521/521, train_loss: 0.441(0.404), train_acc: 85.000(85.882)
01/18 08:17:32 PM [Supernet Training] epoch: 114, train_loss: 0.404, train_acc: 85.882
01/18 08:17:35 PM [Supernet Validation] epoch: 114, val_loss: 0.520, val_acc: 82.610, best_acc: 83.510
01/18 08:17:35 PM 

01/18 08:17:36 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 001/521, train_loss: 0.277(0.277), train_acc: 91.667(91.667)
01/18 08:17:48 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 101/521, train_loss: 0.387(0.394), train_acc: 85.417(86.355)
01/18 08:18:01 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 201/521, train_loss: 0.308(0.395), train_acc: 86.458(86.215)
01/18 08:18:14 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 301/521, train_loss: 0.315(0.400), train_acc: 89.583(86.015)
01/18 08:18:27 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 401/521, train_loss: 0.389(0.402), train_acc: 87.500(85.845)
01/18 08:18:39 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 501/521, train_loss: 0.300(0.402), train_acc: 88.542(85.884)
01/18 08:18:42 PM [Supernet Training] lr: 0.02284 epoch: 115/600, step: 521/521, train_loss: 0.370(0.402), train_acc: 87.500(85.868)
01/18 08:18:42 PM [Supernet Training] epoch: 115, train_loss: 0.402, train_acc: 85.868
01/18 08:18:46 PM [Supernet Validation] epoch: 115, val_loss: 0.519, val_acc: 83.220, best_acc: 83.510
01/18 08:18:46 PM 

01/18 08:18:46 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 001/521, train_loss: 0.403(0.403), train_acc: 87.500(87.500)
01/18 08:18:59 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 101/521, train_loss: 0.393(0.403), train_acc: 81.250(85.963)
01/18 08:19:11 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 201/521, train_loss: 0.273(0.401), train_acc: 92.708(85.925)
01/18 08:19:24 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 301/521, train_loss: 0.342(0.395), train_acc: 90.625(86.071)
01/18 08:19:37 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 401/521, train_loss: 0.342(0.399), train_acc: 87.500(85.905)
01/18 08:19:50 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 501/521, train_loss: 0.519(0.399), train_acc: 82.292(85.878)
01/18 08:19:53 PM [Supernet Training] lr: 0.02280 epoch: 116/600, step: 521/521, train_loss: 0.400(0.399), train_acc: 85.000(85.894)
01/18 08:19:53 PM [Supernet Training] epoch: 116, train_loss: 0.399, train_acc: 85.894
01/18 08:19:56 PM [Supernet Validation] epoch: 116, val_loss: 0.500, val_acc: 83.090, best_acc: 83.510
01/18 08:19:56 PM 

01/18 08:19:57 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 001/521, train_loss: 0.509(0.509), train_acc: 80.208(80.208)
01/18 08:20:09 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 101/521, train_loss: 0.238(0.377), train_acc: 90.625(86.613)
01/18 08:20:22 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 201/521, train_loss: 0.474(0.385), train_acc: 81.250(86.453)
01/18 08:20:35 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 301/521, train_loss: 0.438(0.392), train_acc: 81.250(86.310)
01/18 08:20:48 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 401/521, train_loss: 0.423(0.393), train_acc: 83.333(86.261)
01/18 08:21:00 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 501/521, train_loss: 0.489(0.395), train_acc: 83.333(86.223)
01/18 08:21:03 PM [Supernet Training] lr: 0.02276 epoch: 117/600, step: 521/521, train_loss: 0.505(0.395), train_acc: 80.000(86.184)
01/18 08:21:03 PM [Supernet Training] epoch: 117, train_loss: 0.395, train_acc: 86.184
01/18 08:21:07 PM [Supernet Validation] epoch: 117, val_loss: 0.499, val_acc: 83.320, best_acc: 83.510
01/18 08:21:07 PM 

01/18 08:21:07 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 001/521, train_loss: 0.419(0.419), train_acc: 85.417(85.417)
01/18 08:21:20 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 101/521, train_loss: 0.348(0.388), train_acc: 86.458(86.242)
01/18 08:21:33 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 201/521, train_loss: 0.435(0.392), train_acc: 82.292(86.189)
01/18 08:21:45 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 301/521, train_loss: 0.332(0.389), train_acc: 90.625(86.296)
01/18 08:21:58 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 401/521, train_loss: 0.530(0.389), train_acc: 82.292(86.370)
01/18 08:22:11 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 501/521, train_loss: 0.412(0.391), train_acc: 82.292(86.352)
01/18 08:22:14 PM [Supernet Training] lr: 0.02273 epoch: 118/600, step: 521/521, train_loss: 0.403(0.390), train_acc: 85.000(86.346)
01/18 08:22:14 PM [Supernet Training] epoch: 118, train_loss: 0.390, train_acc: 86.346
01/18 08:22:17 PM [Supernet Validation] epoch: 118, val_loss: 0.531, val_acc: 82.380, best_acc: 83.510
01/18 08:22:17 PM 

01/18 08:22:17 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 001/521, train_loss: 0.517(0.517), train_acc: 81.250(81.250)
01/18 08:22:30 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 101/521, train_loss: 0.607(0.388), train_acc: 80.208(86.603)
01/18 08:22:43 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 201/521, train_loss: 0.488(0.389), train_acc: 81.250(86.432)
01/18 08:22:56 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 301/521, train_loss: 0.323(0.389), train_acc: 88.542(86.451)
01/18 08:23:09 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 401/521, train_loss: 0.454(0.389), train_acc: 88.542(86.466)
01/18 08:23:22 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 501/521, train_loss: 0.422(0.390), train_acc: 81.250(86.406)
01/18 08:23:24 PM [Supernet Training] lr: 0.02269 epoch: 119/600, step: 521/521, train_loss: 0.390(0.391), train_acc: 87.500(86.378)
01/18 08:23:24 PM [Supernet Training] epoch: 119, train_loss: 0.391, train_acc: 86.378
01/18 08:23:28 PM [Supernet Validation] epoch: 119, val_loss: 0.519, val_acc: 83.070, best_acc: 83.510
01/18 08:23:28 PM 

01/18 08:23:28 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 001/521, train_loss: 0.391(0.391), train_acc: 85.417(85.417)
01/18 08:23:41 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 101/521, train_loss: 0.244(0.386), train_acc: 91.667(86.427)
01/18 08:23:54 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 201/521, train_loss: 0.346(0.389), train_acc: 84.375(86.298)
01/18 08:24:07 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 301/521, train_loss: 0.223(0.387), train_acc: 89.583(86.306)
01/18 08:24:19 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 401/521, train_loss: 0.359(0.386), train_acc: 86.458(86.328)
01/18 08:24:32 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 501/521, train_loss: 0.543(0.386), train_acc: 81.250(86.394)
01/18 08:24:35 PM [Supernet Training] lr: 0.02265 epoch: 120/600, step: 521/521, train_loss: 0.523(0.385), train_acc: 77.500(86.414)
01/18 08:24:35 PM [Supernet Training] epoch: 120, train_loss: 0.385, train_acc: 86.414
01/18 08:24:38 PM [Supernet Validation] epoch: 120, val_loss: 0.503, val_acc: 83.150, best_acc: 83.510
01/18 08:24:38 PM 

01/18 08:24:39 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 001/521, train_loss: 0.335(0.335), train_acc: 88.542(88.542)
01/18 08:24:51 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 101/521, train_loss: 0.450(0.385), train_acc: 83.333(86.531)
01/18 08:25:04 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 201/521, train_loss: 0.465(0.383), train_acc: 82.292(86.443)
01/18 08:25:17 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 301/521, train_loss: 0.325(0.384), train_acc: 87.500(86.348)
01/18 08:25:30 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 401/521, train_loss: 0.342(0.390), train_acc: 88.542(86.139)
01/18 08:25:43 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 501/521, train_loss: 0.346(0.388), train_acc: 86.458(86.207)
01/18 08:25:45 PM [Supernet Training] lr: 0.02261 epoch: 121/600, step: 521/521, train_loss: 0.171(0.388), train_acc: 92.500(86.224)
01/18 08:25:45 PM [Supernet Training] epoch: 121, train_loss: 0.388, train_acc: 86.224
01/18 08:25:49 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:25:49 PM [Supernet Validation] epoch: 121, val_loss: 0.485, val_acc: 84.010, best_acc: 84.010
01/18 08:25:49 PM 

01/18 08:25:49 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 001/521, train_loss: 0.324(0.324), train_acc: 90.625(90.625)
01/18 08:26:02 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 101/521, train_loss: 0.503(0.379), train_acc: 83.333(86.685)
01/18 08:26:15 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 201/521, train_loss: 0.438(0.379), train_acc: 83.333(86.723)
01/18 08:26:27 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 301/521, train_loss: 0.406(0.383), train_acc: 91.667(86.514)
01/18 08:26:40 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 401/521, train_loss: 0.353(0.384), train_acc: 85.417(86.500)
01/18 08:26:53 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 501/521, train_loss: 0.343(0.383), train_acc: 87.500(86.529)
01/18 08:26:56 PM [Supernet Training] lr: 0.02257 epoch: 122/600, step: 521/521, train_loss: 0.364(0.384), train_acc: 82.500(86.498)
01/18 08:26:56 PM [Supernet Training] epoch: 122, train_loss: 0.384, train_acc: 86.498
01/18 08:26:59 PM [Supernet Validation] epoch: 122, val_loss: 0.511, val_acc: 83.230, best_acc: 84.010
01/18 08:26:59 PM 

01/18 08:27:00 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 001/521, train_loss: 0.504(0.504), train_acc: 83.333(83.333)
01/18 08:27:12 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 101/521, train_loss: 0.352(0.377), train_acc: 86.458(86.582)
01/18 08:27:25 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 201/521, train_loss: 0.563(0.380), train_acc: 79.167(86.386)
01/18 08:27:38 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 301/521, train_loss: 0.275(0.385), train_acc: 90.625(86.379)
01/18 08:27:51 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 401/521, train_loss: 0.339(0.387), train_acc: 86.458(86.297)
01/18 08:28:04 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 501/521, train_loss: 0.421(0.386), train_acc: 85.417(86.325)
01/18 08:28:06 PM [Supernet Training] lr: 0.02254 epoch: 123/600, step: 521/521, train_loss: 0.396(0.386), train_acc: 82.500(86.322)
01/18 08:28:06 PM [Supernet Training] epoch: 123, train_loss: 0.386, train_acc: 86.322
01/18 08:28:10 PM [Supernet Validation] epoch: 123, val_loss: 0.493, val_acc: 83.740, best_acc: 84.010
01/18 08:28:10 PM 

01/18 08:28:10 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 001/521, train_loss: 0.352(0.352), train_acc: 86.458(86.458)
01/18 08:28:23 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 101/521, train_loss: 0.237(0.379), train_acc: 89.583(86.458)
01/18 08:28:36 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 201/521, train_loss: 0.413(0.383), train_acc: 86.458(86.759)
01/18 08:28:49 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 301/521, train_loss: 0.387(0.379), train_acc: 84.375(86.832)
01/18 08:29:01 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 401/521, train_loss: 0.424(0.376), train_acc: 81.250(86.957)
01/18 08:29:14 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 501/521, train_loss: 0.376(0.376), train_acc: 88.542(86.870)
01/18 08:29:17 PM [Supernet Training] lr: 0.02250 epoch: 124/600, step: 521/521, train_loss: 0.393(0.377), train_acc: 83.750(86.820)
01/18 08:29:17 PM [Supernet Training] epoch: 124, train_loss: 0.377, train_acc: 86.820
01/18 08:29:20 PM [Supernet Validation] epoch: 124, val_loss: 0.514, val_acc: 82.700, best_acc: 84.010
01/18 08:29:20 PM 

01/18 08:29:21 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 001/521, train_loss: 0.364(0.364), train_acc: 87.500(87.500)
01/18 08:29:34 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 101/521, train_loss: 0.472(0.378), train_acc: 86.458(86.809)
01/18 08:29:46 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 201/521, train_loss: 0.239(0.377), train_acc: 92.708(86.961)
01/18 08:29:59 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 301/521, train_loss: 0.408(0.375), train_acc: 86.458(86.884)
01/18 08:30:12 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 401/521, train_loss: 0.416(0.379), train_acc: 86.458(86.760)
01/18 08:30:25 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 501/521, train_loss: 0.364(0.381), train_acc: 86.458(86.716)
01/18 08:30:27 PM [Supernet Training] lr: 0.02246 epoch: 125/600, step: 521/521, train_loss: 0.349(0.380), train_acc: 85.000(86.724)
01/18 08:30:27 PM [Supernet Training] epoch: 125, train_loss: 0.380, train_acc: 86.724
01/18 08:30:31 PM [Supernet Validation] epoch: 125, val_loss: 0.492, val_acc: 83.590, best_acc: 84.010
01/18 08:30:31 PM 

01/18 08:30:31 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 001/521, train_loss: 0.385(0.385), train_acc: 87.500(87.500)
01/18 08:30:44 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 101/521, train_loss: 0.494(0.373), train_acc: 83.333(86.572)
01/18 08:30:57 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 201/521, train_loss: 0.440(0.378), train_acc: 84.375(86.624)
01/18 08:31:10 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 301/521, train_loss: 0.499(0.381), train_acc: 83.333(86.586)
01/18 08:31:23 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 401/521, train_loss: 0.274(0.379), train_acc: 92.708(86.692)
01/18 08:31:35 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 501/521, train_loss: 0.436(0.378), train_acc: 86.458(86.758)
01/18 08:31:38 PM [Supernet Training] lr: 0.02242 epoch: 126/600, step: 521/521, train_loss: 0.341(0.377), train_acc: 88.750(86.758)
01/18 08:31:38 PM [Supernet Training] epoch: 126, train_loss: 0.377, train_acc: 86.758
01/18 08:31:42 PM [Supernet Validation] epoch: 126, val_loss: 0.503, val_acc: 83.700, best_acc: 84.010
01/18 08:31:42 PM 

01/18 08:31:42 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 001/521, train_loss: 0.308(0.308), train_acc: 92.708(92.708)
01/18 08:31:55 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 101/521, train_loss: 0.364(0.356), train_acc: 86.458(87.861)
01/18 08:32:08 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 201/521, train_loss: 0.374(0.366), train_acc: 84.375(87.241)
01/18 08:32:21 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 301/521, train_loss: 0.304(0.367), train_acc: 90.625(87.105)
01/18 08:32:33 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 401/521, train_loss: 0.448(0.371), train_acc: 87.500(86.903)
01/18 08:32:46 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 501/521, train_loss: 0.517(0.373), train_acc: 84.375(86.889)
01/18 08:32:49 PM [Supernet Training] lr: 0.02238 epoch: 127/600, step: 521/521, train_loss: 0.326(0.373), train_acc: 88.750(86.870)
01/18 08:32:49 PM [Supernet Training] epoch: 127, train_loss: 0.373, train_acc: 86.870
01/18 08:32:52 PM [Supernet Validation] epoch: 127, val_loss: 0.500, val_acc: 83.340, best_acc: 84.010
01/18 08:32:52 PM 

01/18 08:32:53 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 001/521, train_loss: 0.259(0.259), train_acc: 89.583(89.583)
01/18 08:33:06 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 101/521, train_loss: 0.220(0.361), train_acc: 90.625(87.706)
01/18 08:33:18 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 201/521, train_loss: 0.377(0.367), train_acc: 88.542(87.365)
01/18 08:33:31 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 301/521, train_loss: 0.389(0.368), train_acc: 86.458(87.175)
01/18 08:33:44 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 401/521, train_loss: 0.337(0.370), train_acc: 86.458(87.074)
01/18 08:33:57 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 501/521, train_loss: 0.339(0.371), train_acc: 88.542(87.026)
01/18 08:33:59 PM [Supernet Training] lr: 0.02234 epoch: 128/600, step: 521/521, train_loss: 0.320(0.371), train_acc: 87.500(87.020)
01/18 08:33:59 PM [Supernet Training] epoch: 128, train_loss: 0.371, train_acc: 87.020
01/18 08:34:03 PM [Supernet Validation] epoch: 128, val_loss: 0.501, val_acc: 83.240, best_acc: 84.010
01/18 08:34:03 PM 

01/18 08:34:03 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 001/521, train_loss: 0.371(0.371), train_acc: 84.375(84.375)
01/18 08:34:16 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 101/521, train_loss: 0.362(0.375), train_acc: 87.500(86.675)
01/18 08:34:29 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 201/521, train_loss: 0.317(0.371), train_acc: 89.583(86.873)
01/18 08:34:42 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 301/521, train_loss: 0.280(0.371), train_acc: 89.583(86.815)
01/18 08:34:54 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 401/521, train_loss: 0.257(0.373), train_acc: 91.667(86.804)
01/18 08:35:07 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 501/521, train_loss: 0.307(0.370), train_acc: 87.500(86.853)
01/18 08:35:10 PM [Supernet Training] lr: 0.02230 epoch: 129/600, step: 521/521, train_loss: 0.271(0.371), train_acc: 90.000(86.830)
01/18 08:35:10 PM [Supernet Training] epoch: 129, train_loss: 0.371, train_acc: 86.830
01/18 08:35:13 PM [Supernet Validation] epoch: 129, val_loss: 0.486, val_acc: 83.730, best_acc: 84.010
01/18 08:35:13 PM 

01/18 08:35:14 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 001/521, train_loss: 0.473(0.473), train_acc: 84.375(84.375)
01/18 08:35:27 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 101/521, train_loss: 0.323(0.362), train_acc: 87.500(87.314)
01/18 08:35:39 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 201/521, train_loss: 0.323(0.364), train_acc: 85.417(87.111)
01/18 08:35:52 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 301/521, train_loss: 0.553(0.367), train_acc: 81.250(87.043)
01/18 08:36:05 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 401/521, train_loss: 0.372(0.365), train_acc: 87.500(87.123)
01/18 08:36:18 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 501/521, train_loss: 0.306(0.367), train_acc: 91.667(87.086)
01/18 08:36:20 PM [Supernet Training] lr: 0.02226 epoch: 130/600, step: 521/521, train_loss: 0.461(0.367), train_acc: 87.500(87.060)
01/18 08:36:20 PM [Supernet Training] epoch: 130, train_loss: 0.367, train_acc: 87.060
01/18 08:36:24 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:36:24 PM [Supernet Validation] epoch: 130, val_loss: 0.480, val_acc: 84.140, best_acc: 84.140
01/18 08:36:24 PM 

01/18 08:36:24 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 001/521, train_loss: 0.429(0.429), train_acc: 86.458(86.458)
01/18 08:36:37 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 101/521, train_loss: 0.343(0.353), train_acc: 86.458(87.624)
01/18 08:36:50 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 201/521, train_loss: 0.491(0.365), train_acc: 81.250(87.241)
01/18 08:37:03 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 301/521, train_loss: 0.230(0.363), train_acc: 94.792(87.365)
01/18 08:37:15 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 401/521, train_loss: 0.419(0.363), train_acc: 85.417(87.290)
01/18 08:37:28 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 501/521, train_loss: 0.445(0.364), train_acc: 87.500(87.282)
01/18 08:37:31 PM [Supernet Training] lr: 0.02221 epoch: 131/600, step: 521/521, train_loss: 0.340(0.364), train_acc: 90.000(87.286)
01/18 08:37:31 PM [Supernet Training] epoch: 131, train_loss: 0.364, train_acc: 87.286
01/18 08:37:35 PM [Supernet Validation] epoch: 131, val_loss: 0.499, val_acc: 83.650, best_acc: 84.140
01/18 08:37:35 PM 

01/18 08:37:35 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 001/521, train_loss: 0.337(0.337), train_acc: 86.458(86.458)
01/18 08:37:48 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 101/521, train_loss: 0.195(0.360), train_acc: 94.792(87.129)
01/18 08:38:01 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 201/521, train_loss: 0.525(0.356), train_acc: 83.333(87.303)
01/18 08:38:13 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 301/521, train_loss: 0.448(0.366), train_acc: 83.333(86.908)
01/18 08:38:26 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 401/521, train_loss: 0.275(0.364), train_acc: 89.583(87.079)
01/18 08:38:39 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 501/521, train_loss: 0.327(0.364), train_acc: 87.500(87.117)
01/18 08:38:42 PM [Supernet Training] lr: 0.02217 epoch: 132/600, step: 521/521, train_loss: 0.498(0.366), train_acc: 83.750(86.980)
01/18 08:38:42 PM [Supernet Training] epoch: 132, train_loss: 0.366, train_acc: 86.980
01/18 08:38:45 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:38:45 PM [Supernet Validation] epoch: 132, val_loss: 0.484, val_acc: 84.310, best_acc: 84.310
01/18 08:38:45 PM 

01/18 08:38:46 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 001/521, train_loss: 0.483(0.483), train_acc: 85.417(85.417)
01/18 08:38:58 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 101/521, train_loss: 0.288(0.368), train_acc: 90.625(87.356)
01/18 08:39:11 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 201/521, train_loss: 0.358(0.362), train_acc: 87.500(87.433)
01/18 08:39:24 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 301/521, train_loss: 0.348(0.362), train_acc: 85.417(87.431)
01/18 08:39:37 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 401/521, train_loss: 0.385(0.364), train_acc: 85.417(87.334)
01/18 08:39:50 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 501/521, train_loss: 0.196(0.363), train_acc: 94.792(87.286)
01/18 08:39:52 PM [Supernet Training] lr: 0.02213 epoch: 133/600, step: 521/521, train_loss: 0.431(0.363), train_acc: 86.250(87.296)
01/18 08:39:52 PM [Supernet Training] epoch: 133, train_loss: 0.363, train_acc: 87.296
01/18 08:39:56 PM [Supernet Validation] epoch: 133, val_loss: 0.487, val_acc: 84.020, best_acc: 84.310
01/18 08:39:56 PM 

01/18 08:39:56 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 001/521, train_loss: 0.316(0.316), train_acc: 86.458(86.458)
01/18 08:40:09 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 101/521, train_loss: 0.311(0.358), train_acc: 89.583(87.655)
01/18 08:40:22 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 201/521, train_loss: 0.353(0.368), train_acc: 88.542(87.272)
01/18 08:40:35 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 301/521, train_loss: 0.322(0.367), train_acc: 87.500(87.234)
01/18 08:40:47 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 401/521, train_loss: 0.284(0.363), train_acc: 87.500(87.238)
01/18 08:41:00 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 501/521, train_loss: 0.341(0.364), train_acc: 88.542(87.257)
01/18 08:41:03 PM [Supernet Training] lr: 0.02209 epoch: 134/600, step: 521/521, train_loss: 0.505(0.365), train_acc: 82.500(87.232)
01/18 08:41:03 PM [Supernet Training] epoch: 134, train_loss: 0.365, train_acc: 87.232
01/18 08:41:06 PM [Supernet Validation] epoch: 134, val_loss: 0.494, val_acc: 83.780, best_acc: 84.310
01/18 08:41:06 PM 

01/18 08:41:07 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 001/521, train_loss: 0.416(0.416), train_acc: 83.333(83.333)
01/18 08:41:20 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 101/521, train_loss: 0.299(0.357), train_acc: 90.625(87.479)
01/18 08:41:33 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 201/521, train_loss: 0.442(0.357), train_acc: 86.458(87.500)
01/18 08:41:45 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 301/521, train_loss: 0.280(0.355), train_acc: 90.625(87.645)
01/18 08:41:58 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 401/521, train_loss: 0.337(0.356), train_acc: 87.500(87.692)
01/18 08:42:11 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 501/521, train_loss: 0.330(0.359), train_acc: 87.500(87.546)
01/18 08:42:13 PM [Supernet Training] lr: 0.02205 epoch: 135/600, step: 521/521, train_loss: 0.421(0.359), train_acc: 87.500(87.514)
01/18 08:42:14 PM [Supernet Training] epoch: 135, train_loss: 0.359, train_acc: 87.514
01/18 08:42:17 PM [Supernet Validation] epoch: 135, val_loss: 0.500, val_acc: 84.040, best_acc: 84.310
01/18 08:42:17 PM 

01/18 08:42:18 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 001/521, train_loss: 0.182(0.182), train_acc: 93.750(93.750)
01/18 08:42:30 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 101/521, train_loss: 0.357(0.343), train_acc: 85.417(87.613)
01/18 08:42:43 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 201/521, train_loss: 0.366(0.355), train_acc: 85.417(87.443)
01/18 08:42:56 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 301/521, train_loss: 0.351(0.354), train_acc: 89.583(87.524)
01/18 08:43:09 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 401/521, train_loss: 0.287(0.357), train_acc: 86.458(87.469)
01/18 08:43:21 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 501/521, train_loss: 0.719(0.359), train_acc: 77.083(87.367)
01/18 08:43:24 PM [Supernet Training] lr: 0.02201 epoch: 136/600, step: 521/521, train_loss: 0.391(0.358), train_acc: 81.250(87.390)
01/18 08:43:24 PM [Supernet Training] epoch: 136, train_loss: 0.358, train_acc: 87.390
01/18 08:43:28 PM [Supernet Validation] epoch: 136, val_loss: 0.483, val_acc: 84.150, best_acc: 84.310
01/18 08:43:28 PM 

01/18 08:43:28 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 89.583(89.583)
01/18 08:43:41 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 101/521, train_loss: 0.222(0.349), train_acc: 89.583(87.717)
01/18 08:43:54 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 201/521, train_loss: 0.544(0.350), train_acc: 78.125(87.718)
01/18 08:44:06 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 301/521, train_loss: 0.291(0.347), train_acc: 88.542(87.888)
01/18 08:44:19 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 401/521, train_loss: 0.267(0.350), train_acc: 90.625(87.773)
01/18 08:44:32 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 501/521, train_loss: 0.312(0.352), train_acc: 87.500(87.650)
01/18 08:44:35 PM [Supernet Training] lr: 0.02196 epoch: 137/600, step: 521/521, train_loss: 0.501(0.354), train_acc: 85.000(87.606)
01/18 08:44:35 PM [Supernet Training] epoch: 137, train_loss: 0.354, train_acc: 87.606
01/18 08:44:38 PM [Supernet Validation] epoch: 137, val_loss: 0.492, val_acc: 84.150, best_acc: 84.310
01/18 08:44:38 PM 

01/18 08:44:39 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 001/521, train_loss: 0.295(0.295), train_acc: 87.500(87.500)
01/18 08:44:51 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 101/521, train_loss: 0.392(0.349), train_acc: 85.417(87.768)
01/18 08:45:04 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 201/521, train_loss: 0.375(0.348), train_acc: 86.458(87.655)
01/18 08:45:17 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 301/521, train_loss: 0.417(0.355), train_acc: 84.375(87.483)
01/18 08:45:30 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 401/521, train_loss: 0.280(0.355), train_acc: 92.708(87.542)
01/18 08:45:43 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 501/521, train_loss: 0.260(0.355), train_acc: 89.583(87.562)
01/18 08:45:45 PM [Supernet Training] lr: 0.02192 epoch: 138/600, step: 521/521, train_loss: 0.594(0.355), train_acc: 77.500(87.552)
01/18 08:45:45 PM [Supernet Training] epoch: 138, train_loss: 0.355, train_acc: 87.552
01/18 08:45:49 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:45:49 PM [Supernet Validation] epoch: 138, val_loss: 0.479, val_acc: 84.420, best_acc: 84.420
01/18 08:45:49 PM 

01/18 08:45:49 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 001/521, train_loss: 0.519(0.519), train_acc: 84.375(84.375)
01/18 08:46:02 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 101/521, train_loss: 0.374(0.353), train_acc: 84.375(87.397)
01/18 08:46:15 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 201/521, train_loss: 0.312(0.349), train_acc: 87.500(87.655)
01/18 08:46:28 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 301/521, train_loss: 0.582(0.346), train_acc: 83.333(87.753)
01/18 08:46:41 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 401/521, train_loss: 0.404(0.349), train_acc: 85.417(87.700)
01/18 08:46:53 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 501/521, train_loss: 0.270(0.352), train_acc: 87.500(87.637)
01/18 08:46:56 PM [Supernet Training] lr: 0.02188 epoch: 139/600, step: 521/521, train_loss: 0.419(0.353), train_acc: 86.250(87.582)
01/18 08:46:56 PM [Supernet Training] epoch: 139, train_loss: 0.353, train_acc: 87.582
01/18 08:47:00 PM [Supernet Validation] epoch: 139, val_loss: 0.493, val_acc: 84.310, best_acc: 84.420
01/18 08:47:00 PM 

01/18 08:47:00 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 001/521, train_loss: 0.365(0.365), train_acc: 87.500(87.500)
01/18 08:47:13 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 101/521, train_loss: 0.448(0.352), train_acc: 81.250(87.345)
01/18 08:47:26 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 201/521, train_loss: 0.306(0.356), train_acc: 90.625(87.438)
01/18 08:47:38 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 301/521, train_loss: 0.227(0.354), train_acc: 91.667(87.576)
01/18 08:47:51 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 401/521, train_loss: 0.331(0.355), train_acc: 90.625(87.516)
01/18 08:48:04 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 501/521, train_loss: 0.359(0.351), train_acc: 86.458(87.648)
01/18 08:48:07 PM [Supernet Training] lr: 0.02183 epoch: 140/600, step: 521/521, train_loss: 0.261(0.350), train_acc: 88.750(87.676)
01/18 08:48:07 PM [Supernet Training] epoch: 140, train_loss: 0.350, train_acc: 87.676
01/18 08:48:10 PM [Supernet Validation] epoch: 140, val_loss: 0.495, val_acc: 83.910, best_acc: 84.420
01/18 08:48:10 PM 

01/18 08:48:11 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 001/521, train_loss: 0.320(0.320), train_acc: 89.583(89.583)
01/18 08:48:24 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 101/521, train_loss: 0.355(0.338), train_acc: 87.500(88.274)
01/18 08:48:36 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 201/521, train_loss: 0.383(0.336), train_acc: 85.417(88.127)
01/18 08:48:49 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 301/521, train_loss: 0.331(0.340), train_acc: 90.625(87.926)
01/18 08:49:02 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 401/521, train_loss: 0.345(0.344), train_acc: 89.583(87.716)
01/18 08:49:15 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 501/521, train_loss: 0.318(0.344), train_acc: 88.542(87.747)
01/18 08:49:17 PM [Supernet Training] lr: 0.02179 epoch: 141/600, step: 521/521, train_loss: 0.199(0.344), train_acc: 93.750(87.764)
01/18 08:49:17 PM [Supernet Training] epoch: 141, train_loss: 0.344, train_acc: 87.764
01/18 08:49:21 PM [Supernet Validation] epoch: 141, val_loss: 0.469, val_acc: 84.420, best_acc: 84.420
01/18 08:49:21 PM 

01/18 08:49:21 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 001/521, train_loss: 0.426(0.426), train_acc: 85.417(85.417)
01/18 08:49:34 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 101/521, train_loss: 0.442(0.346), train_acc: 83.333(87.820)
01/18 08:49:47 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 201/521, train_loss: 0.332(0.348), train_acc: 89.583(87.604)
01/18 08:50:00 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 301/521, train_loss: 0.433(0.350), train_acc: 86.458(87.587)
01/18 08:50:12 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 401/521, train_loss: 0.389(0.346), train_acc: 84.375(87.697)
01/18 08:50:25 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 501/521, train_loss: 0.263(0.346), train_acc: 88.542(87.756)
01/18 08:50:28 PM [Supernet Training] lr: 0.02175 epoch: 142/600, step: 521/521, train_loss: 0.377(0.347), train_acc: 83.750(87.708)
01/18 08:50:28 PM [Supernet Training] epoch: 142, train_loss: 0.347, train_acc: 87.708
01/18 08:50:31 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:50:31 PM [Supernet Validation] epoch: 142, val_loss: 0.479, val_acc: 84.420, best_acc: 84.420
01/18 08:50:31 PM 

01/18 08:50:32 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 001/521, train_loss: 0.452(0.452), train_acc: 80.208(80.208)
01/18 08:50:45 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 101/521, train_loss: 0.347(0.342), train_acc: 88.542(88.005)
01/18 08:50:57 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 201/521, train_loss: 0.300(0.343), train_acc: 92.708(87.977)
01/18 08:51:10 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 301/521, train_loss: 0.316(0.345), train_acc: 86.458(87.891)
01/18 08:51:23 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 401/521, train_loss: 0.306(0.344), train_acc: 86.458(87.921)
01/18 08:51:36 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 501/521, train_loss: 0.416(0.344), train_acc: 82.292(87.943)
01/18 08:51:38 PM [Supernet Training] lr: 0.02170 epoch: 143/600, step: 521/521, train_loss: 0.365(0.344), train_acc: 87.500(87.952)
01/18 08:51:38 PM [Supernet Training] epoch: 143, train_loss: 0.344, train_acc: 87.952
01/18 08:51:42 PM [Supernet Validation] epoch: 143, val_loss: 0.486, val_acc: 84.150, best_acc: 84.420
01/18 08:51:42 PM 

01/18 08:51:42 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 001/521, train_loss: 0.350(0.350), train_acc: 88.542(88.542)
01/18 08:51:55 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 101/521, train_loss: 0.346(0.339), train_acc: 85.417(87.954)
01/18 08:52:08 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 201/521, train_loss: 0.328(0.342), train_acc: 88.542(87.785)
01/18 08:52:20 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 301/521, train_loss: 0.297(0.342), train_acc: 90.625(87.888)
01/18 08:52:33 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 401/521, train_loss: 0.267(0.340), train_acc: 92.708(87.952)
01/18 08:52:46 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 501/521, train_loss: 0.338(0.340), train_acc: 90.625(88.024)
01/18 08:52:48 PM [Supernet Training] lr: 0.02166 epoch: 144/600, step: 521/521, train_loss: 0.300(0.339), train_acc: 88.750(88.040)
01/18 08:52:48 PM [Supernet Training] epoch: 144, train_loss: 0.339, train_acc: 88.040
01/18 08:52:52 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:52:52 PM [Supernet Validation] epoch: 144, val_loss: 0.483, val_acc: 84.470, best_acc: 84.470
01/18 08:52:52 PM 

01/18 08:52:53 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 001/521, train_loss: 0.444(0.444), train_acc: 84.375(84.375)
01/18 08:53:05 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 101/521, train_loss: 0.398(0.329), train_acc: 85.417(88.573)
01/18 08:53:18 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 201/521, train_loss: 0.303(0.338), train_acc: 90.625(88.355)
01/18 08:53:31 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 301/521, train_loss: 0.302(0.338), train_acc: 88.542(88.151)
01/18 08:53:44 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 401/521, train_loss: 0.261(0.340), train_acc: 93.750(88.066)
01/18 08:53:56 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 501/521, train_loss: 0.360(0.338), train_acc: 83.333(88.140)
01/18 08:53:59 PM [Supernet Training] lr: 0.02161 epoch: 145/600, step: 521/521, train_loss: 0.171(0.339), train_acc: 95.000(88.118)
01/18 08:53:59 PM [Supernet Training] epoch: 145, train_loss: 0.339, train_acc: 88.118
01/18 08:54:03 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:54:03 PM [Supernet Validation] epoch: 145, val_loss: 0.465, val_acc: 84.840, best_acc: 84.840
01/18 08:54:03 PM 

01/18 08:54:03 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 001/521, train_loss: 0.439(0.439), train_acc: 83.333(83.333)
01/18 08:54:16 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 101/521, train_loss: 0.240(0.340), train_acc: 91.667(88.057)
01/18 08:54:29 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 201/521, train_loss: 0.291(0.343), train_acc: 91.667(88.174)
01/18 08:54:41 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 301/521, train_loss: 0.343(0.340), train_acc: 84.375(88.313)
01/18 08:54:54 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 401/521, train_loss: 0.376(0.339), train_acc: 85.417(88.243)
01/18 08:55:07 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 501/521, train_loss: 0.425(0.339), train_acc: 86.458(88.194)
01/18 08:55:10 PM [Supernet Training] lr: 0.02157 epoch: 146/600, step: 521/521, train_loss: 0.439(0.340), train_acc: 85.000(88.148)
01/18 08:55:10 PM [Supernet Training] epoch: 146, train_loss: 0.340, train_acc: 88.148
01/18 08:55:13 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:55:13 PM [Supernet Validation] epoch: 146, val_loss: 0.456, val_acc: 84.900, best_acc: 84.900
01/18 08:55:13 PM 

01/18 08:55:14 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 001/521, train_loss: 0.331(0.331), train_acc: 87.500(87.500)
01/18 08:55:26 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 101/521, train_loss: 0.441(0.326), train_acc: 78.125(88.408)
01/18 08:55:39 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 201/521, train_loss: 0.497(0.329), train_acc: 82.292(88.360)
01/18 08:55:52 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 301/521, train_loss: 0.217(0.335), train_acc: 91.667(88.130)
01/18 08:56:05 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 401/521, train_loss: 0.301(0.334), train_acc: 85.417(88.222)
01/18 08:56:18 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 501/521, train_loss: 0.497(0.335), train_acc: 83.333(88.145)
01/18 08:56:20 PM [Supernet Training] lr: 0.02152 epoch: 147/600, step: 521/521, train_loss: 0.358(0.335), train_acc: 87.500(88.114)
01/18 08:56:20 PM [Supernet Training] epoch: 147, train_loss: 0.335, train_acc: 88.114
01/18 08:56:24 PM [Supernet Validation] epoch: 147, val_loss: 0.472, val_acc: 84.640, best_acc: 84.900
01/18 08:56:24 PM 

01/18 08:56:24 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 001/521, train_loss: 0.410(0.410), train_acc: 87.500(87.500)
01/18 08:56:37 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 101/521, train_loss: 0.439(0.330), train_acc: 85.417(88.335)
01/18 08:56:50 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 201/521, train_loss: 0.290(0.334), train_acc: 90.625(88.376)
01/18 08:57:03 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 301/521, train_loss: 0.334(0.337), train_acc: 87.500(88.248)
01/18 08:57:16 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 401/521, train_loss: 0.343(0.336), train_acc: 89.583(88.383)
01/18 08:57:28 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 501/521, train_loss: 0.377(0.335), train_acc: 86.458(88.325)
01/18 08:57:31 PM [Supernet Training] lr: 0.02148 epoch: 148/600, step: 521/521, train_loss: 0.343(0.333), train_acc: 85.000(88.374)
01/18 08:57:31 PM [Supernet Training] epoch: 148, train_loss: 0.333, train_acc: 88.374
01/18 08:57:35 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 08:57:35 PM [Supernet Validation] epoch: 148, val_loss: 0.464, val_acc: 85.130, best_acc: 85.130
01/18 08:57:35 PM 

01/18 08:57:35 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 001/521, train_loss: 0.338(0.338), train_acc: 88.542(88.542)
01/18 08:57:48 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 101/521, train_loss: 0.368(0.315), train_acc: 88.542(89.006)
01/18 08:58:01 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 201/521, train_loss: 0.310(0.322), train_acc: 91.667(88.521)
01/18 08:58:13 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 301/521, train_loss: 0.336(0.328), train_acc: 88.542(88.341)
01/18 08:58:26 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 401/521, train_loss: 0.295(0.331), train_acc: 89.583(88.227)
01/18 08:58:39 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 501/521, train_loss: 0.414(0.333), train_acc: 83.333(88.132)
01/18 08:58:41 PM [Supernet Training] lr: 0.02143 epoch: 149/600, step: 521/521, train_loss: 0.315(0.335), train_acc: 90.000(88.090)
01/18 08:58:41 PM [Supernet Training] epoch: 149, train_loss: 0.335, train_acc: 88.090
01/18 08:58:45 PM [Supernet Validation] epoch: 149, val_loss: 0.470, val_acc: 84.590, best_acc: 85.130
01/18 08:58:45 PM 

01/18 08:58:45 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 001/521, train_loss: 0.344(0.344), train_acc: 90.625(90.625)
01/18 08:58:58 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 101/521, train_loss: 0.355(0.331), train_acc: 90.625(88.779)
01/18 08:59:11 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 201/521, train_loss: 0.320(0.329), train_acc: 88.542(88.578)
01/18 08:59:24 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 301/521, train_loss: 0.276(0.324), train_acc: 93.750(88.704)
01/18 08:59:37 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 401/521, train_loss: 0.281(0.329), train_acc: 90.625(88.544)
01/18 08:59:50 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 501/521, train_loss: 0.302(0.333), train_acc: 88.542(88.367)
01/18 08:59:52 PM [Supernet Training] lr: 0.02138 epoch: 150/600, step: 521/521, train_loss: 0.489(0.333), train_acc: 86.250(88.398)
01/18 08:59:52 PM [Supernet Training] epoch: 150, train_loss: 0.333, train_acc: 88.398
01/18 08:59:56 PM [Supernet Validation] epoch: 150, val_loss: 0.472, val_acc: 84.860, best_acc: 85.130
01/18 08:59:56 PM 

01/18 08:59:56 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 001/521, train_loss: 0.219(0.219), train_acc: 93.750(93.750)
01/18 09:00:09 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 101/521, train_loss: 0.372(0.310), train_acc: 88.542(88.923)
01/18 09:00:22 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 201/521, train_loss: 0.312(0.315), train_acc: 87.500(88.853)
01/18 09:00:35 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 301/521, train_loss: 0.193(0.320), train_acc: 92.708(88.576)
01/18 09:00:47 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 401/521, train_loss: 0.516(0.324), train_acc: 79.167(88.479)
01/18 09:01:00 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 501/521, train_loss: 0.403(0.327), train_acc: 86.458(88.434)
01/18 09:01:03 PM [Supernet Training] lr: 0.02134 epoch: 151/600, step: 521/521, train_loss: 0.302(0.328), train_acc: 88.750(88.404)
01/18 09:01:03 PM [Supernet Training] epoch: 151, train_loss: 0.328, train_acc: 88.404
01/18 09:01:06 PM [Supernet Validation] epoch: 151, val_loss: 0.475, val_acc: 85.080, best_acc: 85.130
01/18 09:01:06 PM 

01/18 09:01:07 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 001/521, train_loss: 0.345(0.345), train_acc: 89.583(89.583)
01/18 09:01:20 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 101/521, train_loss: 0.303(0.332), train_acc: 86.458(88.304)
01/18 09:01:32 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 201/521, train_loss: 0.235(0.336), train_acc: 92.708(88.060)
01/18 09:01:45 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 301/521, train_loss: 0.232(0.330), train_acc: 92.708(88.386)
01/18 09:01:58 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 401/521, train_loss: 0.228(0.328), train_acc: 91.667(88.427)
01/18 09:02:11 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 501/521, train_loss: 0.337(0.328), train_acc: 90.625(88.488)
01/18 09:02:13 PM [Supernet Training] lr: 0.02129 epoch: 152/600, step: 521/521, train_loss: 0.333(0.328), train_acc: 88.750(88.472)
01/18 09:02:14 PM [Supernet Training] epoch: 152, train_loss: 0.328, train_acc: 88.472
01/18 09:02:17 PM [Supernet Validation] epoch: 152, val_loss: 0.475, val_acc: 84.650, best_acc: 85.130
01/18 09:02:17 PM 

01/18 09:02:18 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 001/521, train_loss: 0.313(0.313), train_acc: 89.583(89.583)
01/18 09:02:30 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 101/521, train_loss: 0.321(0.320), train_acc: 89.583(88.490)
01/18 09:02:43 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 201/521, train_loss: 0.401(0.327), train_acc: 86.458(88.314)
01/18 09:02:56 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 301/521, train_loss: 0.246(0.329), train_acc: 89.583(88.268)
01/18 09:03:09 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 401/521, train_loss: 0.514(0.330), train_acc: 83.333(88.274)
01/18 09:03:22 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 501/521, train_loss: 0.496(0.329), train_acc: 81.250(88.246)
01/18 09:03:24 PM [Supernet Training] lr: 0.02125 epoch: 153/600, step: 521/521, train_loss: 0.241(0.329), train_acc: 91.250(88.252)
01/18 09:03:24 PM [Supernet Training] epoch: 153, train_loss: 0.329, train_acc: 88.252
01/18 09:03:28 PM [Supernet Validation] epoch: 153, val_loss: 0.474, val_acc: 84.640, best_acc: 85.130
01/18 09:03:28 PM 

01/18 09:03:28 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 001/521, train_loss: 0.248(0.248), train_acc: 90.625(90.625)
01/18 09:03:41 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 101/521, train_loss: 0.377(0.339), train_acc: 91.667(87.778)
01/18 09:03:54 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 201/521, train_loss: 0.357(0.324), train_acc: 87.500(88.464)
01/18 09:04:06 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 301/521, train_loss: 0.527(0.322), train_acc: 87.500(88.545)
01/18 09:04:19 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 401/521, train_loss: 0.159(0.323), train_acc: 93.750(88.612)
01/18 09:04:32 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 501/521, train_loss: 0.450(0.324), train_acc: 85.417(88.598)
01/18 09:04:35 PM [Supernet Training] lr: 0.02120 epoch: 154/600, step: 521/521, train_loss: 0.304(0.324), train_acc: 90.000(88.606)
01/18 09:04:35 PM [Supernet Training] epoch: 154, train_loss: 0.324, train_acc: 88.606
01/18 09:04:38 PM [Supernet Validation] epoch: 154, val_loss: 0.490, val_acc: 84.080, best_acc: 85.130
01/18 09:04:38 PM 

01/18 09:04:39 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 001/521, train_loss: 0.306(0.306), train_acc: 89.583(89.583)
01/18 09:04:52 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 101/521, train_loss: 0.275(0.332), train_acc: 90.625(88.139)
01/18 09:05:04 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 201/521, train_loss: 0.251(0.321), train_acc: 90.625(88.728)
01/18 09:05:17 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 301/521, train_loss: 0.259(0.325), train_acc: 90.625(88.511)
01/18 09:05:30 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 401/521, train_loss: 0.246(0.323), train_acc: 91.667(88.578)
01/18 09:05:43 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 501/521, train_loss: 0.322(0.325), train_acc: 87.500(88.608)
01/18 09:05:45 PM [Supernet Training] lr: 0.02115 epoch: 155/600, step: 521/521, train_loss: 0.302(0.326), train_acc: 90.000(88.598)
01/18 09:05:45 PM [Supernet Training] epoch: 155, train_loss: 0.326, train_acc: 88.598
01/18 09:05:49 PM [Supernet Validation] epoch: 155, val_loss: 0.475, val_acc: 84.810, best_acc: 85.130
01/18 09:05:49 PM 

01/18 09:05:49 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 001/521, train_loss: 0.342(0.342), train_acc: 85.417(85.417)
01/18 09:06:02 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 101/521, train_loss: 0.352(0.328), train_acc: 87.500(88.449)
01/18 09:06:15 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 201/521, train_loss: 0.511(0.327), train_acc: 86.458(88.511)
01/18 09:06:27 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 301/521, train_loss: 0.276(0.323), train_acc: 88.542(88.618)
01/18 09:06:40 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 401/521, train_loss: 0.378(0.326), train_acc: 84.375(88.547)
01/18 09:06:53 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 501/521, train_loss: 0.275(0.324), train_acc: 89.583(88.581)
01/18 09:06:56 PM [Supernet Training] lr: 0.02110 epoch: 156/600, step: 521/521, train_loss: 0.378(0.324), train_acc: 81.250(88.572)
01/18 09:06:56 PM [Supernet Training] epoch: 156, train_loss: 0.324, train_acc: 88.572
01/18 09:06:59 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 09:06:59 PM [Supernet Validation] epoch: 156, val_loss: 0.447, val_acc: 85.650, best_acc: 85.650
01/18 09:06:59 PM 

01/18 09:07:00 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 001/521, train_loss: 0.195(0.195), train_acc: 90.625(90.625)
01/18 09:07:13 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 101/521, train_loss: 0.301(0.315), train_acc: 83.333(88.965)
01/18 09:07:25 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 201/521, train_loss: 0.411(0.315), train_acc: 85.417(88.998)
01/18 09:07:38 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 301/521, train_loss: 0.172(0.319), train_acc: 94.792(88.843)
01/18 09:07:51 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 401/521, train_loss: 0.295(0.320), train_acc: 90.625(88.734)
01/18 09:08:04 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 501/521, train_loss: 0.377(0.320), train_acc: 87.500(88.772)
01/18 09:08:06 PM [Supernet Training] lr: 0.02106 epoch: 157/600, step: 521/521, train_loss: 0.318(0.319), train_acc: 86.250(88.772)
01/18 09:08:06 PM [Supernet Training] epoch: 157, train_loss: 0.319, train_acc: 88.772
01/18 09:08:10 PM [Supernet Validation] epoch: 157, val_loss: 0.464, val_acc: 84.840, best_acc: 85.650
01/18 09:08:10 PM 

01/18 09:08:10 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 001/521, train_loss: 0.405(0.405), train_acc: 86.458(86.458)
01/18 09:08:23 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 101/521, train_loss: 0.392(0.303), train_acc: 84.375(89.243)
01/18 09:08:36 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 201/521, train_loss: 0.192(0.316), train_acc: 93.750(88.946)
01/18 09:08:49 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 301/521, train_loss: 0.304(0.316), train_acc: 89.583(88.819)
01/18 09:09:01 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 401/521, train_loss: 0.255(0.317), train_acc: 87.500(88.900)
01/18 09:09:14 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 501/521, train_loss: 0.180(0.316), train_acc: 97.917(88.930)
01/18 09:09:17 PM [Supernet Training] lr: 0.02101 epoch: 158/600, step: 521/521, train_loss: 0.385(0.317), train_acc: 85.000(88.888)
01/18 09:09:17 PM [Supernet Training] epoch: 158, train_loss: 0.317, train_acc: 88.888
01/18 09:09:21 PM [Supernet Validation] epoch: 158, val_loss: 0.452, val_acc: 85.460, best_acc: 85.650
01/18 09:09:21 PM 

01/18 09:09:21 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 001/521, train_loss: 0.311(0.311), train_acc: 86.458(86.458)
01/18 09:09:34 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 101/521, train_loss: 0.324(0.311), train_acc: 89.583(88.882)
01/18 09:09:47 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 201/521, train_loss: 0.555(0.318), train_acc: 82.292(88.542)
01/18 09:09:59 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 301/521, train_loss: 0.299(0.317), train_acc: 89.583(88.587)
01/18 09:10:12 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 401/521, train_loss: 0.330(0.317), train_acc: 88.542(88.633)
01/18 09:10:25 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 501/521, train_loss: 0.248(0.319), train_acc: 91.667(88.671)
01/18 09:10:27 PM [Supernet Training] lr: 0.02096 epoch: 159/600, step: 521/521, train_loss: 0.291(0.319), train_acc: 87.500(88.706)
01/18 09:10:27 PM [Supernet Training] epoch: 159, train_loss: 0.319, train_acc: 88.706
01/18 09:10:31 PM [Supernet Validation] epoch: 159, val_loss: 0.459, val_acc: 84.820, best_acc: 85.650
01/18 09:10:31 PM 

01/18 09:10:31 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 001/521, train_loss: 0.445(0.445), train_acc: 88.542(88.542)
01/18 09:10:44 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 101/521, train_loss: 0.324(0.308), train_acc: 89.583(89.470)
01/18 09:10:57 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 201/521, train_loss: 0.409(0.312), train_acc: 87.500(89.226)
01/18 09:11:10 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 301/521, train_loss: 0.410(0.314), train_acc: 86.458(89.030)
01/18 09:11:23 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 401/521, train_loss: 0.317(0.318), train_acc: 89.583(88.856)
01/18 09:11:35 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 501/521, train_loss: 0.295(0.318), train_acc: 89.583(88.881)
01/18 09:11:38 PM [Supernet Training] lr: 0.02091 epoch: 160/600, step: 521/521, train_loss: 0.238(0.319), train_acc: 92.500(88.852)
01/18 09:11:38 PM [Supernet Training] epoch: 160, train_loss: 0.319, train_acc: 88.852
01/18 09:11:42 PM [Supernet Validation] epoch: 160, val_loss: 0.447, val_acc: 85.090, best_acc: 85.650
01/18 09:11:42 PM 

01/18 09:11:42 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 001/521, train_loss: 0.225(0.225), train_acc: 91.667(91.667)
01/18 09:11:55 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 101/521, train_loss: 0.311(0.296), train_acc: 88.542(89.367)
01/18 09:12:07 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 201/521, train_loss: 0.306(0.309), train_acc: 87.500(89.008)
01/18 09:12:20 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 301/521, train_loss: 0.576(0.316), train_acc: 80.208(88.749)
01/18 09:12:33 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 401/521, train_loss: 0.215(0.314), train_acc: 90.625(88.835)
01/18 09:12:46 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 501/521, train_loss: 0.257(0.313), train_acc: 90.625(88.933)
01/18 09:12:48 PM [Supernet Training] lr: 0.02086 epoch: 161/600, step: 521/521, train_loss: 0.255(0.314), train_acc: 87.500(88.882)
01/18 09:12:49 PM [Supernet Training] epoch: 161, train_loss: 0.314, train_acc: 88.882
01/18 09:12:52 PM [Supernet Validation] epoch: 161, val_loss: 0.450, val_acc: 85.500, best_acc: 85.650
01/18 09:12:52 PM 

01/18 09:12:52 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 001/521, train_loss: 0.127(0.127), train_acc: 95.833(95.833)
01/18 09:13:05 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 101/521, train_loss: 0.265(0.307), train_acc: 89.583(89.181)
01/18 09:13:18 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 201/521, train_loss: 0.095(0.310), train_acc: 96.875(88.946)
01/18 09:13:31 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 301/521, train_loss: 0.205(0.311), train_acc: 93.750(88.974)
01/18 09:13:44 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 401/521, train_loss: 0.320(0.309), train_acc: 86.458(89.046)
01/18 09:13:56 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 501/521, train_loss: 0.282(0.312), train_acc: 90.625(88.995)
01/18 09:13:59 PM [Supernet Training] lr: 0.02082 epoch: 162/600, step: 521/521, train_loss: 0.296(0.313), train_acc: 87.500(88.986)
01/18 09:13:59 PM [Supernet Training] epoch: 162, train_loss: 0.313, train_acc: 88.986
01/18 09:14:03 PM [Supernet Validation] epoch: 162, val_loss: 0.451, val_acc: 85.290, best_acc: 85.650
01/18 09:14:03 PM 

01/18 09:14:03 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 001/521, train_loss: 0.216(0.216), train_acc: 92.708(92.708)
01/18 09:14:16 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 101/521, train_loss: 0.217(0.308), train_acc: 93.750(88.965)
01/18 09:14:28 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 201/521, train_loss: 0.431(0.307), train_acc: 82.292(89.070)
01/18 09:14:41 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 301/521, train_loss: 0.363(0.305), train_acc: 83.333(89.133)
01/18 09:14:54 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 401/521, train_loss: 0.327(0.309), train_acc: 89.583(89.064)
01/18 09:15:07 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 501/521, train_loss: 0.435(0.312), train_acc: 90.625(88.991)
01/18 09:15:09 PM [Supernet Training] lr: 0.02077 epoch: 163/600, step: 521/521, train_loss: 0.273(0.313), train_acc: 88.750(88.946)
01/18 09:15:09 PM [Supernet Training] epoch: 163, train_loss: 0.313, train_acc: 88.946
01/18 09:15:13 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 09:15:13 PM [Supernet Validation] epoch: 163, val_loss: 0.438, val_acc: 85.730, best_acc: 85.730
01/18 09:15:13 PM 

01/18 09:15:13 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 001/521, train_loss: 0.195(0.195), train_acc: 92.708(92.708)
01/18 09:15:26 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 101/521, train_loss: 0.411(0.299), train_acc: 84.375(89.666)
01/18 09:15:39 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 201/521, train_loss: 0.295(0.308), train_acc: 90.625(89.189)
01/18 09:15:52 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 301/521, train_loss: 0.353(0.311), train_acc: 86.458(89.172)
01/18 09:16:04 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 401/521, train_loss: 0.254(0.305), train_acc: 93.750(89.324)
01/18 09:16:17 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 501/521, train_loss: 0.342(0.306), train_acc: 88.542(89.232)
01/18 09:16:20 PM [Supernet Training] lr: 0.02072 epoch: 164/600, step: 521/521, train_loss: 0.407(0.306), train_acc: 86.250(89.230)
01/18 09:16:20 PM [Supernet Training] epoch: 164, train_loss: 0.306, train_acc: 89.230
01/18 09:16:23 PM [Supernet Validation] epoch: 164, val_loss: 0.451, val_acc: 85.320, best_acc: 85.730
01/18 09:16:23 PM 

01/18 09:16:24 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 001/521, train_loss: 0.413(0.413), train_acc: 81.250(81.250)
01/18 09:16:37 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 101/521, train_loss: 0.304(0.312), train_acc: 87.500(89.078)
01/18 09:16:49 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 201/521, train_loss: 0.348(0.311), train_acc: 81.250(89.169)
01/18 09:17:02 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 301/521, train_loss: 0.332(0.310), train_acc: 87.500(89.147)
01/18 09:17:15 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 401/521, train_loss: 0.261(0.309), train_acc: 89.583(89.157)
01/18 09:17:28 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 501/521, train_loss: 0.313(0.311), train_acc: 87.500(89.105)
01/18 09:17:30 PM [Supernet Training] lr: 0.02067 epoch: 165/600, step: 521/521, train_loss: 0.252(0.311), train_acc: 91.250(89.092)
01/18 09:17:30 PM [Supernet Training] epoch: 165, train_loss: 0.311, train_acc: 89.092
01/18 09:17:34 PM [Supernet Validation] epoch: 165, val_loss: 0.468, val_acc: 84.870, best_acc: 85.730
01/18 09:17:34 PM 

01/18 09:17:34 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 001/521, train_loss: 0.400(0.400), train_acc: 83.333(83.333)
01/18 09:17:47 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 101/521, train_loss: 0.344(0.311), train_acc: 88.542(88.975)
01/18 09:18:00 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 201/521, train_loss: 0.326(0.308), train_acc: 87.500(89.039)
01/18 09:18:13 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 301/521, train_loss: 0.193(0.310), train_acc: 93.750(88.985)
01/18 09:18:25 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 401/521, train_loss: 0.372(0.309), train_acc: 85.417(89.022)
01/18 09:18:38 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 501/521, train_loss: 0.251(0.309), train_acc: 90.625(89.009)
01/18 09:18:41 PM [Supernet Training] lr: 0.02062 epoch: 166/600, step: 521/521, train_loss: 0.208(0.309), train_acc: 91.250(89.006)
01/18 09:18:41 PM [Supernet Training] epoch: 166, train_loss: 0.309, train_acc: 89.006
01/18 09:18:44 PM [Supernet Validation] epoch: 166, val_loss: 0.464, val_acc: 85.340, best_acc: 85.730
01/18 09:18:44 PM 

01/18 09:18:45 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 001/521, train_loss: 0.319(0.319), train_acc: 87.500(87.500)
01/18 09:18:58 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 101/521, train_loss: 0.340(0.304), train_acc: 87.500(89.264)
01/18 09:19:10 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 201/521, train_loss: 0.262(0.302), train_acc: 90.625(89.314)
01/18 09:19:23 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 301/521, train_loss: 0.347(0.302), train_acc: 89.583(89.362)
01/18 09:19:36 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 401/521, train_loss: 0.367(0.302), train_acc: 89.583(89.331)
01/18 09:19:49 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 501/521, train_loss: 0.225(0.303), train_acc: 90.625(89.336)
01/18 09:19:51 PM [Supernet Training] lr: 0.02057 epoch: 167/600, step: 521/521, train_loss: 0.424(0.302), train_acc: 87.500(89.336)
01/18 09:19:51 PM [Supernet Training] epoch: 167, train_loss: 0.302, train_acc: 89.336
01/18 09:19:55 PM [Supernet Validation] epoch: 167, val_loss: 0.441, val_acc: 85.690, best_acc: 85.730
01/18 09:19:55 PM 

01/18 09:19:55 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 001/521, train_loss: 0.295(0.295), train_acc: 87.500(87.500)
01/18 09:20:08 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 101/521, train_loss: 0.288(0.313), train_acc: 92.708(89.078)
01/18 09:20:21 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 201/521, train_loss: 0.421(0.309), train_acc: 80.208(89.029)
01/18 09:20:33 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 301/521, train_loss: 0.382(0.305), train_acc: 87.500(89.185)
01/18 09:20:46 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 401/521, train_loss: 0.307(0.306), train_acc: 88.542(89.194)
01/18 09:20:59 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 501/521, train_loss: 0.323(0.303), train_acc: 87.500(89.176)
01/18 09:21:02 PM [Supernet Training] lr: 0.02052 epoch: 168/600, step: 521/521, train_loss: 0.215(0.305), train_acc: 88.750(89.104)
01/18 09:21:02 PM [Supernet Training] epoch: 168, train_loss: 0.305, train_acc: 89.104
01/18 09:21:05 PM [Supernet Validation] epoch: 168, val_loss: 0.466, val_acc: 85.320, best_acc: 85.730
01/18 09:21:05 PM 

01/18 09:21:06 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 001/521, train_loss: 0.348(0.348), train_acc: 88.542(88.542)
01/18 09:21:19 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 101/521, train_loss: 0.346(0.296), train_acc: 90.625(89.491)
01/18 09:21:31 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 201/521, train_loss: 0.315(0.301), train_acc: 87.500(89.293)
01/18 09:21:44 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 301/521, train_loss: 0.379(0.301), train_acc: 85.417(89.275)
01/18 09:21:57 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 401/521, train_loss: 0.341(0.302), train_acc: 91.667(89.305)
01/18 09:22:09 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 501/521, train_loss: 0.361(0.303), train_acc: 85.417(89.301)
01/18 09:22:12 PM [Supernet Training] lr: 0.02047 epoch: 169/600, step: 521/521, train_loss: 0.344(0.303), train_acc: 88.750(89.320)
01/18 09:22:12 PM [Supernet Training] epoch: 169, train_loss: 0.303, train_acc: 89.320
01/18 09:22:16 PM [Supernet Validation] epoch: 169, val_loss: 0.468, val_acc: 85.330, best_acc: 85.730
01/18 09:22:16 PM 

01/18 09:22:16 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 87.500(87.500)
01/18 09:22:29 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 101/521, train_loss: 0.350(0.300), train_acc: 89.583(89.439)
01/18 09:22:42 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 201/521, train_loss: 0.300(0.301), train_acc: 88.542(89.293)
01/18 09:22:54 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 301/521, train_loss: 0.243(0.298), train_acc: 90.625(89.466)
01/18 09:23:07 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 401/521, train_loss: 0.321(0.298), train_acc: 89.583(89.500)
01/18 09:23:20 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 501/521, train_loss: 0.314(0.297), train_acc: 88.542(89.533)
01/18 09:23:23 PM [Supernet Training] lr: 0.02042 epoch: 170/600, step: 521/521, train_loss: 0.363(0.298), train_acc: 90.000(89.522)
01/18 09:23:23 PM [Supernet Training] epoch: 170, train_loss: 0.298, train_acc: 89.522
01/18 09:23:26 PM [Supernet Validation] epoch: 170, val_loss: 0.449, val_acc: 85.640, best_acc: 85.730
01/18 09:23:26 PM 

01/18 09:23:27 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 001/521, train_loss: 0.183(0.183), train_acc: 94.792(94.792)
01/18 09:23:40 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 101/521, train_loss: 0.281(0.291), train_acc: 90.625(89.810)
01/18 09:23:52 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 201/521, train_loss: 0.264(0.292), train_acc: 88.542(89.827)
01/18 09:24:05 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 301/521, train_loss: 0.422(0.297), train_acc: 86.458(89.573)
01/18 09:24:18 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 401/521, train_loss: 0.281(0.298), train_acc: 93.750(89.560)
01/18 09:24:31 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 501/521, train_loss: 0.356(0.300), train_acc: 86.458(89.473)
01/18 09:24:33 PM [Supernet Training] lr: 0.02037 epoch: 171/600, step: 521/521, train_loss: 0.192(0.300), train_acc: 92.500(89.480)
01/18 09:24:33 PM [Supernet Training] epoch: 171, train_loss: 0.300, train_acc: 89.480
01/18 09:24:37 PM [Supernet Validation] epoch: 171, val_loss: 0.444, val_acc: 85.640, best_acc: 85.730
01/18 09:24:37 PM 

01/18 09:24:37 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 001/521, train_loss: 0.195(0.195), train_acc: 92.708(92.708)
01/18 09:24:50 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 101/521, train_loss: 0.246(0.297), train_acc: 90.625(89.470)
01/18 09:25:03 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 201/521, train_loss: 0.223(0.299), train_acc: 91.667(89.526)
01/18 09:25:16 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 301/521, train_loss: 0.449(0.296), train_acc: 84.375(89.580)
01/18 09:25:29 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 401/521, train_loss: 0.506(0.297), train_acc: 84.375(89.552)
01/18 09:25:41 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 501/521, train_loss: 0.356(0.300), train_acc: 87.500(89.461)
01/18 09:25:44 PM [Supernet Training] lr: 0.02032 epoch: 172/600, step: 521/521, train_loss: 0.223(0.299), train_acc: 91.250(89.492)
01/18 09:25:44 PM [Supernet Training] epoch: 172, train_loss: 0.299, train_acc: 89.492
01/18 09:25:48 PM [Supernet Validation] epoch: 172, val_loss: 0.466, val_acc: 84.880, best_acc: 85.730
01/18 09:25:48 PM 

01/18 09:25:48 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 001/521, train_loss: 0.266(0.266), train_acc: 92.708(92.708)
01/18 09:26:01 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 101/521, train_loss: 0.190(0.276), train_acc: 91.667(90.047)
01/18 09:26:13 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 201/521, train_loss: 0.361(0.283), train_acc: 88.542(89.941)
01/18 09:26:26 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 301/521, train_loss: 0.159(0.293), train_acc: 94.792(89.608)
01/18 09:26:39 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 401/521, train_loss: 0.239(0.293), train_acc: 90.625(89.664)
01/18 09:26:52 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 501/521, train_loss: 0.273(0.295), train_acc: 86.458(89.583)
01/18 09:26:54 PM [Supernet Training] lr: 0.02026 epoch: 173/600, step: 521/521, train_loss: 0.305(0.295), train_acc: 87.500(89.594)
01/18 09:26:54 PM [Supernet Training] epoch: 173, train_loss: 0.295, train_acc: 89.594
01/18 09:26:58 PM [Supernet Validation] epoch: 173, val_loss: 0.448, val_acc: 85.620, best_acc: 85.730
01/18 09:26:58 PM 

01/18 09:26:58 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 001/521, train_loss: 0.254(0.254), train_acc: 89.583(89.583)
01/18 09:27:11 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 101/521, train_loss: 0.261(0.291), train_acc: 90.625(89.542)
01/18 09:27:24 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 201/521, train_loss: 0.348(0.290), train_acc: 87.500(89.625)
01/18 09:27:37 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 301/521, train_loss: 0.411(0.294), train_acc: 87.500(89.524)
01/18 09:27:49 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 401/521, train_loss: 0.468(0.300), train_acc: 89.583(89.378)
01/18 09:28:02 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 501/521, train_loss: 0.364(0.297), train_acc: 85.417(89.513)
01/18 09:28:05 PM [Supernet Training] lr: 0.02021 epoch: 174/600, step: 521/521, train_loss: 0.296(0.297), train_acc: 88.750(89.530)
01/18 09:28:05 PM [Supernet Training] epoch: 174, train_loss: 0.297, train_acc: 89.530
01/18 09:28:08 PM [Supernet Validation] epoch: 174, val_loss: 0.461, val_acc: 85.510, best_acc: 85.730
01/18 09:28:08 PM 

01/18 09:28:09 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 001/521, train_loss: 0.330(0.330), train_acc: 88.542(88.542)
01/18 09:28:22 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 101/521, train_loss: 0.263(0.281), train_acc: 92.708(89.893)
01/18 09:28:34 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 201/521, train_loss: 0.301(0.292), train_acc: 92.708(89.609)
01/18 09:28:47 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 301/521, train_loss: 0.333(0.288), train_acc: 89.583(89.718)
01/18 09:29:00 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 401/521, train_loss: 0.247(0.293), train_acc: 89.583(89.633)
01/18 09:29:13 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 501/521, train_loss: 0.229(0.295), train_acc: 91.667(89.552)
01/18 09:29:16 PM [Supernet Training] lr: 0.02016 epoch: 175/600, step: 521/521, train_loss: 0.323(0.294), train_acc: 90.000(89.566)
01/18 09:29:16 PM [Supernet Training] epoch: 175, train_loss: 0.294, train_acc: 89.566
01/18 09:29:19 PM [Supernet Validation] epoch: 175, val_loss: 0.437, val_acc: 85.590, best_acc: 85.730
01/18 09:29:19 PM 

01/18 09:29:20 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 001/521, train_loss: 0.231(0.231), train_acc: 90.625(90.625)
01/18 09:29:32 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 101/521, train_loss: 0.310(0.295), train_acc: 90.625(89.635)
01/18 09:29:45 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 201/521, train_loss: 0.283(0.295), train_acc: 92.708(89.666)
01/18 09:29:58 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 301/521, train_loss: 0.310(0.294), train_acc: 88.542(89.608)
01/18 09:30:11 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 401/521, train_loss: 0.254(0.295), train_acc: 90.625(89.633)
01/18 09:30:23 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 501/521, train_loss: 0.291(0.295), train_acc: 89.583(89.596)
01/18 09:30:26 PM [Supernet Training] lr: 0.02011 epoch: 176/600, step: 521/521, train_loss: 0.199(0.295), train_acc: 93.750(89.626)
01/18 09:30:26 PM [Supernet Training] epoch: 176, train_loss: 0.295, train_acc: 89.626
01/18 09:30:30 PM [Supernet Validation] epoch: 176, val_loss: 0.456, val_acc: 85.520, best_acc: 85.730
01/18 09:30:30 PM 

01/18 09:30:30 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 001/521, train_loss: 0.319(0.319), train_acc: 87.500(87.500)
01/18 09:30:43 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 101/521, train_loss: 0.265(0.297), train_acc: 87.500(89.573)
01/18 09:30:56 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 201/521, train_loss: 0.407(0.296), train_acc: 82.292(89.578)
01/18 09:31:08 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 301/521, train_loss: 0.231(0.293), train_acc: 91.667(89.545)
01/18 09:31:21 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 401/521, train_loss: 0.392(0.295), train_acc: 90.625(89.464)
01/18 09:31:34 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 501/521, train_loss: 0.388(0.295), train_acc: 84.375(89.527)
01/18 09:31:36 PM [Supernet Training] lr: 0.02006 epoch: 177/600, step: 521/521, train_loss: 0.388(0.295), train_acc: 90.000(89.516)
01/18 09:31:36 PM [Supernet Training] epoch: 177, train_loss: 0.295, train_acc: 89.516
01/18 09:31:40 PM [Supernet Validation] epoch: 177, val_loss: 0.465, val_acc: 85.450, best_acc: 85.730
01/18 09:31:40 PM 

01/18 09:31:40 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 001/521, train_loss: 0.230(0.230), train_acc: 93.750(93.750)
01/18 09:31:53 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 101/521, train_loss: 0.234(0.297), train_acc: 93.750(89.821)
01/18 09:32:06 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 201/521, train_loss: 0.291(0.292), train_acc: 88.542(89.796)
01/18 09:32:19 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 301/521, train_loss: 0.351(0.288), train_acc: 88.542(89.964)
01/18 09:32:31 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 401/521, train_loss: 0.395(0.290), train_acc: 88.542(89.812)
01/18 09:32:44 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 501/521, train_loss: 0.375(0.291), train_acc: 88.542(89.789)
01/18 09:32:47 PM [Supernet Training] lr: 0.02001 epoch: 178/600, step: 521/521, train_loss: 0.371(0.290), train_acc: 85.000(89.812)
01/18 09:32:47 PM [Supernet Training] epoch: 178, train_loss: 0.290, train_acc: 89.812
01/18 09:32:50 PM [Supernet Validation] epoch: 178, val_loss: 0.454, val_acc: 85.420, best_acc: 85.730
01/18 09:32:50 PM 

01/18 09:32:51 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 001/521, train_loss: 0.252(0.252), train_acc: 91.667(91.667)
01/18 09:33:03 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 101/521, train_loss: 0.233(0.298), train_acc: 91.667(89.501)
01/18 09:33:16 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 201/521, train_loss: 0.279(0.294), train_acc: 93.750(89.749)
01/18 09:33:29 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 301/521, train_loss: 0.164(0.289), train_acc: 93.750(89.874)
01/18 09:33:42 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 401/521, train_loss: 0.243(0.289), train_acc: 90.625(89.781)
01/18 09:33:55 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 501/521, train_loss: 0.252(0.289), train_acc: 92.708(89.833)
01/18 09:33:57 PM [Supernet Training] lr: 0.01995 epoch: 179/600, step: 521/521, train_loss: 0.272(0.290), train_acc: 91.250(89.842)
01/18 09:33:57 PM [Supernet Training] epoch: 179, train_loss: 0.290, train_acc: 89.842
01/18 09:34:01 PM [Supernet Validation] epoch: 179, val_loss: 0.454, val_acc: 85.570, best_acc: 85.730
01/18 09:34:01 PM 

01/18 09:34:01 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 001/521, train_loss: 0.360(0.360), train_acc: 90.625(90.625)
01/18 09:34:14 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 101/521, train_loss: 0.289(0.277), train_acc: 88.542(90.316)
01/18 09:34:27 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 201/521, train_loss: 0.339(0.286), train_acc: 88.542(90.013)
01/18 09:34:40 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 301/521, train_loss: 0.259(0.291), train_acc: 91.667(89.687)
01/18 09:34:52 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 401/521, train_loss: 0.268(0.291), train_acc: 90.625(89.620)
01/18 09:35:05 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 501/521, train_loss: 0.385(0.293), train_acc: 88.542(89.619)
01/18 09:35:08 PM [Supernet Training] lr: 0.01990 epoch: 180/600, step: 521/521, train_loss: 0.175(0.292), train_acc: 93.750(89.674)
01/18 09:35:08 PM [Supernet Training] epoch: 180, train_loss: 0.292, train_acc: 89.674
01/18 09:35:11 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 09:35:11 PM [Supernet Validation] epoch: 180, val_loss: 0.445, val_acc: 86.100, best_acc: 86.100
01/18 09:35:11 PM 

01/18 09:35:12 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 001/521, train_loss: 0.328(0.328), train_acc: 85.417(85.417)
01/18 09:35:25 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 101/521, train_loss: 0.179(0.279), train_acc: 94.792(90.233)
01/18 09:35:37 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 201/521, train_loss: 0.192(0.278), train_acc: 95.833(90.143)
01/18 09:35:50 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 301/521, train_loss: 0.219(0.279), train_acc: 92.708(90.068)
01/18 09:36:03 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 401/521, train_loss: 0.326(0.283), train_acc: 91.667(89.937)
01/18 09:36:16 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 501/521, train_loss: 0.430(0.283), train_acc: 82.292(89.901)
01/18 09:36:18 PM [Supernet Training] lr: 0.01985 epoch: 181/600, step: 521/521, train_loss: 0.187(0.284), train_acc: 91.250(89.874)
01/18 09:36:18 PM [Supernet Training] epoch: 181, train_loss: 0.284, train_acc: 89.874
01/18 09:36:22 PM [Supernet Validation] epoch: 181, val_loss: 0.441, val_acc: 85.670, best_acc: 86.100
01/18 09:36:22 PM 

01/18 09:36:22 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 001/521, train_loss: 0.342(0.342), train_acc: 87.500(87.500)
01/18 09:36:35 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 101/521, train_loss: 0.311(0.280), train_acc: 89.583(90.243)
01/18 09:36:48 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 201/521, train_loss: 0.308(0.288), train_acc: 91.667(89.874)
01/18 09:37:01 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 301/521, train_loss: 0.253(0.285), train_acc: 91.667(89.940)
01/18 09:37:13 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 401/521, train_loss: 0.247(0.287), train_acc: 91.667(89.900)
01/18 09:37:26 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 501/521, train_loss: 0.314(0.288), train_acc: 88.542(89.854)
01/18 09:37:29 PM [Supernet Training] lr: 0.01979 epoch: 182/600, step: 521/521, train_loss: 0.292(0.289), train_acc: 90.000(89.830)
01/18 09:37:29 PM [Supernet Training] epoch: 182, train_loss: 0.289, train_acc: 89.830
01/18 09:37:32 PM [Supernet Validation] epoch: 182, val_loss: 0.462, val_acc: 85.380, best_acc: 86.100
01/18 09:37:32 PM 

01/18 09:37:33 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 001/521, train_loss: 0.304(0.304), train_acc: 88.542(88.542)
01/18 09:37:46 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 101/521, train_loss: 0.389(0.283), train_acc: 88.542(90.068)
01/18 09:37:59 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 201/521, train_loss: 0.281(0.280), train_acc: 87.500(90.081)
01/18 09:38:11 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 301/521, train_loss: 0.216(0.280), train_acc: 91.667(90.141)
01/18 09:38:24 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 401/521, train_loss: 0.390(0.282), train_acc: 85.417(90.015)
01/18 09:38:37 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 501/521, train_loss: 0.338(0.281), train_acc: 89.583(90.026)
01/18 09:38:39 PM [Supernet Training] lr: 0.01974 epoch: 183/600, step: 521/521, train_loss: 0.210(0.282), train_acc: 93.750(89.990)
01/18 09:38:40 PM [Supernet Training] epoch: 183, train_loss: 0.282, train_acc: 89.990
01/18 09:38:43 PM [Supernet Validation] epoch: 183, val_loss: 0.454, val_acc: 85.500, best_acc: 86.100
01/18 09:38:43 PM 

01/18 09:38:43 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 001/521, train_loss: 0.233(0.233), train_acc: 89.583(89.583)
01/18 09:38:56 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 101/521, train_loss: 0.262(0.282), train_acc: 90.625(89.924)
01/18 09:39:09 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 201/521, train_loss: 0.310(0.281), train_acc: 88.542(90.148)
01/18 09:39:22 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 301/521, train_loss: 0.301(0.281), train_acc: 87.500(89.985)
01/18 09:39:35 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 401/521, train_loss: 0.348(0.283), train_acc: 88.542(89.838)
01/18 09:39:48 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 501/521, train_loss: 0.177(0.281), train_acc: 92.708(89.872)
01/18 09:39:50 PM [Supernet Training] lr: 0.01969 epoch: 184/600, step: 521/521, train_loss: 0.356(0.283), train_acc: 88.750(89.860)
01/18 09:39:50 PM [Supernet Training] epoch: 184, train_loss: 0.283, train_acc: 89.860
01/18 09:39:54 PM [Supernet Validation] epoch: 184, val_loss: 0.431, val_acc: 86.050, best_acc: 86.100
01/18 09:39:54 PM 

01/18 09:39:54 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 001/521, train_loss: 0.289(0.289), train_acc: 87.500(87.500)
01/18 09:40:07 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 101/521, train_loss: 0.380(0.287), train_acc: 84.375(89.728)
01/18 09:40:20 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 201/521, train_loss: 0.220(0.281), train_acc: 87.500(89.941)
01/18 09:40:32 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 301/521, train_loss: 0.288(0.282), train_acc: 90.625(89.933)
01/18 09:40:45 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 401/521, train_loss: 0.208(0.280), train_acc: 93.750(89.996)
01/18 09:40:58 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 501/521, train_loss: 0.461(0.283), train_acc: 87.500(89.924)
01/18 09:41:00 PM [Supernet Training] lr: 0.01963 epoch: 185/600, step: 521/521, train_loss: 0.284(0.283), train_acc: 87.500(89.910)
01/18 09:41:00 PM [Supernet Training] epoch: 185, train_loss: 0.283, train_acc: 89.910
01/18 09:41:04 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 09:41:04 PM [Supernet Validation] epoch: 185, val_loss: 0.437, val_acc: 86.110, best_acc: 86.110
01/18 09:41:04 PM 

01/18 09:41:05 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 001/521, train_loss: 0.312(0.312), train_acc: 88.542(88.542)
01/18 09:41:17 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 101/521, train_loss: 0.205(0.269), train_acc: 93.750(90.398)
01/18 09:41:30 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 201/521, train_loss: 0.380(0.277), train_acc: 91.667(90.159)
01/18 09:41:43 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 301/521, train_loss: 0.204(0.275), train_acc: 94.792(90.179)
01/18 09:41:56 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 401/521, train_loss: 0.310(0.278), train_acc: 86.458(90.059)
01/18 09:42:09 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 501/521, train_loss: 0.313(0.281), train_acc: 89.583(89.964)
01/18 09:42:11 PM [Supernet Training] lr: 0.01958 epoch: 186/600, step: 521/521, train_loss: 0.242(0.281), train_acc: 91.250(89.934)
01/18 09:42:11 PM [Supernet Training] epoch: 186, train_loss: 0.281, train_acc: 89.934
01/18 09:42:15 PM [Supernet Validation] epoch: 186, val_loss: 0.453, val_acc: 85.550, best_acc: 86.110
01/18 09:42:15 PM 

01/18 09:42:15 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 001/521, train_loss: 0.181(0.181), train_acc: 91.667(91.667)
01/18 09:42:28 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 101/521, train_loss: 0.156(0.275), train_acc: 96.875(90.336)
01/18 09:42:41 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 201/521, train_loss: 0.337(0.272), train_acc: 87.500(90.438)
01/18 09:42:53 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 301/521, train_loss: 0.204(0.277), train_acc: 92.708(90.130)
01/18 09:43:06 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 401/521, train_loss: 0.306(0.276), train_acc: 92.708(90.129)
01/18 09:43:19 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 501/521, train_loss: 0.288(0.278), train_acc: 90.625(90.093)
01/18 09:43:21 PM [Supernet Training] lr: 0.01953 epoch: 187/600, step: 521/521, train_loss: 0.550(0.278), train_acc: 82.500(90.080)
01/18 09:43:21 PM [Supernet Training] epoch: 187, train_loss: 0.278, train_acc: 90.080
01/18 09:43:25 PM [Supernet Validation] epoch: 187, val_loss: 0.440, val_acc: 85.720, best_acc: 86.110
01/18 09:43:25 PM 

01/18 09:43:25 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 001/521, train_loss: 0.445(0.445), train_acc: 87.500(87.500)
01/18 09:43:38 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 101/521, train_loss: 0.366(0.283), train_acc: 90.625(89.841)
01/18 09:43:51 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 201/521, train_loss: 0.328(0.280), train_acc: 87.500(90.019)
01/18 09:44:04 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 301/521, train_loss: 0.316(0.277), train_acc: 87.500(90.192)
01/18 09:44:17 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 401/521, train_loss: 0.238(0.275), train_acc: 94.792(90.173)
01/18 09:44:29 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 501/521, train_loss: 0.237(0.277), train_acc: 89.583(90.055)
01/18 09:44:32 PM [Supernet Training] lr: 0.01947 epoch: 188/600, step: 521/521, train_loss: 0.258(0.277), train_acc: 92.500(90.048)
01/18 09:44:32 PM [Supernet Training] epoch: 188, train_loss: 0.277, train_acc: 90.048
01/18 09:44:35 PM [Supernet Validation] epoch: 188, val_loss: 0.442, val_acc: 85.750, best_acc: 86.110
01/18 09:44:35 PM 

01/18 09:44:36 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 001/521, train_loss: 0.378(0.378), train_acc: 84.375(84.375)
01/18 09:44:49 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 101/521, train_loss: 0.234(0.278), train_acc: 91.667(90.099)
01/18 09:45:01 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 201/521, train_loss: 0.231(0.273), train_acc: 91.667(90.247)
01/18 09:45:14 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 301/521, train_loss: 0.339(0.272), train_acc: 86.458(90.390)
01/18 09:45:27 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 401/521, train_loss: 0.347(0.274), train_acc: 89.583(90.342)
01/18 09:45:40 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 501/521, train_loss: 0.289(0.276), train_acc: 86.458(90.249)
01/18 09:45:42 PM [Supernet Training] lr: 0.01942 epoch: 189/600, step: 521/521, train_loss: 0.221(0.277), train_acc: 88.750(90.208)
01/18 09:45:42 PM [Supernet Training] epoch: 189, train_loss: 0.277, train_acc: 90.208
01/18 09:45:46 PM [Supernet Validation] epoch: 189, val_loss: 0.465, val_acc: 85.550, best_acc: 86.110
01/18 09:45:46 PM 

01/18 09:45:46 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 001/521, train_loss: 0.280(0.280), train_acc: 88.542(88.542)
01/18 09:45:59 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 101/521, train_loss: 0.383(0.269), train_acc: 87.500(90.563)
01/18 09:46:12 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 201/521, train_loss: 0.294(0.276), train_acc: 88.542(90.366)
01/18 09:46:25 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 301/521, train_loss: 0.255(0.276), train_acc: 89.583(90.331)
01/18 09:46:38 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 401/521, train_loss: 0.216(0.277), train_acc: 91.667(90.238)
01/18 09:46:50 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 501/521, train_loss: 0.272(0.278), train_acc: 91.667(90.170)
01/18 09:46:53 PM [Supernet Training] lr: 0.01936 epoch: 190/600, step: 521/521, train_loss: 0.303(0.278), train_acc: 88.750(90.158)
01/18 09:46:53 PM [Supernet Training] epoch: 190, train_loss: 0.278, train_acc: 90.158
01/18 09:46:57 PM [Supernet Validation] epoch: 190, val_loss: 0.431, val_acc: 85.790, best_acc: 86.110
01/18 09:46:57 PM 

01/18 09:46:57 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 001/521, train_loss: 0.285(0.285), train_acc: 90.625(90.625)
01/18 09:47:10 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 101/521, train_loss: 0.431(0.273), train_acc: 88.542(90.336)
01/18 09:47:22 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 201/521, train_loss: 0.225(0.273), train_acc: 92.708(90.299)
01/18 09:47:35 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 301/521, train_loss: 0.279(0.273), train_acc: 92.708(90.296)
01/18 09:47:48 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 401/521, train_loss: 0.258(0.274), train_acc: 92.708(90.334)
01/18 09:48:01 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 501/521, train_loss: 0.189(0.272), train_acc: 92.708(90.413)
01/18 09:48:03 PM [Supernet Training] lr: 0.01931 epoch: 191/600, step: 521/521, train_loss: 0.427(0.273), train_acc: 87.500(90.404)
01/18 09:48:03 PM [Supernet Training] epoch: 191, train_loss: 0.273, train_acc: 90.404
01/18 09:48:07 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 09:48:07 PM [Supernet Validation] epoch: 191, val_loss: 0.428, val_acc: 86.560, best_acc: 86.560
01/18 09:48:07 PM 

01/18 09:48:08 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 001/521, train_loss: 0.235(0.235), train_acc: 93.750(93.750)
01/18 09:48:20 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 101/521, train_loss: 0.278(0.270), train_acc: 89.583(90.285)
01/18 09:48:33 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 201/521, train_loss: 0.141(0.266), train_acc: 95.833(90.599)
01/18 09:48:46 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 301/521, train_loss: 0.184(0.268), train_acc: 91.667(90.466)
01/18 09:48:59 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 401/521, train_loss: 0.280(0.269), train_acc: 90.625(90.474)
01/18 09:49:12 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 501/521, train_loss: 0.383(0.271), train_acc: 87.500(90.369)
01/18 09:49:14 PM [Supernet Training] lr: 0.01925 epoch: 192/600, step: 521/521, train_loss: 0.297(0.273), train_acc: 88.750(90.338)
01/18 09:49:14 PM [Supernet Training] epoch: 192, train_loss: 0.273, train_acc: 90.338
01/18 09:49:18 PM [Supernet Validation] epoch: 192, val_loss: 0.463, val_acc: 85.580, best_acc: 86.560
01/18 09:49:18 PM 

01/18 09:49:18 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 001/521, train_loss: 0.247(0.247), train_acc: 89.583(89.583)
01/18 09:49:31 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 101/521, train_loss: 0.174(0.262), train_acc: 92.708(90.316)
01/18 09:49:44 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 201/521, train_loss: 0.273(0.267), train_acc: 91.667(90.340)
01/18 09:49:57 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 301/521, train_loss: 0.183(0.271), train_acc: 93.750(90.307)
01/18 09:50:09 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 401/521, train_loss: 0.201(0.272), train_acc: 92.708(90.331)
01/18 09:50:22 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 501/521, train_loss: 0.149(0.272), train_acc: 95.833(90.311)
01/18 09:50:25 PM [Supernet Training] lr: 0.01920 epoch: 193/600, step: 521/521, train_loss: 0.214(0.271), train_acc: 91.250(90.342)
01/18 09:50:25 PM [Supernet Training] epoch: 193, train_loss: 0.271, train_acc: 90.342
01/18 09:50:28 PM [Supernet Validation] epoch: 193, val_loss: 0.448, val_acc: 86.020, best_acc: 86.560
01/18 09:50:28 PM 

01/18 09:50:29 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 001/521, train_loss: 0.203(0.203), train_acc: 93.750(93.750)
01/18 09:50:42 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 101/521, train_loss: 0.267(0.278), train_acc: 91.667(90.089)
01/18 09:50:54 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 201/521, train_loss: 0.312(0.273), train_acc: 90.625(90.231)
01/18 09:51:07 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 301/521, train_loss: 0.384(0.275), train_acc: 90.625(90.179)
01/18 09:51:20 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 401/521, train_loss: 0.139(0.275), train_acc: 95.833(90.225)
01/18 09:51:33 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 501/521, train_loss: 0.179(0.272), train_acc: 96.875(90.361)
01/18 09:51:35 PM [Supernet Training] lr: 0.01914 epoch: 194/600, step: 521/521, train_loss: 0.294(0.272), train_acc: 90.000(90.396)
01/18 09:51:35 PM [Supernet Training] epoch: 194, train_loss: 0.272, train_acc: 90.396
01/18 09:51:39 PM [Supernet Validation] epoch: 194, val_loss: 0.428, val_acc: 86.300, best_acc: 86.560
01/18 09:51:39 PM 

01/18 09:51:39 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 88.542(88.542)
01/18 09:51:52 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 101/521, train_loss: 0.170(0.263), train_acc: 93.750(90.584)
01/18 09:52:05 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 201/521, train_loss: 0.304(0.266), train_acc: 87.500(90.682)
01/18 09:52:18 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 301/521, train_loss: 0.230(0.265), train_acc: 91.667(90.670)
01/18 09:52:30 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 401/521, train_loss: 0.346(0.268), train_acc: 85.417(90.508)
01/18 09:52:43 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 501/521, train_loss: 0.178(0.267), train_acc: 95.833(90.550)
01/18 09:52:46 PM [Supernet Training] lr: 0.01909 epoch: 195/600, step: 521/521, train_loss: 0.480(0.267), train_acc: 80.000(90.554)
01/18 09:52:46 PM [Supernet Training] epoch: 195, train_loss: 0.267, train_acc: 90.554
01/18 09:52:49 PM [Supernet Validation] epoch: 195, val_loss: 0.456, val_acc: 86.000, best_acc: 86.560
01/18 09:52:49 PM 

01/18 09:52:50 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 001/521, train_loss: 0.226(0.226), train_acc: 93.750(93.750)
01/18 09:53:02 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 101/521, train_loss: 0.225(0.249), train_acc: 89.583(91.213)
01/18 09:53:15 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 201/521, train_loss: 0.246(0.259), train_acc: 89.583(90.889)
01/18 09:53:28 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 301/521, train_loss: 0.304(0.264), train_acc: 87.500(90.635)
01/18 09:53:41 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 401/521, train_loss: 0.285(0.266), train_acc: 88.542(90.628)
01/18 09:53:53 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 501/521, train_loss: 0.310(0.268), train_acc: 90.625(90.523)
01/18 09:53:56 PM [Supernet Training] lr: 0.01903 epoch: 196/600, step: 521/521, train_loss: 0.499(0.268), train_acc: 87.500(90.516)
01/18 09:53:56 PM [Supernet Training] epoch: 196, train_loss: 0.268, train_acc: 90.516
01/18 09:54:00 PM [Supernet Validation] epoch: 196, val_loss: 0.443, val_acc: 86.030, best_acc: 86.560
01/18 09:54:00 PM 

01/18 09:54:00 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 001/521, train_loss: 0.363(0.363), train_acc: 86.458(86.458)
01/18 09:54:13 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 101/521, train_loss: 0.344(0.266), train_acc: 90.625(90.821)
01/18 09:54:25 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 201/521, train_loss: 0.338(0.269), train_acc: 87.500(90.501)
01/18 09:54:38 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 301/521, train_loss: 0.227(0.265), train_acc: 93.750(90.563)
01/18 09:54:51 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 401/521, train_loss: 0.200(0.266), train_acc: 91.667(90.578)
01/18 09:55:04 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 501/521, train_loss: 0.202(0.266), train_acc: 90.625(90.538)
01/18 09:55:06 PM [Supernet Training] lr: 0.01898 epoch: 197/600, step: 521/521, train_loss: 0.450(0.266), train_acc: 82.500(90.532)
01/18 09:55:06 PM [Supernet Training] epoch: 197, train_loss: 0.266, train_acc: 90.532
01/18 09:55:10 PM [Supernet Validation] epoch: 197, val_loss: 0.428, val_acc: 86.320, best_acc: 86.560
01/18 09:55:10 PM 

01/18 09:55:10 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 001/521, train_loss: 0.237(0.237), train_acc: 90.625(90.625)
01/18 09:55:23 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 101/521, train_loss: 0.199(0.274), train_acc: 92.708(90.573)
01/18 09:55:36 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 201/521, train_loss: 0.135(0.263), train_acc: 94.792(90.889)
01/18 09:55:49 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 301/521, train_loss: 0.236(0.265), train_acc: 91.667(90.746)
01/18 09:56:02 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 401/521, train_loss: 0.215(0.266), train_acc: 92.708(90.706)
01/18 09:56:15 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 501/521, train_loss: 0.426(0.263), train_acc: 86.458(90.739)
01/18 09:56:17 PM [Supernet Training] lr: 0.01892 epoch: 198/600, step: 521/521, train_loss: 0.367(0.263), train_acc: 88.750(90.754)
01/18 09:56:17 PM [Supernet Training] epoch: 198, train_loss: 0.263, train_acc: 90.754
01/18 09:56:21 PM [Supernet Validation] epoch: 198, val_loss: 0.436, val_acc: 86.300, best_acc: 86.560
01/18 09:56:21 PM 

01/18 09:56:21 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 001/521, train_loss: 0.228(0.228), train_acc: 91.667(91.667)
01/18 09:56:34 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 101/521, train_loss: 0.243(0.263), train_acc: 89.583(90.450)
01/18 09:56:47 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 201/521, train_loss: 0.233(0.266), train_acc: 91.667(90.392)
01/18 09:56:59 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 301/521, train_loss: 0.375(0.266), train_acc: 85.417(90.462)
01/18 09:57:12 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 401/521, train_loss: 0.189(0.266), train_acc: 90.625(90.454)
01/18 09:57:25 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 501/521, train_loss: 0.284(0.268), train_acc: 88.542(90.348)
01/18 09:57:27 PM [Supernet Training] lr: 0.01886 epoch: 199/600, step: 521/521, train_loss: 0.306(0.268), train_acc: 88.750(90.378)
01/18 09:57:27 PM [Supernet Training] epoch: 199, train_loss: 0.268, train_acc: 90.378
01/18 09:57:31 PM [Supernet Validation] epoch: 199, val_loss: 0.437, val_acc: 86.250, best_acc: 86.560
01/18 09:57:31 PM 

01/18 09:57:31 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 001/521, train_loss: 0.232(0.232), train_acc: 93.750(93.750)
01/18 09:57:44 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 101/521, train_loss: 0.394(0.257), train_acc: 86.458(91.099)
01/18 09:57:57 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 201/521, train_loss: 0.151(0.258), train_acc: 94.792(90.967)
01/18 09:58:10 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 301/521, train_loss: 0.197(0.260), train_acc: 95.833(90.878)
01/18 09:58:22 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 401/521, train_loss: 0.276(0.260), train_acc: 92.708(90.874)
01/18 09:58:35 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 501/521, train_loss: 0.153(0.262), train_acc: 93.750(90.729)
01/18 09:58:38 PM [Supernet Training] lr: 0.01881 epoch: 200/600, step: 521/521, train_loss: 0.530(0.263), train_acc: 86.250(90.732)
01/18 09:58:38 PM [Supernet Training] epoch: 200, train_loss: 0.263, train_acc: 90.732
01/18 09:58:41 PM [Supernet Validation] epoch: 200, val_loss: 0.432, val_acc: 86.370, best_acc: 86.560
01/18 09:58:41 PM 

01/18 09:58:42 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 001/521, train_loss: 0.235(0.235), train_acc: 90.625(90.625)
01/18 09:58:55 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 101/521, train_loss: 0.270(0.246), train_acc: 93.750(91.254)
01/18 09:59:08 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 201/521, train_loss: 0.178(0.248), train_acc: 94.792(91.117)
01/18 09:59:20 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 301/521, train_loss: 0.342(0.254), train_acc: 91.667(90.902)
01/18 09:59:33 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 401/521, train_loss: 0.219(0.257), train_acc: 91.667(90.835)
01/18 09:59:46 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 501/521, train_loss: 0.267(0.258), train_acc: 89.583(90.735)
01/18 09:59:48 PM [Supernet Training] lr: 0.01875 epoch: 201/600, step: 521/521, train_loss: 0.582(0.260), train_acc: 80.000(90.678)
01/18 09:59:48 PM [Supernet Training] epoch: 201, train_loss: 0.260, train_acc: 90.678
01/18 09:59:52 PM [Supernet Validation] epoch: 201, val_loss: 0.448, val_acc: 86.250, best_acc: 86.560
01/18 09:59:52 PM 

01/18 09:59:52 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 001/521, train_loss: 0.337(0.337), train_acc: 88.542(88.542)
01/18 10:00:05 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 101/521, train_loss: 0.236(0.252), train_acc: 92.708(91.203)
01/18 10:00:18 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 201/521, train_loss: 0.205(0.254), train_acc: 91.667(91.071)
01/18 10:00:31 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 301/521, train_loss: 0.279(0.255), train_acc: 88.542(91.020)
01/18 10:00:44 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 401/521, train_loss: 0.247(0.255), train_acc: 92.708(90.934)
01/18 10:00:57 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 501/521, train_loss: 0.155(0.259), train_acc: 93.750(90.810)
01/18 10:00:59 PM [Supernet Training] lr: 0.01869 epoch: 202/600, step: 521/521, train_loss: 0.175(0.260), train_acc: 93.750(90.774)
01/18 10:00:59 PM [Supernet Training] epoch: 202, train_loss: 0.260, train_acc: 90.774
01/18 10:01:03 PM [Supernet Validation] epoch: 202, val_loss: 0.427, val_acc: 86.290, best_acc: 86.560
01/18 10:01:03 PM 

01/18 10:01:03 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 001/521, train_loss: 0.341(0.341), train_acc: 91.667(91.667)
01/18 10:01:16 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 101/521, train_loss: 0.451(0.254), train_acc: 88.542(90.976)
01/18 10:01:29 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 201/521, train_loss: 0.195(0.251), train_acc: 94.792(91.221)
01/18 10:01:42 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 301/521, train_loss: 0.329(0.259), train_acc: 86.458(90.878)
01/18 10:01:54 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 401/521, train_loss: 0.198(0.260), train_acc: 92.708(90.833)
01/18 10:02:07 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 501/521, train_loss: 0.311(0.262), train_acc: 90.625(90.658)
01/18 10:02:10 PM [Supernet Training] lr: 0.01864 epoch: 203/600, step: 521/521, train_loss: 0.234(0.262), train_acc: 88.750(90.652)
01/18 10:02:10 PM [Supernet Training] epoch: 203, train_loss: 0.262, train_acc: 90.652
01/18 10:02:13 PM [Supernet Validation] epoch: 203, val_loss: 0.442, val_acc: 86.020, best_acc: 86.560
01/18 10:02:13 PM 

01/18 10:02:14 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 001/521, train_loss: 0.147(0.147), train_acc: 95.833(95.833)
01/18 10:02:26 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 101/521, train_loss: 0.271(0.258), train_acc: 92.708(90.573)
01/18 10:02:39 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 201/521, train_loss: 0.295(0.262), train_acc: 90.625(90.537)
01/18 10:02:52 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 301/521, train_loss: 0.275(0.264), train_acc: 88.542(90.518)
01/18 10:03:05 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 401/521, train_loss: 0.268(0.261), train_acc: 90.625(90.690)
01/18 10:03:18 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 501/521, train_loss: 0.256(0.260), train_acc: 92.708(90.727)
01/18 10:03:20 PM [Supernet Training] lr: 0.01858 epoch: 204/600, step: 521/521, train_loss: 0.366(0.260), train_acc: 85.000(90.732)
01/18 10:03:20 PM [Supernet Training] epoch: 204, train_loss: 0.260, train_acc: 90.732
01/18 10:03:24 PM [Supernet Validation] epoch: 204, val_loss: 0.434, val_acc: 86.260, best_acc: 86.560
01/18 10:03:24 PM 

01/18 10:03:24 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 001/521, train_loss: 0.281(0.281), train_acc: 88.542(88.542)
01/18 10:03:37 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 101/521, train_loss: 0.283(0.233), train_acc: 88.542(91.512)
01/18 10:03:50 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 201/521, train_loss: 0.318(0.241), train_acc: 91.667(91.444)
01/18 10:04:02 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 301/521, train_loss: 0.255(0.249), train_acc: 89.583(91.154)
01/18 10:04:15 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 401/521, train_loss: 0.211(0.252), train_acc: 92.708(91.098)
01/18 10:04:28 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 501/521, train_loss: 0.212(0.254), train_acc: 91.667(91.053)
01/18 10:04:31 PM [Supernet Training] lr: 0.01852 epoch: 205/600, step: 521/521, train_loss: 0.162(0.255), train_acc: 92.500(91.040)
01/18 10:04:31 PM [Supernet Training] epoch: 205, train_loss: 0.255, train_acc: 91.040
01/18 10:04:34 PM [Supernet Validation] epoch: 205, val_loss: 0.421, val_acc: 86.560, best_acc: 86.560
01/18 10:04:34 PM 

01/18 10:04:35 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 001/521, train_loss: 0.317(0.317), train_acc: 89.583(89.583)
01/18 10:04:47 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 101/521, train_loss: 0.262(0.257), train_acc: 91.667(90.759)
01/18 10:05:00 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 201/521, train_loss: 0.180(0.254), train_acc: 90.625(90.951)
01/18 10:05:13 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 301/521, train_loss: 0.235(0.256), train_acc: 93.750(90.853)
01/18 10:05:26 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 401/521, train_loss: 0.312(0.256), train_acc: 90.625(90.812)
01/18 10:05:39 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 501/521, train_loss: 0.209(0.259), train_acc: 90.625(90.777)
01/18 10:05:41 PM [Supernet Training] lr: 0.01846 epoch: 206/600, step: 521/521, train_loss: 0.201(0.258), train_acc: 93.750(90.816)
01/18 10:05:41 PM [Supernet Training] epoch: 206, train_loss: 0.258, train_acc: 90.816
01/18 10:05:45 PM [Supernet Validation] epoch: 206, val_loss: 0.446, val_acc: 85.930, best_acc: 86.560
01/18 10:05:45 PM 

01/18 10:05:45 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 001/521, train_loss: 0.132(0.132), train_acc: 95.833(95.833)
01/18 10:05:58 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 101/521, train_loss: 0.257(0.259), train_acc: 92.708(90.677)
01/18 10:06:11 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 201/521, train_loss: 0.239(0.248), train_acc: 93.750(91.143)
01/18 10:06:24 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 301/521, train_loss: 0.182(0.247), train_acc: 92.708(91.051)
01/18 10:06:36 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 401/521, train_loss: 0.191(0.251), train_acc: 92.708(90.947)
01/18 10:06:49 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 501/521, train_loss: 0.278(0.250), train_acc: 90.625(91.049)
01/18 10:06:52 PM [Supernet Training] lr: 0.01841 epoch: 207/600, step: 521/521, train_loss: 0.294(0.250), train_acc: 91.250(91.018)
01/18 10:06:52 PM [Supernet Training] epoch: 207, train_loss: 0.250, train_acc: 91.018
01/18 10:06:55 PM [Supernet Validation] epoch: 207, val_loss: 0.438, val_acc: 86.260, best_acc: 86.560
01/18 10:06:55 PM 

01/18 10:06:56 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 001/521, train_loss: 0.148(0.148), train_acc: 94.792(94.792)
01/18 10:07:09 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 101/521, train_loss: 0.213(0.245), train_acc: 88.542(91.316)
01/18 10:07:21 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 201/521, train_loss: 0.351(0.243), train_acc: 90.625(91.231)
01/18 10:07:34 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 301/521, train_loss: 0.135(0.248), train_acc: 95.833(91.009)
01/18 10:07:47 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 401/521, train_loss: 0.248(0.249), train_acc: 93.750(91.051)
01/18 10:08:00 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 501/521, train_loss: 0.234(0.250), train_acc: 90.625(90.958)
01/18 10:08:02 PM [Supernet Training] lr: 0.01835 epoch: 208/600, step: 521/521, train_loss: 0.154(0.250), train_acc: 92.500(90.994)
01/18 10:08:02 PM [Supernet Training] epoch: 208, train_loss: 0.250, train_acc: 90.994
01/18 10:08:06 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 10:08:06 PM [Supernet Validation] epoch: 208, val_loss: 0.436, val_acc: 86.690, best_acc: 86.690
01/18 10:08:06 PM 

01/18 10:08:06 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 001/521, train_loss: 0.179(0.179), train_acc: 96.875(96.875)
01/18 10:08:19 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 101/521, train_loss: 0.184(0.252), train_acc: 91.667(91.130)
01/18 10:08:32 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 201/521, train_loss: 0.270(0.252), train_acc: 89.583(91.008)
01/18 10:08:45 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 301/521, train_loss: 0.331(0.251), train_acc: 89.583(90.978)
01/18 10:08:58 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 401/521, train_loss: 0.258(0.254), train_acc: 90.625(90.864)
01/18 10:09:10 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 501/521, train_loss: 0.355(0.254), train_acc: 89.583(90.924)
01/18 10:09:13 PM [Supernet Training] lr: 0.01829 epoch: 209/600, step: 521/521, train_loss: 0.307(0.254), train_acc: 93.750(90.954)
01/18 10:09:13 PM [Supernet Training] epoch: 209, train_loss: 0.254, train_acc: 90.954
01/18 10:09:17 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 10:09:17 PM [Supernet Validation] epoch: 209, val_loss: 0.415, val_acc: 86.930, best_acc: 86.930
01/18 10:09:17 PM 

01/18 10:09:17 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 001/521, train_loss: 0.210(0.210), train_acc: 93.750(93.750)
01/18 10:09:30 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 101/521, train_loss: 0.116(0.237), train_acc: 96.875(91.481)
01/18 10:09:43 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 201/521, train_loss: 0.326(0.251), train_acc: 89.583(91.097)
01/18 10:09:55 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 301/521, train_loss: 0.315(0.258), train_acc: 89.583(90.801)
01/18 10:10:08 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 401/521, train_loss: 0.254(0.253), train_acc: 90.625(90.965)
01/18 10:10:21 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 501/521, train_loss: 0.273(0.251), train_acc: 92.708(91.080)
01/18 10:10:24 PM [Supernet Training] lr: 0.01823 epoch: 210/600, step: 521/521, train_loss: 0.225(0.252), train_acc: 88.750(91.068)
01/18 10:10:24 PM [Supernet Training] epoch: 210, train_loss: 0.252, train_acc: 91.068
01/18 10:10:27 PM [Supernet Validation] epoch: 210, val_loss: 0.434, val_acc: 86.550, best_acc: 86.930
01/18 10:10:27 PM 

01/18 10:10:28 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 001/521, train_loss: 0.358(0.358), train_acc: 88.542(88.542)
01/18 10:10:40 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 101/521, train_loss: 0.156(0.254), train_acc: 95.833(91.130)
01/18 10:10:53 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 201/521, train_loss: 0.264(0.255), train_acc: 89.583(91.086)
01/18 10:11:06 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 301/521, train_loss: 0.318(0.248), train_acc: 89.583(91.348)
01/18 10:11:19 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 401/521, train_loss: 0.256(0.248), train_acc: 91.667(91.316)
01/18 10:11:32 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 501/521, train_loss: 0.229(0.249), train_acc: 90.625(91.261)
01/18 10:11:34 PM [Supernet Training] lr: 0.01817 epoch: 211/600, step: 521/521, train_loss: 0.304(0.249), train_acc: 88.750(91.260)
01/18 10:11:34 PM [Supernet Training] epoch: 211, train_loss: 0.249, train_acc: 91.260
01/18 10:11:38 PM [Supernet Validation] epoch: 211, val_loss: 0.425, val_acc: 86.770, best_acc: 86.930
01/18 10:11:38 PM 

01/18 10:11:38 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 001/521, train_loss: 0.412(0.412), train_acc: 82.292(82.292)
01/18 10:11:51 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 101/521, train_loss: 0.234(0.238), train_acc: 91.667(91.491)
01/18 10:12:04 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 201/521, train_loss: 0.253(0.241), train_acc: 94.792(91.387)
01/18 10:12:16 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 301/521, train_loss: 0.172(0.244), train_acc: 93.750(91.293)
01/18 10:12:29 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 401/521, train_loss: 0.239(0.245), train_acc: 92.708(91.267)
01/18 10:12:42 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 501/521, train_loss: 0.261(0.246), train_acc: 94.792(91.267)
01/18 10:12:44 PM [Supernet Training] lr: 0.01812 epoch: 212/600, step: 521/521, train_loss: 0.295(0.246), train_acc: 90.000(91.306)
01/18 10:12:44 PM [Supernet Training] epoch: 212, train_loss: 0.246, train_acc: 91.306
01/18 10:12:48 PM [Supernet Validation] epoch: 212, val_loss: 0.457, val_acc: 86.200, best_acc: 86.930
01/18 10:12:48 PM 

01/18 10:12:48 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 001/521, train_loss: 0.175(0.175), train_acc: 93.750(93.750)
01/18 10:13:01 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 101/521, train_loss: 0.234(0.240), train_acc: 92.708(91.481)
01/18 10:13:14 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 201/521, train_loss: 0.204(0.243), train_acc: 94.792(91.376)
01/18 10:13:27 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 301/521, train_loss: 0.255(0.248), train_acc: 90.625(91.109)
01/18 10:13:40 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 401/521, train_loss: 0.198(0.246), train_acc: 91.667(91.202)
01/18 10:13:53 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 501/521, train_loss: 0.214(0.249), train_acc: 93.750(91.089)
01/18 10:13:55 PM [Supernet Training] lr: 0.01806 epoch: 213/600, step: 521/521, train_loss: 0.206(0.249), train_acc: 90.000(91.076)
01/18 10:13:55 PM [Supernet Training] epoch: 213, train_loss: 0.249, train_acc: 91.076
01/18 10:13:59 PM [Supernet Validation] epoch: 213, val_loss: 0.435, val_acc: 86.160, best_acc: 86.930
01/18 10:13:59 PM 

01/18 10:13:59 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 001/521, train_loss: 0.336(0.336), train_acc: 91.667(91.667)
01/18 10:14:12 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 101/521, train_loss: 0.189(0.242), train_acc: 92.708(91.625)
01/18 10:14:25 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 201/521, train_loss: 0.263(0.243), train_acc: 90.625(91.563)
01/18 10:14:38 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 301/521, train_loss: 0.322(0.242), train_acc: 85.417(91.559)
01/18 10:14:50 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 401/521, train_loss: 0.200(0.245), train_acc: 93.750(91.487)
01/18 10:15:03 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 501/521, train_loss: 0.139(0.247), train_acc: 94.792(91.384)
01/18 10:15:06 PM [Supernet Training] lr: 0.01800 epoch: 214/600, step: 521/521, train_loss: 0.263(0.248), train_acc: 90.000(91.330)
01/18 10:15:06 PM [Supernet Training] epoch: 214, train_loss: 0.248, train_acc: 91.330
01/18 10:15:09 PM [Supernet Validation] epoch: 214, val_loss: 0.423, val_acc: 86.730, best_acc: 86.930
01/18 10:15:09 PM 

01/18 10:15:10 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 001/521, train_loss: 0.189(0.189), train_acc: 94.792(94.792)
01/18 10:15:22 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 101/521, train_loss: 0.257(0.251), train_acc: 89.583(91.089)
01/18 10:15:35 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 201/521, train_loss: 0.224(0.244), train_acc: 92.708(91.423)
01/18 10:15:48 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 301/521, train_loss: 0.390(0.242), train_acc: 84.375(91.501)
01/18 10:16:01 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 401/521, train_loss: 0.215(0.242), train_acc: 89.583(91.511)
01/18 10:16:14 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 501/521, train_loss: 0.499(0.244), train_acc: 84.375(91.419)
01/18 10:16:16 PM [Supernet Training] lr: 0.01794 epoch: 215/600, step: 521/521, train_loss: 0.429(0.244), train_acc: 83.750(91.418)
01/18 10:16:16 PM [Supernet Training] epoch: 215, train_loss: 0.244, train_acc: 91.418
01/18 10:16:20 PM [Supernet Validation] epoch: 215, val_loss: 0.420, val_acc: 86.780, best_acc: 86.930
01/18 10:16:20 PM 

01/18 10:16:20 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 001/521, train_loss: 0.199(0.199), train_acc: 91.667(91.667)
01/18 10:16:33 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 101/521, train_loss: 0.142(0.240), train_acc: 95.833(91.904)
01/18 10:16:46 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 201/521, train_loss: 0.214(0.241), train_acc: 92.708(91.615)
01/18 10:16:59 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 301/521, train_loss: 0.217(0.243), train_acc: 91.667(91.528)
01/18 10:17:12 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 401/521, train_loss: 0.276(0.240), train_acc: 88.542(91.550)
01/18 10:17:24 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 501/521, train_loss: 0.235(0.242), train_acc: 93.750(91.492)
01/18 10:17:27 PM [Supernet Training] lr: 0.01788 epoch: 216/600, step: 521/521, train_loss: 0.279(0.242), train_acc: 88.750(91.486)
01/18 10:17:27 PM [Supernet Training] epoch: 216, train_loss: 0.242, train_acc: 91.486
01/18 10:17:31 PM [Supernet Validation] epoch: 216, val_loss: 0.441, val_acc: 86.640, best_acc: 86.930
01/18 10:17:31 PM 

01/18 10:17:31 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 001/521, train_loss: 0.157(0.157), train_acc: 91.667(91.667)
01/18 10:17:44 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 101/521, train_loss: 0.150(0.245), train_acc: 94.792(91.182)
01/18 10:17:57 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 201/521, train_loss: 0.258(0.242), train_acc: 92.708(91.428)
01/18 10:18:09 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 301/521, train_loss: 0.226(0.243), train_acc: 92.708(91.404)
01/18 10:18:22 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 401/521, train_loss: 0.199(0.244), train_acc: 92.708(91.316)
01/18 10:18:35 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 501/521, train_loss: 0.268(0.243), train_acc: 88.542(91.386)
01/18 10:18:37 PM [Supernet Training] lr: 0.01782 epoch: 217/600, step: 521/521, train_loss: 0.226(0.242), train_acc: 90.000(91.386)
01/18 10:18:37 PM [Supernet Training] epoch: 217, train_loss: 0.242, train_acc: 91.386
01/18 10:18:41 PM [Supernet Validation] epoch: 217, val_loss: 0.434, val_acc: 86.840, best_acc: 86.930
01/18 10:18:41 PM 

01/18 10:18:41 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 001/521, train_loss: 0.271(0.271), train_acc: 91.667(91.667)
01/18 10:18:54 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 101/521, train_loss: 0.161(0.241), train_acc: 93.750(91.429)
01/18 10:19:07 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 201/521, train_loss: 0.172(0.239), train_acc: 92.708(91.610)
01/18 10:19:20 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 301/521, train_loss: 0.344(0.240), train_acc: 89.583(91.577)
01/18 10:19:32 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 401/521, train_loss: 0.324(0.242), train_acc: 89.583(91.459)
01/18 10:19:45 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 501/521, train_loss: 0.300(0.241), train_acc: 90.625(91.457)
01/18 10:19:48 PM [Supernet Training] lr: 0.01776 epoch: 218/600, step: 521/521, train_loss: 0.323(0.242), train_acc: 87.500(91.406)
01/18 10:19:48 PM [Supernet Training] epoch: 218, train_loss: 0.242, train_acc: 91.406
01/18 10:19:51 PM [Supernet Validation] epoch: 218, val_loss: 0.447, val_acc: 86.360, best_acc: 86.930
01/18 10:19:51 PM 

01/18 10:19:52 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 001/521, train_loss: 0.273(0.273), train_acc: 88.542(88.542)
01/18 10:20:05 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 101/521, train_loss: 0.220(0.247), train_acc: 94.792(91.244)
01/18 10:20:17 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 201/521, train_loss: 0.232(0.247), train_acc: 90.625(91.304)
01/18 10:20:30 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 301/521, train_loss: 0.253(0.248), train_acc: 90.625(91.224)
01/18 10:20:43 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 401/521, train_loss: 0.232(0.249), train_acc: 91.667(91.126)
01/18 10:20:56 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 501/521, train_loss: 0.243(0.247), train_acc: 92.708(91.247)
01/18 10:20:58 PM [Supernet Training] lr: 0.01770 epoch: 219/600, step: 521/521, train_loss: 0.406(0.247), train_acc: 85.000(91.250)
01/18 10:20:58 PM [Supernet Training] epoch: 219, train_loss: 0.247, train_acc: 91.250
01/18 10:21:02 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 10:21:02 PM [Supernet Validation] epoch: 219, val_loss: 0.426, val_acc: 87.170, best_acc: 87.170
01/18 10:21:02 PM 

01/18 10:21:02 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 001/521, train_loss: 0.273(0.273), train_acc: 93.750(93.750)
01/18 10:21:15 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 101/521, train_loss: 0.126(0.232), train_acc: 95.833(91.770)
01/18 10:21:28 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 201/521, train_loss: 0.368(0.243), train_acc: 85.417(91.418)
01/18 10:21:41 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 301/521, train_loss: 0.245(0.240), train_acc: 90.625(91.428)
01/18 10:21:54 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 401/521, train_loss: 0.262(0.240), train_acc: 90.625(91.435)
01/18 10:22:06 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 501/521, train_loss: 0.144(0.239), train_acc: 92.708(91.500)
01/18 10:22:09 PM [Supernet Training] lr: 0.01764 epoch: 220/600, step: 521/521, train_loss: 0.205(0.239), train_acc: 92.500(91.554)
01/18 10:22:09 PM [Supernet Training] epoch: 220, train_loss: 0.239, train_acc: 91.554
01/18 10:22:12 PM [Supernet Validation] epoch: 220, val_loss: 0.434, val_acc: 86.640, best_acc: 87.170
01/18 10:22:12 PM 

01/18 10:22:13 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 001/521, train_loss: 0.190(0.190), train_acc: 94.792(94.792)
01/18 10:22:26 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 101/521, train_loss: 0.218(0.234), train_acc: 90.625(91.801)
01/18 10:22:38 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 201/521, train_loss: 0.273(0.237), train_acc: 88.542(91.459)
01/18 10:22:51 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 301/521, train_loss: 0.276(0.235), train_acc: 94.792(91.601)
01/18 10:23:04 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 401/521, train_loss: 0.251(0.238), train_acc: 90.625(91.485)
01/18 10:23:17 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 501/521, train_loss: 0.144(0.238), train_acc: 93.750(91.556)
01/18 10:23:19 PM [Supernet Training] lr: 0.01758 epoch: 221/600, step: 521/521, train_loss: 0.149(0.239), train_acc: 95.000(91.510)
01/18 10:23:19 PM [Supernet Training] epoch: 221, train_loss: 0.239, train_acc: 91.510
01/18 10:23:23 PM [Supernet Validation] epoch: 221, val_loss: 0.440, val_acc: 86.650, best_acc: 87.170
01/18 10:23:23 PM 

01/18 10:23:23 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 001/521, train_loss: 0.245(0.245), train_acc: 91.667(91.667)
01/18 10:23:36 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 101/521, train_loss: 0.221(0.240), train_acc: 87.500(91.275)
01/18 10:23:49 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 201/521, train_loss: 0.211(0.237), train_acc: 93.750(91.506)
01/18 10:24:02 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 301/521, train_loss: 0.192(0.238), train_acc: 93.750(91.476)
01/18 10:24:14 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 401/521, train_loss: 0.261(0.237), train_acc: 89.583(91.500)
01/18 10:24:27 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 501/521, train_loss: 0.119(0.238), train_acc: 95.833(91.496)
01/18 10:24:30 PM [Supernet Training] lr: 0.01752 epoch: 222/600, step: 521/521, train_loss: 0.238(0.239), train_acc: 92.500(91.498)
01/18 10:24:30 PM [Supernet Training] epoch: 222, train_loss: 0.239, train_acc: 91.498
01/18 10:24:33 PM [Supernet Validation] epoch: 222, val_loss: 0.433, val_acc: 86.640, best_acc: 87.170
01/18 10:24:33 PM 

01/18 10:24:34 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 001/521, train_loss: 0.091(0.091), train_acc: 95.833(95.833)
01/18 10:24:46 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 101/521, train_loss: 0.140(0.241), train_acc: 95.833(91.584)
01/18 10:24:59 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 201/521, train_loss: 0.267(0.232), train_acc: 89.583(91.853)
01/18 10:25:12 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 301/521, train_loss: 0.276(0.231), train_acc: 88.542(91.881)
01/18 10:25:25 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 401/521, train_loss: 0.192(0.233), train_acc: 94.792(91.758)
01/18 10:25:38 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 501/521, train_loss: 0.204(0.232), train_acc: 92.708(91.721)
01/18 10:25:40 PM [Supernet Training] lr: 0.01746 epoch: 223/600, step: 521/521, train_loss: 0.176(0.232), train_acc: 93.750(91.708)
01/18 10:25:40 PM [Supernet Training] epoch: 223, train_loss: 0.232, train_acc: 91.708
01/18 10:25:44 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 10:25:44 PM [Supernet Validation] epoch: 223, val_loss: 0.403, val_acc: 87.660, best_acc: 87.660
01/18 10:25:44 PM 

01/18 10:25:44 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 001/521, train_loss: 0.248(0.248), train_acc: 91.667(91.667)
01/18 10:25:57 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 101/521, train_loss: 0.114(0.239), train_acc: 94.792(91.522)
01/18 10:26:10 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 201/521, train_loss: 0.203(0.235), train_acc: 89.583(91.620)
01/18 10:26:23 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 301/521, train_loss: 0.337(0.235), train_acc: 88.542(91.497)
01/18 10:26:35 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 401/521, train_loss: 0.191(0.235), train_acc: 93.750(91.555)
01/18 10:26:48 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 501/521, train_loss: 0.258(0.235), train_acc: 89.583(91.569)
01/18 10:26:51 PM [Supernet Training] lr: 0.01740 epoch: 224/600, step: 521/521, train_loss: 0.153(0.234), train_acc: 93.750(91.590)
01/18 10:26:51 PM [Supernet Training] epoch: 224, train_loss: 0.234, train_acc: 91.590
01/18 10:26:54 PM [Supernet Validation] epoch: 224, val_loss: 0.425, val_acc: 86.910, best_acc: 87.660
01/18 10:26:54 PM 

01/18 10:26:55 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 001/521, train_loss: 0.251(0.251), train_acc: 88.542(88.542)
01/18 10:27:08 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 101/521, train_loss: 0.233(0.229), train_acc: 92.708(91.790)
01/18 10:27:20 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 201/521, train_loss: 0.220(0.228), train_acc: 90.625(91.884)
01/18 10:27:33 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 301/521, train_loss: 0.374(0.229), train_acc: 84.375(91.819)
01/18 10:27:46 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 401/521, train_loss: 0.258(0.233), train_acc: 91.667(91.711)
01/18 10:27:59 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 501/521, train_loss: 0.394(0.233), train_acc: 87.500(91.690)
01/18 10:28:01 PM [Supernet Training] lr: 0.01734 epoch: 225/600, step: 521/521, train_loss: 0.178(0.234), train_acc: 92.500(91.676)
01/18 10:28:01 PM [Supernet Training] epoch: 225, train_loss: 0.234, train_acc: 91.676
01/18 10:28:05 PM [Supernet Validation] epoch: 225, val_loss: 0.420, val_acc: 87.230, best_acc: 87.660
01/18 10:28:05 PM 

01/18 10:28:05 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 001/521, train_loss: 0.236(0.236), train_acc: 92.708(92.708)
01/18 10:28:18 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 101/521, train_loss: 0.304(0.236), train_acc: 87.500(91.409)
01/18 10:28:31 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 201/521, train_loss: 0.285(0.237), train_acc: 90.625(91.516)
01/18 10:28:44 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 301/521, train_loss: 0.143(0.234), train_acc: 94.792(91.615)
01/18 10:28:57 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 401/521, train_loss: 0.289(0.235), train_acc: 88.542(91.529)
01/18 10:29:09 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 501/521, train_loss: 0.254(0.234), train_acc: 91.667(91.567)
01/18 10:29:12 PM [Supernet Training] lr: 0.01728 epoch: 226/600, step: 521/521, train_loss: 0.317(0.233), train_acc: 90.000(91.588)
01/18 10:29:12 PM [Supernet Training] epoch: 226, train_loss: 0.233, train_acc: 91.588
01/18 10:29:15 PM [Supernet Validation] epoch: 226, val_loss: 0.433, val_acc: 86.470, best_acc: 87.660
01/18 10:29:15 PM 

01/18 10:29:16 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 88.542(88.542)
01/18 10:29:29 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 101/521, train_loss: 0.141(0.231), train_acc: 95.833(91.873)
01/18 10:29:41 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 201/521, train_loss: 0.262(0.230), train_acc: 90.625(91.729)
01/18 10:29:54 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 301/521, train_loss: 0.263(0.231), train_acc: 92.708(91.736)
01/18 10:30:07 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 401/521, train_loss: 0.303(0.229), train_acc: 88.542(91.742)
01/18 10:30:20 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 501/521, train_loss: 0.155(0.232), train_acc: 95.833(91.656)
01/18 10:30:22 PM [Supernet Training] lr: 0.01722 epoch: 227/600, step: 521/521, train_loss: 0.237(0.232), train_acc: 95.000(91.660)
01/18 10:30:22 PM [Supernet Training] epoch: 227, train_loss: 0.232, train_acc: 91.660
01/18 10:30:26 PM [Supernet Validation] epoch: 227, val_loss: 0.453, val_acc: 86.820, best_acc: 87.660
01/18 10:30:26 PM 

01/18 10:30:26 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 001/521, train_loss: 0.186(0.186), train_acc: 93.750(93.750)
01/18 10:30:39 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 101/521, train_loss: 0.125(0.226), train_acc: 95.833(91.852)
01/18 10:30:52 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 201/521, train_loss: 0.230(0.233), train_acc: 92.708(91.641)
01/18 10:31:05 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 301/521, train_loss: 0.209(0.235), train_acc: 92.708(91.608)
01/18 10:31:17 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 401/521, train_loss: 0.293(0.232), train_acc: 89.583(91.695)
01/18 10:31:30 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 501/521, train_loss: 0.373(0.232), train_acc: 84.375(91.687)
01/18 10:31:33 PM [Supernet Training] lr: 0.01716 epoch: 228/600, step: 521/521, train_loss: 0.272(0.232), train_acc: 91.250(91.672)
01/18 10:31:33 PM [Supernet Training] epoch: 228, train_loss: 0.232, train_acc: 91.672
01/18 10:31:36 PM [Supernet Validation] epoch: 228, val_loss: 0.426, val_acc: 86.730, best_acc: 87.660
01/18 10:31:36 PM 

01/18 10:31:37 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 001/521, train_loss: 0.259(0.259), train_acc: 91.667(91.667)
01/18 10:31:50 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 101/521, train_loss: 0.191(0.221), train_acc: 94.792(92.079)
01/18 10:32:02 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 201/521, train_loss: 0.301(0.233), train_acc: 89.583(91.682)
01/18 10:32:15 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 301/521, train_loss: 0.244(0.234), train_acc: 91.667(91.687)
01/18 10:32:28 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 401/521, train_loss: 0.252(0.232), train_acc: 92.708(91.815)
01/18 10:32:41 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 501/521, train_loss: 0.223(0.230), train_acc: 92.708(91.866)
01/18 10:32:43 PM [Supernet Training] lr: 0.01710 epoch: 229/600, step: 521/521, train_loss: 0.141(0.231), train_acc: 96.250(91.838)
01/18 10:32:43 PM [Supernet Training] epoch: 229, train_loss: 0.231, train_acc: 91.838
01/18 10:32:47 PM [Supernet Validation] epoch: 229, val_loss: 0.429, val_acc: 86.930, best_acc: 87.660
01/18 10:32:47 PM 

01/18 10:32:47 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 001/521, train_loss: 0.229(0.229), train_acc: 91.667(91.667)
01/18 10:33:00 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 101/521, train_loss: 0.221(0.226), train_acc: 92.708(91.945)
01/18 10:33:13 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 201/521, train_loss: 0.341(0.227), train_acc: 87.500(91.884)
01/18 10:33:26 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 301/521, train_loss: 0.165(0.231), train_acc: 95.833(91.798)
01/18 10:33:38 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 401/521, train_loss: 0.159(0.233), train_acc: 95.833(91.711)
01/18 10:33:51 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 501/521, train_loss: 0.199(0.233), train_acc: 90.625(91.673)
01/18 10:33:54 PM [Supernet Training] lr: 0.01704 epoch: 230/600, step: 521/521, train_loss: 0.080(0.233), train_acc: 98.750(91.692)
01/18 10:33:54 PM [Supernet Training] epoch: 230, train_loss: 0.233, train_acc: 91.692
01/18 10:33:57 PM [Supernet Validation] epoch: 230, val_loss: 0.412, val_acc: 87.330, best_acc: 87.660
01/18 10:33:57 PM 

01/18 10:33:58 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 001/521, train_loss: 0.266(0.266), train_acc: 88.542(88.542)
01/18 10:34:10 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 101/521, train_loss: 0.292(0.223), train_acc: 91.667(92.079)
01/18 10:34:23 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 201/521, train_loss: 0.183(0.228), train_acc: 94.792(91.931)
01/18 10:34:36 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 301/521, train_loss: 0.416(0.229), train_acc: 91.667(91.954)
01/18 10:34:49 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 401/521, train_loss: 0.206(0.232), train_acc: 91.667(91.825)
01/18 10:35:01 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 501/521, train_loss: 0.138(0.231), train_acc: 96.875(91.821)
01/18 10:35:04 PM [Supernet Training] lr: 0.01698 epoch: 231/600, step: 521/521, train_loss: 0.250(0.231), train_acc: 92.500(91.794)
01/18 10:35:04 PM [Supernet Training] epoch: 231, train_loss: 0.231, train_acc: 91.794
01/18 10:35:08 PM [Supernet Validation] epoch: 231, val_loss: 0.430, val_acc: 86.950, best_acc: 87.660
01/18 10:35:08 PM 

01/18 10:35:08 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 001/521, train_loss: 0.195(0.195), train_acc: 94.792(94.792)
01/18 10:35:21 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 101/521, train_loss: 0.185(0.228), train_acc: 94.792(92.172)
01/18 10:35:33 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 201/521, train_loss: 0.382(0.227), train_acc: 84.375(91.978)
01/18 10:35:46 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 301/521, train_loss: 0.275(0.226), train_acc: 89.583(92.085)
01/18 10:35:59 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 401/521, train_loss: 0.197(0.229), train_acc: 92.708(91.973)
01/18 10:36:12 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 501/521, train_loss: 0.153(0.228), train_acc: 94.792(91.922)
01/18 10:36:14 PM [Supernet Training] lr: 0.01692 epoch: 232/600, step: 521/521, train_loss: 0.121(0.228), train_acc: 93.750(91.888)
01/18 10:36:14 PM [Supernet Training] epoch: 232, train_loss: 0.228, train_acc: 91.888
01/18 10:36:18 PM [Supernet Validation] epoch: 232, val_loss: 0.413, val_acc: 87.130, best_acc: 87.660
01/18 10:36:18 PM 

01/18 10:36:18 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 001/521, train_loss: 0.144(0.144), train_acc: 95.833(95.833)
01/18 10:36:31 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 101/521, train_loss: 0.211(0.217), train_acc: 92.708(92.554)
01/18 10:36:44 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 201/521, train_loss: 0.287(0.218), train_acc: 88.542(92.340)
01/18 10:36:57 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 301/521, train_loss: 0.196(0.220), train_acc: 91.667(92.207)
01/18 10:37:09 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 401/521, train_loss: 0.309(0.222), train_acc: 90.625(92.134)
01/18 10:37:22 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 501/521, train_loss: 0.264(0.225), train_acc: 90.625(91.976)
01/18 10:37:25 PM [Supernet Training] lr: 0.01686 epoch: 233/600, step: 521/521, train_loss: 0.344(0.225), train_acc: 90.000(91.976)
01/18 10:37:25 PM [Supernet Training] epoch: 233, train_loss: 0.225, train_acc: 91.976
01/18 10:37:28 PM [Supernet Validation] epoch: 233, val_loss: 0.426, val_acc: 87.190, best_acc: 87.660
01/18 10:37:28 PM 

01/18 10:37:29 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 001/521, train_loss: 0.344(0.344), train_acc: 87.500(87.500)
01/18 10:37:42 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 101/521, train_loss: 0.216(0.224), train_acc: 91.667(91.790)
01/18 10:37:55 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 201/521, train_loss: 0.244(0.222), train_acc: 90.625(91.957)
01/18 10:38:07 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 301/521, train_loss: 0.289(0.221), train_acc: 90.625(92.075)
01/18 10:38:20 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 401/521, train_loss: 0.164(0.222), train_acc: 94.792(92.074)
01/18 10:38:33 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 501/521, train_loss: 0.183(0.224), train_acc: 92.708(92.043)
01/18 10:38:36 PM [Supernet Training] lr: 0.01680 epoch: 234/600, step: 521/521, train_loss: 0.213(0.224), train_acc: 92.500(92.064)
01/18 10:38:36 PM [Supernet Training] epoch: 234, train_loss: 0.224, train_acc: 92.064
01/18 10:38:39 PM [Supernet Validation] epoch: 234, val_loss: 0.430, val_acc: 86.900, best_acc: 87.660
01/18 10:38:39 PM 

01/18 10:38:40 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 001/521, train_loss: 0.235(0.235), train_acc: 89.583(89.583)
01/18 10:38:52 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 101/521, train_loss: 0.219(0.221), train_acc: 91.667(91.997)
01/18 10:39:05 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 201/521, train_loss: 0.266(0.224), train_acc: 89.583(91.962)
01/18 10:39:18 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 301/521, train_loss: 0.232(0.225), train_acc: 90.625(91.902)
01/18 10:39:31 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 401/521, train_loss: 0.228(0.223), train_acc: 90.625(92.025)
01/18 10:39:44 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 501/521, train_loss: 0.271(0.221), train_acc: 90.625(92.174)
01/18 10:39:46 PM [Supernet Training] lr: 0.01673 epoch: 235/600, step: 521/521, train_loss: 0.165(0.221), train_acc: 95.000(92.166)
01/18 10:39:46 PM [Supernet Training] epoch: 235, train_loss: 0.221, train_acc: 92.166
01/18 10:39:50 PM [Supernet Validation] epoch: 235, val_loss: 0.403, val_acc: 87.230, best_acc: 87.660
01/18 10:39:50 PM 

01/18 10:39:50 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 001/521, train_loss: 0.197(0.197), train_acc: 92.708(92.708)
01/18 10:40:03 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 101/521, train_loss: 0.119(0.222), train_acc: 96.875(91.925)
01/18 10:40:16 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 201/521, train_loss: 0.272(0.223), train_acc: 91.667(91.957)
01/18 10:40:29 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 301/521, train_loss: 0.183(0.222), train_acc: 94.792(92.092)
01/18 10:40:41 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 401/521, train_loss: 0.187(0.223), train_acc: 92.708(92.054)
01/18 10:40:54 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 501/521, train_loss: 0.226(0.221), train_acc: 92.708(92.064)
01/18 10:40:57 PM [Supernet Training] lr: 0.01667 epoch: 236/600, step: 521/521, train_loss: 0.121(0.222), train_acc: 96.250(92.022)
01/18 10:40:57 PM [Supernet Training] epoch: 236, train_loss: 0.222, train_acc: 92.022
01/18 10:41:00 PM [Supernet Validation] epoch: 236, val_loss: 0.421, val_acc: 87.050, best_acc: 87.660
01/18 10:41:00 PM 

01/18 10:41:01 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 001/521, train_loss: 0.210(0.210), train_acc: 94.792(94.792)
01/18 10:41:14 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 101/521, train_loss: 0.283(0.228), train_acc: 87.500(91.986)
01/18 10:41:26 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 201/521, train_loss: 0.163(0.226), train_acc: 93.750(92.061)
01/18 10:41:39 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 301/521, train_loss: 0.251(0.226), train_acc: 89.583(91.975)
01/18 10:41:52 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 401/521, train_loss: 0.318(0.227), train_acc: 86.458(91.856)
01/18 10:42:05 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 501/521, train_loss: 0.296(0.228), train_acc: 91.667(91.891)
01/18 10:42:07 PM [Supernet Training] lr: 0.01661 epoch: 237/600, step: 521/521, train_loss: 0.227(0.228), train_acc: 95.000(91.880)
01/18 10:42:07 PM [Supernet Training] epoch: 237, train_loss: 0.228, train_acc: 91.880
01/18 10:42:11 PM [Supernet Validation] epoch: 237, val_loss: 0.428, val_acc: 86.810, best_acc: 87.660
01/18 10:42:11 PM 

01/18 10:42:11 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 001/521, train_loss: 0.236(0.236), train_acc: 92.708(92.708)
01/18 10:42:24 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 101/521, train_loss: 0.248(0.214), train_acc: 92.708(92.677)
01/18 10:42:37 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 201/521, train_loss: 0.113(0.219), train_acc: 93.750(92.263)
01/18 10:42:50 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 301/521, train_loss: 0.259(0.221), train_acc: 90.625(92.106)
01/18 10:43:03 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 401/521, train_loss: 0.191(0.223), train_acc: 92.708(92.119)
01/18 10:43:15 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 501/521, train_loss: 0.232(0.222), train_acc: 91.667(92.155)
01/18 10:43:18 PM [Supernet Training] lr: 0.01655 epoch: 238/600, step: 521/521, train_loss: 0.173(0.222), train_acc: 95.000(92.134)
01/18 10:43:18 PM [Supernet Training] epoch: 238, train_loss: 0.222, train_acc: 92.134
01/18 10:43:22 PM [Supernet Validation] epoch: 238, val_loss: 0.422, val_acc: 86.950, best_acc: 87.660
01/18 10:43:22 PM 

01/18 10:43:22 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 001/521, train_loss: 0.185(0.185), train_acc: 92.708(92.708)
01/18 10:43:35 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 101/521, train_loss: 0.260(0.227), train_acc: 92.708(91.759)
01/18 10:43:48 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 201/521, train_loss: 0.180(0.218), train_acc: 95.833(92.081)
01/18 10:44:00 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 301/521, train_loss: 0.242(0.221), train_acc: 93.750(92.072)
01/18 10:44:13 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 401/521, train_loss: 0.290(0.223), train_acc: 91.667(92.049)
01/18 10:44:26 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 501/521, train_loss: 0.210(0.223), train_acc: 91.667(92.031)
01/18 10:44:28 PM [Supernet Training] lr: 0.01649 epoch: 239/600, step: 521/521, train_loss: 0.182(0.223), train_acc: 92.500(92.050)
01/18 10:44:29 PM [Supernet Training] epoch: 239, train_loss: 0.223, train_acc: 92.050
01/18 10:44:32 PM [Supernet Validation] epoch: 239, val_loss: 0.436, val_acc: 86.660, best_acc: 87.660
01/18 10:44:32 PM 

01/18 10:44:33 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 001/521, train_loss: 0.286(0.286), train_acc: 86.458(86.458)
01/18 10:44:45 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 101/521, train_loss: 0.316(0.211), train_acc: 89.583(92.770)
01/18 10:44:58 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 201/521, train_loss: 0.125(0.213), train_acc: 95.833(92.527)
01/18 10:45:11 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 301/521, train_loss: 0.208(0.216), train_acc: 93.750(92.386)
01/18 10:45:24 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 401/521, train_loss: 0.199(0.218), train_acc: 95.833(92.316)
01/18 10:45:37 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 501/521, train_loss: 0.204(0.219), train_acc: 93.750(92.216)
01/18 10:45:39 PM [Supernet Training] lr: 0.01642 epoch: 240/600, step: 521/521, train_loss: 0.248(0.220), train_acc: 91.250(92.156)
01/18 10:45:39 PM [Supernet Training] epoch: 240, train_loss: 0.220, train_acc: 92.156
01/18 10:45:43 PM [Supernet Validation] epoch: 240, val_loss: 0.414, val_acc: 87.430, best_acc: 87.660
01/18 10:45:43 PM 

01/18 10:45:43 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 001/521, train_loss: 0.103(0.103), train_acc: 95.833(95.833)
01/18 10:45:56 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 101/521, train_loss: 0.141(0.221), train_acc: 93.750(92.007)
01/18 10:46:09 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 201/521, train_loss: 0.298(0.220), train_acc: 90.625(91.864)
01/18 10:46:22 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 301/521, train_loss: 0.226(0.219), train_acc: 91.667(91.923)
01/18 10:46:34 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 401/521, train_loss: 0.318(0.218), train_acc: 87.500(92.059)
01/18 10:46:47 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 501/521, train_loss: 0.188(0.222), train_acc: 94.792(92.024)
01/18 10:46:50 PM [Supernet Training] lr: 0.01636 epoch: 241/600, step: 521/521, train_loss: 0.102(0.222), train_acc: 95.000(92.018)
01/18 10:46:50 PM [Supernet Training] epoch: 241, train_loss: 0.222, train_acc: 92.018
01/18 10:46:53 PM [Supernet Validation] epoch: 241, val_loss: 0.416, val_acc: 87.320, best_acc: 87.660
01/18 10:46:53 PM 

01/18 10:46:54 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 001/521, train_loss: 0.243(0.243), train_acc: 89.583(89.583)
01/18 10:47:06 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 101/521, train_loss: 0.338(0.216), train_acc: 86.458(91.997)
01/18 10:47:19 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 201/521, train_loss: 0.306(0.221), train_acc: 92.708(91.905)
01/18 10:47:32 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 301/521, train_loss: 0.274(0.220), train_acc: 89.583(92.061)
01/18 10:47:45 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 401/521, train_loss: 0.151(0.221), train_acc: 93.750(92.080)
01/18 10:47:58 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 501/521, train_loss: 0.235(0.219), train_acc: 92.708(92.201)
01/18 10:48:00 PM [Supernet Training] lr: 0.01630 epoch: 242/600, step: 521/521, train_loss: 0.141(0.219), train_acc: 97.500(92.226)
01/18 10:48:00 PM [Supernet Training] epoch: 242, train_loss: 0.219, train_acc: 92.226
01/18 10:48:04 PM [Supernet Validation] epoch: 242, val_loss: 0.408, val_acc: 87.280, best_acc: 87.660
01/18 10:48:04 PM 

01/18 10:48:04 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 001/521, train_loss: 0.238(0.238), train_acc: 91.667(91.667)
01/18 10:48:17 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 101/521, train_loss: 0.158(0.207), train_acc: 94.792(92.523)
01/18 10:48:30 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 201/521, train_loss: 0.134(0.209), train_acc: 96.875(92.651)
01/18 10:48:43 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 301/521, train_loss: 0.174(0.212), train_acc: 94.792(92.438)
01/18 10:48:55 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 401/521, train_loss: 0.072(0.212), train_acc: 97.917(92.449)
01/18 10:49:08 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 501/521, train_loss: 0.207(0.215), train_acc: 92.708(92.347)
01/18 10:49:11 PM [Supernet Training] lr: 0.01624 epoch: 243/600, step: 521/521, train_loss: 0.265(0.215), train_acc: 92.500(92.350)
01/18 10:49:11 PM [Supernet Training] epoch: 243, train_loss: 0.215, train_acc: 92.350
01/18 10:49:14 PM [Supernet Validation] epoch: 243, val_loss: 0.424, val_acc: 87.120, best_acc: 87.660
01/18 10:49:14 PM 

01/18 10:49:15 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 001/521, train_loss: 0.215(0.215), train_acc: 93.750(93.750)
01/18 10:49:28 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 101/521, train_loss: 0.191(0.208), train_acc: 92.708(92.554)
01/18 10:49:40 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 201/521, train_loss: 0.145(0.213), train_acc: 96.875(92.371)
01/18 10:49:53 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 301/521, train_loss: 0.217(0.214), train_acc: 92.708(92.338)
01/18 10:50:06 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 401/521, train_loss: 0.166(0.215), train_acc: 92.708(92.160)
01/18 10:50:19 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 501/521, train_loss: 0.161(0.216), train_acc: 94.792(92.205)
01/18 10:50:21 PM [Supernet Training] lr: 0.01618 epoch: 244/600, step: 521/521, train_loss: 0.199(0.217), train_acc: 91.250(92.188)
01/18 10:50:21 PM [Supernet Training] epoch: 244, train_loss: 0.217, train_acc: 92.188
01/18 10:50:25 PM [Supernet Validation] epoch: 244, val_loss: 0.437, val_acc: 86.610, best_acc: 87.660
01/18 10:50:25 PM 

01/18 10:50:25 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 001/521, train_loss: 0.167(0.167), train_acc: 93.750(93.750)
01/18 10:50:38 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 101/521, train_loss: 0.169(0.226), train_acc: 93.750(92.162)
01/18 10:50:51 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 201/521, train_loss: 0.128(0.215), train_acc: 95.833(92.382)
01/18 10:51:04 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 301/521, train_loss: 0.293(0.218), train_acc: 86.458(92.203)
01/18 10:51:16 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 401/521, train_loss: 0.259(0.215), train_acc: 92.708(92.246)
01/18 10:51:29 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 501/521, train_loss: 0.243(0.215), train_acc: 89.583(92.278)
01/18 10:51:32 PM [Supernet Training] lr: 0.01611 epoch: 245/600, step: 521/521, train_loss: 0.234(0.214), train_acc: 87.500(92.274)
01/18 10:51:32 PM [Supernet Training] epoch: 245, train_loss: 0.214, train_acc: 92.274
01/18 10:51:36 PM [Supernet Validation] epoch: 245, val_loss: 0.428, val_acc: 87.320, best_acc: 87.660
01/18 10:51:36 PM 

01/18 10:51:36 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 001/521, train_loss: 0.274(0.274), train_acc: 89.583(89.583)
01/18 10:51:49 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 101/521, train_loss: 0.189(0.217), train_acc: 93.750(92.213)
01/18 10:52:01 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 201/521, train_loss: 0.372(0.218), train_acc: 87.500(92.133)
01/18 10:52:14 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 301/521, train_loss: 0.088(0.218), train_acc: 96.875(92.148)
01/18 10:52:27 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 401/521, train_loss: 0.087(0.218), train_acc: 97.917(92.202)
01/18 10:52:40 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 501/521, train_loss: 0.198(0.216), train_acc: 92.708(92.261)
01/18 10:52:42 PM [Supernet Training] lr: 0.01605 epoch: 246/600, step: 521/521, train_loss: 0.256(0.217), train_acc: 90.000(92.242)
01/18 10:52:42 PM [Supernet Training] epoch: 246, train_loss: 0.217, train_acc: 92.242
01/18 10:52:46 PM [Supernet Validation] epoch: 246, val_loss: 0.432, val_acc: 86.630, best_acc: 87.660
01/18 10:52:46 PM 

01/18 10:52:46 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 001/521, train_loss: 0.165(0.165), train_acc: 91.667(91.667)
01/18 10:52:59 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 101/521, train_loss: 0.175(0.209), train_acc: 91.667(92.327)
01/18 10:53:12 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 201/521, train_loss: 0.305(0.212), train_acc: 91.667(92.185)
01/18 10:53:25 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 301/521, train_loss: 0.148(0.216), train_acc: 92.708(92.117)
01/18 10:53:37 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 401/521, train_loss: 0.287(0.217), train_acc: 88.542(92.046)
01/18 10:53:50 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 501/521, train_loss: 0.224(0.217), train_acc: 90.625(92.070)
01/18 10:53:53 PM [Supernet Training] lr: 0.01599 epoch: 247/600, step: 521/521, train_loss: 0.127(0.216), train_acc: 97.500(92.116)
01/18 10:53:53 PM [Supernet Training] epoch: 247, train_loss: 0.216, train_acc: 92.116
01/18 10:53:56 PM [Supernet Validation] epoch: 247, val_loss: 0.416, val_acc: 87.450, best_acc: 87.660
01/18 10:53:56 PM 

01/18 10:53:57 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 001/521, train_loss: 0.229(0.229), train_acc: 92.708(92.708)
01/18 10:54:10 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 101/521, train_loss: 0.335(0.199), train_acc: 89.583(92.873)
01/18 10:54:22 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 201/521, train_loss: 0.291(0.204), train_acc: 90.625(92.688)
01/18 10:54:35 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 301/521, train_loss: 0.178(0.201), train_acc: 93.750(92.861)
01/18 10:54:48 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 401/521, train_loss: 0.235(0.203), train_acc: 94.792(92.872)
01/18 10:55:01 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 501/521, train_loss: 0.273(0.204), train_acc: 90.625(92.773)
01/18 10:55:03 PM [Supernet Training] lr: 0.01592 epoch: 248/600, step: 521/521, train_loss: 0.090(0.204), train_acc: 96.250(92.788)
01/18 10:55:03 PM [Supernet Training] epoch: 248, train_loss: 0.204, train_acc: 92.788
01/18 10:55:07 PM [Supernet Validation] epoch: 248, val_loss: 0.423, val_acc: 87.210, best_acc: 87.660
01/18 10:55:07 PM 

01/18 10:55:07 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 001/521, train_loss: 0.220(0.220), train_acc: 91.667(91.667)
01/18 10:55:20 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 101/521, train_loss: 0.224(0.207), train_acc: 91.667(92.440)
01/18 10:55:33 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 201/521, train_loss: 0.267(0.215), train_acc: 92.708(92.299)
01/18 10:55:46 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 301/521, train_loss: 0.326(0.212), train_acc: 91.667(92.411)
01/18 10:55:59 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 401/521, train_loss: 0.208(0.211), train_acc: 93.750(92.467)
01/18 10:56:11 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 501/521, train_loss: 0.263(0.214), train_acc: 92.708(92.320)
01/18 10:56:14 PM [Supernet Training] lr: 0.01586 epoch: 249/600, step: 521/521, train_loss: 0.242(0.214), train_acc: 90.000(92.320)
01/18 10:56:14 PM [Supernet Training] epoch: 249, train_loss: 0.214, train_acc: 92.320
01/18 10:56:17 PM [Supernet Validation] epoch: 249, val_loss: 0.425, val_acc: 86.960, best_acc: 87.660
01/18 10:56:17 PM 

01/18 10:56:18 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 001/521, train_loss: 0.106(0.106), train_acc: 95.833(95.833)
01/18 10:56:31 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 101/521, train_loss: 0.186(0.199), train_acc: 92.708(92.626)
01/18 10:56:43 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 201/521, train_loss: 0.238(0.204), train_acc: 91.667(92.646)
01/18 10:56:56 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 301/521, train_loss: 0.234(0.203), train_acc: 89.583(92.698)
01/18 10:57:09 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 401/521, train_loss: 0.201(0.205), train_acc: 93.750(92.659)
01/18 10:57:22 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 501/521, train_loss: 0.136(0.208), train_acc: 94.792(92.532)
01/18 10:57:24 PM [Supernet Training] lr: 0.01580 epoch: 250/600, step: 521/521, train_loss: 0.319(0.208), train_acc: 87.500(92.532)
01/18 10:57:24 PM [Supernet Training] epoch: 250, train_loss: 0.208, train_acc: 92.532
01/18 10:57:28 PM [Supernet Validation] epoch: 250, val_loss: 0.425, val_acc: 87.080, best_acc: 87.660
01/18 10:57:28 PM 

01/18 10:57:28 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 001/521, train_loss: 0.230(0.230), train_acc: 91.667(91.667)
01/18 10:57:41 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 101/521, train_loss: 0.106(0.200), train_acc: 95.833(92.708)
01/18 10:57:54 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 201/521, train_loss: 0.297(0.204), train_acc: 88.542(92.698)
01/18 10:58:07 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 301/521, train_loss: 0.247(0.207), train_acc: 91.667(92.553)
01/18 10:58:20 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 401/521, train_loss: 0.294(0.208), train_acc: 91.667(92.539)
01/18 10:58:32 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 501/521, train_loss: 0.358(0.210), train_acc: 90.625(92.467)
01/18 10:58:35 PM [Supernet Training] lr: 0.01574 epoch: 251/600, step: 521/521, train_loss: 0.140(0.210), train_acc: 96.250(92.458)
01/18 10:58:35 PM [Supernet Training] epoch: 251, train_loss: 0.210, train_acc: 92.458
01/18 10:58:38 PM [Supernet Validation] epoch: 251, val_loss: 0.419, val_acc: 87.180, best_acc: 87.660
01/18 10:58:38 PM 

01/18 10:58:39 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 001/521, train_loss: 0.246(0.246), train_acc: 87.500(87.500)
01/18 10:58:52 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 101/521, train_loss: 0.205(0.211), train_acc: 93.750(92.038)
01/18 10:59:04 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 201/521, train_loss: 0.215(0.211), train_acc: 92.708(92.185)
01/18 10:59:17 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 301/521, train_loss: 0.261(0.210), train_acc: 91.667(92.293)
01/18 10:59:30 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 401/521, train_loss: 0.325(0.210), train_acc: 87.500(92.389)
01/18 10:59:43 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 501/521, train_loss: 0.183(0.210), train_acc: 92.708(92.409)
01/18 10:59:45 PM [Supernet Training] lr: 0.01567 epoch: 252/600, step: 521/521, train_loss: 0.183(0.210), train_acc: 91.250(92.408)
01/18 10:59:45 PM [Supernet Training] epoch: 252, train_loss: 0.210, train_acc: 92.408
01/18 10:59:49 PM [Supernet Validation] epoch: 252, val_loss: 0.421, val_acc: 86.900, best_acc: 87.660
01/18 10:59:49 PM 

01/18 10:59:49 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 001/521, train_loss: 0.246(0.246), train_acc: 89.583(89.583)
01/18 11:00:02 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 101/521, train_loss: 0.169(0.198), train_acc: 92.708(92.739)
01/18 11:00:15 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 201/521, train_loss: 0.211(0.203), train_acc: 91.667(92.496)
01/18 11:00:28 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 301/521, train_loss: 0.243(0.207), train_acc: 92.708(92.463)
01/18 11:00:41 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 401/521, train_loss: 0.145(0.208), train_acc: 93.750(92.467)
01/18 11:00:53 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 501/521, train_loss: 0.148(0.208), train_acc: 94.792(92.509)
01/18 11:00:56 PM [Supernet Training] lr: 0.01561 epoch: 253/600, step: 521/521, train_loss: 0.223(0.208), train_acc: 91.250(92.522)
01/18 11:00:56 PM [Supernet Training] epoch: 253, train_loss: 0.208, train_acc: 92.522
01/18 11:00:59 PM [Supernet Validation] epoch: 253, val_loss: 0.421, val_acc: 87.220, best_acc: 87.660
01/18 11:00:59 PM 

01/18 11:01:00 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 001/521, train_loss: 0.213(0.213), train_acc: 91.667(91.667)
01/18 11:01:13 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 101/521, train_loss: 0.266(0.204), train_acc: 90.625(92.750)
01/18 11:01:26 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 201/521, train_loss: 0.266(0.205), train_acc: 88.542(92.682)
01/18 11:01:38 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 301/521, train_loss: 0.123(0.205), train_acc: 95.833(92.750)
01/18 11:01:51 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 401/521, train_loss: 0.273(0.206), train_acc: 89.583(92.701)
01/18 11:02:04 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 501/521, train_loss: 0.244(0.206), train_acc: 88.542(92.675)
01/18 11:02:06 PM [Supernet Training] lr: 0.01555 epoch: 254/600, step: 521/521, train_loss: 0.239(0.207), train_acc: 87.500(92.624)
01/18 11:02:07 PM [Supernet Training] epoch: 254, train_loss: 0.207, train_acc: 92.624
01/18 11:02:10 PM [Supernet Validation] epoch: 254, val_loss: 0.425, val_acc: 87.120, best_acc: 87.660
01/18 11:02:10 PM 

01/18 11:02:10 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 001/521, train_loss: 0.163(0.163), train_acc: 94.792(94.792)
01/18 11:02:23 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 101/521, train_loss: 0.190(0.206), train_acc: 95.833(92.523)
01/18 11:02:36 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 201/521, train_loss: 0.232(0.195), train_acc: 91.667(92.983)
01/18 11:02:49 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 301/521, train_loss: 0.203(0.199), train_acc: 90.625(92.881)
01/18 11:03:02 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 401/521, train_loss: 0.229(0.204), train_acc: 91.667(92.740)
01/18 11:03:14 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 501/521, train_loss: 0.259(0.203), train_acc: 93.750(92.723)
01/18 11:03:17 PM [Supernet Training] lr: 0.01548 epoch: 255/600, step: 521/521, train_loss: 0.255(0.203), train_acc: 91.250(92.728)
01/18 11:03:17 PM [Supernet Training] epoch: 255, train_loss: 0.203, train_acc: 92.728
01/18 11:03:21 PM [Supernet Validation] epoch: 255, val_loss: 0.427, val_acc: 86.700, best_acc: 87.660
01/18 11:03:21 PM 

01/18 11:03:21 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 001/521, train_loss: 0.168(0.168), train_acc: 93.750(93.750)
01/18 11:03:34 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 101/521, train_loss: 0.121(0.203), train_acc: 95.833(92.605)
01/18 11:03:47 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 201/521, train_loss: 0.238(0.201), train_acc: 87.500(92.703)
01/18 11:03:59 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 301/521, train_loss: 0.110(0.202), train_acc: 97.917(92.726)
01/18 11:04:12 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 401/521, train_loss: 0.242(0.203), train_acc: 91.667(92.664)
01/18 11:04:25 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 501/521, train_loss: 0.143(0.202), train_acc: 93.750(92.663)
01/18 11:04:27 PM [Supernet Training] lr: 0.01542 epoch: 256/600, step: 521/521, train_loss: 0.140(0.202), train_acc: 96.250(92.666)
01/18 11:04:28 PM [Supernet Training] epoch: 256, train_loss: 0.202, train_acc: 92.666
01/18 11:04:31 PM [Supernet Validation] epoch: 256, val_loss: 0.418, val_acc: 87.510, best_acc: 87.660
01/18 11:04:31 PM 

01/18 11:04:31 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 001/521, train_loss: 0.255(0.255), train_acc: 87.500(87.500)
01/18 11:04:44 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 101/521, train_loss: 0.137(0.210), train_acc: 93.750(92.502)
01/18 11:04:57 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 201/521, train_loss: 0.234(0.208), train_acc: 92.708(92.579)
01/18 11:05:10 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 301/521, train_loss: 0.233(0.210), train_acc: 91.667(92.497)
01/18 11:05:23 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 401/521, train_loss: 0.199(0.209), train_acc: 95.833(92.529)
01/18 11:05:35 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 501/521, train_loss: 0.079(0.207), train_acc: 97.917(92.604)
01/18 11:05:38 PM [Supernet Training] lr: 0.01535 epoch: 257/600, step: 521/521, train_loss: 0.194(0.206), train_acc: 93.750(92.630)
01/18 11:05:38 PM [Supernet Training] epoch: 257, train_loss: 0.206, train_acc: 92.630
01/18 11:05:42 PM [Supernet Validation] epoch: 257, val_loss: 0.425, val_acc: 87.290, best_acc: 87.660
01/18 11:05:42 PM 

01/18 11:05:42 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 001/521, train_loss: 0.101(0.101), train_acc: 96.875(96.875)
01/18 11:05:55 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 101/521, train_loss: 0.180(0.204), train_acc: 91.667(92.729)
01/18 11:06:07 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 201/521, train_loss: 0.208(0.205), train_acc: 91.667(92.605)
01/18 11:06:20 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 301/521, train_loss: 0.246(0.208), train_acc: 91.667(92.549)
01/18 11:06:33 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 401/521, train_loss: 0.185(0.208), train_acc: 94.792(92.537)
01/18 11:06:46 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 501/521, train_loss: 0.226(0.208), train_acc: 89.583(92.486)
01/18 11:06:48 PM [Supernet Training] lr: 0.01529 epoch: 258/600, step: 521/521, train_loss: 0.110(0.209), train_acc: 95.000(92.486)
01/18 11:06:48 PM [Supernet Training] epoch: 258, train_loss: 0.209, train_acc: 92.486
01/18 11:06:52 PM [Supernet Validation] epoch: 258, val_loss: 0.408, val_acc: 87.550, best_acc: 87.660
01/18 11:06:52 PM 

01/18 11:06:52 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 001/521, train_loss: 0.245(0.245), train_acc: 92.708(92.708)
01/18 11:07:05 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 101/521, train_loss: 0.163(0.214), train_acc: 94.792(92.533)
01/18 11:07:18 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 201/521, train_loss: 0.137(0.201), train_acc: 95.833(93.030)
01/18 11:07:31 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 301/521, train_loss: 0.185(0.202), train_acc: 92.708(92.909)
01/18 11:07:43 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 401/521, train_loss: 0.272(0.201), train_acc: 89.583(92.869)
01/18 11:07:56 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 501/521, train_loss: 0.266(0.201), train_acc: 87.500(92.837)
01/18 11:07:59 PM [Supernet Training] lr: 0.01523 epoch: 259/600, step: 521/521, train_loss: 0.263(0.202), train_acc: 92.500(92.828)
01/18 11:07:59 PM [Supernet Training] epoch: 259, train_loss: 0.202, train_acc: 92.828
01/18 11:08:02 PM [Supernet Validation] epoch: 259, val_loss: 0.420, val_acc: 87.600, best_acc: 87.660
01/18 11:08:02 PM 

01/18 11:08:03 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 001/521, train_loss: 0.187(0.187), train_acc: 92.708(92.708)
01/18 11:08:15 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 101/521, train_loss: 0.228(0.192), train_acc: 93.750(93.141)
01/18 11:08:28 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 201/521, train_loss: 0.196(0.200), train_acc: 95.833(92.869)
01/18 11:08:41 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 301/521, train_loss: 0.200(0.200), train_acc: 93.750(92.961)
01/18 11:08:54 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 401/521, train_loss: 0.154(0.197), train_acc: 94.792(92.999)
01/18 11:09:07 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 501/521, train_loss: 0.108(0.198), train_acc: 97.917(92.962)
01/18 11:09:09 PM [Supernet Training] lr: 0.01516 epoch: 260/600, step: 521/521, train_loss: 0.339(0.198), train_acc: 86.250(92.960)
01/18 11:09:09 PM [Supernet Training] epoch: 260, train_loss: 0.198, train_acc: 92.960
01/18 11:09:13 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:09:13 PM [Supernet Validation] epoch: 260, val_loss: 0.419, val_acc: 87.690, best_acc: 87.690
01/18 11:09:13 PM 

01/18 11:09:13 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 001/521, train_loss: 0.170(0.170), train_acc: 92.708(92.708)
01/18 11:09:26 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 101/521, train_loss: 0.136(0.199), train_acc: 95.833(92.925)
01/18 11:09:39 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 201/521, train_loss: 0.150(0.199), train_acc: 92.708(92.988)
01/18 11:09:52 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 301/521, train_loss: 0.172(0.203), train_acc: 91.667(92.833)
01/18 11:10:05 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 401/521, train_loss: 0.274(0.200), train_acc: 89.583(92.903)
01/18 11:10:17 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 501/521, train_loss: 0.219(0.201), train_acc: 90.625(92.816)
01/18 11:10:20 PM [Supernet Training] lr: 0.01510 epoch: 261/600, step: 521/521, train_loss: 0.106(0.201), train_acc: 93.750(92.778)
01/18 11:10:20 PM [Supernet Training] epoch: 261, train_loss: 0.201, train_acc: 92.778
01/18 11:10:24 PM [Supernet Validation] epoch: 261, val_loss: 0.435, val_acc: 87.330, best_acc: 87.690
01/18 11:10:24 PM 

01/18 11:10:24 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 001/521, train_loss: 0.352(0.352), train_acc: 88.542(88.542)
01/18 11:10:37 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 101/521, train_loss: 0.138(0.204), train_acc: 94.792(92.523)
01/18 11:10:50 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 201/521, train_loss: 0.248(0.203), train_acc: 91.667(92.620)
01/18 11:11:02 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 301/521, train_loss: 0.252(0.199), train_acc: 89.583(92.791)
01/18 11:11:15 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 401/521, train_loss: 0.169(0.201), train_acc: 93.750(92.799)
01/18 11:11:28 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 501/521, train_loss: 0.135(0.200), train_acc: 94.792(92.858)
01/18 11:11:30 PM [Supernet Training] lr: 0.01503 epoch: 262/600, step: 521/521, train_loss: 0.259(0.200), train_acc: 95.000(92.844)
01/18 11:11:31 PM [Supernet Training] epoch: 262, train_loss: 0.200, train_acc: 92.844
01/18 11:11:34 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:11:34 PM [Supernet Validation] epoch: 262, val_loss: 0.410, val_acc: 87.760, best_acc: 87.760
01/18 11:11:34 PM 

01/18 11:11:35 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 001/521, train_loss: 0.091(0.091), train_acc: 95.833(95.833)
01/18 11:11:47 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 101/521, train_loss: 0.255(0.196), train_acc: 91.667(93.090)
01/18 11:12:00 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 201/521, train_loss: 0.107(0.199), train_acc: 96.875(92.874)
01/18 11:12:13 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 301/521, train_loss: 0.191(0.196), train_acc: 91.667(93.065)
01/18 11:12:26 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 401/521, train_loss: 0.153(0.196), train_acc: 97.917(93.116)
01/18 11:12:38 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 501/521, train_loss: 0.135(0.199), train_acc: 96.875(92.987)
01/18 11:12:41 PM [Supernet Training] lr: 0.01497 epoch: 263/600, step: 521/521, train_loss: 0.333(0.199), train_acc: 88.750(92.920)
01/18 11:12:41 PM [Supernet Training] epoch: 263, train_loss: 0.199, train_acc: 92.920
01/18 11:12:44 PM [Supernet Validation] epoch: 263, val_loss: 0.435, val_acc: 87.240, best_acc: 87.760
01/18 11:12:44 PM 

01/18 11:12:45 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 001/521, train_loss: 0.163(0.163), train_acc: 93.750(93.750)
01/18 11:12:58 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 101/521, train_loss: 0.290(0.188), train_acc: 91.667(93.080)
01/18 11:13:11 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 201/521, train_loss: 0.236(0.192), train_acc: 89.583(93.087)
01/18 11:13:23 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 301/521, train_loss: 0.241(0.194), train_acc: 92.708(93.075)
01/18 11:13:36 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 401/521, train_loss: 0.158(0.193), train_acc: 92.708(93.106)
01/18 11:13:49 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 501/521, train_loss: 0.131(0.194), train_acc: 96.875(93.060)
01/18 11:13:51 PM [Supernet Training] lr: 0.01491 epoch: 264/600, step: 521/521, train_loss: 0.114(0.193), train_acc: 95.000(93.070)
01/18 11:13:52 PM [Supernet Training] epoch: 264, train_loss: 0.193, train_acc: 93.070
01/18 11:13:55 PM [Supernet Validation] epoch: 264, val_loss: 0.436, val_acc: 87.240, best_acc: 87.760
01/18 11:13:55 PM 

01/18 11:13:55 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 001/521, train_loss: 0.260(0.260), train_acc: 90.625(90.625)
01/18 11:14:08 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 101/521, train_loss: 0.075(0.194), train_acc: 97.917(93.307)
01/18 11:14:21 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 201/521, train_loss: 0.174(0.195), train_acc: 92.708(93.087)
01/18 11:14:34 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 301/521, train_loss: 0.163(0.197), train_acc: 93.750(92.985)
01/18 11:14:47 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 401/521, train_loss: 0.231(0.198), train_acc: 92.708(92.968)
01/18 11:15:00 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 501/521, train_loss: 0.220(0.197), train_acc: 93.750(93.014)
01/18 11:15:02 PM [Supernet Training] lr: 0.01484 epoch: 265/600, step: 521/521, train_loss: 0.331(0.196), train_acc: 87.500(93.022)
01/18 11:15:02 PM [Supernet Training] epoch: 265, train_loss: 0.196, train_acc: 93.022
01/18 11:15:06 PM [Supernet Validation] epoch: 265, val_loss: 0.425, val_acc: 87.300, best_acc: 87.760
01/18 11:15:06 PM 

01/18 11:15:06 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 001/521, train_loss: 0.246(0.246), train_acc: 90.625(90.625)
01/18 11:15:19 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 101/521, train_loss: 0.222(0.210), train_acc: 93.750(92.502)
01/18 11:15:32 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 201/521, train_loss: 0.158(0.200), train_acc: 92.708(92.802)
01/18 11:15:44 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 301/521, train_loss: 0.279(0.196), train_acc: 89.583(92.778)
01/18 11:15:57 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 401/521, train_loss: 0.168(0.196), train_acc: 92.708(92.820)
01/18 11:16:10 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 501/521, train_loss: 0.289(0.197), train_acc: 89.583(92.775)
01/18 11:16:13 PM [Supernet Training] lr: 0.01478 epoch: 266/600, step: 521/521, train_loss: 0.182(0.197), train_acc: 92.500(92.784)
01/18 11:16:13 PM [Supernet Training] epoch: 266, train_loss: 0.197, train_acc: 92.784
01/18 11:16:16 PM [Supernet Validation] epoch: 266, val_loss: 0.423, val_acc: 87.520, best_acc: 87.760
01/18 11:16:16 PM 

01/18 11:16:17 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 93.750(93.750)
01/18 11:16:30 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 101/521, train_loss: 0.252(0.189), train_acc: 90.625(93.183)
01/18 11:16:42 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 201/521, train_loss: 0.258(0.191), train_acc: 90.625(93.128)
01/18 11:16:55 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 301/521, train_loss: 0.115(0.196), train_acc: 95.833(92.930)
01/18 11:17:08 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 401/521, train_loss: 0.239(0.197), train_acc: 91.667(92.999)
01/18 11:17:21 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 501/521, train_loss: 0.145(0.195), train_acc: 94.792(93.081)
01/18 11:17:23 PM [Supernet Training] lr: 0.01471 epoch: 267/600, step: 521/521, train_loss: 0.181(0.197), train_acc: 92.500(93.018)
01/18 11:17:23 PM [Supernet Training] epoch: 267, train_loss: 0.197, train_acc: 93.018
01/18 11:17:27 PM [Supernet Validation] epoch: 267, val_loss: 0.417, val_acc: 87.710, best_acc: 87.760
01/18 11:17:27 PM 

01/18 11:17:27 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 001/521, train_loss: 0.159(0.159), train_acc: 95.833(95.833)
01/18 11:17:40 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 101/521, train_loss: 0.117(0.195), train_acc: 95.833(93.028)
01/18 11:17:53 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 201/521, train_loss: 0.151(0.193), train_acc: 93.750(93.024)
01/18 11:18:05 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 301/521, train_loss: 0.266(0.196), train_acc: 90.625(92.930)
01/18 11:18:18 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 401/521, train_loss: 0.149(0.196), train_acc: 93.750(92.929)
01/18 11:18:31 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 501/521, train_loss: 0.288(0.198), train_acc: 86.458(92.883)
01/18 11:18:33 PM [Supernet Training] lr: 0.01465 epoch: 268/600, step: 521/521, train_loss: 0.202(0.198), train_acc: 95.000(92.858)
01/18 11:18:33 PM [Supernet Training] epoch: 268, train_loss: 0.198, train_acc: 92.858
01/18 11:18:37 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:18:37 PM [Supernet Validation] epoch: 268, val_loss: 0.415, val_acc: 87.840, best_acc: 87.840
01/18 11:18:37 PM 

01/18 11:18:38 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 001/521, train_loss: 0.170(0.170), train_acc: 92.708(92.708)
01/18 11:18:50 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 101/521, train_loss: 0.247(0.201), train_acc: 89.583(92.729)
01/18 11:19:03 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 201/521, train_loss: 0.269(0.206), train_acc: 89.583(92.475)
01/18 11:19:16 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 301/521, train_loss: 0.327(0.199), train_acc: 88.542(92.708)
01/18 11:19:29 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 401/521, train_loss: 0.151(0.200), train_acc: 94.792(92.727)
01/18 11:19:41 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 501/521, train_loss: 0.248(0.198), train_acc: 90.625(92.833)
01/18 11:19:44 PM [Supernet Training] lr: 0.01458 epoch: 269/600, step: 521/521, train_loss: 0.124(0.198), train_acc: 96.250(92.810)
01/18 11:19:44 PM [Supernet Training] epoch: 269, train_loss: 0.198, train_acc: 92.810
01/18 11:19:48 PM [Supernet Validation] epoch: 269, val_loss: 0.416, val_acc: 87.650, best_acc: 87.840
01/18 11:19:48 PM 

01/18 11:19:48 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 001/521, train_loss: 0.187(0.187), train_acc: 92.708(92.708)
01/18 11:20:01 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 101/521, train_loss: 0.135(0.191), train_acc: 94.792(93.111)
01/18 11:20:14 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 201/521, train_loss: 0.248(0.190), train_acc: 91.667(93.252)
01/18 11:20:26 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 301/521, train_loss: 0.243(0.190), train_acc: 91.667(93.207)
01/18 11:20:39 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 401/521, train_loss: 0.165(0.191), train_acc: 91.667(93.160)
01/18 11:20:52 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 501/521, train_loss: 0.148(0.192), train_acc: 93.750(93.130)
01/18 11:20:55 PM [Supernet Training] lr: 0.01452 epoch: 270/600, step: 521/521, train_loss: 0.142(0.193), train_acc: 96.250(93.114)
01/18 11:20:55 PM [Supernet Training] epoch: 270, train_loss: 0.193, train_acc: 93.114
01/18 11:20:58 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:20:58 PM [Supernet Validation] epoch: 270, val_loss: 0.419, val_acc: 88.020, best_acc: 88.020
01/18 11:20:58 PM 

01/18 11:20:59 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 001/521, train_loss: 0.148(0.148), train_acc: 93.750(93.750)
01/18 11:21:12 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 101/521, train_loss: 0.146(0.183), train_acc: 95.833(93.544)
01/18 11:21:24 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 201/521, train_loss: 0.266(0.183), train_acc: 91.667(93.491)
01/18 11:21:37 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 301/521, train_loss: 0.367(0.187), train_acc: 89.583(93.279)
01/18 11:21:50 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 401/521, train_loss: 0.171(0.188), train_acc: 92.708(93.246)
01/18 11:22:03 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 501/521, train_loss: 0.219(0.190), train_acc: 90.625(93.174)
01/18 11:22:05 PM [Supernet Training] lr: 0.01446 epoch: 271/600, step: 521/521, train_loss: 0.299(0.190), train_acc: 96.250(93.164)
01/18 11:22:05 PM [Supernet Training] epoch: 271, train_loss: 0.190, train_acc: 93.164
01/18 11:22:09 PM [Supernet Validation] epoch: 271, val_loss: 0.418, val_acc: 87.660, best_acc: 88.020
01/18 11:22:09 PM 

01/18 11:22:09 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 001/521, train_loss: 0.128(0.128), train_acc: 95.833(95.833)
01/18 11:22:22 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 101/521, train_loss: 0.144(0.186), train_acc: 95.833(93.502)
01/18 11:22:35 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 201/521, train_loss: 0.141(0.190), train_acc: 95.833(93.309)
01/18 11:22:48 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 301/521, train_loss: 0.146(0.190), train_acc: 94.792(93.300)
01/18 11:23:00 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 401/521, train_loss: 0.177(0.189), train_acc: 94.792(93.285)
01/18 11:23:13 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 501/521, train_loss: 0.193(0.189), train_acc: 94.792(93.309)
01/18 11:23:16 PM [Supernet Training] lr: 0.01439 epoch: 272/600, step: 521/521, train_loss: 0.135(0.190), train_acc: 95.000(93.266)
01/18 11:23:16 PM [Supernet Training] epoch: 272, train_loss: 0.190, train_acc: 93.266
01/18 11:23:19 PM [Supernet Validation] epoch: 272, val_loss: 0.429, val_acc: 87.630, best_acc: 88.020
01/18 11:23:19 PM 

01/18 11:23:20 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 001/521, train_loss: 0.231(0.231), train_acc: 93.750(93.750)
01/18 11:23:33 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 101/521, train_loss: 0.159(0.179), train_acc: 94.792(93.657)
01/18 11:23:45 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 201/521, train_loss: 0.144(0.183), train_acc: 94.792(93.646)
01/18 11:23:58 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 301/521, train_loss: 0.162(0.185), train_acc: 94.792(93.584)
01/18 11:24:11 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 401/521, train_loss: 0.396(0.188), train_acc: 89.583(93.394)
01/18 11:24:24 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 501/521, train_loss: 0.305(0.189), train_acc: 86.458(93.345)
01/18 11:24:26 PM [Supernet Training] lr: 0.01433 epoch: 273/600, step: 521/521, train_loss: 0.107(0.189), train_acc: 92.500(93.318)
01/18 11:24:26 PM [Supernet Training] epoch: 273, train_loss: 0.189, train_acc: 93.318
01/18 11:24:30 PM [Supernet Validation] epoch: 273, val_loss: 0.425, val_acc: 87.950, best_acc: 88.020
01/18 11:24:30 PM 

01/18 11:24:30 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 001/521, train_loss: 0.245(0.245), train_acc: 93.750(93.750)
01/18 11:24:43 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 101/521, train_loss: 0.165(0.169), train_acc: 94.792(94.214)
01/18 11:24:56 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 201/521, train_loss: 0.228(0.179), train_acc: 92.708(93.786)
01/18 11:25:09 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 301/521, train_loss: 0.159(0.178), train_acc: 93.750(93.719)
01/18 11:25:22 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 401/521, train_loss: 0.213(0.182), train_acc: 91.667(93.576)
01/18 11:25:34 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 501/521, train_loss: 0.113(0.186), train_acc: 96.875(93.403)
01/18 11:25:37 PM [Supernet Training] lr: 0.01426 epoch: 274/600, step: 521/521, train_loss: 0.196(0.187), train_acc: 92.500(93.384)
01/18 11:25:37 PM [Supernet Training] epoch: 274, train_loss: 0.187, train_acc: 93.384
01/18 11:25:41 PM [Supernet Validation] epoch: 274, val_loss: 0.427, val_acc: 87.530, best_acc: 88.020
01/18 11:25:41 PM 

01/18 11:25:41 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 001/521, train_loss: 0.209(0.209), train_acc: 93.750(93.750)
01/18 11:25:54 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 101/521, train_loss: 0.200(0.181), train_acc: 91.667(93.502)
01/18 11:26:07 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 201/521, train_loss: 0.182(0.183), train_acc: 92.708(93.366)
01/18 11:26:19 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 301/521, train_loss: 0.168(0.185), train_acc: 91.667(93.366)
01/18 11:26:32 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 401/521, train_loss: 0.121(0.191), train_acc: 95.833(93.215)
01/18 11:26:45 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 501/521, train_loss: 0.142(0.190), train_acc: 93.750(93.253)
01/18 11:26:47 PM [Supernet Training] lr: 0.01420 epoch: 275/600, step: 521/521, train_loss: 0.123(0.189), train_acc: 97.500(93.300)
01/18 11:26:47 PM [Supernet Training] epoch: 275, train_loss: 0.189, train_acc: 93.300
01/18 11:26:51 PM [Supernet Validation] epoch: 275, val_loss: 0.425, val_acc: 87.520, best_acc: 88.020
01/18 11:26:51 PM 

01/18 11:26:51 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 001/521, train_loss: 0.209(0.209), train_acc: 92.708(92.708)
01/18 11:27:04 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 101/521, train_loss: 0.163(0.196), train_acc: 92.708(93.007)
01/18 11:27:17 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 201/521, train_loss: 0.221(0.193), train_acc: 92.708(93.071)
01/18 11:27:30 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 301/521, train_loss: 0.258(0.190), train_acc: 92.708(93.162)
01/18 11:27:42 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 401/521, train_loss: 0.229(0.189), train_acc: 94.792(93.246)
01/18 11:27:55 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 501/521, train_loss: 0.150(0.188), train_acc: 94.792(93.293)
01/18 11:27:58 PM [Supernet Training] lr: 0.01413 epoch: 276/600, step: 521/521, train_loss: 0.238(0.188), train_acc: 90.000(93.300)
01/18 11:27:58 PM [Supernet Training] epoch: 276, train_loss: 0.188, train_acc: 93.300
01/18 11:28:01 PM [Supernet Validation] epoch: 276, val_loss: 0.439, val_acc: 87.150, best_acc: 88.020
01/18 11:28:01 PM 

01/18 11:28:02 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 001/521, train_loss: 0.114(0.114), train_acc: 96.875(96.875)
01/18 11:28:14 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 101/521, train_loss: 0.256(0.183), train_acc: 89.583(93.379)
01/18 11:28:27 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 201/521, train_loss: 0.185(0.186), train_acc: 93.750(93.299)
01/18 11:28:40 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 301/521, train_loss: 0.205(0.187), train_acc: 88.542(93.283)
01/18 11:28:53 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 401/521, train_loss: 0.177(0.187), train_acc: 91.667(93.316)
01/18 11:29:05 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 501/521, train_loss: 0.105(0.187), train_acc: 96.875(93.334)
01/18 11:29:08 PM [Supernet Training] lr: 0.01407 epoch: 277/600, step: 521/521, train_loss: 0.244(0.187), train_acc: 91.250(93.338)
01/18 11:29:08 PM [Supernet Training] epoch: 277, train_loss: 0.187, train_acc: 93.338
01/18 11:29:12 PM [Supernet Validation] epoch: 277, val_loss: 0.415, val_acc: 87.900, best_acc: 88.020
01/18 11:29:12 PM 

01/18 11:29:12 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 001/521, train_loss: 0.192(0.192), train_acc: 93.750(93.750)
01/18 11:29:25 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 101/521, train_loss: 0.072(0.181), train_acc: 97.917(93.554)
01/18 11:29:38 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 201/521, train_loss: 0.190(0.183), train_acc: 91.667(93.475)
01/18 11:29:50 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 301/521, train_loss: 0.220(0.181), train_acc: 93.750(93.563)
01/18 11:30:03 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 401/521, train_loss: 0.228(0.182), train_acc: 91.667(93.524)
01/18 11:30:16 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 501/521, train_loss: 0.158(0.184), train_acc: 95.833(93.517)
01/18 11:30:18 PM [Supernet Training] lr: 0.01400 epoch: 278/600, step: 521/521, train_loss: 0.171(0.184), train_acc: 95.000(93.446)
01/18 11:30:19 PM [Supernet Training] epoch: 278, train_loss: 0.184, train_acc: 93.446
01/18 11:30:22 PM [Supernet Validation] epoch: 278, val_loss: 0.428, val_acc: 87.420, best_acc: 88.020
01/18 11:30:22 PM 

01/18 11:30:22 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 001/521, train_loss: 0.191(0.191), train_acc: 92.708(92.708)
01/18 11:30:35 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 101/521, train_loss: 0.289(0.194), train_acc: 91.667(93.059)
01/18 11:30:48 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 201/521, train_loss: 0.152(0.193), train_acc: 96.875(93.144)
01/18 11:31:01 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 301/521, train_loss: 0.127(0.191), train_acc: 95.833(93.245)
01/18 11:31:14 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 401/521, train_loss: 0.195(0.188), train_acc: 92.708(93.303)
01/18 11:31:27 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 501/521, train_loss: 0.104(0.188), train_acc: 95.833(93.261)
01/18 11:31:29 PM [Supernet Training] lr: 0.01394 epoch: 279/600, step: 521/521, train_loss: 0.113(0.187), train_acc: 93.750(93.278)
01/18 11:31:29 PM [Supernet Training] epoch: 279, train_loss: 0.187, train_acc: 93.278
01/18 11:31:33 PM [Supernet Validation] epoch: 279, val_loss: 0.403, val_acc: 87.990, best_acc: 88.020
01/18 11:31:33 PM 

01/18 11:31:33 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 001/521, train_loss: 0.142(0.142), train_acc: 95.833(95.833)
01/18 11:31:46 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 101/521, train_loss: 0.194(0.189), train_acc: 92.708(93.492)
01/18 11:31:59 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 201/521, train_loss: 0.139(0.188), train_acc: 95.833(93.403)
01/18 11:32:11 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 301/521, train_loss: 0.134(0.186), train_acc: 94.792(93.477)
01/18 11:32:24 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 401/521, train_loss: 0.144(0.189), train_acc: 95.833(93.394)
01/18 11:32:37 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 501/521, train_loss: 0.137(0.186), train_acc: 94.792(93.463)
01/18 11:32:40 PM [Supernet Training] lr: 0.01387 epoch: 280/600, step: 521/521, train_loss: 0.174(0.186), train_acc: 96.250(93.472)
01/18 11:32:40 PM [Supernet Training] epoch: 280, train_loss: 0.186, train_acc: 93.472
01/18 11:32:43 PM [Supernet Validation] epoch: 280, val_loss: 0.413, val_acc: 88.000, best_acc: 88.020
01/18 11:32:43 PM 

01/18 11:32:44 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 001/521, train_loss: 0.127(0.127), train_acc: 95.833(95.833)
01/18 11:32:56 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 101/521, train_loss: 0.179(0.176), train_acc: 96.875(93.575)
01/18 11:33:09 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 201/521, train_loss: 0.189(0.180), train_acc: 94.792(93.481)
01/18 11:33:22 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 301/521, train_loss: 0.147(0.178), train_acc: 93.750(93.515)
01/18 11:33:35 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 401/521, train_loss: 0.261(0.184), train_acc: 89.583(93.321)
01/18 11:33:47 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 501/521, train_loss: 0.155(0.183), train_acc: 95.833(93.345)
01/18 11:33:50 PM [Supernet Training] lr: 0.01381 epoch: 281/600, step: 521/521, train_loss: 0.112(0.182), train_acc: 96.250(93.384)
01/18 11:33:50 PM [Supernet Training] epoch: 281, train_loss: 0.182, train_acc: 93.384
01/18 11:33:54 PM [Supernet Validation] epoch: 281, val_loss: 0.425, val_acc: 87.810, best_acc: 88.020
01/18 11:33:54 PM 

01/18 11:33:54 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 001/521, train_loss: 0.116(0.116), train_acc: 95.833(95.833)
01/18 11:34:07 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 101/521, train_loss: 0.101(0.182), train_acc: 96.875(93.368)
01/18 11:34:20 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 201/521, train_loss: 0.259(0.182), train_acc: 91.667(93.377)
01/18 11:34:32 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 301/521, train_loss: 0.272(0.181), train_acc: 91.667(93.459)
01/18 11:34:45 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 401/521, train_loss: 0.136(0.182), train_acc: 93.750(93.392)
01/18 11:34:58 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 501/521, train_loss: 0.177(0.182), train_acc: 94.792(93.390)
01/18 11:35:00 PM [Supernet Training] lr: 0.01374 epoch: 282/600, step: 521/521, train_loss: 0.355(0.182), train_acc: 90.000(93.390)
01/18 11:35:01 PM [Supernet Training] epoch: 282, train_loss: 0.182, train_acc: 93.390
01/18 11:35:04 PM [Supernet Validation] epoch: 282, val_loss: 0.409, val_acc: 88.010, best_acc: 88.020
01/18 11:35:04 PM 

01/18 11:35:04 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 001/521, train_loss: 0.297(0.297), train_acc: 90.625(90.625)
01/18 11:35:17 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 101/521, train_loss: 0.227(0.183), train_acc: 92.708(93.389)
01/18 11:35:30 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 201/521, train_loss: 0.157(0.185), train_acc: 94.792(93.268)
01/18 11:35:43 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 301/521, train_loss: 0.178(0.181), train_acc: 95.833(93.359)
01/18 11:35:56 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 401/521, train_loss: 0.370(0.182), train_acc: 87.500(93.430)
01/18 11:36:09 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 501/521, train_loss: 0.162(0.183), train_acc: 91.667(93.384)
01/18 11:36:11 PM [Supernet Training] lr: 0.01368 epoch: 283/600, step: 521/521, train_loss: 0.096(0.183), train_acc: 96.250(93.394)
01/18 11:36:11 PM [Supernet Training] epoch: 283, train_loss: 0.183, train_acc: 93.394
01/18 11:36:15 PM [Supernet Validation] epoch: 283, val_loss: 0.421, val_acc: 87.890, best_acc: 88.020
01/18 11:36:15 PM 

01/18 11:36:15 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 001/521, train_loss: 0.196(0.196), train_acc: 93.750(93.750)
01/18 11:36:28 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 101/521, train_loss: 0.312(0.182), train_acc: 89.583(93.337)
01/18 11:36:41 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 201/521, train_loss: 0.305(0.181), train_acc: 91.667(93.538)
01/18 11:36:53 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 301/521, train_loss: 0.124(0.179), train_acc: 95.833(93.650)
01/18 11:37:06 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 401/521, train_loss: 0.056(0.180), train_acc: 98.958(93.636)
01/18 11:37:19 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 501/521, train_loss: 0.124(0.180), train_acc: 96.875(93.663)
01/18 11:37:22 PM [Supernet Training] lr: 0.01361 epoch: 284/600, step: 521/521, train_loss: 0.207(0.180), train_acc: 88.750(93.614)
01/18 11:37:22 PM [Supernet Training] epoch: 284, train_loss: 0.180, train_acc: 93.614
01/18 11:37:25 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:37:25 PM [Supernet Validation] epoch: 284, val_loss: 0.413, val_acc: 88.210, best_acc: 88.210
01/18 11:37:25 PM 

01/18 11:37:26 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 96.875(96.875)
01/18 11:37:38 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 101/521, train_loss: 0.288(0.164), train_acc: 89.583(94.214)
01/18 11:37:51 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 201/521, train_loss: 0.152(0.175), train_acc: 92.708(93.709)
01/18 11:38:04 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 301/521, train_loss: 0.259(0.178), train_acc: 90.625(93.632)
01/18 11:38:17 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 401/521, train_loss: 0.098(0.175), train_acc: 97.917(93.664)
01/18 11:38:30 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 501/521, train_loss: 0.098(0.176), train_acc: 96.875(93.629)
01/18 11:38:32 PM [Supernet Training] lr: 0.01355 epoch: 285/600, step: 521/521, train_loss: 0.232(0.176), train_acc: 90.000(93.612)
01/18 11:38:32 PM [Supernet Training] epoch: 285, train_loss: 0.176, train_acc: 93.612
01/18 11:38:36 PM [Supernet Validation] epoch: 285, val_loss: 0.422, val_acc: 87.560, best_acc: 88.210
01/18 11:38:36 PM 

01/18 11:38:36 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 001/521, train_loss: 0.082(0.082), train_acc: 97.917(97.917)
01/18 11:38:49 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 101/521, train_loss: 0.208(0.175), train_acc: 92.708(93.771)
01/18 11:39:02 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 201/521, train_loss: 0.062(0.176), train_acc: 98.958(93.610)
01/18 11:39:15 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 301/521, train_loss: 0.085(0.176), train_acc: 98.958(93.643)
01/18 11:39:28 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 401/521, train_loss: 0.297(0.177), train_acc: 88.542(93.560)
01/18 11:39:40 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 501/521, train_loss: 0.089(0.179), train_acc: 97.917(93.559)
01/18 11:39:43 PM [Supernet Training] lr: 0.01348 epoch: 286/600, step: 521/521, train_loss: 0.232(0.178), train_acc: 90.000(93.578)
01/18 11:39:43 PM [Supernet Training] epoch: 286, train_loss: 0.178, train_acc: 93.578
01/18 11:39:47 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:39:47 PM [Supernet Validation] epoch: 286, val_loss: 0.421, val_acc: 88.320, best_acc: 88.320
01/18 11:39:47 PM 

01/18 11:39:47 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 001/521, train_loss: 0.109(0.109), train_acc: 94.792(94.792)
01/18 11:40:00 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 101/521, train_loss: 0.223(0.171), train_acc: 91.667(93.843)
01/18 11:40:12 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 201/521, train_loss: 0.140(0.176), train_acc: 95.833(93.703)
01/18 11:40:25 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 301/521, train_loss: 0.136(0.172), train_acc: 95.833(93.878)
01/18 11:40:38 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 401/521, train_loss: 0.193(0.175), train_acc: 91.667(93.776)
01/18 11:40:51 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 501/521, train_loss: 0.118(0.175), train_acc: 95.833(93.760)
01/18 11:40:54 PM [Supernet Training] lr: 0.01342 epoch: 287/600, step: 521/521, train_loss: 0.179(0.175), train_acc: 92.500(93.718)
01/18 11:40:54 PM [Supernet Training] epoch: 287, train_loss: 0.175, train_acc: 93.718
01/18 11:40:57 PM [Supernet Validation] epoch: 287, val_loss: 0.415, val_acc: 88.030, best_acc: 88.320
01/18 11:40:57 PM 

01/18 11:40:58 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 001/521, train_loss: 0.235(0.235), train_acc: 91.667(91.667)
01/18 11:41:10 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 101/521, train_loss: 0.135(0.166), train_acc: 94.792(94.224)
01/18 11:41:23 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 201/521, train_loss: 0.146(0.171), train_acc: 93.750(93.812)
01/18 11:41:36 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 301/521, train_loss: 0.248(0.174), train_acc: 90.625(93.740)
01/18 11:41:49 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 401/521, train_loss: 0.127(0.177), train_acc: 94.792(93.698)
01/18 11:42:02 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 501/521, train_loss: 0.115(0.178), train_acc: 96.875(93.621)
01/18 11:42:04 PM [Supernet Training] lr: 0.01335 epoch: 288/600, step: 521/521, train_loss: 0.194(0.177), train_acc: 93.750(93.622)
01/18 11:42:04 PM [Supernet Training] epoch: 288, train_loss: 0.177, train_acc: 93.622
01/18 11:42:08 PM [Supernet Validation] epoch: 288, val_loss: 0.428, val_acc: 87.700, best_acc: 88.320
01/18 11:42:08 PM 

01/18 11:42:08 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 001/521, train_loss: 0.190(0.190), train_acc: 90.625(90.625)
01/18 11:42:21 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 101/521, train_loss: 0.140(0.173), train_acc: 92.708(93.740)
01/18 11:42:34 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 201/521, train_loss: 0.198(0.171), train_acc: 93.750(93.817)
01/18 11:42:47 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 301/521, train_loss: 0.259(0.171), train_acc: 90.625(93.802)
01/18 11:42:59 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 401/521, train_loss: 0.155(0.173), train_acc: 93.750(93.781)
01/18 11:43:12 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 501/521, train_loss: 0.207(0.175), train_acc: 90.625(93.731)
01/18 11:43:15 PM [Supernet Training] lr: 0.01328 epoch: 289/600, step: 521/521, train_loss: 0.313(0.175), train_acc: 91.250(93.732)
01/18 11:43:15 PM [Supernet Training] epoch: 289, train_loss: 0.175, train_acc: 93.732
01/18 11:43:18 PM [Supernet Validation] epoch: 289, val_loss: 0.425, val_acc: 87.740, best_acc: 88.320
01/18 11:43:18 PM 

01/18 11:43:19 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 001/521, train_loss: 0.175(0.175), train_acc: 92.708(92.708)
01/18 11:43:32 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 101/521, train_loss: 0.243(0.161), train_acc: 93.750(94.266)
01/18 11:43:44 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 201/521, train_loss: 0.157(0.169), train_acc: 92.708(94.035)
01/18 11:43:57 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 301/521, train_loss: 0.155(0.173), train_acc: 93.750(93.864)
01/18 11:44:10 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 401/521, train_loss: 0.158(0.176), train_acc: 93.750(93.742)
01/18 11:44:23 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 501/521, train_loss: 0.280(0.177), train_acc: 93.750(93.663)
01/18 11:44:25 PM [Supernet Training] lr: 0.01322 epoch: 290/600, step: 521/521, train_loss: 0.206(0.177), train_acc: 95.000(93.638)
01/18 11:44:25 PM [Supernet Training] epoch: 290, train_loss: 0.177, train_acc: 93.638
01/18 11:44:29 PM [Supernet Validation] epoch: 290, val_loss: 0.420, val_acc: 87.660, best_acc: 88.320
01/18 11:44:29 PM 

01/18 11:44:29 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 001/521, train_loss: 0.093(0.093), train_acc: 94.792(94.792)
01/18 11:44:42 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 101/521, train_loss: 0.248(0.180), train_acc: 91.667(93.575)
01/18 11:44:55 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 201/521, train_loss: 0.155(0.176), train_acc: 93.750(93.667)
01/18 11:45:08 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 301/521, train_loss: 0.155(0.175), train_acc: 93.750(93.726)
01/18 11:45:20 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 401/521, train_loss: 0.147(0.174), train_acc: 94.792(93.745)
01/18 11:45:33 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 501/521, train_loss: 0.149(0.174), train_acc: 95.833(93.717)
01/18 11:45:36 PM [Supernet Training] lr: 0.01315 epoch: 291/600, step: 521/521, train_loss: 0.233(0.174), train_acc: 93.750(93.736)
01/18 11:45:36 PM [Supernet Training] epoch: 291, train_loss: 0.174, train_acc: 93.736
01/18 11:45:39 PM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/18 11:45:39 PM [Supernet Validation] epoch: 291, val_loss: 0.425, val_acc: 88.370, best_acc: 88.370
01/18 11:45:39 PM 

01/18 11:45:40 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 001/521, train_loss: 0.249(0.249), train_acc: 90.625(90.625)
01/18 11:45:53 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 101/521, train_loss: 0.246(0.174), train_acc: 91.667(93.750)
01/18 11:46:05 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 201/521, train_loss: 0.126(0.175), train_acc: 94.792(93.709)
01/18 11:46:18 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 301/521, train_loss: 0.149(0.176), train_acc: 92.708(93.639)
01/18 11:46:31 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 401/521, train_loss: 0.149(0.175), train_acc: 95.833(93.685)
01/18 11:46:44 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 501/521, train_loss: 0.074(0.173), train_acc: 97.917(93.733)
01/18 11:46:46 PM [Supernet Training] lr: 0.01309 epoch: 292/600, step: 521/521, train_loss: 0.133(0.174), train_acc: 96.250(93.722)
01/18 11:46:46 PM [Supernet Training] epoch: 292, train_loss: 0.174, train_acc: 93.722
01/18 11:46:50 PM [Supernet Validation] epoch: 292, val_loss: 0.423, val_acc: 87.650, best_acc: 88.370
01/18 11:46:50 PM 

01/18 11:46:50 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 001/521, train_loss: 0.173(0.173), train_acc: 94.792(94.792)
01/18 11:47:03 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 101/521, train_loss: 0.150(0.177), train_acc: 95.833(93.750)
01/18 11:47:16 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 201/521, train_loss: 0.129(0.178), train_acc: 95.833(93.615)
01/18 11:47:29 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 301/521, train_loss: 0.207(0.176), train_acc: 92.708(93.695)
01/18 11:47:41 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 401/521, train_loss: 0.192(0.174), train_acc: 90.625(93.758)
01/18 11:47:54 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 501/521, train_loss: 0.226(0.175), train_acc: 93.750(93.746)
01/18 11:47:57 PM [Supernet Training] lr: 0.01302 epoch: 293/600, step: 521/521, train_loss: 0.221(0.174), train_acc: 93.750(93.760)
01/18 11:47:57 PM [Supernet Training] epoch: 293, train_loss: 0.174, train_acc: 93.760
01/18 11:48:00 PM [Supernet Validation] epoch: 293, val_loss: 0.421, val_acc: 88.140, best_acc: 88.370
01/18 11:48:00 PM 

01/18 11:48:01 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 001/521, train_loss: 0.260(0.260), train_acc: 91.667(91.667)
01/18 11:48:14 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 101/521, train_loss: 0.142(0.175), train_acc: 96.875(93.616)
01/18 11:48:26 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 201/521, train_loss: 0.289(0.175), train_acc: 92.708(93.605)
01/18 11:48:39 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 301/521, train_loss: 0.162(0.175), train_acc: 92.708(93.657)
01/18 11:48:52 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 401/521, train_loss: 0.204(0.177), train_acc: 94.792(93.560)
01/18 11:49:05 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 501/521, train_loss: 0.115(0.174), train_acc: 96.875(93.638)
01/18 11:49:07 PM [Supernet Training] lr: 0.01296 epoch: 294/600, step: 521/521, train_loss: 0.165(0.175), train_acc: 91.250(93.632)
01/18 11:49:07 PM [Supernet Training] epoch: 294, train_loss: 0.175, train_acc: 93.632
01/18 11:49:11 PM [Supernet Validation] epoch: 294, val_loss: 0.424, val_acc: 87.730, best_acc: 88.370
01/18 11:49:11 PM 

01/18 11:49:11 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 001/521, train_loss: 0.277(0.277), train_acc: 92.708(92.708)
01/18 11:49:24 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 101/521, train_loss: 0.126(0.177), train_acc: 95.833(93.791)
01/18 11:49:37 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 201/521, train_loss: 0.147(0.176), train_acc: 93.750(93.703)
01/18 11:49:50 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 301/521, train_loss: 0.221(0.173), train_acc: 93.750(93.798)
01/18 11:50:02 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 401/521, train_loss: 0.268(0.174), train_acc: 92.708(93.768)
01/18 11:50:15 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 501/521, train_loss: 0.224(0.175), train_acc: 90.625(93.796)
01/18 11:50:18 PM [Supernet Training] lr: 0.01289 epoch: 295/600, step: 521/521, train_loss: 0.053(0.175), train_acc: 98.750(93.788)
01/18 11:50:18 PM [Supernet Training] epoch: 295, train_loss: 0.175, train_acc: 93.788
01/18 11:50:21 PM [Supernet Validation] epoch: 295, val_loss: 0.411, val_acc: 87.970, best_acc: 88.370
01/18 11:50:21 PM 

01/18 11:50:22 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 001/521, train_loss: 0.248(0.248), train_acc: 88.542(88.542)
01/18 11:50:35 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 101/521, train_loss: 0.139(0.174), train_acc: 95.833(93.833)
01/18 11:50:47 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 201/521, train_loss: 0.151(0.173), train_acc: 95.833(93.771)
01/18 11:51:00 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 301/521, train_loss: 0.049(0.172), train_acc: 98.958(93.899)
01/18 11:51:13 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 401/521, train_loss: 0.134(0.172), train_acc: 94.792(93.810)
01/18 11:51:26 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 501/521, train_loss: 0.164(0.171), train_acc: 93.750(93.790)
01/18 11:51:28 PM [Supernet Training] lr: 0.01283 epoch: 296/600, step: 521/521, train_loss: 0.138(0.171), train_acc: 95.000(93.782)
01/18 11:51:28 PM [Supernet Training] epoch: 296, train_loss: 0.171, train_acc: 93.782
01/18 11:51:32 PM [Supernet Validation] epoch: 296, val_loss: 0.417, val_acc: 88.070, best_acc: 88.370
01/18 11:51:32 PM 

01/18 11:51:32 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 001/521, train_loss: 0.197(0.197), train_acc: 92.708(92.708)
01/18 11:51:45 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 101/521, train_loss: 0.152(0.166), train_acc: 91.667(93.740)
01/18 11:51:58 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 201/521, train_loss: 0.197(0.170), train_acc: 93.750(93.688)
01/18 11:52:11 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 301/521, train_loss: 0.153(0.173), train_acc: 94.792(93.653)
01/18 11:52:24 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 401/521, train_loss: 0.117(0.172), train_acc: 94.792(93.654)
01/18 11:52:37 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 501/521, train_loss: 0.210(0.173), train_acc: 92.708(93.671)
01/18 11:52:39 PM [Supernet Training] lr: 0.01276 epoch: 297/600, step: 521/521, train_loss: 0.188(0.172), train_acc: 92.500(93.684)
01/18 11:52:39 PM [Supernet Training] epoch: 297, train_loss: 0.172, train_acc: 93.684
01/18 11:52:43 PM [Supernet Validation] epoch: 297, val_loss: 0.431, val_acc: 87.870, best_acc: 88.370
01/18 11:52:43 PM 

01/18 11:52:43 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 001/521, train_loss: 0.219(0.219), train_acc: 92.708(92.708)
01/18 11:52:56 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 101/521, train_loss: 0.140(0.170), train_acc: 93.750(93.822)
01/18 11:53:09 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 201/521, train_loss: 0.303(0.172), train_acc: 89.583(93.791)
01/18 11:53:21 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 301/521, train_loss: 0.197(0.171), train_acc: 92.708(93.882)
01/18 11:53:34 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 401/521, train_loss: 0.179(0.171), train_acc: 93.750(93.849)
01/18 11:53:47 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 501/521, train_loss: 0.270(0.171), train_acc: 90.625(93.854)
01/18 11:53:50 PM [Supernet Training] lr: 0.01270 epoch: 298/600, step: 521/521, train_loss: 0.165(0.171), train_acc: 96.250(93.846)
01/18 11:53:50 PM [Supernet Training] epoch: 298, train_loss: 0.171, train_acc: 93.846
01/18 11:53:53 PM [Supernet Validation] epoch: 298, val_loss: 0.421, val_acc: 87.960, best_acc: 88.370
01/18 11:53:53 PM 

01/18 11:53:54 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 001/521, train_loss: 0.141(0.141), train_acc: 95.833(95.833)
01/18 11:54:06 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 101/521, train_loss: 0.101(0.181), train_acc: 93.750(93.513)
01/18 11:54:19 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 201/521, train_loss: 0.131(0.174), train_acc: 95.833(93.755)
01/18 11:54:32 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 301/521, train_loss: 0.223(0.172), train_acc: 92.708(93.747)
01/18 11:54:45 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 401/521, train_loss: 0.283(0.172), train_acc: 87.500(93.755)
01/18 11:54:58 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 501/521, train_loss: 0.302(0.170), train_acc: 92.708(93.846)
01/18 11:55:00 PM [Supernet Training] lr: 0.01263 epoch: 299/600, step: 521/521, train_loss: 0.092(0.170), train_acc: 96.250(93.876)
01/18 11:55:00 PM [Supernet Training] epoch: 299, train_loss: 0.170, train_acc: 93.876
01/18 11:55:04 PM [Supernet Validation] epoch: 299, val_loss: 0.418, val_acc: 87.810, best_acc: 88.370
01/18 11:55:04 PM 

01/18 11:55:04 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 001/521, train_loss: 0.121(0.121), train_acc: 95.833(95.833)
01/18 11:55:17 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 101/521, train_loss: 0.211(0.164), train_acc: 91.667(94.173)
01/18 11:55:30 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 201/521, train_loss: 0.191(0.166), train_acc: 92.708(94.014)
01/18 11:55:42 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 301/521, train_loss: 0.138(0.169), train_acc: 95.833(93.916)
01/18 11:55:55 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 401/521, train_loss: 0.138(0.168), train_acc: 94.792(93.999)
01/18 11:56:08 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 501/521, train_loss: 0.235(0.167), train_acc: 90.625(94.087)
01/18 11:56:11 PM [Supernet Training] lr: 0.01257 epoch: 300/600, step: 521/521, train_loss: 0.216(0.168), train_acc: 90.000(94.054)
01/18 11:56:11 PM [Supernet Training] epoch: 300, train_loss: 0.168, train_acc: 94.054
01/18 11:56:14 PM [Supernet Validation] epoch: 300, val_loss: 0.425, val_acc: 88.140, best_acc: 88.370
01/18 11:56:14 PM 

01/18 11:56:15 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 001/521, train_loss: 0.079(0.079), train_acc: 97.917(97.917)
01/18 11:56:27 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 101/521, train_loss: 0.175(0.154), train_acc: 94.792(94.719)
01/18 11:56:40 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 201/521, train_loss: 0.261(0.161), train_acc: 92.708(94.258)
01/18 11:56:53 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 301/521, train_loss: 0.174(0.162), train_acc: 93.750(94.155)
01/18 11:57:06 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 401/521, train_loss: 0.217(0.164), train_acc: 92.708(94.041)
01/18 11:57:19 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 501/521, train_loss: 0.225(0.166), train_acc: 90.625(94.002)
01/18 11:57:21 PM [Supernet Training] lr: 0.01250 epoch: 301/600, step: 521/521, train_loss: 0.230(0.166), train_acc: 90.000(93.972)
01/18 11:57:21 PM [Supernet Training] epoch: 301, train_loss: 0.166, train_acc: 93.972
01/18 11:57:25 PM [Supernet Validation] epoch: 301, val_loss: 0.427, val_acc: 87.900, best_acc: 88.370
01/18 11:57:25 PM 

01/18 11:57:25 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 001/521, train_loss: 0.176(0.176), train_acc: 91.667(91.667)
01/18 11:57:38 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 101/521, train_loss: 0.072(0.156), train_acc: 98.958(94.338)
01/18 11:57:51 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 201/521, train_loss: 0.149(0.163), train_acc: 94.792(94.175)
01/18 11:58:04 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 301/521, train_loss: 0.284(0.164), train_acc: 88.542(94.103)
01/18 11:58:16 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 401/521, train_loss: 0.187(0.165), train_acc: 95.833(94.077)
01/18 11:58:29 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 501/521, train_loss: 0.154(0.165), train_acc: 95.833(94.049)
01/18 11:58:32 PM [Supernet Training] lr: 0.01243 epoch: 302/600, step: 521/521, train_loss: 0.242(0.166), train_acc: 95.000(94.046)
01/18 11:58:32 PM [Supernet Training] epoch: 302, train_loss: 0.166, train_acc: 94.046
01/18 11:58:35 PM [Supernet Validation] epoch: 302, val_loss: 0.431, val_acc: 87.660, best_acc: 88.370
01/18 11:58:35 PM 

01/18 11:58:36 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 001/521, train_loss: 0.141(0.141), train_acc: 93.750(93.750)
01/18 11:58:49 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 101/521, train_loss: 0.174(0.162), train_acc: 93.750(93.863)
01/18 11:59:01 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 201/521, train_loss: 0.279(0.166), train_acc: 91.667(93.947)
01/18 11:59:14 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 301/521, train_loss: 0.164(0.162), train_acc: 94.792(94.082)
01/18 11:59:27 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 401/521, train_loss: 0.138(0.164), train_acc: 94.792(94.044)
01/18 11:59:40 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 501/521, train_loss: 0.325(0.164), train_acc: 88.542(94.043)
01/18 11:59:42 PM [Supernet Training] lr: 0.01237 epoch: 303/600, step: 521/521, train_loss: 0.153(0.164), train_acc: 93.750(94.024)
01/18 11:59:42 PM [Supernet Training] epoch: 303, train_loss: 0.164, train_acc: 94.024
01/18 11:59:46 PM [Supernet Validation] epoch: 303, val_loss: 0.420, val_acc: 88.000, best_acc: 88.370
01/18 11:59:46 PM 

01/18 11:59:46 PM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 001/521, train_loss: 0.210(0.210), train_acc: 91.667(91.667)
01/18 11:59:59 PM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 101/521, train_loss: 0.361(0.174), train_acc: 85.417(93.750)
01/19 12:00:12 AM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 201/521, train_loss: 0.107(0.174), train_acc: 98.958(93.734)
01/19 12:00:25 AM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 301/521, train_loss: 0.299(0.171), train_acc: 90.625(93.819)
01/19 12:00:38 AM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 401/521, train_loss: 0.123(0.171), train_acc: 91.667(93.825)
01/19 12:00:50 AM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 501/521, train_loss: 0.092(0.167), train_acc: 95.833(93.887)
01/19 12:00:53 AM [Supernet Training] lr: 0.01230 epoch: 304/600, step: 521/521, train_loss: 0.177(0.169), train_acc: 95.000(93.844)
01/19 12:00:53 AM [Supernet Training] epoch: 304, train_loss: 0.169, train_acc: 93.844
01/19 12:00:57 AM [Supernet Validation] epoch: 304, val_loss: 0.430, val_acc: 88.020, best_acc: 88.370
01/19 12:00:57 AM 

01/19 12:00:57 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 001/521, train_loss: 0.127(0.127), train_acc: 93.750(93.750)
01/19 12:01:10 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 101/521, train_loss: 0.088(0.172), train_acc: 94.792(93.956)
01/19 12:01:22 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 201/521, train_loss: 0.094(0.166), train_acc: 95.833(94.102)
01/19 12:01:35 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 301/521, train_loss: 0.165(0.163), train_acc: 94.792(94.186)
01/19 12:01:48 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 401/521, train_loss: 0.082(0.162), train_acc: 97.917(94.147)
01/19 12:02:01 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 501/521, train_loss: 0.324(0.163), train_acc: 89.583(94.081)
01/19 12:02:03 AM [Supernet Training] lr: 0.01224 epoch: 305/600, step: 521/521, train_loss: 0.248(0.163), train_acc: 90.000(94.090)
01/19 12:02:03 AM [Supernet Training] epoch: 305, train_loss: 0.163, train_acc: 94.090
01/19 12:02:07 AM [Supernet Validation] epoch: 305, val_loss: 0.428, val_acc: 87.870, best_acc: 88.370
01/19 12:02:07 AM 

01/19 12:02:07 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 001/521, train_loss: 0.245(0.245), train_acc: 92.708(92.708)
01/19 12:02:20 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 101/521, train_loss: 0.143(0.159), train_acc: 93.750(94.307)
01/19 12:02:33 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 201/521, train_loss: 0.133(0.158), train_acc: 95.833(94.356)
01/19 12:02:46 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 301/521, train_loss: 0.157(0.160), train_acc: 96.875(94.228)
01/19 12:02:59 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 401/521, train_loss: 0.172(0.163), train_acc: 94.792(94.171)
01/19 12:03:11 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 501/521, train_loss: 0.116(0.163), train_acc: 98.958(94.110)
01/19 12:03:14 AM [Supernet Training] lr: 0.01217 epoch: 306/600, step: 521/521, train_loss: 0.080(0.163), train_acc: 97.500(94.126)
01/19 12:03:14 AM [Supernet Training] epoch: 306, train_loss: 0.163, train_acc: 94.126
01/19 12:03:18 AM [Supernet Validation] epoch: 306, val_loss: 0.418, val_acc: 88.210, best_acc: 88.370
01/19 12:03:18 AM 

01/19 12:03:18 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 001/521, train_loss: 0.153(0.153), train_acc: 94.792(94.792)
01/19 12:03:31 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 101/521, train_loss: 0.160(0.160), train_acc: 93.750(94.462)
01/19 12:03:44 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 201/521, train_loss: 0.156(0.160), train_acc: 93.750(94.439)
01/19 12:03:56 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 301/521, train_loss: 0.168(0.161), train_acc: 93.750(94.394)
01/19 12:04:09 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 401/521, train_loss: 0.091(0.161), train_acc: 95.833(94.306)
01/19 12:04:22 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 501/521, train_loss: 0.085(0.162), train_acc: 96.875(94.251)
01/19 12:04:25 AM [Supernet Training] lr: 0.01211 epoch: 307/600, step: 521/521, train_loss: 0.037(0.161), train_acc: 98.750(94.272)
01/19 12:04:25 AM [Supernet Training] epoch: 307, train_loss: 0.161, train_acc: 94.272
01/19 12:04:28 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 12:04:28 AM [Supernet Validation] epoch: 307, val_loss: 0.417, val_acc: 88.450, best_acc: 88.450
01/19 12:04:28 AM 

01/19 12:04:29 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 001/521, train_loss: 0.167(0.167), train_acc: 92.708(92.708)
01/19 12:04:41 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 101/521, train_loss: 0.157(0.152), train_acc: 95.833(94.493)
01/19 12:04:54 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 201/521, train_loss: 0.141(0.162), train_acc: 95.833(94.076)
01/19 12:05:07 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 301/521, train_loss: 0.175(0.164), train_acc: 95.833(94.013)
01/19 12:05:20 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 401/521, train_loss: 0.243(0.163), train_acc: 91.667(94.054)
01/19 12:05:33 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 501/521, train_loss: 0.139(0.163), train_acc: 96.875(94.081)
01/19 12:05:35 AM [Supernet Training] lr: 0.01204 epoch: 308/600, step: 521/521, train_loss: 0.202(0.163), train_acc: 91.250(94.084)
01/19 12:05:35 AM [Supernet Training] epoch: 308, train_loss: 0.163, train_acc: 94.084
01/19 12:05:39 AM [Supernet Validation] epoch: 308, val_loss: 0.430, val_acc: 87.910, best_acc: 88.450
01/19 12:05:39 AM 

01/19 12:05:39 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 001/521, train_loss: 0.107(0.107), train_acc: 96.875(96.875)
01/19 12:05:52 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 101/521, train_loss: 0.222(0.163), train_acc: 91.667(94.080)
01/19 12:06:05 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 201/521, train_loss: 0.173(0.164), train_acc: 93.750(93.988)
01/19 12:06:18 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 301/521, train_loss: 0.096(0.164), train_acc: 96.875(93.989)
01/19 12:06:30 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 401/521, train_loss: 0.134(0.162), train_acc: 94.792(94.147)
01/19 12:06:43 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 501/521, train_loss: 0.171(0.161), train_acc: 93.750(94.170)
01/19 12:06:46 AM [Supernet Training] lr: 0.01198 epoch: 309/600, step: 521/521, train_loss: 0.185(0.160), train_acc: 92.500(94.202)
01/19 12:06:46 AM [Supernet Training] epoch: 309, train_loss: 0.160, train_acc: 94.202
01/19 12:06:49 AM [Supernet Validation] epoch: 309, val_loss: 0.427, val_acc: 88.020, best_acc: 88.450
01/19 12:06:49 AM 

01/19 12:06:50 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 001/521, train_loss: 0.128(0.128), train_acc: 95.833(95.833)
01/19 12:07:03 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 101/521, train_loss: 0.194(0.150), train_acc: 93.750(94.565)
01/19 12:07:15 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 201/521, train_loss: 0.171(0.157), train_acc: 94.792(94.315)
01/19 12:07:28 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 301/521, train_loss: 0.247(0.157), train_acc: 89.583(94.245)
01/19 12:07:41 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 401/521, train_loss: 0.208(0.160), train_acc: 91.667(94.158)
01/19 12:07:54 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 501/521, train_loss: 0.124(0.161), train_acc: 95.833(94.143)
01/19 12:07:56 AM [Supernet Training] lr: 0.01191 epoch: 310/600, step: 521/521, train_loss: 0.354(0.162), train_acc: 87.500(94.126)
01/19 12:07:56 AM [Supernet Training] epoch: 310, train_loss: 0.162, train_acc: 94.126
01/19 12:08:00 AM [Supernet Validation] epoch: 310, val_loss: 0.426, val_acc: 87.960, best_acc: 88.450
01/19 12:08:00 AM 

01/19 12:08:00 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 001/521, train_loss: 0.085(0.085), train_acc: 97.917(97.917)
01/19 12:08:13 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 101/521, train_loss: 0.092(0.155), train_acc: 96.875(94.317)
01/19 12:08:26 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 201/521, train_loss: 0.101(0.158), train_acc: 96.875(94.222)
01/19 12:08:38 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 301/521, train_loss: 0.149(0.161), train_acc: 94.792(94.120)
01/19 12:08:51 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 401/521, train_loss: 0.115(0.161), train_acc: 94.792(94.098)
01/19 12:09:04 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 501/521, train_loss: 0.107(0.161), train_acc: 94.792(94.191)
01/19 12:09:07 AM [Supernet Training] lr: 0.01185 epoch: 311/600, step: 521/521, train_loss: 0.267(0.161), train_acc: 93.750(94.184)
01/19 12:09:07 AM [Supernet Training] epoch: 311, train_loss: 0.161, train_acc: 94.184
01/19 12:09:10 AM [Supernet Validation] epoch: 311, val_loss: 0.430, val_acc: 88.240, best_acc: 88.450
01/19 12:09:10 AM 

01/19 12:09:11 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 001/521, train_loss: 0.123(0.123), train_acc: 94.792(94.792)
01/19 12:09:23 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 101/521, train_loss: 0.127(0.163), train_acc: 93.750(94.049)
01/19 12:09:36 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 201/521, train_loss: 0.131(0.161), train_acc: 94.792(94.159)
01/19 12:09:49 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 301/521, train_loss: 0.123(0.160), train_acc: 95.833(94.145)
01/19 12:10:02 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 401/521, train_loss: 0.116(0.160), train_acc: 96.875(94.166)
01/19 12:10:14 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 501/521, train_loss: 0.235(0.162), train_acc: 88.542(94.112)
01/19 12:10:17 AM [Supernet Training] lr: 0.01178 epoch: 312/600, step: 521/521, train_loss: 0.107(0.162), train_acc: 95.000(94.100)
01/19 12:10:17 AM [Supernet Training] epoch: 312, train_loss: 0.162, train_acc: 94.100
01/19 12:10:21 AM [Supernet Validation] epoch: 312, val_loss: 0.428, val_acc: 87.950, best_acc: 88.450
01/19 12:10:21 AM 

01/19 12:10:21 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 001/521, train_loss: 0.163(0.163), train_acc: 93.750(93.750)
01/19 12:10:34 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 101/521, train_loss: 0.221(0.159), train_acc: 91.667(94.183)
01/19 12:10:47 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 201/521, train_loss: 0.167(0.156), train_acc: 93.750(94.320)
01/19 12:10:59 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 301/521, train_loss: 0.158(0.159), train_acc: 93.750(94.279)
01/19 12:11:12 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 401/521, train_loss: 0.152(0.161), train_acc: 93.750(94.197)
01/19 12:11:25 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 501/521, train_loss: 0.101(0.161), train_acc: 94.792(94.170)
01/19 12:11:28 AM [Supernet Training] lr: 0.01172 epoch: 313/600, step: 521/521, train_loss: 0.115(0.161), train_acc: 95.000(94.176)
01/19 12:11:28 AM [Supernet Training] epoch: 313, train_loss: 0.161, train_acc: 94.176
01/19 12:11:31 AM [Supernet Validation] epoch: 313, val_loss: 0.436, val_acc: 88.120, best_acc: 88.450
01/19 12:11:31 AM 

01/19 12:11:32 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 001/521, train_loss: 0.180(0.180), train_acc: 94.792(94.792)
01/19 12:11:44 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 101/521, train_loss: 0.174(0.154), train_acc: 93.750(94.740)
01/19 12:11:57 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 201/521, train_loss: 0.200(0.156), train_acc: 93.750(94.543)
01/19 12:12:10 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 301/521, train_loss: 0.117(0.154), train_acc: 96.875(94.612)
01/19 12:12:23 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 401/521, train_loss: 0.211(0.156), train_acc: 93.750(94.514)
01/19 12:12:35 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 501/521, train_loss: 0.238(0.158), train_acc: 91.667(94.426)
01/19 12:12:38 AM [Supernet Training] lr: 0.01165 epoch: 314/600, step: 521/521, train_loss: 0.215(0.158), train_acc: 91.250(94.422)
01/19 12:12:38 AM [Supernet Training] epoch: 314, train_loss: 0.158, train_acc: 94.422
01/19 12:12:42 AM [Supernet Validation] epoch: 314, val_loss: 0.402, val_acc: 88.370, best_acc: 88.450
01/19 12:12:42 AM 

01/19 12:12:42 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 001/521, train_loss: 0.173(0.173), train_acc: 94.792(94.792)
01/19 12:12:55 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 101/521, train_loss: 0.128(0.145), train_acc: 94.792(94.740)
01/19 12:13:08 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 201/521, train_loss: 0.210(0.153), train_acc: 91.667(94.413)
01/19 12:13:20 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 301/521, train_loss: 0.211(0.158), train_acc: 94.792(94.304)
01/19 12:13:33 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 401/521, train_loss: 0.168(0.158), train_acc: 92.708(94.249)
01/19 12:13:46 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 501/521, train_loss: 0.140(0.157), train_acc: 93.750(94.328)
01/19 12:13:48 AM [Supernet Training] lr: 0.01158 epoch: 315/600, step: 521/521, train_loss: 0.072(0.157), train_acc: 98.750(94.310)
01/19 12:13:48 AM [Supernet Training] epoch: 315, train_loss: 0.157, train_acc: 94.310
01/19 12:13:52 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 12:13:52 AM [Supernet Validation] epoch: 315, val_loss: 0.404, val_acc: 88.680, best_acc: 88.680
01/19 12:13:52 AM 

01/19 12:13:53 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 001/521, train_loss: 0.152(0.152), train_acc: 93.750(93.750)
01/19 12:14:05 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 101/521, train_loss: 0.157(0.146), train_acc: 93.750(94.833)
01/19 12:14:18 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 201/521, train_loss: 0.138(0.152), train_acc: 93.750(94.548)
01/19 12:14:31 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 301/521, train_loss: 0.051(0.155), train_acc: 98.958(94.435)
01/19 12:14:44 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 401/521, train_loss: 0.134(0.155), train_acc: 94.792(94.433)
01/19 12:14:57 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 501/521, train_loss: 0.131(0.156), train_acc: 95.833(94.378)
01/19 12:14:59 AM [Supernet Training] lr: 0.01152 epoch: 316/600, step: 521/521, train_loss: 0.138(0.157), train_acc: 95.000(94.352)
01/19 12:14:59 AM [Supernet Training] epoch: 316, train_loss: 0.157, train_acc: 94.352
01/19 12:15:03 AM [Supernet Validation] epoch: 316, val_loss: 0.443, val_acc: 87.860, best_acc: 88.680
01/19 12:15:03 AM 

01/19 12:15:03 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 001/521, train_loss: 0.255(0.255), train_acc: 91.667(91.667)
01/19 12:15:16 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 101/521, train_loss: 0.155(0.162), train_acc: 95.833(93.833)
01/19 12:15:29 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 201/521, train_loss: 0.110(0.159), train_acc: 93.750(94.071)
01/19 12:15:41 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 301/521, train_loss: 0.258(0.158), train_acc: 92.708(94.113)
01/19 12:15:54 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 401/521, train_loss: 0.210(0.157), train_acc: 93.750(94.171)
01/19 12:16:07 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 501/521, train_loss: 0.144(0.157), train_acc: 93.750(94.147)
01/19 12:16:10 AM [Supernet Training] lr: 0.01145 epoch: 317/600, step: 521/521, train_loss: 0.212(0.157), train_acc: 93.750(94.170)
01/19 12:16:10 AM [Supernet Training] epoch: 317, train_loss: 0.157, train_acc: 94.170
01/19 12:16:13 AM [Supernet Validation] epoch: 317, val_loss: 0.431, val_acc: 88.070, best_acc: 88.680
01/19 12:16:13 AM 

01/19 12:16:14 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 001/521, train_loss: 0.247(0.247), train_acc: 88.542(88.542)
01/19 12:16:27 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 101/521, train_loss: 0.139(0.152), train_acc: 94.792(94.503)
01/19 12:16:39 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 201/521, train_loss: 0.161(0.156), train_acc: 96.875(94.372)
01/19 12:16:52 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 301/521, train_loss: 0.132(0.151), train_acc: 93.750(94.532)
01/19 12:17:05 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 401/521, train_loss: 0.192(0.154), train_acc: 92.708(94.470)
01/19 12:17:18 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 501/521, train_loss: 0.230(0.155), train_acc: 92.708(94.432)
01/19 12:17:20 AM [Supernet Training] lr: 0.01139 epoch: 318/600, step: 521/521, train_loss: 0.123(0.155), train_acc: 97.500(94.422)
01/19 12:17:20 AM [Supernet Training] epoch: 318, train_loss: 0.155, train_acc: 94.422
01/19 12:17:24 AM [Supernet Validation] epoch: 318, val_loss: 0.419, val_acc: 87.970, best_acc: 88.680
01/19 12:17:24 AM 

01/19 12:17:24 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 001/521, train_loss: 0.198(0.198), train_acc: 92.708(92.708)
01/19 12:17:37 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 101/521, train_loss: 0.147(0.157), train_acc: 94.792(94.400)
01/19 12:17:50 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 201/521, train_loss: 0.097(0.155), train_acc: 96.875(94.408)
01/19 12:18:03 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 301/521, train_loss: 0.040(0.158), train_acc: 98.958(94.304)
01/19 12:18:15 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 401/521, train_loss: 0.107(0.157), train_acc: 97.917(94.308)
01/19 12:18:28 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 501/521, train_loss: 0.177(0.157), train_acc: 92.708(94.307)
01/19 12:18:31 AM [Supernet Training] lr: 0.01132 epoch: 319/600, step: 521/521, train_loss: 0.128(0.157), train_acc: 97.500(94.316)
01/19 12:18:31 AM [Supernet Training] epoch: 319, train_loss: 0.157, train_acc: 94.316
01/19 12:18:34 AM [Supernet Validation] epoch: 319, val_loss: 0.423, val_acc: 88.280, best_acc: 88.680
01/19 12:18:34 AM 

01/19 12:18:35 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 001/521, train_loss: 0.135(0.135), train_acc: 94.792(94.792)
01/19 12:18:47 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 101/521, train_loss: 0.111(0.160), train_acc: 95.833(94.616)
01/19 12:19:00 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 201/521, train_loss: 0.165(0.155), train_acc: 91.667(94.600)
01/19 12:19:13 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 301/521, train_loss: 0.091(0.156), train_acc: 97.917(94.529)
01/19 12:19:26 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 401/521, train_loss: 0.169(0.155), train_acc: 92.708(94.511)
01/19 12:19:39 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 501/521, train_loss: 0.156(0.154), train_acc: 94.792(94.484)
01/19 12:19:41 AM [Supernet Training] lr: 0.01126 epoch: 320/600, step: 521/521, train_loss: 0.171(0.154), train_acc: 92.500(94.482)
01/19 12:19:41 AM [Supernet Training] epoch: 320, train_loss: 0.154, train_acc: 94.482
01/19 12:19:45 AM [Supernet Validation] epoch: 320, val_loss: 0.415, val_acc: 88.200, best_acc: 88.680
01/19 12:19:45 AM 

01/19 12:19:45 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 001/521, train_loss: 0.225(0.225), train_acc: 91.667(91.667)
01/19 12:19:58 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 101/521, train_loss: 0.297(0.155), train_acc: 91.667(94.410)
01/19 12:20:11 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 201/521, train_loss: 0.059(0.150), train_acc: 100.000(94.719)
01/19 12:20:24 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 301/521, train_loss: 0.225(0.154), train_acc: 92.708(94.536)
01/19 12:20:36 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 401/521, train_loss: 0.088(0.157), train_acc: 96.875(94.381)
01/19 12:20:49 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 501/521, train_loss: 0.123(0.158), train_acc: 96.875(94.336)
01/19 12:20:52 AM [Supernet Training] lr: 0.01119 epoch: 321/600, step: 521/521, train_loss: 0.246(0.158), train_acc: 90.000(94.320)
01/19 12:20:52 AM [Supernet Training] epoch: 321, train_loss: 0.158, train_acc: 94.320
01/19 12:20:55 AM [Supernet Validation] epoch: 321, val_loss: 0.414, val_acc: 88.560, best_acc: 88.680
01/19 12:20:55 AM 

01/19 12:20:56 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 001/521, train_loss: 0.255(0.255), train_acc: 89.583(89.583)
01/19 12:21:09 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 101/521, train_loss: 0.210(0.156), train_acc: 90.625(94.389)
01/19 12:21:21 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 201/521, train_loss: 0.091(0.150), train_acc: 95.833(94.615)
01/19 12:21:34 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 301/521, train_loss: 0.067(0.148), train_acc: 97.917(94.619)
01/19 12:21:47 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 401/521, train_loss: 0.151(0.147), train_acc: 92.708(94.683)
01/19 12:21:59 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 501/521, train_loss: 0.085(0.150), train_acc: 96.875(94.596)
01/19 12:22:02 AM [Supernet Training] lr: 0.01113 epoch: 322/600, step: 521/521, train_loss: 0.162(0.150), train_acc: 96.250(94.600)
01/19 12:22:02 AM [Supernet Training] epoch: 322, train_loss: 0.150, train_acc: 94.600
01/19 12:22:06 AM [Supernet Validation] epoch: 322, val_loss: 0.423, val_acc: 88.390, best_acc: 88.680
01/19 12:22:06 AM 

01/19 12:22:06 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 001/521, train_loss: 0.234(0.234), train_acc: 91.667(91.667)
01/19 12:22:19 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 101/521, train_loss: 0.154(0.148), train_acc: 94.792(94.616)
01/19 12:22:32 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 201/521, train_loss: 0.064(0.152), train_acc: 97.917(94.538)
01/19 12:22:44 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 301/521, train_loss: 0.171(0.153), train_acc: 93.750(94.504)
01/19 12:22:57 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 401/521, train_loss: 0.180(0.156), train_acc: 92.708(94.423)
01/19 12:23:10 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 501/521, train_loss: 0.185(0.155), train_acc: 94.792(94.436)
01/19 12:23:12 AM [Supernet Training] lr: 0.01106 epoch: 323/600, step: 521/521, train_loss: 0.145(0.155), train_acc: 95.000(94.422)
01/19 12:23:12 AM [Supernet Training] epoch: 323, train_loss: 0.155, train_acc: 94.422
01/19 12:23:16 AM [Supernet Validation] epoch: 323, val_loss: 0.418, val_acc: 88.210, best_acc: 88.680
01/19 12:23:16 AM 

01/19 12:23:16 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 001/521, train_loss: 0.149(0.149), train_acc: 93.750(93.750)
01/19 12:23:29 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 101/521, train_loss: 0.148(0.148), train_acc: 93.750(94.771)
01/19 12:23:42 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 201/521, train_loss: 0.033(0.149), train_acc: 98.958(94.600)
01/19 12:23:55 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 301/521, train_loss: 0.219(0.149), train_acc: 91.667(94.646)
01/19 12:24:08 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 401/521, train_loss: 0.159(0.148), train_acc: 95.833(94.657)
01/19 12:24:20 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 501/521, train_loss: 0.103(0.148), train_acc: 96.875(94.642)
01/19 12:24:23 AM [Supernet Training] lr: 0.01100 epoch: 324/600, step: 521/521, train_loss: 0.241(0.148), train_acc: 93.750(94.638)
01/19 12:24:23 AM [Supernet Training] epoch: 324, train_loss: 0.148, train_acc: 94.638
01/19 12:24:27 AM [Supernet Validation] epoch: 324, val_loss: 0.426, val_acc: 87.980, best_acc: 88.680
01/19 12:24:27 AM 

01/19 12:24:27 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 001/521, train_loss: 0.122(0.122), train_acc: 95.833(95.833)
01/19 12:24:40 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 101/521, train_loss: 0.101(0.149), train_acc: 96.875(94.596)
01/19 12:24:53 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 201/521, train_loss: 0.140(0.146), train_acc: 95.833(94.714)
01/19 12:25:05 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 301/521, train_loss: 0.082(0.147), train_acc: 98.958(94.657)
01/19 12:25:18 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 401/521, train_loss: 0.107(0.147), train_acc: 95.833(94.628)
01/19 12:25:31 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 501/521, train_loss: 0.116(0.149), train_acc: 94.792(94.598)
01/19 12:25:33 AM [Supernet Training] lr: 0.01093 epoch: 325/600, step: 521/521, train_loss: 0.207(0.149), train_acc: 92.500(94.594)
01/19 12:25:33 AM [Supernet Training] epoch: 325, train_loss: 0.149, train_acc: 94.594
01/19 12:25:37 AM [Supernet Validation] epoch: 325, val_loss: 0.418, val_acc: 88.560, best_acc: 88.680
01/19 12:25:37 AM 

01/19 12:25:37 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 001/521, train_loss: 0.175(0.175), train_acc: 95.833(95.833)
01/19 12:25:50 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 101/521, train_loss: 0.107(0.146), train_acc: 94.792(94.627)
01/19 12:26:03 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 201/521, train_loss: 0.213(0.154), train_acc: 91.667(94.413)
01/19 12:26:16 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 301/521, train_loss: 0.153(0.152), train_acc: 93.750(94.421)
01/19 12:26:29 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 401/521, train_loss: 0.178(0.153), train_acc: 93.750(94.407)
01/19 12:26:41 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 501/521, train_loss: 0.227(0.151), train_acc: 91.667(94.484)
01/19 12:26:44 AM [Supernet Training] lr: 0.01087 epoch: 326/600, step: 521/521, train_loss: 0.288(0.151), train_acc: 90.000(94.460)
01/19 12:26:44 AM [Supernet Training] epoch: 326, train_loss: 0.151, train_acc: 94.460
01/19 12:26:47 AM [Supernet Validation] epoch: 326, val_loss: 0.430, val_acc: 87.980, best_acc: 88.680
01/19 12:26:47 AM 

01/19 12:26:48 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 001/521, train_loss: 0.142(0.142), train_acc: 93.750(93.750)
01/19 12:27:01 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 101/521, train_loss: 0.113(0.142), train_acc: 94.792(94.709)
01/19 12:27:13 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 201/521, train_loss: 0.169(0.147), train_acc: 93.750(94.626)
01/19 12:27:26 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 301/521, train_loss: 0.149(0.148), train_acc: 95.833(94.612)
01/19 12:27:39 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 401/521, train_loss: 0.064(0.148), train_acc: 97.917(94.651)
01/19 12:27:52 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 501/521, train_loss: 0.115(0.150), train_acc: 94.792(94.609)
01/19 12:27:54 AM [Supernet Training] lr: 0.01080 epoch: 327/600, step: 521/521, train_loss: 0.242(0.151), train_acc: 93.750(94.594)
01/19 12:27:54 AM [Supernet Training] epoch: 327, train_loss: 0.151, train_acc: 94.594
01/19 12:27:58 AM [Supernet Validation] epoch: 327, val_loss: 0.430, val_acc: 88.230, best_acc: 88.680
01/19 12:27:58 AM 

01/19 12:27:58 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 001/521, train_loss: 0.163(0.163), train_acc: 94.792(94.792)
01/19 12:28:11 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 101/521, train_loss: 0.116(0.143), train_acc: 96.875(94.895)
01/19 12:28:24 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 201/521, train_loss: 0.096(0.146), train_acc: 95.833(94.755)
01/19 12:28:37 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 301/521, train_loss: 0.290(0.150), train_acc: 89.583(94.667)
01/19 12:28:50 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 401/521, train_loss: 0.119(0.147), train_acc: 95.833(94.766)
01/19 12:29:02 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 501/521, train_loss: 0.114(0.149), train_acc: 94.792(94.673)
01/19 12:29:05 AM [Supernet Training] lr: 0.01074 epoch: 328/600, step: 521/521, train_loss: 0.159(0.149), train_acc: 93.750(94.654)
01/19 12:29:05 AM [Supernet Training] epoch: 328, train_loss: 0.149, train_acc: 94.654
01/19 12:29:09 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 12:29:09 AM [Supernet Validation] epoch: 328, val_loss: 0.412, val_acc: 88.730, best_acc: 88.730
01/19 12:29:09 AM 

01/19 12:29:09 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 001/521, train_loss: 0.100(0.100), train_acc: 95.833(95.833)
01/19 12:29:22 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 101/521, train_loss: 0.143(0.147), train_acc: 95.833(94.637)
01/19 12:29:35 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 201/521, train_loss: 0.116(0.145), train_acc: 95.833(94.807)
01/19 12:29:47 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 301/521, train_loss: 0.170(0.145), train_acc: 93.750(94.833)
01/19 12:30:00 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 401/521, train_loss: 0.161(0.146), train_acc: 92.708(94.784)
01/19 12:30:13 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 501/521, train_loss: 0.111(0.145), train_acc: 94.792(94.839)
01/19 12:30:16 AM [Supernet Training] lr: 0.01067 epoch: 329/600, step: 521/521, train_loss: 0.151(0.145), train_acc: 96.250(94.820)
01/19 12:30:16 AM [Supernet Training] epoch: 329, train_loss: 0.145, train_acc: 94.820
01/19 12:30:19 AM [Supernet Validation] epoch: 329, val_loss: 0.429, val_acc: 88.200, best_acc: 88.730
01/19 12:30:19 AM 

01/19 12:30:20 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 001/521, train_loss: 0.106(0.106), train_acc: 94.792(94.792)
01/19 12:30:33 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 101/521, train_loss: 0.060(0.156), train_acc: 98.958(94.554)
01/19 12:30:45 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 201/521, train_loss: 0.117(0.149), train_acc: 95.833(94.662)
01/19 12:30:58 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 301/521, train_loss: 0.149(0.150), train_acc: 94.792(94.608)
01/19 12:31:11 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 401/521, train_loss: 0.186(0.150), train_acc: 94.792(94.625)
01/19 12:31:24 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 501/521, train_loss: 0.146(0.151), train_acc: 94.792(94.605)
01/19 12:31:26 AM [Supernet Training] lr: 0.01061 epoch: 330/600, step: 521/521, train_loss: 0.197(0.150), train_acc: 93.750(94.618)
01/19 12:31:26 AM [Supernet Training] epoch: 330, train_loss: 0.150, train_acc: 94.618
01/19 12:31:30 AM [Supernet Validation] epoch: 330, val_loss: 0.432, val_acc: 87.880, best_acc: 88.730
01/19 12:31:30 AM 

01/19 12:31:30 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 001/521, train_loss: 0.086(0.086), train_acc: 97.917(97.917)
01/19 12:31:43 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 101/521, train_loss: 0.176(0.152), train_acc: 95.833(94.534)
01/19 12:31:56 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 201/521, train_loss: 0.164(0.147), train_acc: 93.750(94.750)
01/19 12:32:09 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 301/521, train_loss: 0.143(0.147), train_acc: 94.792(94.716)
01/19 12:32:22 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 401/521, train_loss: 0.095(0.145), train_acc: 97.917(94.750)
01/19 12:32:34 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 501/521, train_loss: 0.077(0.145), train_acc: 96.875(94.721)
01/19 12:32:37 AM [Supernet Training] lr: 0.01054 epoch: 331/600, step: 521/521, train_loss: 0.180(0.145), train_acc: 92.500(94.696)
01/19 12:32:37 AM [Supernet Training] epoch: 331, train_loss: 0.145, train_acc: 94.696
01/19 12:32:41 AM [Supernet Validation] epoch: 331, val_loss: 0.415, val_acc: 88.660, best_acc: 88.730
01/19 12:32:41 AM 

01/19 12:32:41 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 001/521, train_loss: 0.079(0.079), train_acc: 97.917(97.917)
01/19 12:32:54 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 101/521, train_loss: 0.097(0.158), train_acc: 94.792(94.420)
01/19 12:33:06 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 201/521, train_loss: 0.184(0.151), train_acc: 94.792(94.698)
01/19 12:33:19 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 301/521, train_loss: 0.177(0.149), train_acc: 94.792(94.778)
01/19 12:33:32 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 401/521, train_loss: 0.138(0.147), train_acc: 94.792(94.823)
01/19 12:33:45 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 501/521, train_loss: 0.186(0.147), train_acc: 93.750(94.842)
01/19 12:33:47 AM [Supernet Training] lr: 0.01048 epoch: 332/600, step: 521/521, train_loss: 0.095(0.146), train_acc: 96.250(94.858)
01/19 12:33:47 AM [Supernet Training] epoch: 332, train_loss: 0.146, train_acc: 94.858
01/19 12:33:51 AM [Supernet Validation] epoch: 332, val_loss: 0.415, val_acc: 88.460, best_acc: 88.730
01/19 12:33:51 AM 

01/19 12:33:51 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 001/521, train_loss: 0.114(0.114), train_acc: 96.875(96.875)
01/19 12:34:04 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 101/521, train_loss: 0.139(0.144), train_acc: 94.792(94.843)
01/19 12:34:17 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 201/521, train_loss: 0.166(0.146), train_acc: 94.792(94.729)
01/19 12:34:30 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 301/521, train_loss: 0.077(0.145), train_acc: 97.917(94.729)
01/19 12:34:43 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 401/521, train_loss: 0.143(0.146), train_acc: 93.750(94.675)
01/19 12:34:55 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 501/521, train_loss: 0.108(0.146), train_acc: 94.792(94.727)
01/19 12:34:58 AM [Supernet Training] lr: 0.01042 epoch: 333/600, step: 521/521, train_loss: 0.113(0.146), train_acc: 96.250(94.738)
01/19 12:34:58 AM [Supernet Training] epoch: 333, train_loss: 0.146, train_acc: 94.738
01/19 12:35:02 AM [Supernet Validation] epoch: 333, val_loss: 0.418, val_acc: 88.530, best_acc: 88.730
01/19 12:35:02 AM 

01/19 12:35:02 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 001/521, train_loss: 0.151(0.151), train_acc: 92.708(92.708)
01/19 12:35:15 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 101/521, train_loss: 0.069(0.145), train_acc: 97.917(94.678)
01/19 12:35:27 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 201/521, train_loss: 0.123(0.147), train_acc: 93.750(94.558)
01/19 12:35:40 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 301/521, train_loss: 0.128(0.143), train_acc: 96.875(94.709)
01/19 12:35:53 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 401/521, train_loss: 0.201(0.143), train_acc: 93.750(94.779)
01/19 12:36:06 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 501/521, train_loss: 0.154(0.144), train_acc: 96.875(94.763)
01/19 12:36:08 AM [Supernet Training] lr: 0.01035 epoch: 334/600, step: 521/521, train_loss: 0.120(0.144), train_acc: 96.250(94.790)
01/19 12:36:08 AM [Supernet Training] epoch: 334, train_loss: 0.144, train_acc: 94.790
01/19 12:36:12 AM [Supernet Validation] epoch: 334, val_loss: 0.421, val_acc: 88.440, best_acc: 88.730
01/19 12:36:12 AM 

01/19 12:36:12 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 001/521, train_loss: 0.129(0.129), train_acc: 95.833(95.833)
01/19 12:36:25 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 101/521, train_loss: 0.103(0.140), train_acc: 95.833(94.936)
01/19 12:36:38 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 201/521, train_loss: 0.109(0.145), train_acc: 93.750(94.766)
01/19 12:36:51 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 301/521, train_loss: 0.270(0.145), train_acc: 92.708(94.740)
01/19 12:37:03 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 401/521, train_loss: 0.078(0.143), train_acc: 97.917(94.812)
01/19 12:37:16 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 501/521, train_loss: 0.288(0.144), train_acc: 87.500(94.767)
01/19 12:37:19 AM [Supernet Training] lr: 0.01029 epoch: 335/600, step: 521/521, train_loss: 0.104(0.143), train_acc: 96.250(94.782)
01/19 12:37:19 AM [Supernet Training] epoch: 335, train_loss: 0.143, train_acc: 94.782
01/19 12:37:22 AM [Supernet Validation] epoch: 335, val_loss: 0.422, val_acc: 88.480, best_acc: 88.730
01/19 12:37:22 AM 

01/19 12:37:23 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 001/521, train_loss: 0.090(0.090), train_acc: 95.833(95.833)
01/19 12:37:36 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 101/521, train_loss: 0.184(0.142), train_acc: 92.708(94.802)
01/19 12:37:48 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 201/521, train_loss: 0.214(0.144), train_acc: 91.667(94.792)
01/19 12:38:01 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 301/521, train_loss: 0.068(0.143), train_acc: 98.958(94.861)
01/19 12:38:14 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 401/521, train_loss: 0.111(0.139), train_acc: 95.833(95.018)
01/19 12:38:27 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 501/521, train_loss: 0.277(0.141), train_acc: 90.625(94.985)
01/19 12:38:29 AM [Supernet Training] lr: 0.01022 epoch: 336/600, step: 521/521, train_loss: 0.169(0.141), train_acc: 93.750(94.958)
01/19 12:38:29 AM [Supernet Training] epoch: 336, train_loss: 0.141, train_acc: 94.958
01/19 12:38:33 AM [Supernet Validation] epoch: 336, val_loss: 0.418, val_acc: 88.430, best_acc: 88.730
01/19 12:38:33 AM 

01/19 12:38:33 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 001/521, train_loss: 0.198(0.198), train_acc: 94.792(94.792)
01/19 12:38:46 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 101/521, train_loss: 0.163(0.148), train_acc: 92.708(94.658)
01/19 12:38:59 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 201/521, train_loss: 0.147(0.144), train_acc: 94.792(94.843)
01/19 12:39:12 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 301/521, train_loss: 0.168(0.142), train_acc: 92.708(94.916)
01/19 12:39:25 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 401/521, train_loss: 0.155(0.142), train_acc: 95.833(94.885)
01/19 12:39:37 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 501/521, train_loss: 0.238(0.144), train_acc: 92.708(94.846)
01/19 12:39:40 AM [Supernet Training] lr: 0.01016 epoch: 337/600, step: 521/521, train_loss: 0.272(0.144), train_acc: 92.500(94.800)
01/19 12:39:40 AM [Supernet Training] epoch: 337, train_loss: 0.144, train_acc: 94.800
01/19 12:39:43 AM [Supernet Validation] epoch: 337, val_loss: 0.443, val_acc: 88.380, best_acc: 88.730
01/19 12:39:43 AM 

01/19 12:39:44 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 001/521, train_loss: 0.118(0.118), train_acc: 96.875(96.875)
01/19 12:39:57 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 101/521, train_loss: 0.165(0.142), train_acc: 94.792(94.874)
01/19 12:40:10 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 201/521, train_loss: 0.104(0.139), train_acc: 94.792(94.895)
01/19 12:40:22 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 301/521, train_loss: 0.076(0.137), train_acc: 97.917(95.013)
01/19 12:40:35 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 401/521, train_loss: 0.109(0.140), train_acc: 95.833(94.906)
01/19 12:40:48 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 501/521, train_loss: 0.164(0.140), train_acc: 93.750(94.850)
01/19 12:40:50 AM [Supernet Training] lr: 0.01009 epoch: 338/600, step: 521/521, train_loss: 0.087(0.141), train_acc: 98.750(94.836)
01/19 12:40:50 AM [Supernet Training] epoch: 338, train_loss: 0.141, train_acc: 94.836
01/19 12:40:54 AM [Supernet Validation] epoch: 338, val_loss: 0.435, val_acc: 88.370, best_acc: 88.730
01/19 12:40:54 AM 

01/19 12:40:54 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 001/521, train_loss: 0.133(0.133), train_acc: 96.875(96.875)
01/19 12:41:07 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 101/521, train_loss: 0.174(0.135), train_acc: 94.792(95.050)
01/19 12:41:20 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 201/521, train_loss: 0.183(0.136), train_acc: 93.750(95.097)
01/19 12:41:33 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 301/521, train_loss: 0.136(0.141), train_acc: 96.875(94.944)
01/19 12:41:46 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 401/521, train_loss: 0.075(0.139), train_acc: 97.917(95.005)
01/19 12:41:58 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 501/521, train_loss: 0.149(0.140), train_acc: 93.750(94.970)
01/19 12:42:01 AM [Supernet Training] lr: 0.01003 epoch: 339/600, step: 521/521, train_loss: 0.108(0.140), train_acc: 96.250(95.006)
01/19 12:42:01 AM [Supernet Training] epoch: 339, train_loss: 0.140, train_acc: 95.006
01/19 12:42:05 AM [Supernet Validation] epoch: 339, val_loss: 0.425, val_acc: 88.340, best_acc: 88.730
01/19 12:42:05 AM 

01/19 12:42:05 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 001/521, train_loss: 0.281(0.281), train_acc: 89.583(89.583)
01/19 12:42:18 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 101/521, train_loss: 0.173(0.141), train_acc: 94.792(95.050)
01/19 12:42:31 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 201/521, train_loss: 0.148(0.141), train_acc: 96.875(95.004)
01/19 12:42:43 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 301/521, train_loss: 0.101(0.141), train_acc: 96.875(95.017)
01/19 12:42:56 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 401/521, train_loss: 0.118(0.144), train_acc: 93.750(94.805)
01/19 12:43:09 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 501/521, train_loss: 0.084(0.142), train_acc: 94.792(94.883)
01/19 12:43:12 AM [Supernet Training] lr: 0.00997 epoch: 340/600, step: 521/521, train_loss: 0.060(0.142), train_acc: 97.500(94.896)
01/19 12:43:12 AM [Supernet Training] epoch: 340, train_loss: 0.142, train_acc: 94.896
01/19 12:43:15 AM [Supernet Validation] epoch: 340, val_loss: 0.431, val_acc: 88.280, best_acc: 88.730
01/19 12:43:15 AM 

01/19 12:43:16 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 97.917(97.917)
01/19 12:43:28 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 101/521, train_loss: 0.175(0.133), train_acc: 94.792(95.297)
01/19 12:43:41 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 201/521, train_loss: 0.176(0.138), train_acc: 94.792(95.154)
01/19 12:43:54 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 301/521, train_loss: 0.184(0.133), train_acc: 94.792(95.321)
01/19 12:44:07 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 401/521, train_loss: 0.185(0.136), train_acc: 92.708(95.210)
01/19 12:44:19 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 501/521, train_loss: 0.079(0.139), train_acc: 97.917(95.087)
01/19 12:44:22 AM [Supernet Training] lr: 0.00990 epoch: 341/600, step: 521/521, train_loss: 0.103(0.139), train_acc: 95.000(95.088)
01/19 12:44:22 AM [Supernet Training] epoch: 341, train_loss: 0.139, train_acc: 95.088
01/19 12:44:26 AM [Supernet Validation] epoch: 341, val_loss: 0.425, val_acc: 88.610, best_acc: 88.730
01/19 12:44:26 AM 

01/19 12:44:26 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 001/521, train_loss: 0.224(0.224), train_acc: 94.792(94.792)
01/19 12:44:39 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 101/521, train_loss: 0.095(0.129), train_acc: 96.875(95.472)
01/19 12:44:52 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 201/521, train_loss: 0.212(0.128), train_acc: 92.708(95.408)
01/19 12:45:04 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 301/521, train_loss: 0.140(0.134), train_acc: 94.792(95.210)
01/19 12:45:17 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 401/521, train_loss: 0.168(0.135), train_acc: 93.750(95.171)
01/19 12:45:30 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 501/521, train_loss: 0.085(0.137), train_acc: 97.917(95.077)
01/19 12:45:32 AM [Supernet Training] lr: 0.00984 epoch: 342/600, step: 521/521, train_loss: 0.093(0.137), train_acc: 96.250(95.086)
01/19 12:45:32 AM [Supernet Training] epoch: 342, train_loss: 0.137, train_acc: 95.086
01/19 12:45:36 AM [Supernet Validation] epoch: 342, val_loss: 0.425, val_acc: 88.600, best_acc: 88.730
01/19 12:45:36 AM 

01/19 12:45:36 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 001/521, train_loss: 0.247(0.247), train_acc: 93.750(93.750)
01/19 12:45:49 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 101/521, train_loss: 0.209(0.141), train_acc: 93.750(94.884)
01/19 12:46:02 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 201/521, train_loss: 0.156(0.138), train_acc: 95.833(95.030)
01/19 12:46:15 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 301/521, train_loss: 0.179(0.139), train_acc: 91.667(94.985)
01/19 12:46:28 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 401/521, train_loss: 0.104(0.137), train_acc: 95.833(95.023)
01/19 12:46:40 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 501/521, train_loss: 0.291(0.138), train_acc: 86.458(94.973)
01/19 12:46:43 AM [Supernet Training] lr: 0.00977 epoch: 343/600, step: 521/521, train_loss: 0.159(0.139), train_acc: 93.750(94.976)
01/19 12:46:43 AM [Supernet Training] epoch: 343, train_loss: 0.139, train_acc: 94.976
01/19 12:46:47 AM [Supernet Validation] epoch: 343, val_loss: 0.427, val_acc: 88.510, best_acc: 88.730
01/19 12:46:47 AM 

01/19 12:46:47 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 001/521, train_loss: 0.134(0.134), train_acc: 91.667(91.667)
01/19 12:47:00 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 101/521, train_loss: 0.163(0.133), train_acc: 92.708(95.235)
01/19 12:47:13 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 201/521, train_loss: 0.095(0.137), train_acc: 96.875(95.103)
01/19 12:47:25 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 301/521, train_loss: 0.108(0.136), train_acc: 94.792(95.148)
01/19 12:47:38 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 401/521, train_loss: 0.057(0.138), train_acc: 98.958(95.077)
01/19 12:47:51 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 501/521, train_loss: 0.111(0.140), train_acc: 95.833(94.989)
01/19 12:47:53 AM [Supernet Training] lr: 0.00971 epoch: 344/600, step: 521/521, train_loss: 0.109(0.140), train_acc: 97.500(94.992)
01/19 12:47:54 AM [Supernet Training] epoch: 344, train_loss: 0.140, train_acc: 94.992
01/19 12:47:57 AM [Supernet Validation] epoch: 344, val_loss: 0.430, val_acc: 88.010, best_acc: 88.730
01/19 12:47:57 AM 

01/19 12:47:58 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 94.792(94.792)
01/19 12:48:10 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 101/521, train_loss: 0.172(0.144), train_acc: 93.750(94.699)
01/19 12:48:23 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 201/521, train_loss: 0.156(0.145), train_acc: 93.750(94.672)
01/19 12:48:36 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 301/521, train_loss: 0.131(0.143), train_acc: 94.792(94.750)
01/19 12:48:49 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 401/521, train_loss: 0.129(0.142), train_acc: 94.792(94.792)
01/19 12:49:01 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 501/521, train_loss: 0.162(0.141), train_acc: 94.792(94.869)
01/19 12:49:04 AM [Supernet Training] lr: 0.00965 epoch: 345/600, step: 521/521, train_loss: 0.128(0.141), train_acc: 95.000(94.858)
01/19 12:49:04 AM [Supernet Training] epoch: 345, train_loss: 0.141, train_acc: 94.858
01/19 12:49:08 AM [Supernet Validation] epoch: 345, val_loss: 0.416, val_acc: 88.590, best_acc: 88.730
01/19 12:49:08 AM 

01/19 12:49:08 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 001/521, train_loss: 0.172(0.172), train_acc: 93.750(93.750)
01/19 12:49:21 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 101/521, train_loss: 0.141(0.135), train_acc: 94.792(95.050)
01/19 12:49:34 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 201/521, train_loss: 0.211(0.136), train_acc: 92.708(94.947)
01/19 12:49:47 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 301/521, train_loss: 0.193(0.136), train_acc: 92.708(94.992)
01/19 12:49:59 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 401/521, train_loss: 0.165(0.135), train_acc: 91.667(94.984)
01/19 12:50:12 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 501/521, train_loss: 0.177(0.137), train_acc: 93.750(94.962)
01/19 12:50:15 AM [Supernet Training] lr: 0.00958 epoch: 346/600, step: 521/521, train_loss: 0.144(0.137), train_acc: 96.250(94.956)
01/19 12:50:15 AM [Supernet Training] epoch: 346, train_loss: 0.137, train_acc: 94.956
01/19 12:50:19 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 12:50:19 AM [Supernet Validation] epoch: 346, val_loss: 0.429, val_acc: 88.860, best_acc: 88.860
01/19 12:50:19 AM 

01/19 12:50:19 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 001/521, train_loss: 0.159(0.159), train_acc: 92.708(92.708)
01/19 12:50:32 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 101/521, train_loss: 0.128(0.140), train_acc: 94.792(94.854)
01/19 12:50:45 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 201/521, train_loss: 0.179(0.137), train_acc: 92.708(95.040)
01/19 12:50:57 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 301/521, train_loss: 0.126(0.134), train_acc: 94.792(95.055)
01/19 12:51:10 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 401/521, train_loss: 0.081(0.134), train_acc: 95.833(95.059)
01/19 12:51:23 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 501/521, train_loss: 0.120(0.137), train_acc: 96.875(94.950)
01/19 12:51:26 AM [Supernet Training] lr: 0.00952 epoch: 347/600, step: 521/521, train_loss: 0.152(0.137), train_acc: 95.000(94.954)
01/19 12:51:26 AM [Supernet Training] epoch: 347, train_loss: 0.137, train_acc: 94.954
01/19 12:51:29 AM [Supernet Validation] epoch: 347, val_loss: 0.418, val_acc: 88.810, best_acc: 88.860
01/19 12:51:29 AM 

01/19 12:51:30 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 001/521, train_loss: 0.137(0.137), train_acc: 93.750(93.750)
01/19 12:51:42 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 101/521, train_loss: 0.213(0.141), train_acc: 93.750(94.854)
01/19 12:51:55 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 201/521, train_loss: 0.182(0.139), train_acc: 93.750(95.035)
01/19 12:52:08 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 301/521, train_loss: 0.095(0.138), train_acc: 98.958(94.979)
01/19 12:52:21 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 401/521, train_loss: 0.076(0.137), train_acc: 97.917(95.038)
01/19 12:52:33 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 501/521, train_loss: 0.074(0.136), train_acc: 97.917(95.018)
01/19 12:52:36 AM [Supernet Training] lr: 0.00945 epoch: 348/600, step: 521/521, train_loss: 0.153(0.137), train_acc: 93.750(95.012)
01/19 12:52:36 AM [Supernet Training] epoch: 348, train_loss: 0.137, train_acc: 95.012
01/19 12:52:39 AM [Supernet Validation] epoch: 348, val_loss: 0.429, val_acc: 88.270, best_acc: 88.860
01/19 12:52:39 AM 

01/19 12:52:40 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 001/521, train_loss: 0.161(0.161), train_acc: 93.750(93.750)
01/19 12:52:53 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 101/521, train_loss: 0.135(0.124), train_acc: 92.708(95.596)
01/19 12:53:05 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 201/521, train_loss: 0.152(0.124), train_acc: 92.708(95.553)
01/19 12:53:18 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 301/521, train_loss: 0.115(0.126), train_acc: 93.750(95.511)
01/19 12:53:31 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 401/521, train_loss: 0.168(0.128), train_acc: 93.750(95.402)
01/19 12:53:44 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 501/521, train_loss: 0.217(0.131), train_acc: 92.708(95.307)
01/19 12:53:46 AM [Supernet Training] lr: 0.00939 epoch: 349/600, step: 521/521, train_loss: 0.198(0.131), train_acc: 91.250(95.294)
01/19 12:53:46 AM [Supernet Training] epoch: 349, train_loss: 0.131, train_acc: 95.294
01/19 12:53:50 AM [Supernet Validation] epoch: 349, val_loss: 0.452, val_acc: 87.760, best_acc: 88.860
01/19 12:53:50 AM 

01/19 12:53:50 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 001/521, train_loss: 0.097(0.097), train_acc: 95.833(95.833)
01/19 12:54:03 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 101/521, train_loss: 0.126(0.128), train_acc: 94.792(95.194)
01/19 12:54:16 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 201/521, train_loss: 0.082(0.130), train_acc: 95.833(95.061)
01/19 12:54:29 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 301/521, train_loss: 0.155(0.132), train_acc: 93.750(95.093)
01/19 12:54:41 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 401/521, train_loss: 0.130(0.135), train_acc: 93.750(95.080)
01/19 12:54:54 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 501/521, train_loss: 0.047(0.135), train_acc: 97.917(95.093)
01/19 12:54:57 AM [Supernet Training] lr: 0.00933 epoch: 350/600, step: 521/521, train_loss: 0.040(0.135), train_acc: 100.000(95.092)
01/19 12:54:57 AM [Supernet Training] epoch: 350, train_loss: 0.135, train_acc: 95.092
01/19 12:55:00 AM [Supernet Validation] epoch: 350, val_loss: 0.440, val_acc: 88.470, best_acc: 88.860
01/19 12:55:00 AM 

01/19 12:55:01 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 95.833(95.833)
01/19 12:55:14 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 101/521, train_loss: 0.134(0.127), train_acc: 93.750(95.122)
01/19 12:55:26 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 201/521, train_loss: 0.049(0.125), train_acc: 100.000(95.258)
01/19 12:55:39 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 301/521, train_loss: 0.161(0.131), train_acc: 93.750(95.138)
01/19 12:55:52 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 401/521, train_loss: 0.103(0.129), train_acc: 95.833(95.259)
01/19 12:56:05 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 501/521, train_loss: 0.132(0.130), train_acc: 94.792(95.232)
01/19 12:56:07 AM [Supernet Training] lr: 0.00926 epoch: 351/600, step: 521/521, train_loss: 0.071(0.130), train_acc: 98.750(95.228)
01/19 12:56:07 AM [Supernet Training] epoch: 351, train_loss: 0.130, train_acc: 95.228
01/19 12:56:11 AM [Supernet Validation] epoch: 351, val_loss: 0.423, val_acc: 88.840, best_acc: 88.860
01/19 12:56:11 AM 

01/19 12:56:11 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 001/521, train_loss: 0.124(0.124), train_acc: 93.750(93.750)
01/19 12:56:24 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 101/521, train_loss: 0.121(0.128), train_acc: 94.792(95.266)
01/19 12:56:37 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 201/521, train_loss: 0.091(0.131), train_acc: 95.833(95.072)
01/19 12:56:50 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 301/521, train_loss: 0.167(0.134), train_acc: 94.792(95.048)
01/19 12:57:03 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 401/521, train_loss: 0.114(0.135), train_acc: 94.792(95.038)
01/19 12:57:15 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 501/521, train_loss: 0.100(0.133), train_acc: 97.917(95.145)
01/19 12:57:18 AM [Supernet Training] lr: 0.00920 epoch: 352/600, step: 521/521, train_loss: 0.210(0.133), train_acc: 93.750(95.150)
01/19 12:57:18 AM [Supernet Training] epoch: 352, train_loss: 0.133, train_acc: 95.150
01/19 12:57:22 AM [Supernet Validation] epoch: 352, val_loss: 0.422, val_acc: 88.820, best_acc: 88.860
01/19 12:57:22 AM 

01/19 12:57:22 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 001/521, train_loss: 0.112(0.112), train_acc: 96.875(96.875)
01/19 12:57:35 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 101/521, train_loss: 0.111(0.137), train_acc: 94.792(95.101)
01/19 12:57:47 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 201/521, train_loss: 0.120(0.134), train_acc: 96.875(95.134)
01/19 12:58:00 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 301/521, train_loss: 0.058(0.133), train_acc: 100.000(95.138)
01/19 12:58:13 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 401/521, train_loss: 0.098(0.133), train_acc: 95.833(95.140)
01/19 12:58:26 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 501/521, train_loss: 0.114(0.133), train_acc: 95.833(95.137)
01/19 12:58:28 AM [Supernet Training] lr: 0.00914 epoch: 353/600, step: 521/521, train_loss: 0.126(0.133), train_acc: 93.750(95.146)
01/19 12:58:28 AM [Supernet Training] epoch: 353, train_loss: 0.133, train_acc: 95.146
01/19 12:58:32 AM [Supernet Validation] epoch: 353, val_loss: 0.434, val_acc: 88.710, best_acc: 88.860
01/19 12:58:32 AM 

01/19 12:58:32 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 001/521, train_loss: 0.114(0.114), train_acc: 97.917(97.917)
01/19 12:58:45 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 101/521, train_loss: 0.172(0.135), train_acc: 93.750(95.266)
01/19 12:58:58 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 201/521, train_loss: 0.204(0.131), train_acc: 95.833(95.227)
01/19 12:59:11 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 301/521, train_loss: 0.064(0.128), train_acc: 97.917(95.321)
01/19 12:59:23 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 401/521, train_loss: 0.100(0.130), train_acc: 94.792(95.262)
01/19 12:59:36 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 501/521, train_loss: 0.096(0.130), train_acc: 97.917(95.276)
01/19 12:59:39 AM [Supernet Training] lr: 0.00908 epoch: 354/600, step: 521/521, train_loss: 0.076(0.130), train_acc: 97.500(95.286)
01/19 12:59:39 AM [Supernet Training] epoch: 354, train_loss: 0.130, train_acc: 95.286
01/19 12:59:42 AM [Supernet Validation] epoch: 354, val_loss: 0.437, val_acc: 88.740, best_acc: 88.860
01/19 12:59:42 AM 

01/19 12:59:43 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 001/521, train_loss: 0.119(0.119), train_acc: 93.750(93.750)
01/19 12:59:55 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 101/521, train_loss: 0.103(0.123), train_acc: 96.875(95.462)
01/19 01:00:08 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 201/521, train_loss: 0.152(0.126), train_acc: 95.833(95.434)
01/19 01:00:21 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 301/521, train_loss: 0.138(0.127), train_acc: 94.792(95.293)
01/19 01:00:34 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 401/521, train_loss: 0.223(0.129), train_acc: 94.792(95.275)
01/19 01:00:47 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 501/521, train_loss: 0.193(0.131), train_acc: 92.708(95.239)
01/19 01:00:49 AM [Supernet Training] lr: 0.00901 epoch: 355/600, step: 521/521, train_loss: 0.089(0.131), train_acc: 95.000(95.244)
01/19 01:00:49 AM [Supernet Training] epoch: 355, train_loss: 0.131, train_acc: 95.244
01/19 01:00:53 AM [Supernet Validation] epoch: 355, val_loss: 0.435, val_acc: 88.570, best_acc: 88.860
01/19 01:00:53 AM 

01/19 01:00:53 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 001/521, train_loss: 0.066(0.066), train_acc: 97.917(97.917)
01/19 01:01:06 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 101/521, train_loss: 0.117(0.134), train_acc: 96.875(95.039)
01/19 01:01:19 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 201/521, train_loss: 0.129(0.126), train_acc: 93.750(95.346)
01/19 01:01:31 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 301/521, train_loss: 0.140(0.127), train_acc: 94.792(95.338)
01/19 01:01:44 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 401/521, train_loss: 0.150(0.130), train_acc: 95.833(95.296)
01/19 01:01:57 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 501/521, train_loss: 0.116(0.129), train_acc: 96.875(95.328)
01/19 01:01:59 AM [Supernet Training] lr: 0.00895 epoch: 356/600, step: 521/521, train_loss: 0.125(0.130), train_acc: 93.750(95.328)
01/19 01:01:59 AM [Supernet Training] epoch: 356, train_loss: 0.130, train_acc: 95.328
01/19 01:02:03 AM [Supernet Validation] epoch: 356, val_loss: 0.425, val_acc: 88.760, best_acc: 88.860
01/19 01:02:03 AM 

01/19 01:02:03 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 001/521, train_loss: 0.084(0.084), train_acc: 95.833(95.833)
01/19 01:02:16 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 101/521, train_loss: 0.235(0.122), train_acc: 91.667(95.596)
01/19 01:02:29 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 201/521, train_loss: 0.157(0.123), train_acc: 94.792(95.548)
01/19 01:02:42 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 301/521, train_loss: 0.057(0.126), train_acc: 97.917(95.394)
01/19 01:02:55 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 401/521, train_loss: 0.104(0.126), train_acc: 96.875(95.428)
01/19 01:03:07 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 501/521, train_loss: 0.147(0.128), train_acc: 94.792(95.397)
01/19 01:03:10 AM [Supernet Training] lr: 0.00889 epoch: 357/600, step: 521/521, train_loss: 0.202(0.129), train_acc: 91.250(95.368)
01/19 01:03:10 AM [Supernet Training] epoch: 357, train_loss: 0.129, train_acc: 95.368
01/19 01:03:14 AM [Supernet Validation] epoch: 357, val_loss: 0.436, val_acc: 88.320, best_acc: 88.860
01/19 01:03:14 AM 

01/19 01:03:14 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 001/521, train_loss: 0.146(0.146), train_acc: 96.875(96.875)
01/19 01:03:27 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 101/521, train_loss: 0.157(0.123), train_acc: 94.792(95.441)
01/19 01:03:40 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 201/521, train_loss: 0.331(0.124), train_acc: 90.625(95.471)
01/19 01:03:52 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 301/521, train_loss: 0.121(0.126), train_acc: 95.833(95.525)
01/19 01:04:05 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 401/521, train_loss: 0.066(0.127), train_acc: 96.875(95.470)
01/19 01:04:18 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 501/521, train_loss: 0.319(0.126), train_acc: 89.583(95.480)
01/19 01:04:20 AM [Supernet Training] lr: 0.00882 epoch: 358/600, step: 521/521, train_loss: 0.089(0.125), train_acc: 98.750(95.520)
01/19 01:04:20 AM [Supernet Training] epoch: 358, train_loss: 0.125, train_acc: 95.520
01/19 01:04:24 AM [Supernet Validation] epoch: 358, val_loss: 0.436, val_acc: 88.550, best_acc: 88.860
01/19 01:04:24 AM 

01/19 01:04:24 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 001/521, train_loss: 0.169(0.169), train_acc: 95.833(95.833)
01/19 01:04:37 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 101/521, train_loss: 0.037(0.127), train_acc: 97.917(95.545)
01/19 01:04:50 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 201/521, train_loss: 0.054(0.126), train_acc: 98.958(95.465)
01/19 01:05:03 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 301/521, train_loss: 0.030(0.128), train_acc: 98.958(95.387)
01/19 01:05:15 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 401/521, train_loss: 0.071(0.127), train_acc: 97.917(95.374)
01/19 01:05:28 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 501/521, train_loss: 0.069(0.128), train_acc: 97.917(95.316)
01/19 01:05:31 AM [Supernet Training] lr: 0.00876 epoch: 359/600, step: 521/521, train_loss: 0.120(0.128), train_acc: 96.250(95.308)
01/19 01:05:31 AM [Supernet Training] epoch: 359, train_loss: 0.128, train_acc: 95.308
01/19 01:05:34 AM [Supernet Validation] epoch: 359, val_loss: 0.437, val_acc: 88.440, best_acc: 88.860
01/19 01:05:34 AM 

01/19 01:05:35 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 001/521, train_loss: 0.109(0.109), train_acc: 94.792(94.792)
01/19 01:05:48 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 101/521, train_loss: 0.140(0.123), train_acc: 92.708(95.493)
01/19 01:06:00 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 201/521, train_loss: 0.105(0.125), train_acc: 94.792(95.481)
01/19 01:06:13 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 301/521, train_loss: 0.128(0.126), train_acc: 94.792(95.491)
01/19 01:06:26 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 401/521, train_loss: 0.094(0.126), train_acc: 95.833(95.438)
01/19 01:06:39 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 501/521, train_loss: 0.159(0.125), train_acc: 94.792(95.447)
01/19 01:06:41 AM [Supernet Training] lr: 0.00870 epoch: 360/600, step: 521/521, train_loss: 0.074(0.125), train_acc: 98.750(95.442)
01/19 01:06:41 AM [Supernet Training] epoch: 360, train_loss: 0.125, train_acc: 95.442
01/19 01:06:45 AM [Supernet Validation] epoch: 360, val_loss: 0.437, val_acc: 88.500, best_acc: 88.860
01/19 01:06:45 AM 

01/19 01:06:45 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 001/521, train_loss: 0.214(0.214), train_acc: 93.750(93.750)
01/19 01:06:58 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 101/521, train_loss: 0.078(0.137), train_acc: 97.917(94.905)
01/19 01:07:11 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 201/521, train_loss: 0.124(0.135), train_acc: 93.750(94.973)
01/19 01:07:24 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 301/521, train_loss: 0.132(0.132), train_acc: 96.875(95.010)
01/19 01:07:36 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 401/521, train_loss: 0.094(0.132), train_acc: 95.833(95.080)
01/19 01:07:49 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 501/521, train_loss: 0.073(0.132), train_acc: 98.958(95.143)
01/19 01:07:52 AM [Supernet Training] lr: 0.00864 epoch: 361/600, step: 521/521, train_loss: 0.237(0.132), train_acc: 90.000(95.158)
01/19 01:07:52 AM [Supernet Training] epoch: 361, train_loss: 0.132, train_acc: 95.158
01/19 01:07:55 AM [Supernet Validation] epoch: 361, val_loss: 0.422, val_acc: 88.760, best_acc: 88.860
01/19 01:07:55 AM 

01/19 01:07:56 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 001/521, train_loss: 0.125(0.125), train_acc: 93.750(93.750)
01/19 01:08:08 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 101/521, train_loss: 0.055(0.123), train_acc: 97.917(95.307)
01/19 01:08:21 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 201/521, train_loss: 0.129(0.125), train_acc: 93.750(95.289)
01/19 01:08:34 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 301/521, train_loss: 0.162(0.125), train_acc: 93.750(95.439)
01/19 01:08:47 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 401/521, train_loss: 0.072(0.125), train_acc: 97.917(95.454)
01/19 01:09:00 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 501/521, train_loss: 0.122(0.124), train_acc: 94.792(95.447)
01/19 01:09:02 AM [Supernet Training] lr: 0.00858 epoch: 362/600, step: 521/521, train_loss: 0.159(0.125), train_acc: 93.750(95.408)
01/19 01:09:02 AM [Supernet Training] epoch: 362, train_loss: 0.125, train_acc: 95.408
01/19 01:09:06 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 01:09:06 AM [Supernet Validation] epoch: 362, val_loss: 0.425, val_acc: 88.860, best_acc: 88.860
01/19 01:09:06 AM 

01/19 01:09:06 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 001/521, train_loss: 0.037(0.037), train_acc: 98.958(98.958)
01/19 01:09:19 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 101/521, train_loss: 0.187(0.120), train_acc: 91.667(95.627)
01/19 01:09:32 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 201/521, train_loss: 0.098(0.125), train_acc: 95.833(95.362)
01/19 01:09:45 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 301/521, train_loss: 0.147(0.125), train_acc: 94.792(95.383)
01/19 01:09:58 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 401/521, train_loss: 0.120(0.125), train_acc: 93.750(95.431)
01/19 01:10:10 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 501/521, train_loss: 0.139(0.124), train_acc: 95.833(95.459)
01/19 01:10:13 AM [Supernet Training] lr: 0.00851 epoch: 363/600, step: 521/521, train_loss: 0.208(0.124), train_acc: 91.250(95.454)
01/19 01:10:13 AM [Supernet Training] epoch: 363, train_loss: 0.124, train_acc: 95.454
01/19 01:10:17 AM [Supernet Validation] epoch: 363, val_loss: 0.431, val_acc: 88.600, best_acc: 88.860
01/19 01:10:17 AM 

01/19 01:10:17 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 001/521, train_loss: 0.084(0.084), train_acc: 98.958(98.958)
01/19 01:10:30 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 101/521, train_loss: 0.129(0.119), train_acc: 96.875(95.782)
01/19 01:10:43 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 201/521, train_loss: 0.151(0.125), train_acc: 94.792(95.533)
01/19 01:10:55 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 301/521, train_loss: 0.168(0.124), train_acc: 95.833(95.532)
01/19 01:11:08 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 401/521, train_loss: 0.083(0.125), train_acc: 96.875(95.506)
01/19 01:11:21 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 501/521, train_loss: 0.145(0.124), train_acc: 95.833(95.546)
01/19 01:11:23 AM [Supernet Training] lr: 0.00845 epoch: 364/600, step: 521/521, train_loss: 0.117(0.124), train_acc: 96.250(95.552)
01/19 01:11:23 AM [Supernet Training] epoch: 364, train_loss: 0.124, train_acc: 95.552
01/19 01:11:27 AM [Supernet Validation] epoch: 364, val_loss: 0.440, val_acc: 88.610, best_acc: 88.860
01/19 01:11:27 AM 

01/19 01:11:27 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 001/521, train_loss: 0.132(0.132), train_acc: 94.792(94.792)
01/19 01:11:40 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 101/521, train_loss: 0.128(0.125), train_acc: 94.792(95.390)
01/19 01:11:53 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 201/521, train_loss: 0.282(0.123), train_acc: 90.625(95.642)
01/19 01:12:06 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 301/521, train_loss: 0.070(0.124), train_acc: 98.958(95.522)
01/19 01:12:19 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 401/521, train_loss: 0.168(0.125), train_acc: 94.792(95.472)
01/19 01:12:31 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 501/521, train_loss: 0.134(0.124), train_acc: 96.875(95.467)
01/19 01:12:34 AM [Supernet Training] lr: 0.00839 epoch: 365/600, step: 521/521, train_loss: 0.035(0.124), train_acc: 100.000(95.464)
01/19 01:12:34 AM [Supernet Training] epoch: 365, train_loss: 0.124, train_acc: 95.464
01/19 01:12:38 AM [Supernet Validation] epoch: 365, val_loss: 0.425, val_acc: 88.780, best_acc: 88.860
01/19 01:12:38 AM 

01/19 01:12:38 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 001/521, train_loss: 0.113(0.113), train_acc: 95.833(95.833)
01/19 01:12:51 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 101/521, train_loss: 0.095(0.120), train_acc: 96.875(95.617)
01/19 01:13:04 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 201/521, train_loss: 0.173(0.121), train_acc: 92.708(95.548)
01/19 01:13:16 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 301/521, train_loss: 0.084(0.124), train_acc: 95.833(95.439)
01/19 01:13:29 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 401/521, train_loss: 0.087(0.121), train_acc: 96.875(95.600)
01/19 01:13:42 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 501/521, train_loss: 0.125(0.126), train_acc: 96.875(95.472)
01/19 01:13:45 AM [Supernet Training] lr: 0.00833 epoch: 366/600, step: 521/521, train_loss: 0.292(0.126), train_acc: 90.000(95.450)
01/19 01:13:45 AM [Supernet Training] epoch: 366, train_loss: 0.126, train_acc: 95.450
01/19 01:13:48 AM [Supernet Validation] epoch: 366, val_loss: 0.430, val_acc: 88.540, best_acc: 88.860
01/19 01:13:48 AM 

01/19 01:13:49 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 001/521, train_loss: 0.087(0.087), train_acc: 97.917(97.917)
01/19 01:14:01 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 101/521, train_loss: 0.074(0.121), train_acc: 97.917(95.596)
01/19 01:14:14 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 201/521, train_loss: 0.170(0.122), train_acc: 92.708(95.600)
01/19 01:14:27 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 301/521, train_loss: 0.074(0.121), train_acc: 97.917(95.678)
01/19 01:14:40 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 401/521, train_loss: 0.106(0.122), train_acc: 96.875(95.610)
01/19 01:14:52 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 501/521, train_loss: 0.059(0.125), train_acc: 98.958(95.497)
01/19 01:14:55 AM [Supernet Training] lr: 0.00827 epoch: 367/600, step: 521/521, train_loss: 0.052(0.125), train_acc: 97.500(95.490)
01/19 01:14:55 AM [Supernet Training] epoch: 367, train_loss: 0.125, train_acc: 95.490
01/19 01:14:58 AM [Supernet Validation] epoch: 367, val_loss: 0.429, val_acc: 88.770, best_acc: 88.860
01/19 01:14:58 AM 

01/19 01:14:59 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 001/521, train_loss: 0.094(0.094), train_acc: 96.875(96.875)
01/19 01:15:12 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 101/521, train_loss: 0.157(0.121), train_acc: 93.750(95.421)
01/19 01:15:24 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 201/521, train_loss: 0.089(0.122), train_acc: 95.833(95.414)
01/19 01:15:37 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 301/521, train_loss: 0.094(0.122), train_acc: 95.833(95.446)
01/19 01:15:50 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 401/521, train_loss: 0.065(0.121), train_acc: 98.958(95.467)
01/19 01:16:03 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 501/521, train_loss: 0.113(0.121), train_acc: 96.875(95.461)
01/19 01:16:05 AM [Supernet Training] lr: 0.00820 epoch: 368/600, step: 521/521, train_loss: 0.102(0.120), train_acc: 95.000(95.488)
01/19 01:16:05 AM [Supernet Training] epoch: 368, train_loss: 0.120, train_acc: 95.488
01/19 01:16:09 AM [Supernet Validation] epoch: 368, val_loss: 0.425, val_acc: 88.810, best_acc: 88.860
01/19 01:16:09 AM 

01/19 01:16:09 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 001/521, train_loss: 0.150(0.150), train_acc: 94.792(94.792)
01/19 01:16:22 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 101/521, train_loss: 0.081(0.129), train_acc: 95.833(95.349)
01/19 01:16:35 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 201/521, train_loss: 0.148(0.126), train_acc: 94.792(95.393)
01/19 01:16:48 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 301/521, train_loss: 0.046(0.124), train_acc: 98.958(95.415)
01/19 01:17:01 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 401/521, train_loss: 0.114(0.124), train_acc: 95.833(95.457)
01/19 01:17:13 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 501/521, train_loss: 0.129(0.124), train_acc: 93.750(95.461)
01/19 01:17:16 AM [Supernet Training] lr: 0.00814 epoch: 369/600, step: 521/521, train_loss: 0.088(0.125), train_acc: 96.250(95.434)
01/19 01:17:16 AM [Supernet Training] epoch: 369, train_loss: 0.125, train_acc: 95.434
01/19 01:17:20 AM [Supernet Validation] epoch: 369, val_loss: 0.433, val_acc: 88.380, best_acc: 88.860
01/19 01:17:20 AM 

01/19 01:17:20 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 001/521, train_loss: 0.062(0.062), train_acc: 97.917(97.917)
01/19 01:17:33 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 101/521, train_loss: 0.039(0.118), train_acc: 97.917(95.875)
01/19 01:17:46 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 201/521, train_loss: 0.065(0.122), train_acc: 97.917(95.621)
01/19 01:17:58 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 301/521, train_loss: 0.103(0.126), train_acc: 94.792(95.525)
01/19 01:18:11 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 401/521, train_loss: 0.237(0.125), train_acc: 92.708(95.496)
01/19 01:18:24 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 501/521, train_loss: 0.243(0.125), train_acc: 91.667(95.521)
01/19 01:18:27 AM [Supernet Training] lr: 0.00808 epoch: 370/600, step: 521/521, train_loss: 0.181(0.124), train_acc: 90.000(95.526)
01/19 01:18:27 AM [Supernet Training] epoch: 370, train_loss: 0.124, train_acc: 95.526
01/19 01:18:30 AM [Supernet Validation] epoch: 370, val_loss: 0.441, val_acc: 88.600, best_acc: 88.860
01/19 01:18:30 AM 

01/19 01:18:31 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 001/521, train_loss: 0.117(0.117), train_acc: 95.833(95.833)
01/19 01:18:43 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 101/521, train_loss: 0.130(0.124), train_acc: 95.833(95.565)
01/19 01:18:56 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 201/521, train_loss: 0.100(0.121), train_acc: 94.792(95.636)
01/19 01:19:09 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 301/521, train_loss: 0.104(0.121), train_acc: 97.917(95.674)
01/19 01:19:22 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 401/521, train_loss: 0.058(0.123), train_acc: 98.958(95.548)
01/19 01:19:35 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 501/521, train_loss: 0.098(0.122), train_acc: 94.792(95.546)
01/19 01:19:37 AM [Supernet Training] lr: 0.00802 epoch: 371/600, step: 521/521, train_loss: 0.148(0.123), train_acc: 96.250(95.552)
01/19 01:19:37 AM [Supernet Training] epoch: 371, train_loss: 0.123, train_acc: 95.552
01/19 01:19:41 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 01:19:41 AM [Supernet Validation] epoch: 371, val_loss: 0.424, val_acc: 88.890, best_acc: 88.890
01/19 01:19:41 AM 

01/19 01:19:41 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 001/521, train_loss: 0.037(0.037), train_acc: 98.958(98.958)
01/19 01:19:54 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 101/521, train_loss: 0.090(0.115), train_acc: 95.833(95.792)
01/19 01:20:07 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 201/521, train_loss: 0.074(0.115), train_acc: 96.875(95.844)
01/19 01:20:19 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 301/521, train_loss: 0.043(0.118), train_acc: 98.958(95.750)
01/19 01:20:32 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 401/521, train_loss: 0.089(0.119), train_acc: 95.833(95.667)
01/19 01:20:45 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 501/521, train_loss: 0.102(0.119), train_acc: 96.875(95.665)
01/19 01:20:48 AM [Supernet Training] lr: 0.00796 epoch: 372/600, step: 521/521, train_loss: 0.033(0.118), train_acc: 100.000(95.668)
01/19 01:20:48 AM [Supernet Training] epoch: 372, train_loss: 0.118, train_acc: 95.668
01/19 01:20:51 AM [Supernet Validation] epoch: 372, val_loss: 0.446, val_acc: 88.540, best_acc: 88.890
01/19 01:20:51 AM 

01/19 01:20:52 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 96.875(96.875)
01/19 01:21:05 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 101/521, train_loss: 0.149(0.122), train_acc: 94.792(95.565)
01/19 01:21:17 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 201/521, train_loss: 0.182(0.125), train_acc: 94.792(95.393)
01/19 01:21:30 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 301/521, train_loss: 0.096(0.123), train_acc: 96.875(95.511)
01/19 01:21:43 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 401/521, train_loss: 0.158(0.122), train_acc: 93.750(95.605)
01/19 01:21:56 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 501/521, train_loss: 0.090(0.122), train_acc: 96.875(95.619)
01/19 01:21:58 AM [Supernet Training] lr: 0.00790 epoch: 373/600, step: 521/521, train_loss: 0.059(0.121), train_acc: 98.750(95.640)
01/19 01:21:58 AM [Supernet Training] epoch: 373, train_loss: 0.121, train_acc: 95.640
01/19 01:22:02 AM [Supernet Validation] epoch: 373, val_loss: 0.445, val_acc: 88.620, best_acc: 88.890
01/19 01:22:02 AM 

01/19 01:22:02 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 001/521, train_loss: 0.216(0.216), train_acc: 93.750(93.750)
01/19 01:22:15 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 101/521, train_loss: 0.100(0.123), train_acc: 93.750(95.534)
01/19 01:22:28 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 201/521, train_loss: 0.188(0.129), train_acc: 91.667(95.351)
01/19 01:22:41 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 301/521, train_loss: 0.135(0.127), train_acc: 95.833(95.460)
01/19 01:22:53 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 401/521, train_loss: 0.120(0.124), train_acc: 95.833(95.563)
01/19 01:23:06 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 501/521, train_loss: 0.121(0.123), train_acc: 97.917(95.646)
01/19 01:23:09 AM [Supernet Training] lr: 0.00784 epoch: 374/600, step: 521/521, train_loss: 0.153(0.123), train_acc: 92.500(95.634)
01/19 01:23:09 AM [Supernet Training] epoch: 374, train_loss: 0.123, train_acc: 95.634
01/19 01:23:12 AM [Supernet Validation] epoch: 374, val_loss: 0.444, val_acc: 88.300, best_acc: 88.890
01/19 01:23:12 AM 

01/19 01:23:13 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 001/521, train_loss: 0.130(0.130), train_acc: 93.750(93.750)
01/19 01:23:25 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 101/521, train_loss: 0.064(0.114), train_acc: 98.958(95.833)
01/19 01:23:38 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 201/521, train_loss: 0.095(0.115), train_acc: 96.875(95.818)
01/19 01:23:51 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 301/521, train_loss: 0.328(0.121), train_acc: 86.458(95.650)
01/19 01:24:04 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 401/521, train_loss: 0.074(0.120), train_acc: 96.875(95.659)
01/19 01:24:17 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 501/521, train_loss: 0.190(0.120), train_acc: 92.708(95.652)
01/19 01:24:19 AM [Supernet Training] lr: 0.00778 epoch: 375/600, step: 521/521, train_loss: 0.079(0.119), train_acc: 98.750(95.686)
01/19 01:24:19 AM [Supernet Training] epoch: 375, train_loss: 0.119, train_acc: 95.686
01/19 01:24:23 AM [Supernet Validation] epoch: 375, val_loss: 0.443, val_acc: 88.660, best_acc: 88.890
01/19 01:24:23 AM 

01/19 01:24:23 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 001/521, train_loss: 0.068(0.068), train_acc: 96.875(96.875)
01/19 01:24:36 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 101/521, train_loss: 0.103(0.116), train_acc: 95.833(95.741)
01/19 01:24:49 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 201/521, train_loss: 0.113(0.117), train_acc: 96.875(95.667)
01/19 01:25:02 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 301/521, train_loss: 0.154(0.119), train_acc: 93.750(95.577)
01/19 01:25:14 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 401/521, train_loss: 0.091(0.121), train_acc: 98.958(95.579)
01/19 01:25:27 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 501/521, train_loss: 0.158(0.119), train_acc: 95.833(95.607)
01/19 01:25:30 AM [Supernet Training] lr: 0.00772 epoch: 376/600, step: 521/521, train_loss: 0.222(0.119), train_acc: 96.250(95.636)
01/19 01:25:30 AM [Supernet Training] epoch: 376, train_loss: 0.119, train_acc: 95.636
01/19 01:25:33 AM [Supernet Validation] epoch: 376, val_loss: 0.432, val_acc: 88.690, best_acc: 88.890
01/19 01:25:33 AM 

01/19 01:25:34 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 001/521, train_loss: 0.078(0.078), train_acc: 97.917(97.917)
01/19 01:25:46 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 101/521, train_loss: 0.115(0.115), train_acc: 96.875(95.916)
01/19 01:25:59 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 201/521, train_loss: 0.137(0.116), train_acc: 94.792(95.864)
01/19 01:26:12 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 301/521, train_loss: 0.161(0.115), train_acc: 94.792(95.864)
01/19 01:26:25 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 401/521, train_loss: 0.101(0.115), train_acc: 94.792(95.909)
01/19 01:26:38 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 501/521, train_loss: 0.158(0.116), train_acc: 93.750(95.892)
01/19 01:26:40 AM [Supernet Training] lr: 0.00766 epoch: 377/600, step: 521/521, train_loss: 0.085(0.116), train_acc: 96.250(95.876)
01/19 01:26:40 AM [Supernet Training] epoch: 377, train_loss: 0.116, train_acc: 95.876
01/19 01:26:44 AM [Supernet Validation] epoch: 377, val_loss: 0.432, val_acc: 88.480, best_acc: 88.890
01/19 01:26:44 AM 

01/19 01:26:44 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 001/521, train_loss: 0.145(0.145), train_acc: 95.833(95.833)
01/19 01:26:57 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 101/521, train_loss: 0.070(0.113), train_acc: 97.917(96.019)
01/19 01:27:10 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 201/521, train_loss: 0.097(0.114), train_acc: 96.875(95.937)
01/19 01:27:22 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 301/521, train_loss: 0.059(0.115), train_acc: 97.917(95.878)
01/19 01:27:35 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 401/521, train_loss: 0.105(0.117), train_acc: 94.792(95.815)
01/19 01:27:48 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 501/521, train_loss: 0.108(0.116), train_acc: 96.875(95.796)
01/19 01:27:50 AM [Supernet Training] lr: 0.00760 epoch: 378/600, step: 521/521, train_loss: 0.098(0.116), train_acc: 96.250(95.818)
01/19 01:27:51 AM [Supernet Training] epoch: 378, train_loss: 0.116, train_acc: 95.818
01/19 01:27:54 AM [Supernet Validation] epoch: 378, val_loss: 0.434, val_acc: 88.500, best_acc: 88.890
01/19 01:27:54 AM 

01/19 01:27:55 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 001/521, train_loss: 0.093(0.093), train_acc: 96.875(96.875)
01/19 01:28:07 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 101/521, train_loss: 0.120(0.112), train_acc: 95.833(95.823)
01/19 01:28:20 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 201/521, train_loss: 0.134(0.120), train_acc: 92.708(95.693)
01/19 01:28:33 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 301/521, train_loss: 0.078(0.117), train_acc: 96.875(95.709)
01/19 01:28:46 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 401/521, train_loss: 0.048(0.117), train_acc: 98.958(95.737)
01/19 01:28:58 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 501/521, train_loss: 0.069(0.118), train_acc: 97.917(95.686)
01/19 01:29:01 AM [Supernet Training] lr: 0.00754 epoch: 379/600, step: 521/521, train_loss: 0.068(0.119), train_acc: 100.000(95.678)
01/19 01:29:01 AM [Supernet Training] epoch: 379, train_loss: 0.119, train_acc: 95.678
01/19 01:29:05 AM [Supernet Validation] epoch: 379, val_loss: 0.438, val_acc: 88.810, best_acc: 88.890
01/19 01:29:05 AM 

01/19 01:29:05 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 001/521, train_loss: 0.264(0.264), train_acc: 92.708(92.708)
01/19 01:29:18 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 101/521, train_loss: 0.113(0.124), train_acc: 95.833(95.369)
01/19 01:29:30 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 201/521, train_loss: 0.111(0.119), train_acc: 93.750(95.528)
01/19 01:29:43 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 301/521, train_loss: 0.051(0.116), train_acc: 97.917(95.688)
01/19 01:29:56 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 401/521, train_loss: 0.073(0.118), train_acc: 96.875(95.615)
01/19 01:30:09 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 501/521, train_loss: 0.106(0.117), train_acc: 95.833(95.669)
01/19 01:30:11 AM [Supernet Training] lr: 0.00748 epoch: 380/600, step: 521/521, train_loss: 0.098(0.116), train_acc: 96.250(95.662)
01/19 01:30:11 AM [Supernet Training] epoch: 380, train_loss: 0.116, train_acc: 95.662
01/19 01:30:15 AM [Supernet Validation] epoch: 380, val_loss: 0.433, val_acc: 88.670, best_acc: 88.890
01/19 01:30:15 AM 

01/19 01:30:15 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 001/521, train_loss: 0.160(0.160), train_acc: 94.792(94.792)
01/19 01:30:28 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 101/521, train_loss: 0.053(0.112), train_acc: 97.917(95.844)
01/19 01:30:41 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 201/521, train_loss: 0.086(0.115), train_acc: 97.917(95.709)
01/19 01:30:54 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 301/521, train_loss: 0.050(0.118), train_acc: 97.917(95.615)
01/19 01:31:06 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 401/521, train_loss: 0.216(0.117), train_acc: 91.667(95.649)
01/19 01:31:19 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 501/521, train_loss: 0.160(0.117), train_acc: 94.792(95.677)
01/19 01:31:22 AM [Supernet Training] lr: 0.00742 epoch: 381/600, step: 521/521, train_loss: 0.080(0.117), train_acc: 96.250(95.670)
01/19 01:31:22 AM [Supernet Training] epoch: 381, train_loss: 0.117, train_acc: 95.670
01/19 01:31:25 AM [Supernet Validation] epoch: 381, val_loss: 0.451, val_acc: 88.540, best_acc: 88.890
01/19 01:31:25 AM 

01/19 01:31:26 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 001/521, train_loss: 0.122(0.122), train_acc: 95.833(95.833)
01/19 01:31:39 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 101/521, train_loss: 0.154(0.114), train_acc: 93.750(95.792)
01/19 01:31:51 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 201/521, train_loss: 0.107(0.112), train_acc: 96.875(95.890)
01/19 01:32:04 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 301/521, train_loss: 0.140(0.113), train_acc: 93.750(95.826)
01/19 01:32:17 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 401/521, train_loss: 0.067(0.114), train_acc: 98.958(95.815)
01/19 01:32:30 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 501/521, train_loss: 0.184(0.115), train_acc: 94.792(95.794)
01/19 01:32:32 AM [Supernet Training] lr: 0.00736 epoch: 382/600, step: 521/521, train_loss: 0.136(0.115), train_acc: 93.750(95.786)
01/19 01:32:32 AM [Supernet Training] epoch: 382, train_loss: 0.115, train_acc: 95.786
01/19 01:32:36 AM [Supernet Validation] epoch: 382, val_loss: 0.439, val_acc: 88.890, best_acc: 88.890
01/19 01:32:36 AM 

01/19 01:32:36 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 001/521, train_loss: 0.084(0.084), train_acc: 95.833(95.833)
01/19 01:32:49 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 101/521, train_loss: 0.042(0.108), train_acc: 98.958(96.060)
01/19 01:33:02 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 201/521, train_loss: 0.152(0.113), train_acc: 92.708(95.927)
01/19 01:33:15 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 301/521, train_loss: 0.095(0.113), train_acc: 95.833(95.920)
01/19 01:33:27 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 401/521, train_loss: 0.104(0.114), train_acc: 95.833(95.877)
01/19 01:33:40 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 501/521, train_loss: 0.138(0.114), train_acc: 95.833(95.879)
01/19 01:33:43 AM [Supernet Training] lr: 0.00730 epoch: 383/600, step: 521/521, train_loss: 0.051(0.114), train_acc: 97.500(95.876)
01/19 01:33:43 AM [Supernet Training] epoch: 383, train_loss: 0.114, train_acc: 95.876
01/19 01:33:46 AM [Supernet Validation] epoch: 383, val_loss: 0.457, val_acc: 88.080, best_acc: 88.890
01/19 01:33:46 AM 

01/19 01:33:47 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 001/521, train_loss: 0.090(0.090), train_acc: 97.917(97.917)
01/19 01:34:00 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 101/521, train_loss: 0.119(0.119), train_acc: 94.792(95.689)
01/19 01:34:12 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 201/521, train_loss: 0.084(0.116), train_acc: 97.917(95.725)
01/19 01:34:25 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 301/521, train_loss: 0.101(0.118), train_acc: 96.875(95.646)
01/19 01:34:38 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 401/521, train_loss: 0.106(0.117), train_acc: 94.792(95.722)
01/19 01:34:51 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 501/521, train_loss: 0.081(0.116), train_acc: 96.875(95.690)
01/19 01:34:53 AM [Supernet Training] lr: 0.00724 epoch: 384/600, step: 521/521, train_loss: 0.121(0.116), train_acc: 93.750(95.690)
01/19 01:34:54 AM [Supernet Training] epoch: 384, train_loss: 0.116, train_acc: 95.690
01/19 01:34:57 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 01:34:57 AM [Supernet Validation] epoch: 384, val_loss: 0.430, val_acc: 89.040, best_acc: 89.040
01/19 01:34:57 AM 

01/19 01:34:58 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 001/521, train_loss: 0.070(0.070), train_acc: 95.833(95.833)
01/19 01:35:10 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 101/521, train_loss: 0.104(0.114), train_acc: 94.792(95.802)
01/19 01:35:23 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 201/521, train_loss: 0.206(0.114), train_acc: 93.750(95.880)
01/19 01:35:36 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 301/521, train_loss: 0.095(0.115), train_acc: 94.792(95.764)
01/19 01:35:49 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 401/521, train_loss: 0.120(0.115), train_acc: 94.792(95.771)
01/19 01:36:01 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 501/521, train_loss: 0.099(0.114), train_acc: 97.917(95.844)
01/19 01:36:04 AM [Supernet Training] lr: 0.00718 epoch: 385/600, step: 521/521, train_loss: 0.155(0.114), train_acc: 91.250(95.844)
01/19 01:36:04 AM [Supernet Training] epoch: 385, train_loss: 0.114, train_acc: 95.844
01/19 01:36:08 AM [Supernet Validation] epoch: 385, val_loss: 0.439, val_acc: 88.480, best_acc: 89.040
01/19 01:36:08 AM 

01/19 01:36:08 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 001/521, train_loss: 0.265(0.265), train_acc: 93.750(93.750)
01/19 01:36:21 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 101/521, train_loss: 0.066(0.116), train_acc: 98.958(95.895)
01/19 01:36:34 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 201/521, train_loss: 0.095(0.115), train_acc: 95.833(95.890)
01/19 01:36:46 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 301/521, train_loss: 0.076(0.115), train_acc: 98.958(95.871)
01/19 01:36:59 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 401/521, train_loss: 0.131(0.113), train_acc: 94.792(95.961)
01/19 01:37:12 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 501/521, train_loss: 0.111(0.114), train_acc: 94.792(95.925)
01/19 01:37:14 AM [Supernet Training] lr: 0.00712 epoch: 386/600, step: 521/521, train_loss: 0.143(0.114), train_acc: 96.250(95.928)
01/19 01:37:15 AM [Supernet Training] epoch: 386, train_loss: 0.114, train_acc: 95.928
01/19 01:37:18 AM [Supernet Validation] epoch: 386, val_loss: 0.432, val_acc: 88.810, best_acc: 89.040
01/19 01:37:18 AM 

01/19 01:37:18 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 001/521, train_loss: 0.138(0.138), train_acc: 94.792(94.792)
01/19 01:37:31 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 101/521, train_loss: 0.119(0.111), train_acc: 93.750(95.833)
01/19 01:37:44 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 201/521, train_loss: 0.122(0.110), train_acc: 94.792(95.953)
01/19 01:37:57 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 301/521, train_loss: 0.101(0.108), train_acc: 96.875(96.062)
01/19 01:38:10 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 401/521, train_loss: 0.053(0.111), train_acc: 97.917(95.966)
01/19 01:38:23 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 501/521, train_loss: 0.123(0.112), train_acc: 95.833(95.935)
01/19 01:38:25 AM [Supernet Training] lr: 0.00706 epoch: 387/600, step: 521/521, train_loss: 0.156(0.112), train_acc: 96.250(95.948)
01/19 01:38:25 AM [Supernet Training] epoch: 387, train_loss: 0.112, train_acc: 95.948
01/19 01:38:29 AM [Supernet Validation] epoch: 387, val_loss: 0.450, val_acc: 88.610, best_acc: 89.040
01/19 01:38:29 AM 

01/19 01:38:29 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 001/521, train_loss: 0.063(0.063), train_acc: 97.917(97.917)
01/19 01:38:42 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 101/521, train_loss: 0.082(0.110), train_acc: 97.917(95.936)
01/19 01:38:55 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 201/521, train_loss: 0.099(0.108), train_acc: 97.917(96.072)
01/19 01:39:07 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 301/521, train_loss: 0.073(0.109), train_acc: 97.917(96.044)
01/19 01:39:20 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 401/521, train_loss: 0.155(0.109), train_acc: 93.750(96.075)
01/19 01:39:33 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 501/521, train_loss: 0.239(0.112), train_acc: 92.708(95.975)
01/19 01:39:36 AM [Supernet Training] lr: 0.00700 epoch: 388/600, step: 521/521, train_loss: 0.108(0.112), train_acc: 95.000(95.970)
01/19 01:39:36 AM [Supernet Training] epoch: 388, train_loss: 0.112, train_acc: 95.970
01/19 01:39:39 AM [Supernet Validation] epoch: 388, val_loss: 0.452, val_acc: 88.560, best_acc: 89.040
01/19 01:39:39 AM 

01/19 01:39:40 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 97.917(97.917)
01/19 01:39:52 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 101/521, train_loss: 0.077(0.105), train_acc: 97.917(96.163)
01/19 01:40:05 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 201/521, train_loss: 0.070(0.107), train_acc: 97.917(96.035)
01/19 01:40:18 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 301/521, train_loss: 0.086(0.109), train_acc: 96.875(96.010)
01/19 01:40:31 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 401/521, train_loss: 0.200(0.108), train_acc: 91.667(95.963)
01/19 01:40:44 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 501/521, train_loss: 0.057(0.110), train_acc: 98.958(95.935)
01/19 01:40:46 AM [Supernet Training] lr: 0.00694 epoch: 389/600, step: 521/521, train_loss: 0.253(0.110), train_acc: 92.500(95.926)
01/19 01:40:46 AM [Supernet Training] epoch: 389, train_loss: 0.110, train_acc: 95.926
01/19 01:40:50 AM [Supernet Validation] epoch: 389, val_loss: 0.459, val_acc: 88.660, best_acc: 89.040
01/19 01:40:50 AM 

01/19 01:40:50 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 001/521, train_loss: 0.192(0.192), train_acc: 90.625(90.625)
01/19 01:41:03 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 101/521, train_loss: 0.049(0.119), train_acc: 98.958(95.741)
01/19 01:41:16 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 201/521, train_loss: 0.075(0.113), train_acc: 97.917(95.942)
01/19 01:41:28 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 301/521, train_loss: 0.052(0.115), train_acc: 98.958(95.878)
01/19 01:41:41 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 401/521, train_loss: 0.075(0.114), train_acc: 97.917(95.927)
01/19 01:41:54 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 501/521, train_loss: 0.143(0.113), train_acc: 94.792(95.991)
01/19 01:41:57 AM [Supernet Training] lr: 0.00688 epoch: 390/600, step: 521/521, train_loss: 0.176(0.113), train_acc: 93.750(96.012)
01/19 01:41:57 AM [Supernet Training] epoch: 390, train_loss: 0.113, train_acc: 96.012
01/19 01:42:00 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 01:42:00 AM [Supernet Validation] epoch: 390, val_loss: 0.435, val_acc: 89.270, best_acc: 89.270
01/19 01:42:00 AM 

01/19 01:42:01 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 001/521, train_loss: 0.163(0.163), train_acc: 95.833(95.833)
01/19 01:42:14 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 101/521, train_loss: 0.050(0.112), train_acc: 98.958(95.916)
01/19 01:42:26 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 201/521, train_loss: 0.103(0.113), train_acc: 95.833(95.854)
01/19 01:42:39 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 301/521, train_loss: 0.136(0.111), train_acc: 95.833(95.937)
01/19 01:42:52 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 401/521, train_loss: 0.117(0.113), train_acc: 95.833(95.867)
01/19 01:43:05 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 501/521, train_loss: 0.077(0.113), train_acc: 95.833(95.865)
01/19 01:43:07 AM [Supernet Training] lr: 0.00683 epoch: 391/600, step: 521/521, train_loss: 0.133(0.113), train_acc: 93.750(95.878)
01/19 01:43:08 AM [Supernet Training] epoch: 391, train_loss: 0.113, train_acc: 95.878
01/19 01:43:11 AM [Supernet Validation] epoch: 391, val_loss: 0.423, val_acc: 89.070, best_acc: 89.270
01/19 01:43:11 AM 

01/19 01:43:11 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 001/521, train_loss: 0.121(0.121), train_acc: 94.792(94.792)
01/19 01:43:24 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 101/521, train_loss: 0.037(0.113), train_acc: 98.958(95.916)
01/19 01:43:37 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 201/521, train_loss: 0.079(0.110), train_acc: 97.917(96.046)
01/19 01:43:50 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 301/521, train_loss: 0.046(0.110), train_acc: 98.958(96.041)
01/19 01:44:03 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 401/521, train_loss: 0.079(0.111), train_acc: 96.875(95.963)
01/19 01:44:16 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 501/521, train_loss: 0.190(0.111), train_acc: 93.750(95.977)
01/19 01:44:18 AM [Supernet Training] lr: 0.00677 epoch: 392/600, step: 521/521, train_loss: 0.031(0.111), train_acc: 98.750(95.960)
01/19 01:44:18 AM [Supernet Training] epoch: 392, train_loss: 0.111, train_acc: 95.960
01/19 01:44:22 AM [Supernet Validation] epoch: 392, val_loss: 0.442, val_acc: 88.860, best_acc: 89.270
01/19 01:44:22 AM 

01/19 01:44:22 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 001/521, train_loss: 0.054(0.054), train_acc: 97.917(97.917)
01/19 01:44:35 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 101/521, train_loss: 0.069(0.104), train_acc: 96.875(96.163)
01/19 01:44:48 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 201/521, train_loss: 0.107(0.108), train_acc: 93.750(95.978)
01/19 01:45:01 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 301/521, train_loss: 0.101(0.107), train_acc: 95.833(96.069)
01/19 01:45:13 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 401/521, train_loss: 0.123(0.109), train_acc: 96.875(96.015)
01/19 01:45:26 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 501/521, train_loss: 0.079(0.110), train_acc: 97.917(95.960)
01/19 01:45:29 AM [Supernet Training] lr: 0.00671 epoch: 393/600, step: 521/521, train_loss: 0.065(0.110), train_acc: 98.750(95.958)
01/19 01:45:29 AM [Supernet Training] epoch: 393, train_loss: 0.110, train_acc: 95.958
01/19 01:45:32 AM [Supernet Validation] epoch: 393, val_loss: 0.451, val_acc: 88.650, best_acc: 89.270
01/19 01:45:32 AM 

01/19 01:45:33 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 001/521, train_loss: 0.086(0.086), train_acc: 96.875(96.875)
01/19 01:45:45 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 101/521, train_loss: 0.051(0.115), train_acc: 98.958(95.833)
01/19 01:45:58 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 201/521, train_loss: 0.053(0.108), train_acc: 96.875(95.973)
01/19 01:46:11 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 301/521, train_loss: 0.166(0.108), train_acc: 93.750(95.986)
01/19 01:46:24 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 401/521, train_loss: 0.134(0.109), train_acc: 92.708(95.961)
01/19 01:46:37 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 501/521, train_loss: 0.124(0.110), train_acc: 93.750(95.991)
01/19 01:46:39 AM [Supernet Training] lr: 0.00665 epoch: 394/600, step: 521/521, train_loss: 0.061(0.110), train_acc: 96.250(96.010)
01/19 01:46:39 AM [Supernet Training] epoch: 394, train_loss: 0.110, train_acc: 96.010
01/19 01:46:43 AM [Supernet Validation] epoch: 394, val_loss: 0.436, val_acc: 88.960, best_acc: 89.270
01/19 01:46:43 AM 

01/19 01:46:43 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 98.958(98.958)
01/19 01:46:56 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 101/521, train_loss: 0.121(0.110), train_acc: 94.792(95.957)
01/19 01:47:09 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 201/521, train_loss: 0.044(0.111), train_acc: 98.958(95.994)
01/19 01:47:21 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 301/521, train_loss: 0.086(0.107), train_acc: 97.917(96.069)
01/19 01:47:34 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 401/521, train_loss: 0.044(0.107), train_acc: 98.958(96.129)
01/19 01:47:47 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 501/521, train_loss: 0.130(0.107), train_acc: 93.750(96.116)
01/19 01:47:50 AM [Supernet Training] lr: 0.00659 epoch: 395/600, step: 521/521, train_loss: 0.064(0.107), train_acc: 97.500(96.112)
01/19 01:47:50 AM [Supernet Training] epoch: 395, train_loss: 0.107, train_acc: 96.112
01/19 01:47:53 AM [Supernet Validation] epoch: 395, val_loss: 0.444, val_acc: 88.590, best_acc: 89.270
01/19 01:47:53 AM 

01/19 01:47:54 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 001/521, train_loss: 0.111(0.111), train_acc: 94.792(94.792)
01/19 01:48:06 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 101/521, train_loss: 0.106(0.109), train_acc: 95.833(95.947)
01/19 01:48:19 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 201/521, train_loss: 0.116(0.109), train_acc: 95.833(96.004)
01/19 01:48:32 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 301/521, train_loss: 0.069(0.110), train_acc: 95.833(95.982)
01/19 01:48:45 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 401/521, train_loss: 0.144(0.109), train_acc: 93.750(95.992)
01/19 01:48:58 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 501/521, train_loss: 0.068(0.108), train_acc: 98.958(96.014)
01/19 01:49:00 AM [Supernet Training] lr: 0.00654 epoch: 396/600, step: 521/521, train_loss: 0.074(0.108), train_acc: 96.250(96.028)
01/19 01:49:00 AM [Supernet Training] epoch: 396, train_loss: 0.108, train_acc: 96.028
01/19 01:49:04 AM [Supernet Validation] epoch: 396, val_loss: 0.433, val_acc: 88.780, best_acc: 89.270
01/19 01:49:04 AM 

01/19 01:49:04 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 001/521, train_loss: 0.115(0.115), train_acc: 94.792(94.792)
01/19 01:49:17 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 101/521, train_loss: 0.097(0.107), train_acc: 93.750(96.194)
01/19 01:49:30 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 201/521, train_loss: 0.027(0.104), train_acc: 100.000(96.357)
01/19 01:49:43 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 301/521, train_loss: 0.117(0.109), train_acc: 95.833(96.145)
01/19 01:49:55 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 401/521, train_loss: 0.131(0.107), train_acc: 97.917(96.200)
01/19 01:50:08 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 501/521, train_loss: 0.139(0.109), train_acc: 96.875(96.116)
01/19 01:50:11 AM [Supernet Training] lr: 0.00648 epoch: 397/600, step: 521/521, train_loss: 0.073(0.108), train_acc: 96.250(96.142)
01/19 01:50:11 AM [Supernet Training] epoch: 397, train_loss: 0.108, train_acc: 96.142
01/19 01:50:14 AM [Supernet Validation] epoch: 397, val_loss: 0.445, val_acc: 88.880, best_acc: 89.270
01/19 01:50:14 AM 

01/19 01:50:15 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 001/521, train_loss: 0.061(0.061), train_acc: 96.875(96.875)
01/19 01:50:28 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 101/521, train_loss: 0.073(0.103), train_acc: 96.875(96.205)
01/19 01:50:40 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 201/521, train_loss: 0.149(0.100), train_acc: 95.833(96.409)
01/19 01:50:53 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 301/521, train_loss: 0.155(0.101), train_acc: 93.750(96.366)
01/19 01:51:06 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 401/521, train_loss: 0.145(0.103), train_acc: 95.833(96.259)
01/19 01:51:19 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 501/521, train_loss: 0.111(0.106), train_acc: 96.875(96.174)
01/19 01:51:21 AM [Supernet Training] lr: 0.00642 epoch: 398/600, step: 521/521, train_loss: 0.108(0.106), train_acc: 97.500(96.194)
01/19 01:51:21 AM [Supernet Training] epoch: 398, train_loss: 0.106, train_acc: 96.194
01/19 01:51:25 AM [Supernet Validation] epoch: 398, val_loss: 0.432, val_acc: 88.850, best_acc: 89.270
01/19 01:51:25 AM 

01/19 01:51:25 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 001/521, train_loss: 0.145(0.145), train_acc: 95.833(95.833)
01/19 01:51:38 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 101/521, train_loss: 0.050(0.104), train_acc: 98.958(96.194)
01/19 01:51:51 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 201/521, train_loss: 0.112(0.104), train_acc: 94.792(96.222)
01/19 01:52:03 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 301/521, train_loss: 0.064(0.104), train_acc: 96.875(96.183)
01/19 01:52:16 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 401/521, train_loss: 0.068(0.104), train_acc: 95.833(96.233)
01/19 01:52:29 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 501/521, train_loss: 0.115(0.105), train_acc: 96.875(96.168)
01/19 01:52:32 AM [Supernet Training] lr: 0.00636 epoch: 399/600, step: 521/521, train_loss: 0.143(0.105), train_acc: 95.000(96.170)
01/19 01:52:32 AM [Supernet Training] epoch: 399, train_loss: 0.105, train_acc: 96.170
01/19 01:52:35 AM [Supernet Validation] epoch: 399, val_loss: 0.444, val_acc: 89.160, best_acc: 89.270
01/19 01:52:35 AM 

01/19 01:52:36 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 001/521, train_loss: 0.195(0.195), train_acc: 93.750(93.750)
01/19 01:52:48 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 101/521, train_loss: 0.059(0.108), train_acc: 98.958(96.009)
01/19 01:53:01 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 201/521, train_loss: 0.092(0.109), train_acc: 96.875(96.046)
01/19 01:53:14 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 301/521, train_loss: 0.123(0.107), train_acc: 96.875(96.127)
01/19 01:53:27 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 401/521, train_loss: 0.052(0.108), train_acc: 96.875(96.098)
01/19 01:53:40 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 501/521, train_loss: 0.131(0.107), train_acc: 94.792(96.133)
01/19 01:53:42 AM [Supernet Training] lr: 0.00631 epoch: 400/600, step: 521/521, train_loss: 0.072(0.107), train_acc: 97.500(96.142)
01/19 01:53:42 AM [Supernet Training] epoch: 400, train_loss: 0.107, train_acc: 96.142
01/19 01:53:46 AM [Supernet Validation] epoch: 400, val_loss: 0.453, val_acc: 88.660, best_acc: 89.270
01/19 01:53:46 AM 

01/19 01:53:46 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 001/521, train_loss: 0.090(0.090), train_acc: 95.833(95.833)
01/19 01:53:59 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 101/521, train_loss: 0.099(0.102), train_acc: 96.875(96.236)
01/19 01:54:12 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 201/521, train_loss: 0.104(0.106), train_acc: 97.917(96.077)
01/19 01:54:25 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 301/521, train_loss: 0.070(0.107), train_acc: 95.833(96.065)
01/19 01:54:37 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 401/521, train_loss: 0.086(0.106), train_acc: 97.917(96.142)
01/19 01:54:50 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 501/521, train_loss: 0.114(0.105), train_acc: 96.875(96.203)
01/19 01:54:53 AM [Supernet Training] lr: 0.00625 epoch: 401/600, step: 521/521, train_loss: 0.063(0.104), train_acc: 98.750(96.232)
01/19 01:54:53 AM [Supernet Training] epoch: 401, train_loss: 0.104, train_acc: 96.232
01/19 01:54:56 AM [Supernet Validation] epoch: 401, val_loss: 0.444, val_acc: 88.840, best_acc: 89.270
01/19 01:54:56 AM 

01/19 01:54:57 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 001/521, train_loss: 0.074(0.074), train_acc: 98.958(98.958)
01/19 01:55:09 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 101/521, train_loss: 0.120(0.103), train_acc: 93.750(96.328)
01/19 01:55:22 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 201/521, train_loss: 0.070(0.101), train_acc: 96.875(96.258)
01/19 01:55:35 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 301/521, train_loss: 0.134(0.101), train_acc: 95.833(96.214)
01/19 01:55:48 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 401/521, train_loss: 0.036(0.101), train_acc: 100.000(96.252)
01/19 01:56:01 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 501/521, train_loss: 0.118(0.101), train_acc: 97.917(96.266)
01/19 01:56:03 AM [Supernet Training] lr: 0.00619 epoch: 402/600, step: 521/521, train_loss: 0.208(0.101), train_acc: 93.750(96.258)
01/19 01:56:03 AM [Supernet Training] epoch: 402, train_loss: 0.101, train_acc: 96.258
01/19 01:56:07 AM [Supernet Validation] epoch: 402, val_loss: 0.468, val_acc: 88.610, best_acc: 89.270
01/19 01:56:07 AM 

01/19 01:56:07 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 001/521, train_loss: 0.135(0.135), train_acc: 92.708(92.708)
01/19 01:56:20 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 101/521, train_loss: 0.095(0.108), train_acc: 97.917(96.246)
01/19 01:56:33 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 201/521, train_loss: 0.037(0.105), train_acc: 98.958(96.274)
01/19 01:56:45 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 301/521, train_loss: 0.174(0.102), train_acc: 93.750(96.283)
01/19 01:56:58 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 401/521, train_loss: 0.057(0.103), train_acc: 97.917(96.280)
01/19 01:57:11 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 501/521, train_loss: 0.072(0.104), train_acc: 96.875(96.243)
01/19 01:57:14 AM [Supernet Training] lr: 0.00614 epoch: 403/600, step: 521/521, train_loss: 0.067(0.104), train_acc: 97.500(96.198)
01/19 01:57:14 AM [Supernet Training] epoch: 403, train_loss: 0.104, train_acc: 96.198
01/19 01:57:17 AM [Supernet Validation] epoch: 403, val_loss: 0.443, val_acc: 88.930, best_acc: 89.270
01/19 01:57:17 AM 

01/19 01:57:18 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 001/521, train_loss: 0.065(0.065), train_acc: 97.917(97.917)
01/19 01:57:30 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 101/521, train_loss: 0.130(0.095), train_acc: 94.792(96.524)
01/19 01:57:43 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 201/521, train_loss: 0.126(0.103), train_acc: 94.792(96.212)
01/19 01:57:56 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 301/521, train_loss: 0.132(0.105), train_acc: 95.833(96.183)
01/19 01:58:08 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 401/521, train_loss: 0.109(0.105), train_acc: 95.833(96.145)
01/19 01:58:21 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 501/521, train_loss: 0.201(0.105), train_acc: 93.750(96.122)
01/19 01:58:24 AM [Supernet Training] lr: 0.00608 epoch: 404/600, step: 521/521, train_loss: 0.078(0.105), train_acc: 96.250(96.110)
01/19 01:58:24 AM [Supernet Training] epoch: 404, train_loss: 0.105, train_acc: 96.110
01/19 01:58:27 AM [Supernet Validation] epoch: 404, val_loss: 0.456, val_acc: 88.540, best_acc: 89.270
01/19 01:58:27 AM 

01/19 01:58:28 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 001/521, train_loss: 0.093(0.093), train_acc: 97.917(97.917)
01/19 01:58:41 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 101/521, train_loss: 0.086(0.103), train_acc: 94.792(96.308)
01/19 01:58:53 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 201/521, train_loss: 0.112(0.101), train_acc: 94.792(96.315)
01/19 01:59:06 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 301/521, train_loss: 0.034(0.102), train_acc: 98.958(96.311)
01/19 01:59:19 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 401/521, train_loss: 0.074(0.103), train_acc: 95.833(96.285)
01/19 01:59:32 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 501/521, train_loss: 0.129(0.102), train_acc: 94.792(96.278)
01/19 01:59:34 AM [Supernet Training] lr: 0.00602 epoch: 405/600, step: 521/521, train_loss: 0.125(0.103), train_acc: 93.750(96.254)
01/19 01:59:34 AM [Supernet Training] epoch: 405, train_loss: 0.103, train_acc: 96.254
01/19 01:59:38 AM [Supernet Validation] epoch: 405, val_loss: 0.449, val_acc: 88.910, best_acc: 89.270
01/19 01:59:38 AM 

01/19 01:59:38 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 001/521, train_loss: 0.103(0.103), train_acc: 97.917(97.917)
01/19 01:59:51 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 101/521, train_loss: 0.271(0.096), train_acc: 92.708(96.504)
01/19 02:00:04 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 201/521, train_loss: 0.105(0.103), train_acc: 93.750(96.258)
01/19 02:00:17 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 301/521, train_loss: 0.025(0.102), train_acc: 98.958(96.262)
01/19 02:00:29 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 401/521, train_loss: 0.130(0.103), train_acc: 94.792(96.246)
01/19 02:00:42 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 501/521, train_loss: 0.087(0.102), train_acc: 95.833(96.278)
01/19 02:00:45 AM [Supernet Training] lr: 0.00597 epoch: 406/600, step: 521/521, train_loss: 0.038(0.102), train_acc: 100.000(96.300)
01/19 02:00:45 AM [Supernet Training] epoch: 406, train_loss: 0.102, train_acc: 96.300
01/19 02:00:48 AM [Supernet Validation] epoch: 406, val_loss: 0.444, val_acc: 88.720, best_acc: 89.270
01/19 02:00:48 AM 

01/19 02:00:49 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 001/521, train_loss: 0.109(0.109), train_acc: 93.750(93.750)
01/19 02:01:02 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 101/521, train_loss: 0.069(0.097), train_acc: 96.875(96.504)
01/19 02:01:14 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 201/521, train_loss: 0.122(0.102), train_acc: 95.833(96.310)
01/19 02:01:27 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 301/521, train_loss: 0.054(0.104), train_acc: 97.917(96.276)
01/19 02:01:40 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 401/521, train_loss: 0.091(0.104), train_acc: 96.875(96.244)
01/19 02:01:53 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 501/521, train_loss: 0.054(0.105), train_acc: 98.958(96.178)
01/19 02:01:55 AM [Supernet Training] lr: 0.00591 epoch: 407/600, step: 521/521, train_loss: 0.060(0.105), train_acc: 98.750(96.172)
01/19 02:01:56 AM [Supernet Training] epoch: 407, train_loss: 0.105, train_acc: 96.172
01/19 02:01:59 AM [Supernet Validation] epoch: 407, val_loss: 0.446, val_acc: 89.080, best_acc: 89.270
01/19 02:01:59 AM 

01/19 02:01:59 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 001/521, train_loss: 0.122(0.122), train_acc: 95.833(95.833)
01/19 02:02:12 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 101/521, train_loss: 0.105(0.103), train_acc: 96.875(96.339)
01/19 02:02:25 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 201/521, train_loss: 0.134(0.105), train_acc: 94.792(96.263)
01/19 02:02:38 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 301/521, train_loss: 0.151(0.103), train_acc: 93.750(96.262)
01/19 02:02:51 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 401/521, train_loss: 0.039(0.101), train_acc: 97.917(96.342)
01/19 02:03:04 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 501/521, train_loss: 0.071(0.100), train_acc: 95.833(96.370)
01/19 02:03:06 AM [Supernet Training] lr: 0.00586 epoch: 408/600, step: 521/521, train_loss: 0.130(0.101), train_acc: 93.750(96.362)
01/19 02:03:06 AM [Supernet Training] epoch: 408, train_loss: 0.101, train_acc: 96.362
01/19 02:03:10 AM [Supernet Validation] epoch: 408, val_loss: 0.452, val_acc: 89.020, best_acc: 89.270
01/19 02:03:10 AM 

01/19 02:03:10 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 001/521, train_loss: 0.099(0.099), train_acc: 96.875(96.875)
01/19 02:03:23 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 101/521, train_loss: 0.084(0.103), train_acc: 95.833(96.256)
01/19 02:03:36 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 201/521, train_loss: 0.079(0.102), train_acc: 96.875(96.383)
01/19 02:03:49 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 301/521, train_loss: 0.057(0.101), train_acc: 97.917(96.328)
01/19 02:04:01 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 401/521, train_loss: 0.089(0.101), train_acc: 97.917(96.293)
01/19 02:04:14 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 501/521, train_loss: 0.130(0.101), train_acc: 93.750(96.287)
01/19 02:04:17 AM [Supernet Training] lr: 0.00580 epoch: 409/600, step: 521/521, train_loss: 0.107(0.101), train_acc: 95.000(96.276)
01/19 02:04:17 AM [Supernet Training] epoch: 409, train_loss: 0.101, train_acc: 96.276
01/19 02:04:20 AM [Supernet Validation] epoch: 409, val_loss: 0.447, val_acc: 88.560, best_acc: 89.270
01/19 02:04:20 AM 

01/19 02:04:21 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 001/521, train_loss: 0.140(0.140), train_acc: 93.750(93.750)
01/19 02:04:33 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 101/521, train_loss: 0.142(0.103), train_acc: 94.792(96.122)
01/19 02:04:46 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 201/521, train_loss: 0.090(0.099), train_acc: 95.833(96.367)
01/19 02:04:59 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 301/521, train_loss: 0.062(0.101), train_acc: 97.917(96.252)
01/19 02:05:12 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 401/521, train_loss: 0.042(0.100), train_acc: 98.958(96.283)
01/19 02:05:25 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 501/521, train_loss: 0.129(0.100), train_acc: 98.958(96.307)
01/19 02:05:27 AM [Supernet Training] lr: 0.00575 epoch: 410/600, step: 521/521, train_loss: 0.036(0.100), train_acc: 98.750(96.300)
01/19 02:05:27 AM [Supernet Training] epoch: 410, train_loss: 0.100, train_acc: 96.300
01/19 02:05:31 AM [Supernet Validation] epoch: 410, val_loss: 0.446, val_acc: 88.500, best_acc: 89.270
01/19 02:05:31 AM 

01/19 02:05:31 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 001/521, train_loss: 0.134(0.134), train_acc: 95.833(95.833)
01/19 02:05:44 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 101/521, train_loss: 0.079(0.100), train_acc: 94.792(96.359)
01/19 02:05:57 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 201/521, train_loss: 0.135(0.100), train_acc: 94.792(96.279)
01/19 02:06:09 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 301/521, train_loss: 0.145(0.097), train_acc: 93.750(96.401)
01/19 02:06:22 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 401/521, train_loss: 0.131(0.097), train_acc: 94.792(96.402)
01/19 02:06:35 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 501/521, train_loss: 0.163(0.097), train_acc: 93.750(96.415)
01/19 02:06:37 AM [Supernet Training] lr: 0.00569 epoch: 411/600, step: 521/521, train_loss: 0.117(0.097), train_acc: 96.250(96.436)
01/19 02:06:38 AM [Supernet Training] epoch: 411, train_loss: 0.097, train_acc: 96.436
01/19 02:06:41 AM [Supernet Validation] epoch: 411, val_loss: 0.439, val_acc: 88.960, best_acc: 89.270
01/19 02:06:41 AM 

01/19 02:06:41 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 95.833(95.833)
01/19 02:06:54 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 101/521, train_loss: 0.036(0.091), train_acc: 98.958(96.576)
01/19 02:07:07 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 201/521, train_loss: 0.117(0.095), train_acc: 95.833(96.507)
01/19 02:07:20 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 301/521, train_loss: 0.051(0.096), train_acc: 96.875(96.501)
01/19 02:07:33 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 401/521, train_loss: 0.131(0.096), train_acc: 94.792(96.496)
01/19 02:07:45 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 501/521, train_loss: 0.069(0.097), train_acc: 98.958(96.507)
01/19 02:07:48 AM [Supernet Training] lr: 0.00564 epoch: 412/600, step: 521/521, train_loss: 0.103(0.097), train_acc: 96.250(96.482)
01/19 02:07:48 AM [Supernet Training] epoch: 412, train_loss: 0.097, train_acc: 96.482
01/19 02:07:52 AM [Supernet Validation] epoch: 412, val_loss: 0.444, val_acc: 88.870, best_acc: 89.270
01/19 02:07:52 AM 

01/19 02:07:52 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 001/521, train_loss: 0.089(0.089), train_acc: 96.875(96.875)
01/19 02:08:05 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 101/521, train_loss: 0.088(0.098), train_acc: 96.875(96.586)
01/19 02:08:17 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 201/521, train_loss: 0.084(0.101), train_acc: 97.917(96.533)
01/19 02:08:30 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 301/521, train_loss: 0.116(0.101), train_acc: 95.833(96.477)
01/19 02:08:43 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 401/521, train_loss: 0.155(0.100), train_acc: 92.708(96.462)
01/19 02:08:56 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 501/521, train_loss: 0.109(0.101), train_acc: 94.792(96.411)
01/19 02:08:58 AM [Supernet Training] lr: 0.00558 epoch: 413/600, step: 521/521, train_loss: 0.171(0.102), train_acc: 95.000(96.390)
01/19 02:08:58 AM [Supernet Training] epoch: 413, train_loss: 0.102, train_acc: 96.390
01/19 02:09:02 AM [Supernet Validation] epoch: 413, val_loss: 0.444, val_acc: 89.110, best_acc: 89.270
01/19 02:09:02 AM 

01/19 02:09:02 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 001/521, train_loss: 0.124(0.124), train_acc: 95.833(95.833)
01/19 02:09:15 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 101/521, train_loss: 0.085(0.099), train_acc: 96.875(96.246)
01/19 02:09:28 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 201/521, train_loss: 0.051(0.097), train_acc: 98.958(96.341)
01/19 02:09:41 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 301/521, train_loss: 0.077(0.101), train_acc: 97.917(96.280)
01/19 02:09:54 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 401/521, train_loss: 0.145(0.102), train_acc: 95.833(96.246)
01/19 02:10:06 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 501/521, train_loss: 0.024(0.101), train_acc: 100.000(96.295)
01/19 02:10:09 AM [Supernet Training] lr: 0.00553 epoch: 414/600, step: 521/521, train_loss: 0.056(0.100), train_acc: 97.500(96.326)
01/19 02:10:09 AM [Supernet Training] epoch: 414, train_loss: 0.100, train_acc: 96.326
01/19 02:10:13 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 02:10:13 AM [Supernet Validation] epoch: 414, val_loss: 0.442, val_acc: 89.290, best_acc: 89.290
01/19 02:10:13 AM 

01/19 02:10:13 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 001/521, train_loss: 0.107(0.107), train_acc: 95.833(95.833)
01/19 02:10:26 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 101/521, train_loss: 0.055(0.099), train_acc: 97.917(96.462)
01/19 02:10:39 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 201/521, train_loss: 0.185(0.101), train_acc: 95.833(96.248)
01/19 02:10:51 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 301/521, train_loss: 0.027(0.100), train_acc: 98.958(96.321)
01/19 02:11:04 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 401/521, train_loss: 0.171(0.101), train_acc: 94.792(96.350)
01/19 02:11:17 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 501/521, train_loss: 0.060(0.099), train_acc: 96.875(96.434)
01/19 02:11:20 AM [Supernet Training] lr: 0.00547 epoch: 415/600, step: 521/521, train_loss: 0.062(0.099), train_acc: 96.250(96.448)
01/19 02:11:20 AM [Supernet Training] epoch: 415, train_loss: 0.099, train_acc: 96.448
01/19 02:11:23 AM [Supernet Validation] epoch: 415, val_loss: 0.436, val_acc: 88.860, best_acc: 89.290
01/19 02:11:23 AM 

01/19 02:11:24 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 001/521, train_loss: 0.064(0.064), train_acc: 97.917(97.917)
01/19 02:11:36 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 101/521, train_loss: 0.136(0.092), train_acc: 92.708(96.586)
01/19 02:11:49 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 201/521, train_loss: 0.083(0.095), train_acc: 95.833(96.471)
01/19 02:12:02 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 301/521, train_loss: 0.107(0.094), train_acc: 94.792(96.505)
01/19 02:12:15 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 401/521, train_loss: 0.049(0.094), train_acc: 97.917(96.504)
01/19 02:12:28 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 501/521, train_loss: 0.092(0.095), train_acc: 97.917(96.488)
01/19 02:12:30 AM [Supernet Training] lr: 0.00542 epoch: 416/600, step: 521/521, train_loss: 0.152(0.095), train_acc: 95.000(96.470)
01/19 02:12:30 AM [Supernet Training] epoch: 416, train_loss: 0.095, train_acc: 96.470
01/19 02:12:34 AM [Supernet Validation] epoch: 416, val_loss: 0.452, val_acc: 88.960, best_acc: 89.290
01/19 02:12:34 AM 

01/19 02:12:34 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 001/521, train_loss: 0.087(0.087), train_acc: 95.833(95.833)
01/19 02:12:47 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 101/521, train_loss: 0.121(0.089), train_acc: 96.875(96.803)
01/19 02:13:00 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 201/521, train_loss: 0.090(0.092), train_acc: 96.875(96.606)
01/19 02:13:12 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 301/521, train_loss: 0.060(0.095), train_acc: 96.875(96.536)
01/19 02:13:25 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 401/521, train_loss: 0.084(0.095), train_acc: 94.792(96.542)
01/19 02:13:38 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 501/521, train_loss: 0.070(0.095), train_acc: 97.917(96.492)
01/19 02:13:41 AM [Supernet Training] lr: 0.00537 epoch: 417/600, step: 521/521, train_loss: 0.083(0.096), train_acc: 96.250(96.474)
01/19 02:13:41 AM [Supernet Training] epoch: 417, train_loss: 0.096, train_acc: 96.474
01/19 02:13:44 AM [Supernet Validation] epoch: 417, val_loss: 0.454, val_acc: 88.930, best_acc: 89.290
01/19 02:13:44 AM 

01/19 02:13:45 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 001/521, train_loss: 0.069(0.069), train_acc: 96.875(96.875)
01/19 02:13:57 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 101/521, train_loss: 0.115(0.100), train_acc: 96.875(96.236)
01/19 02:14:10 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 201/521, train_loss: 0.086(0.094), train_acc: 97.917(96.559)
01/19 02:14:23 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 301/521, train_loss: 0.112(0.094), train_acc: 93.750(96.470)
01/19 02:14:36 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 401/521, train_loss: 0.093(0.095), train_acc: 96.875(96.444)
01/19 02:14:48 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 501/521, train_loss: 0.160(0.097), train_acc: 94.792(96.378)
01/19 02:14:51 AM [Supernet Training] lr: 0.00531 epoch: 418/600, step: 521/521, train_loss: 0.072(0.098), train_acc: 98.750(96.384)
01/19 02:14:51 AM [Supernet Training] epoch: 418, train_loss: 0.098, train_acc: 96.384
01/19 02:14:54 AM [Supernet Validation] epoch: 418, val_loss: 0.446, val_acc: 88.890, best_acc: 89.290
01/19 02:14:54 AM 

01/19 02:14:55 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 001/521, train_loss: 0.130(0.130), train_acc: 95.833(95.833)
01/19 02:15:08 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 101/521, train_loss: 0.135(0.096), train_acc: 94.792(96.390)
01/19 02:15:20 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 201/521, train_loss: 0.152(0.096), train_acc: 95.833(96.455)
01/19 02:15:33 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 301/521, train_loss: 0.031(0.097), train_acc: 98.958(96.422)
01/19 02:15:46 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 401/521, train_loss: 0.073(0.098), train_acc: 97.917(96.366)
01/19 02:15:59 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 501/521, train_loss: 0.121(0.099), train_acc: 95.833(96.322)
01/19 02:16:01 AM [Supernet Training] lr: 0.00526 epoch: 419/600, step: 521/521, train_loss: 0.035(0.100), train_acc: 98.750(96.312)
01/19 02:16:01 AM [Supernet Training] epoch: 419, train_loss: 0.100, train_acc: 96.312
01/19 02:16:05 AM [Supernet Validation] epoch: 419, val_loss: 0.439, val_acc: 88.950, best_acc: 89.290
01/19 02:16:05 AM 

01/19 02:16:05 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 001/521, train_loss: 0.138(0.138), train_acc: 94.792(94.792)
01/19 02:16:18 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 101/521, train_loss: 0.124(0.100), train_acc: 94.792(96.411)
01/19 02:16:31 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 201/521, train_loss: 0.121(0.104), train_acc: 97.917(96.227)
01/19 02:16:44 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 301/521, train_loss: 0.049(0.102), train_acc: 97.917(96.332)
01/19 02:16:56 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 401/521, train_loss: 0.075(0.100), train_acc: 97.917(96.402)
01/19 02:17:09 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 501/521, train_loss: 0.101(0.099), train_acc: 94.792(96.386)
01/19 02:17:12 AM [Supernet Training] lr: 0.00521 epoch: 420/600, step: 521/521, train_loss: 0.154(0.099), train_acc: 93.750(96.404)
01/19 02:17:12 AM [Supernet Training] epoch: 420, train_loss: 0.099, train_acc: 96.404
01/19 02:17:16 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 02:17:16 AM [Supernet Validation] epoch: 420, val_loss: 0.433, val_acc: 89.420, best_acc: 89.420
01/19 02:17:16 AM 

01/19 02:17:16 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 96.875(96.875)
01/19 02:17:29 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 101/521, train_loss: 0.079(0.092), train_acc: 97.917(96.741)
01/19 02:17:41 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 201/521, train_loss: 0.038(0.095), train_acc: 100.000(96.517)
01/19 02:17:54 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 301/521, train_loss: 0.035(0.097), train_acc: 100.000(96.446)
01/19 02:18:07 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 401/521, train_loss: 0.085(0.097), train_acc: 95.833(96.452)
01/19 02:18:20 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 501/521, train_loss: 0.096(0.095), train_acc: 92.708(96.507)
01/19 02:18:22 AM [Supernet Training] lr: 0.00515 epoch: 421/600, step: 521/521, train_loss: 0.035(0.095), train_acc: 100.000(96.506)
01/19 02:18:22 AM [Supernet Training] epoch: 421, train_loss: 0.095, train_acc: 96.506
01/19 02:18:26 AM [Supernet Validation] epoch: 421, val_loss: 0.453, val_acc: 88.910, best_acc: 89.420
01/19 02:18:26 AM 

01/19 02:18:26 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 001/521, train_loss: 0.079(0.079), train_acc: 98.958(98.958)
01/19 02:18:39 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 101/521, train_loss: 0.028(0.096), train_acc: 98.958(96.555)
01/19 02:18:52 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 201/521, train_loss: 0.127(0.095), train_acc: 96.875(96.517)
01/19 02:19:05 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 301/521, train_loss: 0.077(0.094), train_acc: 97.917(96.519)
01/19 02:19:18 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 401/521, train_loss: 0.068(0.093), train_acc: 95.833(96.563)
01/19 02:19:30 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 501/521, train_loss: 0.041(0.093), train_acc: 98.958(96.553)
01/19 02:19:33 AM [Supernet Training] lr: 0.00510 epoch: 422/600, step: 521/521, train_loss: 0.051(0.094), train_acc: 98.750(96.524)
01/19 02:19:33 AM [Supernet Training] epoch: 422, train_loss: 0.094, train_acc: 96.524
01/19 02:19:37 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 02:19:37 AM [Supernet Validation] epoch: 422, val_loss: 0.436, val_acc: 89.560, best_acc: 89.560
01/19 02:19:37 AM 

01/19 02:19:37 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 001/521, train_loss: 0.084(0.084), train_acc: 96.875(96.875)
01/19 02:19:50 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 101/521, train_loss: 0.094(0.099), train_acc: 98.958(96.442)
01/19 02:20:03 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 201/521, train_loss: 0.079(0.095), train_acc: 95.833(96.559)
01/19 02:20:15 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 301/521, train_loss: 0.084(0.094), train_acc: 98.958(96.609)
01/19 02:20:28 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 401/521, train_loss: 0.085(0.094), train_acc: 96.875(96.592)
01/19 02:20:41 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 501/521, train_loss: 0.131(0.095), train_acc: 93.750(96.522)
01/19 02:20:43 AM [Supernet Training] lr: 0.00505 epoch: 423/600, step: 521/521, train_loss: 0.053(0.095), train_acc: 98.750(96.554)
01/19 02:20:44 AM [Supernet Training] epoch: 423, train_loss: 0.095, train_acc: 96.554
01/19 02:20:47 AM [Supernet Validation] epoch: 423, val_loss: 0.445, val_acc: 89.100, best_acc: 89.560
01/19 02:20:47 AM 

01/19 02:20:47 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 96.875(96.875)
01/19 02:21:00 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 101/521, train_loss: 0.070(0.094), train_acc: 98.958(96.720)
01/19 02:21:13 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 201/521, train_loss: 0.164(0.091), train_acc: 94.792(96.720)
01/19 02:21:26 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 301/521, train_loss: 0.109(0.092), train_acc: 95.833(96.654)
01/19 02:21:39 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 401/521, train_loss: 0.048(0.093), train_acc: 98.958(96.631)
01/19 02:21:52 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 501/521, train_loss: 0.078(0.093), train_acc: 96.875(96.642)
01/19 02:21:54 AM [Supernet Training] lr: 0.00499 epoch: 424/600, step: 521/521, train_loss: 0.042(0.093), train_acc: 98.750(96.646)
01/19 02:21:54 AM [Supernet Training] epoch: 424, train_loss: 0.093, train_acc: 96.646
01/19 02:21:58 AM [Supernet Validation] epoch: 424, val_loss: 0.451, val_acc: 88.750, best_acc: 89.560
01/19 02:21:58 AM 

01/19 02:21:58 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 001/521, train_loss: 0.069(0.069), train_acc: 98.958(98.958)
01/19 02:22:11 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 101/521, train_loss: 0.109(0.096), train_acc: 95.833(96.514)
01/19 02:22:24 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 201/521, train_loss: 0.054(0.097), train_acc: 97.917(96.502)
01/19 02:22:37 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 301/521, train_loss: 0.042(0.096), train_acc: 100.000(96.494)
01/19 02:22:49 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 401/521, train_loss: 0.036(0.096), train_acc: 100.000(96.509)
01/19 02:23:02 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 501/521, train_loss: 0.100(0.097), train_acc: 96.875(96.507)
01/19 02:23:05 AM [Supernet Training] lr: 0.00494 epoch: 425/600, step: 521/521, train_loss: 0.228(0.097), train_acc: 93.750(96.502)
01/19 02:23:05 AM [Supernet Training] epoch: 425, train_loss: 0.097, train_acc: 96.502
01/19 02:23:08 AM [Supernet Validation] epoch: 425, val_loss: 0.451, val_acc: 89.300, best_acc: 89.560
01/19 02:23:08 AM 

01/19 02:23:09 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 001/521, train_loss: 0.125(0.125), train_acc: 93.750(93.750)
01/19 02:23:21 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 101/521, train_loss: 0.080(0.092), train_acc: 96.875(96.607)
01/19 02:23:34 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 201/521, train_loss: 0.059(0.091), train_acc: 98.958(96.642)
01/19 02:23:47 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 301/521, train_loss: 0.057(0.089), train_acc: 98.958(96.761)
01/19 02:24:00 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 401/521, train_loss: 0.054(0.087), train_acc: 97.917(96.828)
01/19 02:24:13 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 501/521, train_loss: 0.084(0.089), train_acc: 97.917(96.775)
01/19 02:24:15 AM [Supernet Training] lr: 0.00489 epoch: 426/600, step: 521/521, train_loss: 0.108(0.089), train_acc: 93.750(96.762)
01/19 02:24:15 AM [Supernet Training] epoch: 426, train_loss: 0.089, train_acc: 96.762
01/19 02:24:19 AM [Supernet Validation] epoch: 426, val_loss: 0.433, val_acc: 89.480, best_acc: 89.560
01/19 02:24:19 AM 

01/19 02:24:19 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 001/521, train_loss: 0.240(0.240), train_acc: 92.708(92.708)
01/19 02:24:32 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 101/521, train_loss: 0.062(0.095), train_acc: 96.875(96.566)
01/19 02:24:45 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 201/521, train_loss: 0.018(0.098), train_acc: 100.000(96.476)
01/19 02:24:58 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 301/521, train_loss: 0.176(0.098), train_acc: 92.708(96.484)
01/19 02:25:11 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 401/521, train_loss: 0.056(0.096), train_acc: 96.875(96.548)
01/19 02:25:23 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 501/521, train_loss: 0.042(0.095), train_acc: 98.958(96.576)
01/19 02:25:26 AM [Supernet Training] lr: 0.00484 epoch: 427/600, step: 521/521, train_loss: 0.100(0.095), train_acc: 97.500(96.576)
01/19 02:25:26 AM [Supernet Training] epoch: 427, train_loss: 0.095, train_acc: 96.576
01/19 02:25:29 AM [Supernet Validation] epoch: 427, val_loss: 0.460, val_acc: 88.860, best_acc: 89.560
01/19 02:25:29 AM 

01/19 02:25:30 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 001/521, train_loss: 0.051(0.051), train_acc: 98.958(98.958)
01/19 02:25:43 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 101/521, train_loss: 0.131(0.095), train_acc: 97.917(96.648)
01/19 02:25:55 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 201/521, train_loss: 0.048(0.097), train_acc: 97.917(96.600)
01/19 02:26:08 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 301/521, train_loss: 0.109(0.097), train_acc: 96.875(96.560)
01/19 02:26:21 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 401/521, train_loss: 0.073(0.096), train_acc: 98.958(96.540)
01/19 02:26:34 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 501/521, train_loss: 0.030(0.094), train_acc: 98.958(96.582)
01/19 02:26:36 AM [Supernet Training] lr: 0.00479 epoch: 428/600, step: 521/521, train_loss: 0.140(0.094), train_acc: 95.000(96.616)
01/19 02:26:37 AM [Supernet Training] epoch: 428, train_loss: 0.094, train_acc: 96.616
01/19 02:26:40 AM [Supernet Validation] epoch: 428, val_loss: 0.427, val_acc: 89.430, best_acc: 89.560
01/19 02:26:40 AM 

01/19 02:26:40 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 001/521, train_loss: 0.062(0.062), train_acc: 96.875(96.875)
01/19 02:26:53 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 101/521, train_loss: 0.108(0.088), train_acc: 94.792(96.844)
01/19 02:27:06 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 201/521, train_loss: 0.108(0.090), train_acc: 96.875(96.808)
01/19 02:27:19 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 301/521, train_loss: 0.069(0.091), train_acc: 97.917(96.785)
01/19 02:27:32 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 401/521, train_loss: 0.140(0.092), train_acc: 91.667(96.719)
01/19 02:27:45 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 501/521, train_loss: 0.042(0.092), train_acc: 98.958(96.707)
01/19 02:27:47 AM [Supernet Training] lr: 0.00474 epoch: 429/600, step: 521/521, train_loss: 0.115(0.092), train_acc: 97.500(96.698)
01/19 02:27:47 AM [Supernet Training] epoch: 429, train_loss: 0.092, train_acc: 96.698
01/19 02:27:51 AM [Supernet Validation] epoch: 429, val_loss: 0.436, val_acc: 89.320, best_acc: 89.560
01/19 02:27:51 AM 

01/19 02:27:51 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 001/521, train_loss: 0.108(0.108), train_acc: 94.792(94.792)
01/19 02:28:04 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 101/521, train_loss: 0.125(0.095), train_acc: 94.792(96.658)
01/19 02:28:17 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 201/521, train_loss: 0.022(0.092), train_acc: 98.958(96.714)
01/19 02:28:29 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 301/521, train_loss: 0.210(0.094), train_acc: 95.833(96.636)
01/19 02:28:42 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 401/521, train_loss: 0.108(0.091), train_acc: 93.750(96.711)
01/19 02:28:55 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 501/521, train_loss: 0.075(0.093), train_acc: 97.917(96.632)
01/19 02:28:57 AM [Supernet Training] lr: 0.00468 epoch: 430/600, step: 521/521, train_loss: 0.023(0.093), train_acc: 98.750(96.620)
01/19 02:28:57 AM [Supernet Training] epoch: 430, train_loss: 0.093, train_acc: 96.620
01/19 02:29:01 AM [Supernet Validation] epoch: 430, val_loss: 0.450, val_acc: 89.190, best_acc: 89.560
01/19 02:29:01 AM 

01/19 02:29:01 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 001/521, train_loss: 0.099(0.099), train_acc: 94.792(94.792)
01/19 02:29:14 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 101/521, train_loss: 0.038(0.090), train_acc: 98.958(96.731)
01/19 02:29:27 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 201/521, train_loss: 0.090(0.090), train_acc: 94.792(96.756)
01/19 02:29:40 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 301/521, train_loss: 0.055(0.091), train_acc: 96.875(96.660)
01/19 02:29:53 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 401/521, train_loss: 0.094(0.092), train_acc: 95.833(96.688)
01/19 02:30:05 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 501/521, train_loss: 0.040(0.091), train_acc: 98.958(96.742)
01/19 02:30:08 AM [Supernet Training] lr: 0.00463 epoch: 431/600, step: 521/521, train_loss: 0.070(0.092), train_acc: 97.500(96.730)
01/19 02:30:08 AM [Supernet Training] epoch: 431, train_loss: 0.092, train_acc: 96.730
01/19 02:30:12 AM [Supernet Validation] epoch: 431, val_loss: 0.457, val_acc: 89.150, best_acc: 89.560
01/19 02:30:12 AM 

01/19 02:30:12 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 001/521, train_loss: 0.063(0.063), train_acc: 97.917(97.917)
01/19 02:30:25 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 101/521, train_loss: 0.039(0.083), train_acc: 98.958(97.102)
01/19 02:30:38 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 201/521, train_loss: 0.158(0.090), train_acc: 94.792(96.782)
01/19 02:30:50 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 301/521, train_loss: 0.068(0.092), train_acc: 95.833(96.726)
01/19 02:31:03 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 401/521, train_loss: 0.056(0.091), train_acc: 98.958(96.802)
01/19 02:31:16 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 501/521, train_loss: 0.064(0.091), train_acc: 98.958(96.750)
01/19 02:31:18 AM [Supernet Training] lr: 0.00458 epoch: 432/600, step: 521/521, train_loss: 0.092(0.091), train_acc: 96.250(96.730)
01/19 02:31:18 AM [Supernet Training] epoch: 432, train_loss: 0.091, train_acc: 96.730
01/19 02:31:22 AM [Supernet Validation] epoch: 432, val_loss: 0.454, val_acc: 89.180, best_acc: 89.560
01/19 02:31:22 AM 

01/19 02:31:22 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 001/521, train_loss: 0.074(0.074), train_acc: 98.958(98.958)
01/19 02:31:35 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 101/521, train_loss: 0.085(0.094), train_acc: 97.917(96.627)
01/19 02:31:48 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 201/521, train_loss: 0.112(0.094), train_acc: 95.833(96.559)
01/19 02:32:01 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 301/521, train_loss: 0.125(0.092), train_acc: 94.792(96.640)
01/19 02:32:14 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 401/521, train_loss: 0.072(0.092), train_acc: 97.917(96.662)
01/19 02:32:26 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 501/521, train_loss: 0.176(0.093), train_acc: 94.792(96.574)
01/19 02:32:29 AM [Supernet Training] lr: 0.00453 epoch: 433/600, step: 521/521, train_loss: 0.053(0.094), train_acc: 97.500(96.554)
01/19 02:32:29 AM [Supernet Training] epoch: 433, train_loss: 0.094, train_acc: 96.554
01/19 02:32:33 AM [Supernet Validation] epoch: 433, val_loss: 0.464, val_acc: 88.880, best_acc: 89.560
01/19 02:32:33 AM 

01/19 02:32:33 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 001/521, train_loss: 0.050(0.050), train_acc: 98.958(98.958)
01/19 02:32:46 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 101/521, train_loss: 0.077(0.087), train_acc: 96.875(96.896)
01/19 02:32:59 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 201/521, train_loss: 0.198(0.087), train_acc: 95.833(96.901)
01/19 02:33:11 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 301/521, train_loss: 0.110(0.089), train_acc: 94.792(96.795)
01/19 02:33:24 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 401/521, train_loss: 0.195(0.091), train_acc: 92.708(96.727)
01/19 02:33:37 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 501/521, train_loss: 0.085(0.090), train_acc: 97.917(96.763)
01/19 02:33:39 AM [Supernet Training] lr: 0.00448 epoch: 434/600, step: 521/521, train_loss: 0.063(0.090), train_acc: 98.750(96.770)
01/19 02:33:40 AM [Supernet Training] epoch: 434, train_loss: 0.090, train_acc: 96.770
01/19 02:33:43 AM [Supernet Validation] epoch: 434, val_loss: 0.448, val_acc: 89.220, best_acc: 89.560
01/19 02:33:43 AM 

01/19 02:33:43 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 001/521, train_loss: 0.054(0.054), train_acc: 97.917(97.917)
01/19 02:33:56 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 101/521, train_loss: 0.037(0.093), train_acc: 98.958(96.566)
01/19 02:34:09 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 201/521, train_loss: 0.108(0.091), train_acc: 94.792(96.642)
01/19 02:34:22 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 301/521, train_loss: 0.068(0.090), train_acc: 97.917(96.709)
01/19 02:34:35 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 401/521, train_loss: 0.080(0.091), train_acc: 97.917(96.657)
01/19 02:34:47 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 501/521, train_loss: 0.083(0.091), train_acc: 94.792(96.667)
01/19 02:34:50 AM [Supernet Training] lr: 0.00443 epoch: 435/600, step: 521/521, train_loss: 0.122(0.092), train_acc: 95.000(96.648)
01/19 02:34:50 AM [Supernet Training] epoch: 435, train_loss: 0.092, train_acc: 96.648
01/19 02:34:53 AM [Supernet Validation] epoch: 435, val_loss: 0.453, val_acc: 88.940, best_acc: 89.560
01/19 02:34:53 AM 

01/19 02:34:54 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 97.917(97.917)
01/19 02:35:07 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 101/521, train_loss: 0.145(0.088), train_acc: 93.750(96.731)
01/19 02:35:19 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 201/521, train_loss: 0.096(0.091), train_acc: 96.875(96.611)
01/19 02:35:32 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 301/521, train_loss: 0.082(0.091), train_acc: 95.833(96.650)
01/19 02:35:45 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 401/521, train_loss: 0.073(0.090), train_acc: 97.917(96.748)
01/19 02:35:58 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 501/521, train_loss: 0.045(0.090), train_acc: 97.917(96.769)
01/19 02:36:00 AM [Supernet Training] lr: 0.00438 epoch: 436/600, step: 521/521, train_loss: 0.098(0.090), train_acc: 97.500(96.758)
01/19 02:36:00 AM [Supernet Training] epoch: 436, train_loss: 0.090, train_acc: 96.758
01/19 02:36:04 AM [Supernet Validation] epoch: 436, val_loss: 0.453, val_acc: 88.850, best_acc: 89.560
01/19 02:36:04 AM 

01/19 02:36:04 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
01/19 02:36:17 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 101/521, train_loss: 0.065(0.092), train_acc: 96.875(96.648)
01/19 02:36:30 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 201/521, train_loss: 0.057(0.089), train_acc: 97.917(96.735)
01/19 02:36:42 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 301/521, train_loss: 0.130(0.090), train_acc: 94.792(96.688)
01/19 02:36:55 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 401/521, train_loss: 0.099(0.089), train_acc: 95.833(96.737)
01/19 02:37:08 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 501/521, train_loss: 0.091(0.089), train_acc: 96.875(96.721)
01/19 02:37:10 AM [Supernet Training] lr: 0.00433 epoch: 437/600, step: 521/521, train_loss: 0.037(0.089), train_acc: 98.750(96.740)
01/19 02:37:11 AM [Supernet Training] epoch: 437, train_loss: 0.089, train_acc: 96.740
01/19 02:37:14 AM [Supernet Validation] epoch: 437, val_loss: 0.457, val_acc: 89.030, best_acc: 89.560
01/19 02:37:14 AM 

01/19 02:37:14 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 001/521, train_loss: 0.105(0.105), train_acc: 94.792(94.792)
01/19 02:37:27 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 101/521, train_loss: 0.066(0.086), train_acc: 98.958(96.782)
01/19 02:37:40 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 201/521, train_loss: 0.121(0.085), train_acc: 94.792(96.844)
01/19 02:37:53 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 301/521, train_loss: 0.066(0.087), train_acc: 97.917(96.820)
01/19 02:38:06 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 401/521, train_loss: 0.098(0.088), train_acc: 95.833(96.771)
01/19 02:38:19 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 501/521, train_loss: 0.093(0.089), train_acc: 97.917(96.723)
01/19 02:38:21 AM [Supernet Training] lr: 0.00428 epoch: 438/600, step: 521/521, train_loss: 0.191(0.089), train_acc: 92.500(96.714)
01/19 02:38:21 AM [Supernet Training] epoch: 438, train_loss: 0.089, train_acc: 96.714
01/19 02:38:25 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 02:38:25 AM [Supernet Validation] epoch: 438, val_loss: 0.447, val_acc: 89.740, best_acc: 89.740
01/19 02:38:25 AM 

01/19 02:38:25 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 97.917(97.917)
01/19 02:38:38 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 101/521, train_loss: 0.075(0.086), train_acc: 95.833(97.009)
01/19 02:38:51 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 201/521, train_loss: 0.210(0.092), train_acc: 91.667(96.745)
01/19 02:39:04 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 301/521, train_loss: 0.104(0.089), train_acc: 95.833(96.775)
01/19 02:39:16 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 401/521, train_loss: 0.087(0.089), train_acc: 96.875(96.756)
01/19 02:39:29 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 501/521, train_loss: 0.099(0.090), train_acc: 95.833(96.732)
01/19 02:39:32 AM [Supernet Training] lr: 0.00423 epoch: 439/600, step: 521/521, train_loss: 0.051(0.090), train_acc: 98.750(96.722)
01/19 02:39:32 AM [Supernet Training] epoch: 439, train_loss: 0.090, train_acc: 96.722
01/19 02:39:35 AM [Supernet Validation] epoch: 439, val_loss: 0.467, val_acc: 88.830, best_acc: 89.740
01/19 02:39:35 AM 

01/19 02:39:36 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 96.875(96.875)
01/19 02:39:48 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 101/521, train_loss: 0.087(0.093), train_acc: 97.917(96.638)
01/19 02:40:01 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 201/521, train_loss: 0.072(0.086), train_acc: 96.875(96.922)
01/19 02:40:14 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 301/521, train_loss: 0.153(0.086), train_acc: 97.917(96.889)
01/19 02:40:27 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 401/521, train_loss: 0.094(0.085), train_acc: 96.875(96.924)
01/19 02:40:39 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 501/521, train_loss: 0.165(0.086), train_acc: 92.708(96.869)
01/19 02:40:42 AM [Supernet Training] lr: 0.00418 epoch: 440/600, step: 521/521, train_loss: 0.093(0.087), train_acc: 97.500(96.860)
01/19 02:40:42 AM [Supernet Training] epoch: 440, train_loss: 0.087, train_acc: 96.860
01/19 02:40:46 AM [Supernet Validation] epoch: 440, val_loss: 0.472, val_acc: 88.800, best_acc: 89.740
01/19 02:40:46 AM 

01/19 02:40:46 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 001/521, train_loss: 0.121(0.121), train_acc: 94.792(94.792)
01/19 02:40:59 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 101/521, train_loss: 0.100(0.090), train_acc: 94.792(96.566)
01/19 02:41:11 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 201/521, train_loss: 0.149(0.088), train_acc: 93.750(96.688)
01/19 02:41:24 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 301/521, train_loss: 0.047(0.087), train_acc: 97.917(96.750)
01/19 02:41:37 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 401/521, train_loss: 0.060(0.087), train_acc: 97.917(96.774)
01/19 02:41:50 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 501/521, train_loss: 0.010(0.088), train_acc: 100.000(96.727)
01/19 02:41:52 AM [Supernet Training] lr: 0.00414 epoch: 441/600, step: 521/521, train_loss: 0.036(0.089), train_acc: 97.500(96.732)
01/19 02:41:52 AM [Supernet Training] epoch: 441, train_loss: 0.089, train_acc: 96.732
01/19 02:41:56 AM [Supernet Validation] epoch: 441, val_loss: 0.453, val_acc: 88.960, best_acc: 89.740
01/19 02:41:56 AM 

01/19 02:41:56 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 001/521, train_loss: 0.058(0.058), train_acc: 97.917(97.917)
01/19 02:42:09 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 101/521, train_loss: 0.050(0.082), train_acc: 98.958(97.071)
01/19 02:42:22 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 201/521, train_loss: 0.098(0.081), train_acc: 96.875(97.139)
01/19 02:42:34 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 301/521, train_loss: 0.161(0.083), train_acc: 94.792(97.086)
01/19 02:42:47 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 401/521, train_loss: 0.105(0.085), train_acc: 95.833(96.966)
01/19 02:43:00 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 501/521, train_loss: 0.035(0.086), train_acc: 98.958(96.917)
01/19 02:43:03 AM [Supernet Training] lr: 0.00409 epoch: 442/600, step: 521/521, train_loss: 0.037(0.086), train_acc: 98.750(96.906)
01/19 02:43:03 AM [Supernet Training] epoch: 442, train_loss: 0.086, train_acc: 96.906
01/19 02:43:06 AM [Supernet Validation] epoch: 442, val_loss: 0.448, val_acc: 89.190, best_acc: 89.740
01/19 02:43:06 AM 

01/19 02:43:07 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 96.875(96.875)
01/19 02:43:19 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 101/521, train_loss: 0.121(0.081), train_acc: 94.792(97.019)
01/19 02:43:32 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 201/521, train_loss: 0.148(0.086), train_acc: 94.792(96.906)
01/19 02:43:45 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 301/521, train_loss: 0.124(0.087), train_acc: 96.875(96.816)
01/19 02:43:58 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 401/521, train_loss: 0.109(0.087), train_acc: 96.875(96.815)
01/19 02:44:11 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 501/521, train_loss: 0.020(0.088), train_acc: 100.000(96.827)
01/19 02:44:13 AM [Supernet Training] lr: 0.00404 epoch: 443/600, step: 521/521, train_loss: 0.082(0.088), train_acc: 98.750(96.818)
01/19 02:44:13 AM [Supernet Training] epoch: 443, train_loss: 0.088, train_acc: 96.818
01/19 02:44:17 AM [Supernet Validation] epoch: 443, val_loss: 0.436, val_acc: 89.310, best_acc: 89.740
01/19 02:44:17 AM 

01/19 02:44:17 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 001/521, train_loss: 0.073(0.073), train_acc: 96.875(96.875)
01/19 02:44:30 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 101/521, train_loss: 0.026(0.085), train_acc: 100.000(96.937)
01/19 02:44:43 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 201/521, train_loss: 0.084(0.087), train_acc: 95.833(96.896)
01/19 02:44:56 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 301/521, train_loss: 0.151(0.087), train_acc: 94.792(96.844)
01/19 02:45:08 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 401/521, train_loss: 0.078(0.089), train_acc: 95.833(96.748)
01/19 02:45:21 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 501/521, train_loss: 0.056(0.089), train_acc: 98.958(96.736)
01/19 02:45:24 AM [Supernet Training] lr: 0.00399 epoch: 444/600, step: 521/521, train_loss: 0.119(0.089), train_acc: 97.500(96.732)
01/19 02:45:24 AM [Supernet Training] epoch: 444, train_loss: 0.089, train_acc: 96.732
01/19 02:45:27 AM [Supernet Validation] epoch: 444, val_loss: 0.452, val_acc: 89.190, best_acc: 89.740
01/19 02:45:27 AM 

01/19 02:45:28 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 001/521, train_loss: 0.139(0.139), train_acc: 94.792(94.792)
01/19 02:45:40 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 101/521, train_loss: 0.163(0.090), train_acc: 92.708(96.669)
01/19 02:45:53 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 201/521, train_loss: 0.042(0.090), train_acc: 97.917(96.745)
01/19 02:46:06 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 301/521, train_loss: 0.132(0.087), train_acc: 95.833(96.820)
01/19 02:46:19 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 401/521, train_loss: 0.083(0.086), train_acc: 95.833(96.833)
01/19 02:46:32 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 501/521, train_loss: 0.086(0.085), train_acc: 96.875(96.902)
01/19 02:46:34 AM [Supernet Training] lr: 0.00394 epoch: 445/600, step: 521/521, train_loss: 0.056(0.085), train_acc: 98.750(96.916)
01/19 02:46:34 AM [Supernet Training] epoch: 445, train_loss: 0.085, train_acc: 96.916
01/19 02:46:38 AM [Supernet Validation] epoch: 445, val_loss: 0.447, val_acc: 89.090, best_acc: 89.740
01/19 02:46:38 AM 

01/19 02:46:38 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 96.875(96.875)
01/19 02:46:51 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 101/521, train_loss: 0.089(0.082), train_acc: 95.833(97.061)
01/19 02:47:04 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 201/521, train_loss: 0.109(0.086), train_acc: 95.833(96.885)
01/19 02:47:16 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 301/521, train_loss: 0.050(0.087), train_acc: 97.917(96.833)
01/19 02:47:29 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 401/521, train_loss: 0.290(0.087), train_acc: 91.667(96.833)
01/19 02:47:42 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 501/521, train_loss: 0.071(0.087), train_acc: 95.833(96.829)
01/19 02:47:44 AM [Supernet Training] lr: 0.00390 epoch: 446/600, step: 521/521, train_loss: 0.096(0.087), train_acc: 96.250(96.830)
01/19 02:47:44 AM [Supernet Training] epoch: 446, train_loss: 0.087, train_acc: 96.830
01/19 02:47:48 AM [Supernet Validation] epoch: 446, val_loss: 0.466, val_acc: 88.970, best_acc: 89.740
01/19 02:47:48 AM 

01/19 02:47:48 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 001/521, train_loss: 0.089(0.089), train_acc: 96.875(96.875)
01/19 02:48:01 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 101/521, train_loss: 0.114(0.084), train_acc: 94.792(97.050)
01/19 02:48:14 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 201/521, train_loss: 0.093(0.086), train_acc: 95.833(96.891)
01/19 02:48:27 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 301/521, train_loss: 0.049(0.085), train_acc: 98.958(96.930)
01/19 02:48:40 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 401/521, train_loss: 0.083(0.085), train_acc: 97.917(96.906)
01/19 02:48:52 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 501/521, train_loss: 0.093(0.086), train_acc: 95.833(96.860)
01/19 02:48:55 AM [Supernet Training] lr: 0.00385 epoch: 447/600, step: 521/521, train_loss: 0.067(0.085), train_acc: 96.250(96.880)
01/19 02:48:55 AM [Supernet Training] epoch: 447, train_loss: 0.085, train_acc: 96.880
01/19 02:48:59 AM [Supernet Validation] epoch: 447, val_loss: 0.457, val_acc: 89.660, best_acc: 89.740
01/19 02:48:59 AM 

01/19 02:48:59 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 001/521, train_loss: 0.089(0.089), train_acc: 95.833(95.833)
01/19 02:49:12 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 101/521, train_loss: 0.083(0.088), train_acc: 96.875(96.906)
01/19 02:49:24 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 201/521, train_loss: 0.052(0.084), train_acc: 96.875(96.994)
01/19 02:49:37 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 301/521, train_loss: 0.085(0.084), train_acc: 96.875(96.958)
01/19 02:49:50 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 401/521, train_loss: 0.050(0.083), train_acc: 95.833(97.031)
01/19 02:50:03 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 501/521, train_loss: 0.112(0.083), train_acc: 95.833(97.027)
01/19 02:50:05 AM [Supernet Training] lr: 0.00380 epoch: 448/600, step: 521/521, train_loss: 0.075(0.083), train_acc: 97.500(97.034)
01/19 02:50:05 AM [Supernet Training] epoch: 448, train_loss: 0.083, train_acc: 97.034
01/19 02:50:09 AM [Supernet Validation] epoch: 448, val_loss: 0.447, val_acc: 89.490, best_acc: 89.740
01/19 02:50:09 AM 

01/19 02:50:09 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 001/521, train_loss: 0.027(0.027), train_acc: 100.000(100.000)
01/19 02:50:22 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 101/521, train_loss: 0.115(0.084), train_acc: 96.875(96.958)
01/19 02:50:35 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 201/521, train_loss: 0.086(0.087), train_acc: 96.875(96.844)
01/19 02:50:48 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 301/521, train_loss: 0.093(0.086), train_acc: 95.833(96.844)
01/19 02:51:01 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 401/521, train_loss: 0.079(0.085), train_acc: 94.792(96.888)
01/19 02:51:13 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 501/521, train_loss: 0.081(0.086), train_acc: 94.792(96.877)
01/19 02:51:16 AM [Supernet Training] lr: 0.00375 epoch: 449/600, step: 521/521, train_loss: 0.069(0.086), train_acc: 97.500(96.876)
01/19 02:51:16 AM [Supernet Training] epoch: 449, train_loss: 0.086, train_acc: 96.876
01/19 02:51:20 AM [Supernet Validation] epoch: 449, val_loss: 0.462, val_acc: 88.920, best_acc: 89.740
01/19 02:51:20 AM 

01/19 02:51:20 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 001/521, train_loss: 0.173(0.173), train_acc: 93.750(93.750)
01/19 02:51:33 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 101/521, train_loss: 0.072(0.089), train_acc: 97.917(96.792)
01/19 02:51:46 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 201/521, train_loss: 0.022(0.087), train_acc: 100.000(96.870)
01/19 02:51:58 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 301/521, train_loss: 0.105(0.087), train_acc: 97.917(96.861)
01/19 02:52:11 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 401/521, train_loss: 0.179(0.086), train_acc: 92.708(96.823)
01/19 02:52:24 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 501/521, train_loss: 0.044(0.085), train_acc: 100.000(96.883)
01/19 02:52:26 AM [Supernet Training] lr: 0.00371 epoch: 450/600, step: 521/521, train_loss: 0.047(0.085), train_acc: 97.500(96.882)
01/19 02:52:26 AM [Supernet Training] epoch: 450, train_loss: 0.085, train_acc: 96.882
01/19 02:52:30 AM [Supernet Validation] epoch: 450, val_loss: 0.445, val_acc: 89.400, best_acc: 89.740
01/19 02:52:30 AM 

01/19 02:52:30 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 001/521, train_loss: 0.051(0.051), train_acc: 98.958(98.958)
01/19 02:52:43 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 101/521, train_loss: 0.042(0.081), train_acc: 98.958(96.978)
01/19 02:52:56 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 201/521, train_loss: 0.155(0.080), train_acc: 93.750(97.046)
01/19 02:53:09 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 301/521, train_loss: 0.033(0.082), train_acc: 98.958(96.944)
01/19 02:53:21 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 401/521, train_loss: 0.071(0.082), train_acc: 97.917(96.893)
01/19 02:53:34 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 501/521, train_loss: 0.112(0.084), train_acc: 94.792(96.881)
01/19 02:53:37 AM [Supernet Training] lr: 0.00366 epoch: 451/600, step: 521/521, train_loss: 0.040(0.083), train_acc: 100.000(96.896)
01/19 02:53:37 AM [Supernet Training] epoch: 451, train_loss: 0.083, train_acc: 96.896
01/19 02:53:40 AM [Supernet Validation] epoch: 451, val_loss: 0.464, val_acc: 89.400, best_acc: 89.740
01/19 02:53:40 AM 

01/19 02:53:41 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 001/521, train_loss: 0.061(0.061), train_acc: 96.875(96.875)
01/19 02:53:54 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 101/521, train_loss: 0.054(0.084), train_acc: 98.958(96.906)
01/19 02:54:06 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 201/521, train_loss: 0.042(0.088), train_acc: 97.917(96.828)
01/19 02:54:19 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 301/521, train_loss: 0.071(0.088), train_acc: 98.958(96.809)
01/19 02:54:32 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 401/521, train_loss: 0.036(0.085), train_acc: 97.917(96.911)
01/19 02:54:45 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 501/521, train_loss: 0.107(0.084), train_acc: 95.833(96.975)
01/19 02:54:47 AM [Supernet Training] lr: 0.00362 epoch: 452/600, step: 521/521, train_loss: 0.068(0.083), train_acc: 98.750(96.994)
01/19 02:54:48 AM [Supernet Training] epoch: 452, train_loss: 0.083, train_acc: 96.994
01/19 02:54:51 AM [Supernet Validation] epoch: 452, val_loss: 0.464, val_acc: 89.180, best_acc: 89.740
01/19 02:54:51 AM 

01/19 02:54:51 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 001/521, train_loss: 0.068(0.068), train_acc: 98.958(98.958)
01/19 02:55:04 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 101/521, train_loss: 0.034(0.083), train_acc: 97.917(97.081)
01/19 02:55:17 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 201/521, train_loss: 0.064(0.083), train_acc: 96.875(97.150)
01/19 02:55:30 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 301/521, train_loss: 0.084(0.084), train_acc: 95.833(97.058)
01/19 02:55:43 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 401/521, train_loss: 0.059(0.085), train_acc: 97.917(97.010)
01/19 02:55:56 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 501/521, train_loss: 0.056(0.085), train_acc: 97.917(96.975)
01/19 02:55:58 AM [Supernet Training] lr: 0.00357 epoch: 453/600, step: 521/521, train_loss: 0.152(0.085), train_acc: 93.750(96.986)
01/19 02:55:58 AM [Supernet Training] epoch: 453, train_loss: 0.085, train_acc: 96.986
01/19 02:56:02 AM [Supernet Validation] epoch: 453, val_loss: 0.456, val_acc: 89.570, best_acc: 89.740
01/19 02:56:02 AM 

01/19 02:56:02 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 001/521, train_loss: 0.045(0.045), train_acc: 98.958(98.958)
01/19 02:56:15 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 101/521, train_loss: 0.082(0.080), train_acc: 97.917(97.195)
01/19 02:56:28 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 201/521, train_loss: 0.119(0.082), train_acc: 95.833(97.046)
01/19 02:56:40 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 301/521, train_loss: 0.132(0.085), train_acc: 95.833(96.962)
01/19 02:56:53 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 401/521, train_loss: 0.063(0.084), train_acc: 96.875(96.950)
01/19 02:57:06 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 501/521, train_loss: 0.113(0.084), train_acc: 95.833(96.950)
01/19 02:57:08 AM [Supernet Training] lr: 0.00352 epoch: 454/600, step: 521/521, train_loss: 0.079(0.085), train_acc: 97.500(96.938)
01/19 02:57:09 AM [Supernet Training] epoch: 454, train_loss: 0.085, train_acc: 96.938
01/19 02:57:12 AM [Supernet Validation] epoch: 454, val_loss: 0.450, val_acc: 89.290, best_acc: 89.740
01/19 02:57:12 AM 

01/19 02:57:12 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 100.000(100.000)
01/19 02:57:25 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 101/521, train_loss: 0.102(0.074), train_acc: 94.792(97.494)
01/19 02:57:38 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 201/521, train_loss: 0.110(0.077), train_acc: 94.792(97.321)
01/19 02:57:51 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 301/521, train_loss: 0.056(0.079), train_acc: 97.917(97.204)
01/19 02:58:03 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 401/521, train_loss: 0.070(0.082), train_acc: 96.875(97.119)
01/19 02:58:16 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 501/521, train_loss: 0.093(0.082), train_acc: 94.792(97.075)
01/19 02:58:19 AM [Supernet Training] lr: 0.00348 epoch: 455/600, step: 521/521, train_loss: 0.039(0.082), train_acc: 98.750(97.064)
01/19 02:58:19 AM [Supernet Training] epoch: 455, train_loss: 0.082, train_acc: 97.064
01/19 02:58:22 AM [Supernet Validation] epoch: 455, val_loss: 0.435, val_acc: 89.250, best_acc: 89.740
01/19 02:58:22 AM 

01/19 02:58:23 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 001/521, train_loss: 0.049(0.049), train_acc: 97.917(97.917)
01/19 02:58:36 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 101/521, train_loss: 0.057(0.078), train_acc: 96.875(97.050)
01/19 02:58:48 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 201/521, train_loss: 0.081(0.081), train_acc: 94.792(96.979)
01/19 02:59:01 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 301/521, train_loss: 0.081(0.083), train_acc: 97.917(96.979)
01/19 02:59:14 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 401/521, train_loss: 0.062(0.083), train_acc: 97.917(96.992)
01/19 02:59:27 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 501/521, train_loss: 0.159(0.081), train_acc: 95.833(97.058)
01/19 02:59:29 AM [Supernet Training] lr: 0.00343 epoch: 456/600, step: 521/521, train_loss: 0.086(0.082), train_acc: 98.750(97.042)
01/19 02:59:30 AM [Supernet Training] epoch: 456, train_loss: 0.082, train_acc: 97.042
01/19 02:59:33 AM [Supernet Validation] epoch: 456, val_loss: 0.471, val_acc: 88.640, best_acc: 89.740
01/19 02:59:33 AM 

01/19 02:59:34 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 001/521, train_loss: 0.020(0.020), train_acc: 100.000(100.000)
01/19 02:59:46 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 101/521, train_loss: 0.089(0.075), train_acc: 96.875(97.215)
01/19 02:59:59 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 201/521, train_loss: 0.053(0.075), train_acc: 98.958(97.295)
01/19 03:00:12 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 301/521, train_loss: 0.106(0.077), train_acc: 97.917(97.235)
01/19 03:00:25 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 401/521, train_loss: 0.072(0.078), train_acc: 95.833(97.226)
01/19 03:00:37 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 501/521, train_loss: 0.124(0.080), train_acc: 95.833(97.139)
01/19 03:00:40 AM [Supernet Training] lr: 0.00339 epoch: 457/600, step: 521/521, train_loss: 0.052(0.080), train_acc: 98.750(97.144)
01/19 03:00:40 AM [Supernet Training] epoch: 457, train_loss: 0.080, train_acc: 97.144
01/19 03:00:43 AM [Supernet Validation] epoch: 457, val_loss: 0.473, val_acc: 89.100, best_acc: 89.740
01/19 03:00:43 AM 

01/19 03:00:44 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 001/521, train_loss: 0.066(0.066), train_acc: 97.917(97.917)
01/19 03:00:57 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 101/521, train_loss: 0.077(0.081), train_acc: 96.875(96.947)
01/19 03:01:09 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 201/521, train_loss: 0.027(0.079), train_acc: 98.958(97.056)
01/19 03:01:22 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 301/521, train_loss: 0.064(0.080), train_acc: 98.958(97.069)
01/19 03:01:35 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 401/521, train_loss: 0.081(0.081), train_acc: 96.875(97.067)
01/19 03:01:48 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 501/521, train_loss: 0.017(0.081), train_acc: 100.000(97.079)
01/19 03:01:50 AM [Supernet Training] lr: 0.00334 epoch: 458/600, step: 521/521, train_loss: 0.068(0.081), train_acc: 97.500(97.082)
01/19 03:01:50 AM [Supernet Training] epoch: 458, train_loss: 0.081, train_acc: 97.082
01/19 03:01:54 AM [Supernet Validation] epoch: 458, val_loss: 0.464, val_acc: 89.240, best_acc: 89.740
01/19 03:01:54 AM 

01/19 03:01:54 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 001/521, train_loss: 0.241(0.241), train_acc: 89.583(89.583)
01/19 03:02:07 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 101/521, train_loss: 0.036(0.078), train_acc: 98.958(97.164)
01/19 03:02:20 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 201/521, train_loss: 0.090(0.079), train_acc: 96.875(97.103)
01/19 03:02:33 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 301/521, train_loss: 0.046(0.080), train_acc: 96.875(97.058)
01/19 03:02:46 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 401/521, train_loss: 0.115(0.080), train_acc: 93.750(97.114)
01/19 03:02:58 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 501/521, train_loss: 0.092(0.080), train_acc: 96.875(97.147)
01/19 03:03:01 AM [Supernet Training] lr: 0.00330 epoch: 459/600, step: 521/521, train_loss: 0.083(0.079), train_acc: 96.250(97.172)
01/19 03:03:01 AM [Supernet Training] epoch: 459, train_loss: 0.079, train_acc: 97.172
01/19 03:03:04 AM [Supernet Validation] epoch: 459, val_loss: 0.452, val_acc: 89.190, best_acc: 89.740
01/19 03:03:04 AM 

01/19 03:03:05 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 001/521, train_loss: 0.098(0.098), train_acc: 95.833(95.833)
01/19 03:03:18 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 101/521, train_loss: 0.082(0.081), train_acc: 96.875(97.009)
01/19 03:03:30 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 201/521, train_loss: 0.017(0.084), train_acc: 100.000(96.927)
01/19 03:03:43 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 301/521, train_loss: 0.113(0.081), train_acc: 93.750(96.996)
01/19 03:03:56 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 401/521, train_loss: 0.132(0.081), train_acc: 94.792(97.036)
01/19 03:04:09 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 501/521, train_loss: 0.107(0.081), train_acc: 94.792(97.062)
01/19 03:04:11 AM [Supernet Training] lr: 0.00325 epoch: 460/600, step: 521/521, train_loss: 0.018(0.082), train_acc: 100.000(97.048)
01/19 03:04:11 AM [Supernet Training] epoch: 460, train_loss: 0.082, train_acc: 97.048
01/19 03:04:15 AM [Supernet Validation] epoch: 460, val_loss: 0.452, val_acc: 89.460, best_acc: 89.740
01/19 03:04:15 AM 

01/19 03:04:15 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 001/521, train_loss: 0.052(0.052), train_acc: 98.958(98.958)
01/19 03:04:28 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 101/521, train_loss: 0.075(0.078), train_acc: 96.875(97.081)
01/19 03:04:41 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 201/521, train_loss: 0.039(0.078), train_acc: 98.958(97.113)
01/19 03:04:54 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 301/521, train_loss: 0.065(0.080), train_acc: 96.875(97.076)
01/19 03:05:06 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 401/521, train_loss: 0.185(0.080), train_acc: 92.708(97.130)
01/19 03:05:19 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 501/521, train_loss: 0.084(0.081), train_acc: 96.875(97.075)
01/19 03:05:22 AM [Supernet Training] lr: 0.00321 epoch: 461/600, step: 521/521, train_loss: 0.043(0.081), train_acc: 98.750(97.072)
01/19 03:05:22 AM [Supernet Training] epoch: 461, train_loss: 0.081, train_acc: 97.072
01/19 03:05:25 AM [Supernet Validation] epoch: 461, val_loss: 0.456, val_acc: 89.290, best_acc: 89.740
01/19 03:05:25 AM 

01/19 03:05:26 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 001/521, train_loss: 0.107(0.107), train_acc: 95.833(95.833)
01/19 03:05:38 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 101/521, train_loss: 0.141(0.081), train_acc: 95.833(97.061)
01/19 03:05:51 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 201/521, train_loss: 0.120(0.079), train_acc: 95.833(97.165)
01/19 03:06:04 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 301/521, train_loss: 0.102(0.080), train_acc: 94.792(97.051)
01/19 03:06:17 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 401/521, train_loss: 0.087(0.078), train_acc: 96.875(97.137)
01/19 03:06:30 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 501/521, train_loss: 0.044(0.078), train_acc: 98.958(97.112)
01/19 03:06:32 AM [Supernet Training] lr: 0.00317 epoch: 462/600, step: 521/521, train_loss: 0.038(0.079), train_acc: 98.750(97.110)
01/19 03:06:32 AM [Supernet Training] epoch: 462, train_loss: 0.079, train_acc: 97.110
01/19 03:06:36 AM [Supernet Validation] epoch: 462, val_loss: 0.449, val_acc: 89.440, best_acc: 89.740
01/19 03:06:36 AM 

01/19 03:06:36 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 001/521, train_loss: 0.129(0.129), train_acc: 94.792(94.792)
01/19 03:06:49 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 101/521, train_loss: 0.060(0.079), train_acc: 98.958(97.092)
01/19 03:07:02 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 201/521, train_loss: 0.110(0.079), train_acc: 97.917(97.134)
01/19 03:07:15 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 301/521, train_loss: 0.150(0.080), train_acc: 96.875(97.107)
01/19 03:07:28 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 401/521, train_loss: 0.073(0.080), train_acc: 98.958(97.057)
01/19 03:07:41 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 501/521, train_loss: 0.031(0.080), train_acc: 100.000(97.068)
01/19 03:07:43 AM [Supernet Training] lr: 0.00312 epoch: 463/600, step: 521/521, train_loss: 0.100(0.079), train_acc: 96.250(97.078)
01/19 03:07:43 AM [Supernet Training] epoch: 463, train_loss: 0.079, train_acc: 97.078
01/19 03:07:47 AM [Supernet Validation] epoch: 463, val_loss: 0.454, val_acc: 89.410, best_acc: 89.740
01/19 03:07:47 AM 

01/19 03:07:47 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 001/521, train_loss: 0.144(0.144), train_acc: 97.917(97.917)
01/19 03:08:00 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 101/521, train_loss: 0.135(0.077), train_acc: 95.833(97.339)
01/19 03:08:13 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 201/521, train_loss: 0.022(0.079), train_acc: 100.000(97.248)
01/19 03:08:26 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 301/521, train_loss: 0.101(0.082), train_acc: 96.875(97.186)
01/19 03:08:38 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 401/521, train_loss: 0.069(0.081), train_acc: 95.833(97.179)
01/19 03:08:51 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 501/521, train_loss: 0.081(0.081), train_acc: 97.917(97.174)
01/19 03:08:54 AM [Supernet Training] lr: 0.00308 epoch: 464/600, step: 521/521, train_loss: 0.117(0.081), train_acc: 98.750(97.176)
01/19 03:08:54 AM [Supernet Training] epoch: 464, train_loss: 0.081, train_acc: 97.176
01/19 03:08:57 AM [Supernet Validation] epoch: 464, val_loss: 0.449, val_acc: 89.550, best_acc: 89.740
01/19 03:08:57 AM 

01/19 03:08:58 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 001/521, train_loss: 0.222(0.222), train_acc: 95.833(95.833)
01/19 03:09:10 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 101/521, train_loss: 0.040(0.079), train_acc: 98.958(97.298)
01/19 03:09:23 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 201/521, train_loss: 0.105(0.079), train_acc: 96.875(97.217)
01/19 03:09:36 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 301/521, train_loss: 0.031(0.079), train_acc: 98.958(97.190)
01/19 03:09:49 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 401/521, train_loss: 0.095(0.079), train_acc: 96.875(97.187)
01/19 03:10:01 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 501/521, train_loss: 0.073(0.079), train_acc: 96.875(97.135)
01/19 03:10:04 AM [Supernet Training] lr: 0.00304 epoch: 465/600, step: 521/521, train_loss: 0.068(0.079), train_acc: 96.250(97.116)
01/19 03:10:04 AM [Supernet Training] epoch: 465, train_loss: 0.079, train_acc: 97.116
01/19 03:10:08 AM [Supernet Validation] epoch: 465, val_loss: 0.443, val_acc: 89.540, best_acc: 89.740
01/19 03:10:08 AM 

01/19 03:10:08 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 001/521, train_loss: 0.080(0.080), train_acc: 97.917(97.917)
01/19 03:10:21 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 101/521, train_loss: 0.092(0.085), train_acc: 96.875(97.019)
01/19 03:10:34 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 201/521, train_loss: 0.053(0.079), train_acc: 98.958(97.191)
01/19 03:10:46 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 301/521, train_loss: 0.073(0.078), train_acc: 96.875(97.200)
01/19 03:10:59 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 401/521, train_loss: 0.054(0.078), train_acc: 97.917(97.171)
01/19 03:11:12 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 501/521, train_loss: 0.035(0.079), train_acc: 97.917(97.110)
01/19 03:11:14 AM [Supernet Training] lr: 0.00299 epoch: 466/600, step: 521/521, train_loss: 0.148(0.079), train_acc: 93.750(97.108)
01/19 03:11:14 AM [Supernet Training] epoch: 466, train_loss: 0.079, train_acc: 97.108
01/19 03:11:18 AM [Supernet Validation] epoch: 466, val_loss: 0.442, val_acc: 89.560, best_acc: 89.740
01/19 03:11:18 AM 

01/19 03:11:18 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 001/521, train_loss: 0.024(0.024), train_acc: 98.958(98.958)
01/19 03:11:31 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 101/521, train_loss: 0.071(0.076), train_acc: 96.875(97.133)
01/19 03:11:44 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 201/521, train_loss: 0.082(0.077), train_acc: 96.875(97.129)
01/19 03:11:57 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 301/521, train_loss: 0.049(0.078), train_acc: 97.917(97.096)
01/19 03:12:10 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 401/521, train_loss: 0.064(0.079), train_acc: 97.917(97.098)
01/19 03:12:22 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 501/521, train_loss: 0.102(0.079), train_acc: 96.875(97.089)
01/19 03:12:25 AM [Supernet Training] lr: 0.00295 epoch: 467/600, step: 521/521, train_loss: 0.136(0.079), train_acc: 93.750(97.072)
01/19 03:12:25 AM [Supernet Training] epoch: 467, train_loss: 0.079, train_acc: 97.072
01/19 03:12:28 AM [Supernet Validation] epoch: 467, val_loss: 0.459, val_acc: 89.180, best_acc: 89.740
01/19 03:12:28 AM 

01/19 03:12:29 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 001/521, train_loss: 0.075(0.075), train_acc: 97.917(97.917)
01/19 03:12:42 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 101/521, train_loss: 0.080(0.071), train_acc: 97.917(97.442)
01/19 03:12:54 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 201/521, train_loss: 0.122(0.076), train_acc: 95.833(97.253)
01/19 03:13:07 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 301/521, train_loss: 0.049(0.077), train_acc: 97.917(97.252)
01/19 03:13:20 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 401/521, train_loss: 0.084(0.077), train_acc: 98.958(97.244)
01/19 03:13:33 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 501/521, train_loss: 0.074(0.076), train_acc: 97.917(97.253)
01/19 03:13:36 AM [Supernet Training] lr: 0.00291 epoch: 468/600, step: 521/521, train_loss: 0.057(0.076), train_acc: 97.500(97.276)
01/19 03:13:36 AM [Supernet Training] epoch: 468, train_loss: 0.076, train_acc: 97.276
01/19 03:13:39 AM [Supernet Validation] epoch: 468, val_loss: 0.455, val_acc: 89.610, best_acc: 89.740
01/19 03:13:39 AM 

01/19 03:13:40 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 001/521, train_loss: 0.043(0.043), train_acc: 98.958(98.958)
01/19 03:13:52 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 101/521, train_loss: 0.099(0.078), train_acc: 97.917(97.195)
01/19 03:14:05 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 201/521, train_loss: 0.103(0.080), train_acc: 97.917(97.139)
01/19 03:14:18 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 301/521, train_loss: 0.052(0.078), train_acc: 98.958(97.200)
01/19 03:14:31 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 401/521, train_loss: 0.033(0.080), train_acc: 98.958(97.153)
01/19 03:14:44 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 501/521, train_loss: 0.043(0.079), train_acc: 98.958(97.195)
01/19 03:14:46 AM [Supernet Training] lr: 0.00287 epoch: 469/600, step: 521/521, train_loss: 0.035(0.079), train_acc: 100.000(97.202)
01/19 03:14:46 AM [Supernet Training] epoch: 469, train_loss: 0.079, train_acc: 97.202
01/19 03:14:50 AM [Supernet Validation] epoch: 469, val_loss: 0.456, val_acc: 89.040, best_acc: 89.740
01/19 03:14:50 AM 

01/19 03:14:50 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 001/521, train_loss: 0.020(0.020), train_acc: 100.000(100.000)
01/19 03:15:03 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 101/521, train_loss: 0.117(0.077), train_acc: 94.792(97.308)
01/19 03:15:16 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 201/521, train_loss: 0.043(0.079), train_acc: 98.958(97.129)
01/19 03:15:28 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 301/521, train_loss: 0.041(0.079), train_acc: 98.958(97.124)
01/19 03:15:41 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 401/521, train_loss: 0.075(0.078), train_acc: 96.875(97.156)
01/19 03:15:54 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 501/521, train_loss: 0.111(0.079), train_acc: 95.833(97.118)
01/19 03:15:57 AM [Supernet Training] lr: 0.00283 epoch: 470/600, step: 521/521, train_loss: 0.027(0.078), train_acc: 100.000(97.138)
01/19 03:15:57 AM [Supernet Training] epoch: 470, train_loss: 0.078, train_acc: 97.138
01/19 03:16:00 AM [Supernet Validation] epoch: 470, val_loss: 0.446, val_acc: 89.220, best_acc: 89.740
01/19 03:16:00 AM 

01/19 03:16:01 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 001/521, train_loss: 0.100(0.100), train_acc: 96.875(96.875)
01/19 03:16:13 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 101/521, train_loss: 0.090(0.072), train_acc: 94.792(97.422)
01/19 03:16:26 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 201/521, train_loss: 0.074(0.077), train_acc: 95.833(97.274)
01/19 03:16:39 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 301/521, train_loss: 0.103(0.077), train_acc: 95.833(97.207)
01/19 03:16:52 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 401/521, train_loss: 0.053(0.078), train_acc: 97.917(97.195)
01/19 03:17:05 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 501/521, train_loss: 0.047(0.079), train_acc: 98.958(97.120)
01/19 03:17:07 AM [Supernet Training] lr: 0.00279 epoch: 471/600, step: 521/521, train_loss: 0.016(0.078), train_acc: 100.000(97.138)
01/19 03:17:07 AM [Supernet Training] epoch: 471, train_loss: 0.078, train_acc: 97.138
01/19 03:17:11 AM [Supernet Validation] epoch: 471, val_loss: 0.454, val_acc: 89.310, best_acc: 89.740
01/19 03:17:11 AM 

01/19 03:17:11 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 001/521, train_loss: 0.043(0.043), train_acc: 98.958(98.958)
01/19 03:17:24 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 101/521, train_loss: 0.148(0.076), train_acc: 92.708(97.360)
01/19 03:17:37 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 201/521, train_loss: 0.132(0.077), train_acc: 96.875(97.284)
01/19 03:17:49 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 301/521, train_loss: 0.180(0.079), train_acc: 94.792(97.135)
01/19 03:18:02 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 401/521, train_loss: 0.035(0.080), train_acc: 100.000(97.137)
01/19 03:18:15 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 501/521, train_loss: 0.074(0.079), train_acc: 97.917(97.160)
01/19 03:18:18 AM [Supernet Training] lr: 0.00274 epoch: 472/600, step: 521/521, train_loss: 0.071(0.079), train_acc: 97.500(97.136)
01/19 03:18:18 AM [Supernet Training] epoch: 472, train_loss: 0.079, train_acc: 97.136
01/19 03:18:21 AM [Supernet Validation] epoch: 472, val_loss: 0.463, val_acc: 89.490, best_acc: 89.740
01/19 03:18:21 AM 

01/19 03:18:22 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 001/521, train_loss: 0.100(0.100), train_acc: 95.833(95.833)
01/19 03:18:34 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 101/521, train_loss: 0.026(0.075), train_acc: 98.958(97.257)
01/19 03:18:47 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 201/521, train_loss: 0.050(0.074), train_acc: 98.958(97.305)
01/19 03:19:00 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 301/521, train_loss: 0.080(0.075), train_acc: 96.875(97.242)
01/19 03:19:13 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 401/521, train_loss: 0.039(0.076), train_acc: 98.958(97.254)
01/19 03:19:26 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 501/521, train_loss: 0.070(0.076), train_acc: 96.875(97.231)
01/19 03:19:28 AM [Supernet Training] lr: 0.00270 epoch: 473/600, step: 521/521, train_loss: 0.033(0.076), train_acc: 98.750(97.220)
01/19 03:19:28 AM [Supernet Training] epoch: 473, train_loss: 0.076, train_acc: 97.220
01/19 03:19:32 AM [Supernet Validation] epoch: 473, val_loss: 0.442, val_acc: 89.700, best_acc: 89.740
01/19 03:19:32 AM 

01/19 03:19:32 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 001/521, train_loss: 0.046(0.046), train_acc: 98.958(98.958)
01/19 03:19:45 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 101/521, train_loss: 0.028(0.076), train_acc: 98.958(97.164)
01/19 03:19:58 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 201/521, train_loss: 0.045(0.076), train_acc: 98.958(97.150)
01/19 03:20:11 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 301/521, train_loss: 0.086(0.076), train_acc: 96.875(97.176)
01/19 03:20:23 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 401/521, train_loss: 0.220(0.078), train_acc: 92.708(97.145)
01/19 03:20:36 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 501/521, train_loss: 0.051(0.077), train_acc: 97.917(97.208)
01/19 03:20:39 AM [Supernet Training] lr: 0.00266 epoch: 474/600, step: 521/521, train_loss: 0.143(0.077), train_acc: 95.000(97.202)
01/19 03:20:39 AM [Supernet Training] epoch: 474, train_loss: 0.077, train_acc: 97.202
01/19 03:20:42 AM [Supernet Validation] epoch: 474, val_loss: 0.461, val_acc: 89.050, best_acc: 89.740
01/19 03:20:42 AM 

01/19 03:20:43 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 001/521, train_loss: 0.085(0.085), train_acc: 96.875(96.875)
01/19 03:20:55 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 101/521, train_loss: 0.100(0.076), train_acc: 94.792(97.226)
01/19 03:21:08 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 201/521, train_loss: 0.083(0.071), train_acc: 94.792(97.424)
01/19 03:21:21 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 301/521, train_loss: 0.011(0.072), train_acc: 100.000(97.359)
01/19 03:21:34 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 401/521, train_loss: 0.036(0.072), train_acc: 98.958(97.408)
01/19 03:21:47 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 501/521, train_loss: 0.084(0.073), train_acc: 98.958(97.366)
01/19 03:21:49 AM [Supernet Training] lr: 0.00262 epoch: 475/600, step: 521/521, train_loss: 0.128(0.074), train_acc: 96.250(97.332)
01/19 03:21:49 AM [Supernet Training] epoch: 475, train_loss: 0.074, train_acc: 97.332
01/19 03:21:53 AM [Supernet Validation] epoch: 475, val_loss: 0.468, val_acc: 89.160, best_acc: 89.740
01/19 03:21:53 AM 

01/19 03:21:53 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 001/521, train_loss: 0.129(0.129), train_acc: 94.792(94.792)
01/19 03:22:06 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 101/521, train_loss: 0.061(0.073), train_acc: 98.958(97.288)
01/19 03:22:19 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 201/521, train_loss: 0.041(0.077), train_acc: 97.917(97.196)
01/19 03:22:32 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 301/521, train_loss: 0.038(0.077), train_acc: 98.958(97.252)
01/19 03:22:44 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 401/521, train_loss: 0.052(0.075), train_acc: 98.958(97.285)
01/19 03:22:57 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 501/521, train_loss: 0.063(0.075), train_acc: 98.958(97.287)
01/19 03:23:00 AM [Supernet Training] lr: 0.00258 epoch: 476/600, step: 521/521, train_loss: 0.104(0.075), train_acc: 97.500(97.300)
01/19 03:23:00 AM [Supernet Training] epoch: 476, train_loss: 0.075, train_acc: 97.300
01/19 03:23:03 AM [Supernet Validation] epoch: 476, val_loss: 0.482, val_acc: 88.940, best_acc: 89.740
01/19 03:23:03 AM 

01/19 03:23:04 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 001/521, train_loss: 0.046(0.046), train_acc: 98.958(98.958)
01/19 03:23:16 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 101/521, train_loss: 0.129(0.076), train_acc: 95.833(97.081)
01/19 03:23:29 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 201/521, train_loss: 0.112(0.076), train_acc: 96.875(97.238)
01/19 03:23:42 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 301/521, train_loss: 0.023(0.075), train_acc: 100.000(97.266)
01/19 03:23:55 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 401/521, train_loss: 0.109(0.078), train_acc: 94.792(97.184)
01/19 03:24:08 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 501/521, train_loss: 0.059(0.078), train_acc: 97.917(97.185)
01/19 03:24:10 AM [Supernet Training] lr: 0.00254 epoch: 477/600, step: 521/521, train_loss: 0.111(0.078), train_acc: 93.750(97.160)
01/19 03:24:10 AM [Supernet Training] epoch: 477, train_loss: 0.078, train_acc: 97.160
01/19 03:24:14 AM [Supernet Validation] epoch: 477, val_loss: 0.459, val_acc: 89.570, best_acc: 89.740
01/19 03:24:14 AM 

01/19 03:24:14 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 001/521, train_loss: 0.072(0.072), train_acc: 98.958(98.958)
01/19 03:24:27 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 101/521, train_loss: 0.067(0.073), train_acc: 97.917(97.349)
01/19 03:24:40 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 201/521, train_loss: 0.049(0.074), train_acc: 97.917(97.207)
01/19 03:24:52 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 301/521, train_loss: 0.068(0.074), train_acc: 98.958(97.238)
01/19 03:25:05 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 401/521, train_loss: 0.031(0.073), train_acc: 97.917(97.249)
01/19 03:25:18 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 501/521, train_loss: 0.131(0.074), train_acc: 95.833(97.264)
01/19 03:25:20 AM [Supernet Training] lr: 0.00250 epoch: 478/600, step: 521/521, train_loss: 0.121(0.074), train_acc: 95.000(97.272)
01/19 03:25:21 AM [Supernet Training] epoch: 478, train_loss: 0.074, train_acc: 97.272
01/19 03:25:24 AM [Supernet Validation] epoch: 478, val_loss: 0.461, val_acc: 89.360, best_acc: 89.740
01/19 03:25:24 AM 

01/19 03:25:25 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
01/19 03:25:37 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 101/521, train_loss: 0.082(0.072), train_acc: 95.833(97.525)
01/19 03:25:50 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 201/521, train_loss: 0.020(0.070), train_acc: 100.000(97.502)
01/19 03:26:03 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 301/521, train_loss: 0.037(0.072), train_acc: 98.958(97.432)
01/19 03:26:16 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 401/521, train_loss: 0.095(0.073), train_acc: 96.875(97.402)
01/19 03:26:29 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 501/521, train_loss: 0.146(0.075), train_acc: 94.792(97.324)
01/19 03:26:31 AM [Supernet Training] lr: 0.00246 epoch: 479/600, step: 521/521, train_loss: 0.026(0.075), train_acc: 98.750(97.304)
01/19 03:26:31 AM [Supernet Training] epoch: 479, train_loss: 0.075, train_acc: 97.304
01/19 03:26:35 AM [Supernet Validation] epoch: 479, val_loss: 0.460, val_acc: 89.580, best_acc: 89.740
01/19 03:26:35 AM 

01/19 03:26:35 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 001/521, train_loss: 0.122(0.122), train_acc: 93.750(93.750)
01/19 03:26:48 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 101/521, train_loss: 0.092(0.080), train_acc: 96.875(97.112)
01/19 03:27:01 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 201/521, train_loss: 0.066(0.078), train_acc: 97.917(97.155)
01/19 03:27:13 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 301/521, train_loss: 0.075(0.077), train_acc: 96.875(97.218)
01/19 03:27:26 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 401/521, train_loss: 0.072(0.078), train_acc: 96.875(97.197)
01/19 03:27:39 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 501/521, train_loss: 0.075(0.079), train_acc: 97.917(97.199)
01/19 03:27:42 AM [Supernet Training] lr: 0.00243 epoch: 480/600, step: 521/521, train_loss: 0.082(0.079), train_acc: 96.250(97.214)
01/19 03:27:42 AM [Supernet Training] epoch: 480, train_loss: 0.079, train_acc: 97.214
01/19 03:27:45 AM [Supernet Validation] epoch: 480, val_loss: 0.469, val_acc: 89.530, best_acc: 89.740
01/19 03:27:45 AM 

01/19 03:27:46 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 001/521, train_loss: 0.090(0.090), train_acc: 96.875(96.875)
01/19 03:27:59 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 101/521, train_loss: 0.106(0.081), train_acc: 96.875(97.050)
01/19 03:28:11 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 201/521, train_loss: 0.050(0.077), train_acc: 96.875(97.258)
01/19 03:28:24 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 301/521, train_loss: 0.082(0.076), train_acc: 96.875(97.214)
01/19 03:28:37 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 401/521, train_loss: 0.126(0.076), train_acc: 93.750(97.197)
01/19 03:28:50 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 501/521, train_loss: 0.040(0.075), train_acc: 98.958(97.239)
01/19 03:28:52 AM [Supernet Training] lr: 0.00239 epoch: 481/600, step: 521/521, train_loss: 0.025(0.076), train_acc: 98.750(97.238)
01/19 03:28:52 AM [Supernet Training] epoch: 481, train_loss: 0.076, train_acc: 97.238
01/19 03:28:56 AM [Supernet Validation] epoch: 481, val_loss: 0.466, val_acc: 89.440, best_acc: 89.740
01/19 03:28:56 AM 

01/19 03:28:56 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 98.958(98.958)
01/19 03:29:09 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 101/521, train_loss: 0.043(0.078), train_acc: 98.958(97.123)
01/19 03:29:22 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 201/521, train_loss: 0.068(0.074), train_acc: 96.875(97.196)
01/19 03:29:34 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 301/521, train_loss: 0.028(0.076), train_acc: 98.958(97.169)
01/19 03:29:47 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 401/521, train_loss: 0.167(0.076), train_acc: 94.792(97.150)
01/19 03:30:00 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 501/521, train_loss: 0.206(0.076), train_acc: 93.750(97.162)
01/19 03:30:02 AM [Supernet Training] lr: 0.00235 epoch: 482/600, step: 521/521, train_loss: 0.167(0.075), train_acc: 93.750(97.172)
01/19 03:30:03 AM [Supernet Training] epoch: 482, train_loss: 0.075, train_acc: 97.172
01/19 03:30:06 AM [Supernet Validation] epoch: 482, val_loss: 0.464, val_acc: 89.010, best_acc: 89.740
01/19 03:30:06 AM 

01/19 03:30:06 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 001/521, train_loss: 0.080(0.080), train_acc: 94.792(94.792)
01/19 03:30:19 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 101/521, train_loss: 0.015(0.071), train_acc: 100.000(97.463)
01/19 03:30:32 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 201/521, train_loss: 0.019(0.073), train_acc: 100.000(97.367)
01/19 03:30:45 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 301/521, train_loss: 0.075(0.074), train_acc: 96.875(97.297)
01/19 03:30:58 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 401/521, train_loss: 0.039(0.074), train_acc: 97.917(97.353)
01/19 03:31:10 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 501/521, train_loss: 0.009(0.075), train_acc: 100.000(97.322)
01/19 03:31:13 AM [Supernet Training] lr: 0.00231 epoch: 483/600, step: 521/521, train_loss: 0.089(0.075), train_acc: 96.250(97.320)
01/19 03:31:13 AM [Supernet Training] epoch: 483, train_loss: 0.075, train_acc: 97.320
01/19 03:31:17 AM [Supernet Validation] epoch: 483, val_loss: 0.468, val_acc: 89.110, best_acc: 89.740
01/19 03:31:17 AM 

01/19 03:31:17 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 96.875(96.875)
01/19 03:31:30 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 101/521, train_loss: 0.097(0.077), train_acc: 95.833(97.174)
01/19 03:31:42 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 201/521, train_loss: 0.206(0.077), train_acc: 93.750(97.207)
01/19 03:31:55 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 301/521, train_loss: 0.053(0.077), train_acc: 96.875(97.152)
01/19 03:32:08 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 401/521, train_loss: 0.073(0.077), train_acc: 96.875(97.174)
01/19 03:32:21 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 501/521, train_loss: 0.077(0.076), train_acc: 97.917(97.214)
01/19 03:32:23 AM [Supernet Training] lr: 0.00227 epoch: 484/600, step: 521/521, train_loss: 0.041(0.075), train_acc: 98.750(97.240)
01/19 03:32:24 AM [Supernet Training] epoch: 484, train_loss: 0.075, train_acc: 97.240
01/19 03:32:27 AM [Supernet Validation] epoch: 484, val_loss: 0.472, val_acc: 89.210, best_acc: 89.740
01/19 03:32:27 AM 

01/19 03:32:27 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 98.958(98.958)
01/19 03:32:40 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 101/521, train_loss: 0.100(0.069), train_acc: 95.833(97.535)
01/19 03:32:53 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 201/521, train_loss: 0.125(0.072), train_acc: 93.750(97.440)
01/19 03:33:06 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 301/521, train_loss: 0.033(0.073), train_acc: 97.917(97.356)
01/19 03:33:18 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 401/521, train_loss: 0.076(0.072), train_acc: 96.875(97.376)
01/19 03:33:31 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 501/521, train_loss: 0.086(0.072), train_acc: 95.833(97.376)
01/19 03:33:34 AM [Supernet Training] lr: 0.00224 epoch: 485/600, step: 521/521, train_loss: 0.072(0.072), train_acc: 96.250(97.368)
01/19 03:33:34 AM [Supernet Training] epoch: 485, train_loss: 0.072, train_acc: 97.368
01/19 03:33:37 AM [Supernet Validation] epoch: 485, val_loss: 0.475, val_acc: 88.820, best_acc: 89.740
01/19 03:33:37 AM 

01/19 03:33:38 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 001/521, train_loss: 0.142(0.142), train_acc: 94.792(94.792)
01/19 03:33:51 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 101/521, train_loss: 0.113(0.077), train_acc: 93.750(97.174)
01/19 03:34:03 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 201/521, train_loss: 0.091(0.075), train_acc: 96.875(97.326)
01/19 03:34:16 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 301/521, train_loss: 0.070(0.075), train_acc: 97.917(97.363)
01/19 03:34:29 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 401/521, train_loss: 0.046(0.074), train_acc: 97.917(97.410)
01/19 03:34:42 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 501/521, train_loss: 0.084(0.074), train_acc: 97.917(97.382)
01/19 03:34:44 AM [Supernet Training] lr: 0.00220 epoch: 486/600, step: 521/521, train_loss: 0.071(0.075), train_acc: 95.000(97.364)
01/19 03:34:44 AM [Supernet Training] epoch: 486, train_loss: 0.075, train_acc: 97.364
01/19 03:34:48 AM [Supernet Validation] epoch: 486, val_loss: 0.466, val_acc: 89.280, best_acc: 89.740
01/19 03:34:48 AM 

01/19 03:34:48 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 001/521, train_loss: 0.033(0.033), train_acc: 98.958(98.958)
01/19 03:35:01 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 101/521, train_loss: 0.078(0.068), train_acc: 95.833(97.545)
01/19 03:35:14 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 201/521, train_loss: 0.055(0.068), train_acc: 97.917(97.497)
01/19 03:35:27 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 301/521, train_loss: 0.091(0.070), train_acc: 95.833(97.398)
01/19 03:35:40 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 401/521, train_loss: 0.099(0.071), train_acc: 95.833(97.426)
01/19 03:35:52 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 501/521, train_loss: 0.075(0.073), train_acc: 96.875(97.357)
01/19 03:35:55 AM [Supernet Training] lr: 0.00216 epoch: 487/600, step: 521/521, train_loss: 0.138(0.073), train_acc: 96.250(97.376)
01/19 03:35:55 AM [Supernet Training] epoch: 487, train_loss: 0.073, train_acc: 97.376
01/19 03:35:59 AM [Supernet Validation] epoch: 487, val_loss: 0.464, val_acc: 89.260, best_acc: 89.740
01/19 03:35:59 AM 

01/19 03:35:59 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 98.958(98.958)
01/19 03:36:12 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 101/521, train_loss: 0.147(0.072), train_acc: 93.750(97.329)
01/19 03:36:25 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 201/521, train_loss: 0.044(0.073), train_acc: 97.917(97.388)
01/19 03:36:37 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 301/521, train_loss: 0.117(0.072), train_acc: 93.750(97.380)
01/19 03:36:50 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 401/521, train_loss: 0.025(0.073), train_acc: 98.958(97.369)
01/19 03:37:03 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 501/521, train_loss: 0.028(0.075), train_acc: 98.958(97.285)
01/19 03:37:05 AM [Supernet Training] lr: 0.00212 epoch: 488/600, step: 521/521, train_loss: 0.126(0.075), train_acc: 93.750(97.282)
01/19 03:37:06 AM [Supernet Training] epoch: 488, train_loss: 0.075, train_acc: 97.282
01/19 03:37:09 AM [Supernet Validation] epoch: 488, val_loss: 0.475, val_acc: 89.180, best_acc: 89.740
01/19 03:37:09 AM 

01/19 03:37:09 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 001/521, train_loss: 0.013(0.013), train_acc: 100.000(100.000)
01/19 03:37:22 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 101/521, train_loss: 0.029(0.075), train_acc: 98.958(97.226)
01/19 03:37:35 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 201/521, train_loss: 0.119(0.072), train_acc: 94.792(97.321)
01/19 03:37:48 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 301/521, train_loss: 0.034(0.073), train_acc: 98.958(97.318)
01/19 03:38:01 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 401/521, train_loss: 0.064(0.072), train_acc: 98.958(97.343)
01/19 03:38:14 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 501/521, train_loss: 0.063(0.073), train_acc: 97.917(97.362)
01/19 03:38:16 AM [Supernet Training] lr: 0.00209 epoch: 489/600, step: 521/521, train_loss: 0.067(0.073), train_acc: 97.500(97.374)
01/19 03:38:16 AM [Supernet Training] epoch: 489, train_loss: 0.073, train_acc: 97.374
01/19 03:38:20 AM [Supernet Validation] epoch: 489, val_loss: 0.459, val_acc: 89.320, best_acc: 89.740
01/19 03:38:20 AM 

01/19 03:38:20 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 001/521, train_loss: 0.112(0.112), train_acc: 94.792(94.792)
01/19 03:38:33 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 101/521, train_loss: 0.172(0.071), train_acc: 92.708(97.463)
01/19 03:38:46 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 201/521, train_loss: 0.171(0.073), train_acc: 95.833(97.321)
01/19 03:38:58 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 301/521, train_loss: 0.062(0.074), train_acc: 97.917(97.308)
01/19 03:39:11 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 401/521, train_loss: 0.067(0.074), train_acc: 97.917(97.278)
01/19 03:39:24 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 501/521, train_loss: 0.047(0.073), train_acc: 96.875(97.355)
01/19 03:39:27 AM [Supernet Training] lr: 0.00205 epoch: 490/600, step: 521/521, train_loss: 0.082(0.073), train_acc: 97.500(97.348)
01/19 03:39:27 AM [Supernet Training] epoch: 490, train_loss: 0.073, train_acc: 97.348
01/19 03:39:30 AM [Supernet Validation] epoch: 490, val_loss: 0.463, val_acc: 89.440, best_acc: 89.740
01/19 03:39:30 AM 

01/19 03:39:31 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
01/19 03:39:43 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 101/521, train_loss: 0.054(0.072), train_acc: 98.958(97.411)
01/19 03:39:56 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 201/521, train_loss: 0.109(0.072), train_acc: 95.833(97.316)
01/19 03:40:09 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 301/521, train_loss: 0.036(0.072), train_acc: 97.917(97.356)
01/19 03:40:22 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 401/521, train_loss: 0.046(0.073), train_acc: 98.958(97.288)
01/19 03:40:35 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 501/521, train_loss: 0.059(0.073), train_acc: 97.917(97.285)
01/19 03:40:37 AM [Supernet Training] lr: 0.00202 epoch: 491/600, step: 521/521, train_loss: 0.130(0.074), train_acc: 95.000(97.274)
01/19 03:40:37 AM [Supernet Training] epoch: 491, train_loss: 0.074, train_acc: 97.274
01/19 03:40:41 AM [Supernet Validation] epoch: 491, val_loss: 0.475, val_acc: 89.300, best_acc: 89.740
01/19 03:40:41 AM 

01/19 03:40:41 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 001/521, train_loss: 0.114(0.114), train_acc: 94.792(94.792)
01/19 03:40:54 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 101/521, train_loss: 0.073(0.071), train_acc: 96.875(97.288)
01/19 03:41:07 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 201/521, train_loss: 0.090(0.072), train_acc: 96.875(97.378)
01/19 03:41:20 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 301/521, train_loss: 0.062(0.073), train_acc: 96.875(97.339)
01/19 03:41:32 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 401/521, train_loss: 0.054(0.073), train_acc: 97.917(97.311)
01/19 03:41:45 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 501/521, train_loss: 0.045(0.074), train_acc: 98.958(97.272)
01/19 03:41:48 AM [Supernet Training] lr: 0.00198 epoch: 492/600, step: 521/521, train_loss: 0.089(0.074), train_acc: 95.000(97.276)
01/19 03:41:48 AM [Supernet Training] epoch: 492, train_loss: 0.074, train_acc: 97.276
01/19 03:41:51 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 03:41:51 AM [Supernet Validation] epoch: 492, val_loss: 0.455, val_acc: 89.900, best_acc: 89.900
01/19 03:41:51 AM 

01/19 03:41:52 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 001/521, train_loss: 0.079(0.079), train_acc: 97.917(97.917)
01/19 03:42:04 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 101/521, train_loss: 0.046(0.071), train_acc: 97.917(97.370)
01/19 03:42:17 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 201/521, train_loss: 0.102(0.071), train_acc: 93.750(97.341)
01/19 03:42:30 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 301/521, train_loss: 0.079(0.072), train_acc: 97.917(97.349)
01/19 03:42:43 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 401/521, train_loss: 0.045(0.072), train_acc: 97.917(97.348)
01/19 03:42:56 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 501/521, train_loss: 0.075(0.072), train_acc: 96.875(97.380)
01/19 03:42:58 AM [Supernet Training] lr: 0.00195 epoch: 493/600, step: 521/521, train_loss: 0.096(0.072), train_acc: 97.500(97.380)
01/19 03:42:58 AM [Supernet Training] epoch: 493, train_loss: 0.072, train_acc: 97.380
01/19 03:43:02 AM [Supernet Validation] epoch: 493, val_loss: 0.446, val_acc: 89.580, best_acc: 89.900
01/19 03:43:02 AM 

01/19 03:43:02 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 001/521, train_loss: 0.047(0.047), train_acc: 98.958(98.958)
01/19 03:43:15 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 101/521, train_loss: 0.079(0.068), train_acc: 97.917(97.587)
01/19 03:43:28 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 201/521, train_loss: 0.110(0.069), train_acc: 96.875(97.595)
01/19 03:43:41 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 301/521, train_loss: 0.042(0.070), train_acc: 98.958(97.529)
01/19 03:43:53 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 401/521, train_loss: 0.090(0.071), train_acc: 96.875(97.483)
01/19 03:44:06 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 501/521, train_loss: 0.036(0.071), train_acc: 97.917(97.451)
01/19 03:44:09 AM [Supernet Training] lr: 0.00191 epoch: 494/600, step: 521/521, train_loss: 0.072(0.071), train_acc: 97.500(97.472)
01/19 03:44:09 AM [Supernet Training] epoch: 494, train_loss: 0.071, train_acc: 97.472
01/19 03:44:12 AM [Supernet Validation] epoch: 494, val_loss: 0.468, val_acc: 89.090, best_acc: 89.900
01/19 03:44:12 AM 

01/19 03:44:13 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 001/521, train_loss: 0.087(0.087), train_acc: 97.917(97.917)
01/19 03:44:26 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 101/521, train_loss: 0.105(0.073), train_acc: 95.833(97.205)
01/19 03:44:38 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 201/521, train_loss: 0.152(0.073), train_acc: 93.750(97.305)
01/19 03:44:51 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 301/521, train_loss: 0.069(0.071), train_acc: 97.917(97.418)
01/19 03:45:04 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 401/521, train_loss: 0.087(0.071), train_acc: 97.917(97.415)
01/19 03:45:17 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 501/521, train_loss: 0.124(0.071), train_acc: 94.792(97.441)
01/19 03:45:19 AM [Supernet Training] lr: 0.00188 epoch: 495/600, step: 521/521, train_loss: 0.036(0.071), train_acc: 98.750(97.430)
01/19 03:45:19 AM [Supernet Training] epoch: 495, train_loss: 0.071, train_acc: 97.430
01/19 03:45:23 AM [Supernet Validation] epoch: 495, val_loss: 0.466, val_acc: 89.150, best_acc: 89.900
01/19 03:45:23 AM 

01/19 03:45:23 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 001/521, train_loss: 0.062(0.062), train_acc: 98.958(98.958)
01/19 03:45:36 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 101/521, train_loss: 0.035(0.070), train_acc: 98.958(97.587)
01/19 03:45:49 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 201/521, train_loss: 0.046(0.073), train_acc: 98.958(97.409)
01/19 03:46:02 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 301/521, train_loss: 0.049(0.072), train_acc: 97.917(97.436)
01/19 03:46:15 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 401/521, train_loss: 0.054(0.074), train_acc: 98.958(97.366)
01/19 03:46:27 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 501/521, train_loss: 0.068(0.073), train_acc: 98.958(97.405)
01/19 03:46:30 AM [Supernet Training] lr: 0.00184 epoch: 496/600, step: 521/521, train_loss: 0.104(0.074), train_acc: 97.500(97.386)
01/19 03:46:30 AM [Supernet Training] epoch: 496, train_loss: 0.074, train_acc: 97.386
01/19 03:46:34 AM [Supernet Validation] epoch: 496, val_loss: 0.456, val_acc: 89.350, best_acc: 89.900
01/19 03:46:34 AM 

01/19 03:46:34 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 001/521, train_loss: 0.071(0.071), train_acc: 96.875(96.875)
01/19 03:46:47 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 101/521, train_loss: 0.080(0.071), train_acc: 95.833(97.401)
01/19 03:47:00 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 201/521, train_loss: 0.040(0.073), train_acc: 97.917(97.331)
01/19 03:47:12 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 301/521, train_loss: 0.092(0.070), train_acc: 96.875(97.436)
01/19 03:47:25 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 401/521, train_loss: 0.042(0.069), train_acc: 98.958(97.413)
01/19 03:47:38 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 501/521, train_loss: 0.091(0.068), train_acc: 95.833(97.499)
01/19 03:47:40 AM [Supernet Training] lr: 0.00181 epoch: 497/600, step: 521/521, train_loss: 0.032(0.069), train_acc: 100.000(97.494)
01/19 03:47:40 AM [Supernet Training] epoch: 497, train_loss: 0.069, train_acc: 97.494
01/19 03:47:44 AM [Supernet Validation] epoch: 497, val_loss: 0.460, val_acc: 89.310, best_acc: 89.900
01/19 03:47:44 AM 

01/19 03:47:44 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 98.958(98.958)
01/19 03:47:57 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 101/521, train_loss: 0.077(0.074), train_acc: 97.917(97.329)
01/19 03:48:10 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 201/521, train_loss: 0.085(0.072), train_acc: 96.875(97.393)
01/19 03:48:23 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 301/521, train_loss: 0.021(0.072), train_acc: 98.958(97.346)
01/19 03:48:35 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 401/521, train_loss: 0.076(0.072), train_acc: 97.917(97.348)
01/19 03:48:48 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 501/521, train_loss: 0.030(0.070), train_acc: 97.917(97.407)
01/19 03:48:51 AM [Supernet Training] lr: 0.00177 epoch: 498/600, step: 521/521, train_loss: 0.034(0.070), train_acc: 98.750(97.432)
01/19 03:48:51 AM [Supernet Training] epoch: 498, train_loss: 0.070, train_acc: 97.432
01/19 03:48:55 AM [Supernet Validation] epoch: 498, val_loss: 0.456, val_acc: 89.740, best_acc: 89.900
01/19 03:48:55 AM 

01/19 03:48:55 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 001/521, train_loss: 0.046(0.046), train_acc: 97.917(97.917)
01/19 03:49:08 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 101/521, train_loss: 0.042(0.070), train_acc: 98.958(97.473)
01/19 03:49:20 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 201/521, train_loss: 0.053(0.070), train_acc: 96.875(97.435)
01/19 03:49:33 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 301/521, train_loss: 0.049(0.070), train_acc: 97.917(97.408)
01/19 03:49:46 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 401/521, train_loss: 0.082(0.071), train_acc: 95.833(97.376)
01/19 03:49:59 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 501/521, train_loss: 0.111(0.072), train_acc: 97.917(97.330)
01/19 03:50:01 AM [Supernet Training] lr: 0.00174 epoch: 499/600, step: 521/521, train_loss: 0.046(0.072), train_acc: 98.750(97.342)
01/19 03:50:01 AM [Supernet Training] epoch: 499, train_loss: 0.072, train_acc: 97.342
01/19 03:50:05 AM [Supernet Validation] epoch: 499, val_loss: 0.464, val_acc: 89.260, best_acc: 89.900
01/19 03:50:05 AM 

01/19 03:50:05 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 001/521, train_loss: 0.072(0.072), train_acc: 97.917(97.917)
01/19 03:50:18 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 101/521, train_loss: 0.043(0.074), train_acc: 100.000(97.257)
01/19 03:50:31 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 201/521, train_loss: 0.076(0.072), train_acc: 95.833(97.404)
01/19 03:50:44 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 301/521, train_loss: 0.047(0.071), train_acc: 98.958(97.436)
01/19 03:50:56 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 401/521, train_loss: 0.054(0.071), train_acc: 97.917(97.428)
01/19 03:51:09 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 501/521, train_loss: 0.042(0.070), train_acc: 100.000(97.470)
01/19 03:51:12 AM [Supernet Training] lr: 0.00171 epoch: 500/600, step: 521/521, train_loss: 0.015(0.069), train_acc: 100.000(97.474)
01/19 03:51:12 AM [Supernet Training] epoch: 500, train_loss: 0.069, train_acc: 97.474
01/19 03:51:15 AM [Supernet Validation] epoch: 500, val_loss: 0.467, val_acc: 89.450, best_acc: 89.900
01/19 03:51:15 AM 

01/19 03:51:16 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 001/521, train_loss: 0.092(0.092), train_acc: 97.917(97.917)
01/19 03:51:28 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 101/521, train_loss: 0.082(0.069), train_acc: 96.875(97.535)
01/19 03:51:41 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 201/521, train_loss: 0.101(0.069), train_acc: 97.917(97.487)
01/19 03:51:54 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 301/521, train_loss: 0.037(0.068), train_acc: 97.917(97.519)
01/19 03:52:07 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 401/521, train_loss: 0.048(0.070), train_acc: 97.917(97.488)
01/19 03:52:20 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 501/521, train_loss: 0.048(0.070), train_acc: 98.958(97.515)
01/19 03:52:22 AM [Supernet Training] lr: 0.00167 epoch: 501/600, step: 521/521, train_loss: 0.101(0.070), train_acc: 95.000(97.508)
01/19 03:52:22 AM [Supernet Training] epoch: 501, train_loss: 0.070, train_acc: 97.508
01/19 03:52:26 AM [Supernet Validation] epoch: 501, val_loss: 0.475, val_acc: 89.520, best_acc: 89.900
01/19 03:52:26 AM 

01/19 03:52:26 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 001/521, train_loss: 0.125(0.125), train_acc: 93.750(93.750)
01/19 03:52:39 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 101/521, train_loss: 0.070(0.069), train_acc: 96.875(97.525)
01/19 03:52:52 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 201/521, train_loss: 0.028(0.068), train_acc: 98.958(97.538)
01/19 03:53:05 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 301/521, train_loss: 0.016(0.067), train_acc: 100.000(97.578)
01/19 03:53:17 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 401/521, train_loss: 0.041(0.067), train_acc: 97.917(97.576)
01/19 03:53:30 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 501/521, train_loss: 0.077(0.066), train_acc: 98.958(97.640)
01/19 03:53:33 AM [Supernet Training] lr: 0.00164 epoch: 502/600, step: 521/521, train_loss: 0.151(0.066), train_acc: 93.750(97.620)
01/19 03:53:33 AM [Supernet Training] epoch: 502, train_loss: 0.066, train_acc: 97.620
01/19 03:53:36 AM [Supernet Validation] epoch: 502, val_loss: 0.453, val_acc: 89.450, best_acc: 89.900
01/19 03:53:36 AM 

01/19 03:53:37 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 001/521, train_loss: 0.087(0.087), train_acc: 95.833(95.833)
01/19 03:53:50 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 101/521, train_loss: 0.109(0.074), train_acc: 94.792(97.463)
01/19 03:54:02 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 201/521, train_loss: 0.081(0.071), train_acc: 94.792(97.507)
01/19 03:54:15 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 301/521, train_loss: 0.092(0.070), train_acc: 97.917(97.553)
01/19 03:54:28 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 401/521, train_loss: 0.047(0.070), train_acc: 97.917(97.582)
01/19 03:54:41 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 501/521, train_loss: 0.045(0.069), train_acc: 98.958(97.605)
01/19 03:54:43 AM [Supernet Training] lr: 0.00161 epoch: 503/600, step: 521/521, train_loss: 0.084(0.069), train_acc: 96.250(97.562)
01/19 03:54:43 AM [Supernet Training] epoch: 503, train_loss: 0.069, train_acc: 97.562
01/19 03:54:47 AM [Supernet Validation] epoch: 503, val_loss: 0.453, val_acc: 89.580, best_acc: 89.900
01/19 03:54:47 AM 

01/19 03:54:47 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 001/521, train_loss: 0.067(0.067), train_acc: 98.958(98.958)
01/19 03:55:00 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 101/521, train_loss: 0.094(0.063), train_acc: 95.833(97.855)
01/19 03:55:13 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 201/521, train_loss: 0.051(0.068), train_acc: 97.917(97.590)
01/19 03:55:25 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 301/521, train_loss: 0.081(0.068), train_acc: 96.875(97.598)
01/19 03:55:38 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 401/521, train_loss: 0.032(0.068), train_acc: 98.958(97.608)
01/19 03:55:51 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 501/521, train_loss: 0.052(0.068), train_acc: 98.958(97.609)
01/19 03:55:54 AM [Supernet Training] lr: 0.00158 epoch: 504/600, step: 521/521, train_loss: 0.080(0.068), train_acc: 95.000(97.614)
01/19 03:55:54 AM [Supernet Training] epoch: 504, train_loss: 0.068, train_acc: 97.614
01/19 03:55:57 AM [Supernet Validation] epoch: 504, val_loss: 0.466, val_acc: 89.600, best_acc: 89.900
01/19 03:55:57 AM 

01/19 03:55:58 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 98.958(98.958)
01/19 03:56:10 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 101/521, train_loss: 0.035(0.070), train_acc: 100.000(97.391)
01/19 03:56:23 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 201/521, train_loss: 0.120(0.070), train_acc: 96.875(97.398)
01/19 03:56:36 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 301/521, train_loss: 0.040(0.070), train_acc: 100.000(97.460)
01/19 03:56:49 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 401/521, train_loss: 0.043(0.070), train_acc: 97.917(97.472)
01/19 03:57:02 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 501/521, train_loss: 0.039(0.070), train_acc: 98.958(97.476)
01/19 03:57:04 AM [Supernet Training] lr: 0.00155 epoch: 505/600, step: 521/521, train_loss: 0.123(0.070), train_acc: 96.250(97.462)
01/19 03:57:04 AM [Supernet Training] epoch: 505, train_loss: 0.070, train_acc: 97.462
01/19 03:57:08 AM [Supernet Validation] epoch: 505, val_loss: 0.474, val_acc: 89.390, best_acc: 89.900
01/19 03:57:08 AM 

01/19 03:57:08 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
01/19 03:57:21 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 101/521, train_loss: 0.063(0.067), train_acc: 98.958(97.587)
01/19 03:57:34 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 201/521, train_loss: 0.031(0.069), train_acc: 98.958(97.487)
01/19 03:57:46 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 301/521, train_loss: 0.081(0.069), train_acc: 96.875(97.522)
01/19 03:57:59 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 401/521, train_loss: 0.048(0.069), train_acc: 98.958(97.504)
01/19 03:58:12 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 501/521, train_loss: 0.015(0.070), train_acc: 100.000(97.441)
01/19 03:58:14 AM [Supernet Training] lr: 0.00151 epoch: 506/600, step: 521/521, train_loss: 0.163(0.070), train_acc: 97.500(97.426)
01/19 03:58:15 AM [Supernet Training] epoch: 506, train_loss: 0.070, train_acc: 97.426
01/19 03:58:18 AM [Supernet Validation] epoch: 506, val_loss: 0.445, val_acc: 89.430, best_acc: 89.900
01/19 03:58:18 AM 

01/19 03:58:19 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 001/521, train_loss: 0.097(0.097), train_acc: 96.875(96.875)
01/19 03:58:31 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 101/521, train_loss: 0.072(0.071), train_acc: 96.875(97.432)
01/19 03:58:44 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 201/521, train_loss: 0.068(0.067), train_acc: 96.875(97.569)
01/19 03:58:57 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 301/521, train_loss: 0.063(0.067), train_acc: 97.917(97.574)
01/19 03:59:10 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 401/521, train_loss: 0.127(0.069), train_acc: 94.792(97.537)
01/19 03:59:22 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 501/521, train_loss: 0.032(0.070), train_acc: 100.000(97.524)
01/19 03:59:25 AM [Supernet Training] lr: 0.00148 epoch: 507/600, step: 521/521, train_loss: 0.102(0.070), train_acc: 97.500(97.514)
01/19 03:59:25 AM [Supernet Training] epoch: 507, train_loss: 0.070, train_acc: 97.514
01/19 03:59:29 AM [Supernet Validation] epoch: 507, val_loss: 0.462, val_acc: 89.510, best_acc: 89.900
01/19 03:59:29 AM 

01/19 03:59:29 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 001/521, train_loss: 0.030(0.030), train_acc: 98.958(98.958)
01/19 03:59:42 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 101/521, train_loss: 0.031(0.065), train_acc: 98.958(97.628)
01/19 03:59:55 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 201/521, train_loss: 0.075(0.068), train_acc: 97.917(97.481)
01/19 04:00:07 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 301/521, train_loss: 0.043(0.068), train_acc: 100.000(97.543)
01/19 04:00:20 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 401/521, train_loss: 0.031(0.068), train_acc: 98.958(97.514)
01/19 04:00:33 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 501/521, train_loss: 0.067(0.068), train_acc: 97.917(97.536)
01/19 04:00:36 AM [Supernet Training] lr: 0.00145 epoch: 508/600, step: 521/521, train_loss: 0.090(0.068), train_acc: 97.500(97.544)
01/19 04:00:36 AM [Supernet Training] epoch: 508, train_loss: 0.068, train_acc: 97.544
01/19 04:00:39 AM [Supernet Validation] epoch: 508, val_loss: 0.463, val_acc: 89.610, best_acc: 89.900
01/19 04:00:39 AM 

01/19 04:00:40 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 001/521, train_loss: 0.117(0.117), train_acc: 94.792(94.792)
01/19 04:00:52 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 101/521, train_loss: 0.045(0.068), train_acc: 98.958(97.628)
01/19 04:01:05 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 201/521, train_loss: 0.062(0.068), train_acc: 97.917(97.569)
01/19 04:01:18 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 301/521, train_loss: 0.064(0.067), train_acc: 96.875(97.595)
01/19 04:01:31 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 401/521, train_loss: 0.047(0.066), train_acc: 97.917(97.602)
01/19 04:01:44 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 501/521, train_loss: 0.045(0.067), train_acc: 97.917(97.580)
01/19 04:01:46 AM [Supernet Training] lr: 0.00142 epoch: 509/600, step: 521/521, train_loss: 0.093(0.066), train_acc: 96.250(97.600)
01/19 04:01:46 AM [Supernet Training] epoch: 509, train_loss: 0.066, train_acc: 97.600
01/19 04:01:50 AM [Supernet Validation] epoch: 509, val_loss: 0.461, val_acc: 89.570, best_acc: 89.900
01/19 04:01:50 AM 

01/19 04:01:50 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 001/521, train_loss: 0.058(0.058), train_acc: 96.875(96.875)
01/19 04:02:03 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 101/521, train_loss: 0.034(0.068), train_acc: 98.958(97.556)
01/19 04:02:16 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 201/521, train_loss: 0.027(0.065), train_acc: 98.958(97.621)
01/19 04:02:28 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 301/521, train_loss: 0.016(0.065), train_acc: 100.000(97.578)
01/19 04:02:41 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 401/521, train_loss: 0.035(0.066), train_acc: 97.917(97.561)
01/19 04:02:54 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 501/521, train_loss: 0.064(0.066), train_acc: 95.833(97.532)
01/19 04:02:56 AM [Supernet Training] lr: 0.00139 epoch: 510/600, step: 521/521, train_loss: 0.041(0.066), train_acc: 100.000(97.560)
01/19 04:02:56 AM [Supernet Training] epoch: 510, train_loss: 0.066, train_acc: 97.560
01/19 04:03:00 AM [Supernet Validation] epoch: 510, val_loss: 0.470, val_acc: 89.250, best_acc: 89.900
01/19 04:03:00 AM 

01/19 04:03:00 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 001/521, train_loss: 0.033(0.033), train_acc: 98.958(98.958)
01/19 04:03:13 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 101/521, train_loss: 0.049(0.062), train_acc: 96.875(97.896)
01/19 04:03:26 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 201/521, train_loss: 0.097(0.063), train_acc: 97.917(97.823)
01/19 04:03:39 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 301/521, train_loss: 0.029(0.065), train_acc: 100.000(97.723)
01/19 04:03:52 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 401/521, train_loss: 0.019(0.066), train_acc: 98.958(97.683)
01/19 04:04:04 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 501/521, train_loss: 0.061(0.066), train_acc: 95.833(97.657)
01/19 04:04:07 AM [Supernet Training] lr: 0.00136 epoch: 511/600, step: 521/521, train_loss: 0.031(0.066), train_acc: 98.750(97.638)
01/19 04:04:07 AM [Supernet Training] epoch: 511, train_loss: 0.066, train_acc: 97.638
01/19 04:04:10 AM [Supernet Validation] epoch: 511, val_loss: 0.474, val_acc: 89.360, best_acc: 89.900
01/19 04:04:10 AM 

01/19 04:04:11 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 97.917(97.917)
01/19 04:04:24 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 101/521, train_loss: 0.077(0.070), train_acc: 96.875(97.463)
01/19 04:04:37 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 201/521, train_loss: 0.052(0.070), train_acc: 96.875(97.481)
01/19 04:04:49 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 301/521, train_loss: 0.027(0.069), train_acc: 98.958(97.456)
01/19 04:05:02 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 401/521, train_loss: 0.098(0.069), train_acc: 97.917(97.470)
01/19 04:05:15 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 501/521, train_loss: 0.042(0.069), train_acc: 96.875(97.459)
01/19 04:05:17 AM [Supernet Training] lr: 0.00133 epoch: 512/600, step: 521/521, train_loss: 0.086(0.068), train_acc: 96.250(97.466)
01/19 04:05:18 AM [Supernet Training] epoch: 512, train_loss: 0.068, train_acc: 97.466
01/19 04:05:21 AM [Supernet Validation] epoch: 512, val_loss: 0.471, val_acc: 89.200, best_acc: 89.900
01/19 04:05:21 AM 

01/19 04:05:22 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 001/521, train_loss: 0.038(0.038), train_acc: 97.917(97.917)
01/19 04:05:34 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 101/521, train_loss: 0.058(0.079), train_acc: 97.917(97.174)
01/19 04:05:47 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 201/521, train_loss: 0.042(0.075), train_acc: 98.958(97.341)
01/19 04:06:00 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 301/521, train_loss: 0.035(0.073), train_acc: 98.958(97.287)
01/19 04:06:13 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 401/521, train_loss: 0.069(0.072), train_acc: 97.917(97.356)
01/19 04:06:25 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 501/521, train_loss: 0.067(0.071), train_acc: 96.875(97.409)
01/19 04:06:28 AM [Supernet Training] lr: 0.00130 epoch: 513/600, step: 521/521, train_loss: 0.069(0.071), train_acc: 97.500(97.396)
01/19 04:06:28 AM [Supernet Training] epoch: 513, train_loss: 0.071, train_acc: 97.396
01/19 04:06:32 AM [Supernet Validation] epoch: 513, val_loss: 0.472, val_acc: 89.600, best_acc: 89.900
01/19 04:06:32 AM 

01/19 04:06:32 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 001/521, train_loss: 0.061(0.061), train_acc: 96.875(96.875)
01/19 04:06:45 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 101/521, train_loss: 0.039(0.064), train_acc: 97.917(97.700)
01/19 04:06:58 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 201/521, train_loss: 0.086(0.066), train_acc: 96.875(97.585)
01/19 04:07:10 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 301/521, train_loss: 0.034(0.064), train_acc: 98.958(97.702)
01/19 04:07:23 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 401/521, train_loss: 0.021(0.065), train_acc: 100.000(97.652)
01/19 04:07:36 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 501/521, train_loss: 0.074(0.066), train_acc: 98.958(97.611)
01/19 04:07:38 AM [Supernet Training] lr: 0.00127 epoch: 514/600, step: 521/521, train_loss: 0.071(0.066), train_acc: 96.250(97.598)
01/19 04:07:38 AM [Supernet Training] epoch: 514, train_loss: 0.066, train_acc: 97.598
01/19 04:07:42 AM [Supernet Validation] epoch: 514, val_loss: 0.455, val_acc: 89.450, best_acc: 89.900
01/19 04:07:42 AM 

01/19 04:07:42 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 001/521, train_loss: 0.137(0.137), train_acc: 94.792(94.792)
01/19 04:07:55 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 101/521, train_loss: 0.083(0.066), train_acc: 94.792(97.514)
01/19 04:08:08 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 201/521, train_loss: 0.036(0.067), train_acc: 98.958(97.476)
01/19 04:08:21 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 301/521, train_loss: 0.056(0.070), train_acc: 97.917(97.373)
01/19 04:08:33 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 401/521, train_loss: 0.156(0.070), train_acc: 92.708(97.408)
01/19 04:08:46 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 501/521, train_loss: 0.080(0.068), train_acc: 96.875(97.470)
01/19 04:08:49 AM [Supernet Training] lr: 0.00125 epoch: 515/600, step: 521/521, train_loss: 0.033(0.068), train_acc: 100.000(97.492)
01/19 04:08:49 AM [Supernet Training] epoch: 515, train_loss: 0.068, train_acc: 97.492
01/19 04:08:52 AM [Supernet Validation] epoch: 515, val_loss: 0.470, val_acc: 89.330, best_acc: 89.900
01/19 04:08:52 AM 

01/19 04:08:53 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 001/521, train_loss: 0.082(0.082), train_acc: 95.833(95.833)
01/19 04:09:05 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 101/521, train_loss: 0.036(0.067), train_acc: 97.917(97.731)
01/19 04:09:18 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 201/521, train_loss: 0.035(0.064), train_acc: 100.000(97.782)
01/19 04:09:31 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 301/521, train_loss: 0.056(0.065), train_acc: 97.917(97.754)
01/19 04:09:44 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 401/521, train_loss: 0.067(0.066), train_acc: 97.917(97.727)
01/19 04:09:57 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 501/521, train_loss: 0.042(0.067), train_acc: 97.917(97.636)
01/19 04:09:59 AM [Supernet Training] lr: 0.00122 epoch: 516/600, step: 521/521, train_loss: 0.044(0.067), train_acc: 98.750(97.622)
01/19 04:09:59 AM [Supernet Training] epoch: 516, train_loss: 0.067, train_acc: 97.622
01/19 04:10:03 AM [Supernet Validation] epoch: 516, val_loss: 0.456, val_acc: 89.640, best_acc: 89.900
01/19 04:10:03 AM 

01/19 04:10:03 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 001/521, train_loss: 0.043(0.043), train_acc: 98.958(98.958)
01/19 04:10:16 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 101/521, train_loss: 0.041(0.071), train_acc: 98.958(97.432)
01/19 04:10:29 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 201/521, train_loss: 0.088(0.069), train_acc: 96.875(97.455)
01/19 04:10:42 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 301/521, train_loss: 0.085(0.069), train_acc: 96.875(97.494)
01/19 04:10:54 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 401/521, train_loss: 0.109(0.068), train_acc: 96.875(97.566)
01/19 04:11:07 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 501/521, train_loss: 0.114(0.067), train_acc: 95.833(97.582)
01/19 04:11:10 AM [Supernet Training] lr: 0.00119 epoch: 517/600, step: 521/521, train_loss: 0.082(0.067), train_acc: 96.250(97.560)
01/19 04:11:10 AM [Supernet Training] epoch: 517, train_loss: 0.067, train_acc: 97.560
01/19 04:11:13 AM [Supernet Validation] epoch: 517, val_loss: 0.475, val_acc: 89.520, best_acc: 89.900
01/19 04:11:13 AM 

01/19 04:11:14 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 001/521, train_loss: 0.074(0.074), train_acc: 96.875(96.875)
01/19 04:11:27 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 101/521, train_loss: 0.060(0.064), train_acc: 98.958(97.772)
01/19 04:11:39 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 201/521, train_loss: 0.046(0.064), train_acc: 97.917(97.683)
01/19 04:11:52 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 301/521, train_loss: 0.054(0.065), train_acc: 97.917(97.643)
01/19 04:12:05 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 401/521, train_loss: 0.116(0.066), train_acc: 93.750(97.641)
01/19 04:12:18 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 501/521, train_loss: 0.048(0.065), train_acc: 98.958(97.651)
01/19 04:12:20 AM [Supernet Training] lr: 0.00116 epoch: 518/600, step: 521/521, train_loss: 0.159(0.065), train_acc: 95.000(97.652)
01/19 04:12:20 AM [Supernet Training] epoch: 518, train_loss: 0.065, train_acc: 97.652
01/19 04:12:24 AM [Supernet Validation] epoch: 518, val_loss: 0.450, val_acc: 89.600, best_acc: 89.900
01/19 04:12:24 AM 

01/19 04:12:24 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 001/521, train_loss: 0.062(0.062), train_acc: 98.958(98.958)
01/19 04:12:37 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 101/521, train_loss: 0.067(0.065), train_acc: 97.917(97.752)
01/19 04:12:50 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 201/521, train_loss: 0.047(0.065), train_acc: 96.875(97.715)
01/19 04:13:03 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 301/521, train_loss: 0.101(0.067), train_acc: 96.875(97.612)
01/19 04:13:15 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 401/521, train_loss: 0.063(0.066), train_acc: 96.875(97.652)
01/19 04:13:28 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 501/521, train_loss: 0.018(0.066), train_acc: 98.958(97.624)
01/19 04:13:31 AM [Supernet Training] lr: 0.00113 epoch: 519/600, step: 521/521, train_loss: 0.145(0.066), train_acc: 96.250(97.622)
01/19 04:13:31 AM [Supernet Training] epoch: 519, train_loss: 0.066, train_acc: 97.622
01/19 04:13:34 AM [Supernet Validation] epoch: 519, val_loss: 0.481, val_acc: 89.670, best_acc: 89.900
01/19 04:13:34 AM 

01/19 04:13:35 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 001/521, train_loss: 0.063(0.063), train_acc: 96.875(96.875)
01/19 04:13:47 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 101/521, train_loss: 0.044(0.066), train_acc: 98.958(97.556)
01/19 04:14:00 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 201/521, train_loss: 0.069(0.066), train_acc: 95.833(97.590)
01/19 04:14:13 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 301/521, train_loss: 0.033(0.067), train_acc: 100.000(97.619)
01/19 04:14:26 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 401/521, train_loss: 0.061(0.067), train_acc: 97.917(97.584)
01/19 04:14:38 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 501/521, train_loss: 0.088(0.067), train_acc: 96.875(97.551)
01/19 04:14:41 AM [Supernet Training] lr: 0.00111 epoch: 520/600, step: 521/521, train_loss: 0.046(0.067), train_acc: 98.750(97.544)
01/19 04:14:41 AM [Supernet Training] epoch: 520, train_loss: 0.067, train_acc: 97.544
01/19 04:14:45 AM [Supernet Validation] epoch: 520, val_loss: 0.476, val_acc: 89.420, best_acc: 89.900
01/19 04:14:45 AM 

01/19 04:14:45 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
01/19 04:14:58 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 101/521, train_loss: 0.071(0.066), train_acc: 96.875(97.597)
01/19 04:15:11 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 201/521, train_loss: 0.058(0.067), train_acc: 97.917(97.492)
01/19 04:15:23 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 301/521, train_loss: 0.033(0.066), train_acc: 97.917(97.557)
01/19 04:15:36 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 401/521, train_loss: 0.069(0.065), train_acc: 96.875(97.634)
01/19 04:15:49 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 501/521, train_loss: 0.041(0.063), train_acc: 100.000(97.705)
01/19 04:15:51 AM [Supernet Training] lr: 0.00108 epoch: 521/600, step: 521/521, train_loss: 0.036(0.063), train_acc: 100.000(97.722)
01/19 04:15:51 AM [Supernet Training] epoch: 521, train_loss: 0.063, train_acc: 97.722
01/19 04:15:55 AM [Supernet Validation] epoch: 521, val_loss: 0.460, val_acc: 89.660, best_acc: 89.900
01/19 04:15:55 AM 

01/19 04:15:55 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 001/521, train_loss: 0.082(0.082), train_acc: 97.917(97.917)
01/19 04:16:08 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 101/521, train_loss: 0.026(0.065), train_acc: 98.958(97.690)
01/19 04:16:21 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 201/521, train_loss: 0.125(0.069), train_acc: 94.792(97.507)
01/19 04:16:34 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 301/521, train_loss: 0.138(0.067), train_acc: 95.833(97.571)
01/19 04:16:46 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 401/521, train_loss: 0.026(0.068), train_acc: 100.000(97.522)
01/19 04:16:59 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 501/521, train_loss: 0.036(0.069), train_acc: 97.917(97.488)
01/19 04:17:02 AM [Supernet Training] lr: 0.00105 epoch: 522/600, step: 521/521, train_loss: 0.065(0.068), train_acc: 97.500(97.506)
01/19 04:17:02 AM [Supernet Training] epoch: 522, train_loss: 0.068, train_acc: 97.506
01/19 04:17:05 AM [Supernet Validation] epoch: 522, val_loss: 0.466, val_acc: 89.370, best_acc: 89.900
01/19 04:17:05 AM 

01/19 04:17:06 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 001/521, train_loss: 0.073(0.073), train_acc: 95.833(95.833)
01/19 04:17:19 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 101/521, train_loss: 0.160(0.070), train_acc: 94.792(97.453)
01/19 04:17:31 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 201/521, train_loss: 0.060(0.068), train_acc: 97.917(97.585)
01/19 04:17:44 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 301/521, train_loss: 0.121(0.069), train_acc: 94.792(97.553)
01/19 04:17:57 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 401/521, train_loss: 0.022(0.067), train_acc: 98.958(97.595)
01/19 04:18:10 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 501/521, train_loss: 0.066(0.068), train_acc: 97.917(97.582)
01/19 04:18:12 AM [Supernet Training] lr: 0.00103 epoch: 523/600, step: 521/521, train_loss: 0.069(0.067), train_acc: 98.750(97.584)
01/19 04:18:12 AM [Supernet Training] epoch: 523, train_loss: 0.067, train_acc: 97.584
01/19 04:18:16 AM [Supernet Validation] epoch: 523, val_loss: 0.475, val_acc: 89.350, best_acc: 89.900
01/19 04:18:16 AM 

01/19 04:18:16 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 001/521, train_loss: 0.061(0.061), train_acc: 96.875(96.875)
01/19 04:18:29 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 101/521, train_loss: 0.010(0.066), train_acc: 100.000(97.607)
01/19 04:18:42 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 201/521, train_loss: 0.089(0.067), train_acc: 95.833(97.538)
01/19 04:18:55 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 301/521, train_loss: 0.017(0.066), train_acc: 100.000(97.612)
01/19 04:19:08 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 401/521, train_loss: 0.042(0.067), train_acc: 98.958(97.584)
01/19 04:19:20 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 501/521, train_loss: 0.034(0.065), train_acc: 97.917(97.653)
01/19 04:19:23 AM [Supernet Training] lr: 0.00100 epoch: 524/600, step: 521/521, train_loss: 0.073(0.065), train_acc: 97.500(97.648)
01/19 04:19:23 AM [Supernet Training] epoch: 524, train_loss: 0.065, train_acc: 97.648
01/19 04:19:27 AM [Supernet Validation] epoch: 524, val_loss: 0.473, val_acc: 89.670, best_acc: 89.900
01/19 04:19:27 AM 

01/19 04:19:27 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 001/521, train_loss: 0.070(0.070), train_acc: 98.958(98.958)
01/19 04:19:40 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 101/521, train_loss: 0.048(0.066), train_acc: 97.917(97.628)
01/19 04:19:52 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 201/521, train_loss: 0.040(0.064), train_acc: 98.958(97.663)
01/19 04:20:05 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 301/521, train_loss: 0.077(0.063), train_acc: 96.875(97.719)
01/19 04:20:18 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 401/521, train_loss: 0.068(0.063), train_acc: 96.875(97.678)
01/19 04:20:31 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 501/521, train_loss: 0.078(0.065), train_acc: 95.833(97.613)
01/19 04:20:33 AM [Supernet Training] lr: 0.00098 epoch: 525/600, step: 521/521, train_loss: 0.039(0.065), train_acc: 100.000(97.638)
01/19 04:20:34 AM [Supernet Training] epoch: 525, train_loss: 0.065, train_acc: 97.638
01/19 04:20:37 AM [Supernet Validation] epoch: 525, val_loss: 0.459, val_acc: 89.650, best_acc: 89.900
01/19 04:20:37 AM 

01/19 04:20:37 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 001/521, train_loss: 0.035(0.035), train_acc: 98.958(98.958)
01/19 04:20:50 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 101/521, train_loss: 0.057(0.067), train_acc: 97.917(97.649)
01/19 04:21:03 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 201/521, train_loss: 0.041(0.066), train_acc: 97.917(97.632)
01/19 04:21:16 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 301/521, train_loss: 0.042(0.067), train_acc: 98.958(97.526)
01/19 04:21:29 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 401/521, train_loss: 0.043(0.067), train_acc: 98.958(97.566)
01/19 04:21:41 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 501/521, train_loss: 0.056(0.066), train_acc: 98.958(97.615)
01/19 04:21:44 AM [Supernet Training] lr: 0.00095 epoch: 526/600, step: 521/521, train_loss: 0.060(0.066), train_acc: 97.500(97.608)
01/19 04:21:44 AM [Supernet Training] epoch: 526, train_loss: 0.066, train_acc: 97.608
01/19 04:21:48 AM [Supernet Validation] epoch: 526, val_loss: 0.473, val_acc: 89.480, best_acc: 89.900
01/19 04:21:48 AM 

01/19 04:21:48 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 001/521, train_loss: 0.110(0.110), train_acc: 95.833(95.833)
01/19 04:22:01 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 101/521, train_loss: 0.114(0.069), train_acc: 95.833(97.679)
01/19 04:22:14 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 201/521, train_loss: 0.071(0.063), train_acc: 97.917(97.875)
01/19 04:22:26 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 301/521, train_loss: 0.093(0.062), train_acc: 97.917(97.920)
01/19 04:22:39 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 401/521, train_loss: 0.083(0.062), train_acc: 97.917(97.930)
01/19 04:22:52 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 501/521, train_loss: 0.092(0.064), train_acc: 97.917(97.846)
01/19 04:22:55 AM [Supernet Training] lr: 0.00093 epoch: 527/600, step: 521/521, train_loss: 0.173(0.064), train_acc: 96.250(97.842)
01/19 04:22:55 AM [Supernet Training] epoch: 527, train_loss: 0.064, train_acc: 97.842
01/19 04:22:58 AM [Supernet Validation] epoch: 527, val_loss: 0.445, val_acc: 89.700, best_acc: 89.900
01/19 04:22:58 AM 

01/19 04:22:59 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 001/521, train_loss: 0.081(0.081), train_acc: 97.917(97.917)
01/19 04:23:11 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 101/521, train_loss: 0.033(0.070), train_acc: 98.958(97.432)
01/19 04:23:24 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 201/521, train_loss: 0.154(0.071), train_acc: 94.792(97.440)
01/19 04:23:37 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 301/521, train_loss: 0.065(0.070), train_acc: 97.917(97.470)
01/19 04:23:50 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 401/521, train_loss: 0.025(0.069), train_acc: 98.958(97.446)
01/19 04:24:03 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 501/521, train_loss: 0.048(0.068), train_acc: 96.875(97.515)
01/19 04:24:05 AM [Supernet Training] lr: 0.00090 epoch: 528/600, step: 521/521, train_loss: 0.082(0.068), train_acc: 96.250(97.506)
01/19 04:24:05 AM [Supernet Training] epoch: 528, train_loss: 0.068, train_acc: 97.506
01/19 04:24:09 AM [Supernet Validation] epoch: 528, val_loss: 0.456, val_acc: 89.180, best_acc: 89.900
01/19 04:24:09 AM 

01/19 04:24:09 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 001/521, train_loss: 0.049(0.049), train_acc: 98.958(98.958)
01/19 04:24:22 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 101/521, train_loss: 0.032(0.062), train_acc: 98.958(97.752)
01/19 04:24:35 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 201/521, train_loss: 0.076(0.064), train_acc: 96.875(97.658)
01/19 04:24:47 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 301/521, train_loss: 0.159(0.064), train_acc: 94.792(97.612)
01/19 04:25:00 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 401/521, train_loss: 0.074(0.064), train_acc: 95.833(97.621)
01/19 04:25:13 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 501/521, train_loss: 0.059(0.064), train_acc: 98.958(97.626)
01/19 04:25:16 AM [Supernet Training] lr: 0.00088 epoch: 529/600, step: 521/521, train_loss: 0.093(0.064), train_acc: 95.000(97.614)
01/19 04:25:16 AM [Supernet Training] epoch: 529, train_loss: 0.064, train_acc: 97.614
01/19 04:25:19 AM [Supernet Validation] epoch: 529, val_loss: 0.467, val_acc: 89.250, best_acc: 89.900
01/19 04:25:19 AM 

01/19 04:25:20 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 001/521, train_loss: 0.072(0.072), train_acc: 97.917(97.917)
01/19 04:25:32 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 101/521, train_loss: 0.131(0.068), train_acc: 95.833(97.504)
01/19 04:25:45 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 201/521, train_loss: 0.027(0.067), train_acc: 100.000(97.575)
01/19 04:25:58 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 301/521, train_loss: 0.069(0.066), train_acc: 96.875(97.667)
01/19 04:26:11 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 401/521, train_loss: 0.092(0.066), train_acc: 96.875(97.662)
01/19 04:26:23 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 501/521, train_loss: 0.040(0.066), train_acc: 98.958(97.638)
01/19 04:26:26 AM [Supernet Training] lr: 0.00085 epoch: 530/600, step: 521/521, train_loss: 0.062(0.066), train_acc: 97.500(97.644)
01/19 04:26:26 AM [Supernet Training] epoch: 530, train_loss: 0.066, train_acc: 97.644
01/19 04:26:29 AM [Supernet Validation] epoch: 530, val_loss: 0.446, val_acc: 89.540, best_acc: 89.900
01/19 04:26:29 AM 

01/19 04:26:30 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 001/521, train_loss: 0.057(0.057), train_acc: 97.917(97.917)
01/19 04:26:43 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 101/521, train_loss: 0.099(0.062), train_acc: 95.833(97.752)
01/19 04:26:55 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 201/521, train_loss: 0.016(0.064), train_acc: 100.000(97.647)
01/19 04:27:08 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 301/521, train_loss: 0.064(0.065), train_acc: 98.958(97.661)
01/19 04:27:21 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 401/521, train_loss: 0.032(0.065), train_acc: 98.958(97.654)
01/19 04:27:34 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 501/521, train_loss: 0.175(0.064), train_acc: 92.708(97.700)
01/19 04:27:36 AM [Supernet Training] lr: 0.00083 epoch: 531/600, step: 521/521, train_loss: 0.024(0.064), train_acc: 100.000(97.712)
01/19 04:27:36 AM [Supernet Training] epoch: 531, train_loss: 0.064, train_acc: 97.712
01/19 04:27:40 AM [Supernet Validation] epoch: 531, val_loss: 0.454, val_acc: 89.620, best_acc: 89.900
01/19 04:27:40 AM 

01/19 04:27:40 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 100.000(100.000)
01/19 04:27:53 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 101/521, train_loss: 0.055(0.071), train_acc: 97.917(97.380)
01/19 04:28:06 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 201/521, train_loss: 0.149(0.068), train_acc: 93.750(97.445)
01/19 04:28:18 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 301/521, train_loss: 0.116(0.069), train_acc: 95.833(97.449)
01/19 04:28:31 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 401/521, train_loss: 0.034(0.068), train_acc: 96.875(97.506)
01/19 04:28:44 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 501/521, train_loss: 0.038(0.067), train_acc: 98.958(97.517)
01/19 04:28:46 AM [Supernet Training] lr: 0.00081 epoch: 532/600, step: 521/521, train_loss: 0.030(0.067), train_acc: 100.000(97.520)
01/19 04:28:47 AM [Supernet Training] epoch: 532, train_loss: 0.067, train_acc: 97.520
01/19 04:28:50 AM [Supernet Validation] epoch: 532, val_loss: 0.460, val_acc: 89.670, best_acc: 89.900
01/19 04:28:50 AM 

01/19 04:28:50 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 001/521, train_loss: 0.094(0.094), train_acc: 96.875(96.875)
01/19 04:29:03 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 101/521, train_loss: 0.053(0.063), train_acc: 97.917(97.649)
01/19 04:29:16 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 201/521, train_loss: 0.157(0.063), train_acc: 94.792(97.699)
01/19 04:29:29 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 301/521, train_loss: 0.061(0.062), train_acc: 96.875(97.740)
01/19 04:29:42 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 401/521, train_loss: 0.105(0.063), train_acc: 95.833(97.730)
01/19 04:29:54 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 501/521, train_loss: 0.051(0.064), train_acc: 98.958(97.688)
01/19 04:29:57 AM [Supernet Training] lr: 0.00078 epoch: 533/600, step: 521/521, train_loss: 0.093(0.064), train_acc: 95.000(97.680)
01/19 04:29:57 AM [Supernet Training] epoch: 533, train_loss: 0.064, train_acc: 97.680
01/19 04:30:00 AM [Supernet Validation] epoch: 533, val_loss: 0.470, val_acc: 89.740, best_acc: 89.900
01/19 04:30:00 AM 

01/19 04:30:01 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 001/521, train_loss: 0.074(0.074), train_acc: 97.917(97.917)
01/19 04:30:14 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 101/521, train_loss: 0.079(0.067), train_acc: 97.917(97.618)
01/19 04:30:26 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 201/521, train_loss: 0.025(0.066), train_acc: 100.000(97.616)
01/19 04:30:39 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 301/521, train_loss: 0.053(0.066), train_acc: 97.917(97.598)
01/19 04:30:52 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 401/521, train_loss: 0.061(0.066), train_acc: 97.917(97.613)
01/19 04:31:05 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 501/521, train_loss: 0.083(0.066), train_acc: 95.833(97.621)
01/19 04:31:07 AM [Supernet Training] lr: 0.00076 epoch: 534/600, step: 521/521, train_loss: 0.039(0.065), train_acc: 98.750(97.656)
01/19 04:31:07 AM [Supernet Training] epoch: 534, train_loss: 0.065, train_acc: 97.656
01/19 04:31:11 AM [Supernet Validation] epoch: 534, val_loss: 0.486, val_acc: 89.630, best_acc: 89.900
01/19 04:31:11 AM 

01/19 04:31:11 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 001/521, train_loss: 0.094(0.094), train_acc: 94.792(94.792)
01/19 04:31:24 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 101/521, train_loss: 0.109(0.071), train_acc: 96.875(97.257)
01/19 04:31:37 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 201/521, train_loss: 0.028(0.067), train_acc: 98.958(97.398)
01/19 04:31:50 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 301/521, train_loss: 0.097(0.065), train_acc: 96.875(97.529)
01/19 04:32:02 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 401/521, train_loss: 0.079(0.066), train_acc: 96.875(97.563)
01/19 04:32:15 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 501/521, train_loss: 0.058(0.064), train_acc: 97.917(97.646)
01/19 04:32:17 AM [Supernet Training] lr: 0.00074 epoch: 535/600, step: 521/521, train_loss: 0.036(0.065), train_acc: 100.000(97.620)
01/19 04:32:18 AM [Supernet Training] epoch: 535, train_loss: 0.065, train_acc: 97.620
01/19 04:32:21 AM [Supernet Validation] epoch: 535, val_loss: 0.473, val_acc: 89.470, best_acc: 89.900
01/19 04:32:21 AM 

01/19 04:32:21 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 100.000(100.000)
01/19 04:32:34 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 101/521, train_loss: 0.249(0.059), train_acc: 92.708(97.824)
01/19 04:32:47 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 201/521, train_loss: 0.104(0.063), train_acc: 97.917(97.699)
01/19 04:33:00 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 301/521, train_loss: 0.051(0.065), train_acc: 97.917(97.629)
01/19 04:33:13 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 401/521, train_loss: 0.068(0.065), train_acc: 97.917(97.670)
01/19 04:33:25 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 501/521, train_loss: 0.081(0.066), train_acc: 97.917(97.646)
01/19 04:33:28 AM [Supernet Training] lr: 0.00072 epoch: 536/600, step: 521/521, train_loss: 0.196(0.066), train_acc: 92.500(97.636)
01/19 04:33:28 AM [Supernet Training] epoch: 536, train_loss: 0.066, train_acc: 97.636
01/19 04:33:32 AM [Supernet Validation] epoch: 536, val_loss: 0.470, val_acc: 89.470, best_acc: 89.900
01/19 04:33:32 AM 

01/19 04:33:32 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 001/521, train_loss: 0.044(0.044), train_acc: 98.958(98.958)
01/19 04:33:45 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 101/521, train_loss: 0.032(0.060), train_acc: 98.958(97.979)
01/19 04:33:58 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 201/521, train_loss: 0.077(0.063), train_acc: 96.875(97.761)
01/19 04:34:10 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 301/521, train_loss: 0.073(0.063), train_acc: 96.875(97.761)
01/19 04:34:23 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 401/521, train_loss: 0.067(0.063), train_acc: 96.875(97.696)
01/19 04:34:36 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 501/521, train_loss: 0.121(0.062), train_acc: 94.792(97.740)
01/19 04:34:38 AM [Supernet Training] lr: 0.00070 epoch: 537/600, step: 521/521, train_loss: 0.031(0.062), train_acc: 100.000(97.734)
01/19 04:34:38 AM [Supernet Training] epoch: 537, train_loss: 0.062, train_acc: 97.734
01/19 04:34:42 AM [Supernet Validation] epoch: 537, val_loss: 0.473, val_acc: 89.050, best_acc: 89.900
01/19 04:34:42 AM 

01/19 04:34:42 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 98.958(98.958)
01/19 04:34:55 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 101/521, train_loss: 0.044(0.062), train_acc: 100.000(97.772)
01/19 04:35:08 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 201/521, train_loss: 0.033(0.065), train_acc: 98.958(97.683)
01/19 04:35:21 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 301/521, train_loss: 0.242(0.067), train_acc: 93.750(97.526)
01/19 04:35:33 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 401/521, train_loss: 0.032(0.066), train_acc: 97.917(97.574)
01/19 04:35:46 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 501/521, train_loss: 0.039(0.065), train_acc: 96.875(97.588)
01/19 04:35:49 AM [Supernet Training] lr: 0.00067 epoch: 538/600, step: 521/521, train_loss: 0.048(0.065), train_acc: 96.250(97.612)
01/19 04:35:49 AM [Supernet Training] epoch: 538, train_loss: 0.065, train_acc: 97.612
01/19 04:35:52 AM [Supernet Validation] epoch: 538, val_loss: 0.482, val_acc: 89.530, best_acc: 89.900
01/19 04:35:52 AM 

01/19 04:35:53 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 98.958(98.958)
01/19 04:36:06 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 101/521, train_loss: 0.062(0.061), train_acc: 97.917(97.762)
01/19 04:36:18 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 201/521, train_loss: 0.090(0.062), train_acc: 95.833(97.772)
01/19 04:36:31 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 301/521, train_loss: 0.081(0.062), train_acc: 94.792(97.744)
01/19 04:36:44 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 401/521, train_loss: 0.012(0.062), train_acc: 100.000(97.766)
01/19 04:36:57 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 501/521, train_loss: 0.049(0.064), train_acc: 98.958(97.696)
01/19 04:36:59 AM [Supernet Training] lr: 0.00065 epoch: 539/600, step: 521/521, train_loss: 0.088(0.064), train_acc: 96.250(97.692)
01/19 04:36:59 AM [Supernet Training] epoch: 539, train_loss: 0.064, train_acc: 97.692
01/19 04:37:03 AM [Supernet Validation] epoch: 539, val_loss: 0.472, val_acc: 89.670, best_acc: 89.900
01/19 04:37:03 AM 

01/19 04:37:03 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 001/521, train_loss: 0.018(0.018), train_acc: 100.000(100.000)
01/19 04:37:16 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 101/521, train_loss: 0.044(0.066), train_acc: 98.958(97.710)
01/19 04:37:29 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 201/521, train_loss: 0.040(0.067), train_acc: 97.917(97.616)
01/19 04:37:42 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 301/521, train_loss: 0.055(0.065), train_acc: 96.875(97.699)
01/19 04:37:54 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 401/521, train_loss: 0.031(0.065), train_acc: 98.958(97.683)
01/19 04:38:07 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 501/521, train_loss: 0.012(0.065), train_acc: 100.000(97.678)
01/19 04:38:10 AM [Supernet Training] lr: 0.00063 epoch: 540/600, step: 521/521, train_loss: 0.123(0.065), train_acc: 95.000(97.670)
01/19 04:38:10 AM [Supernet Training] epoch: 540, train_loss: 0.065, train_acc: 97.670
01/19 04:38:13 AM [Supernet Validation] epoch: 540, val_loss: 0.477, val_acc: 89.500, best_acc: 89.900
01/19 04:38:13 AM 

01/19 04:38:14 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 001/521, train_loss: 0.032(0.032), train_acc: 98.958(98.958)
01/19 04:38:26 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 101/521, train_loss: 0.064(0.062), train_acc: 98.958(97.844)
01/19 04:38:39 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 201/521, train_loss: 0.046(0.064), train_acc: 96.875(97.725)
01/19 04:38:52 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 301/521, train_loss: 0.067(0.064), train_acc: 96.875(97.688)
01/19 04:39:05 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 401/521, train_loss: 0.132(0.062), train_acc: 94.792(97.748)
01/19 04:39:17 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 501/521, train_loss: 0.028(0.063), train_acc: 98.958(97.730)
01/19 04:39:20 AM [Supernet Training] lr: 0.00061 epoch: 541/600, step: 521/521, train_loss: 0.044(0.063), train_acc: 98.750(97.718)
01/19 04:39:20 AM [Supernet Training] epoch: 541, train_loss: 0.063, train_acc: 97.718
01/19 04:39:24 AM [Supernet Validation] epoch: 541, val_loss: 0.452, val_acc: 89.620, best_acc: 89.900
01/19 04:39:24 AM 

01/19 04:39:24 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 001/521, train_loss: 0.080(0.080), train_acc: 96.875(96.875)
01/19 04:39:37 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 101/521, train_loss: 0.014(0.068), train_acc: 100.000(97.525)
01/19 04:39:50 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 201/521, train_loss: 0.041(0.064), train_acc: 100.000(97.740)
01/19 04:40:02 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 301/521, train_loss: 0.125(0.064), train_acc: 94.792(97.726)
01/19 04:40:15 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 401/521, train_loss: 0.061(0.064), train_acc: 98.958(97.722)
01/19 04:40:28 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 501/521, train_loss: 0.026(0.064), train_acc: 98.958(97.707)
01/19 04:40:31 AM [Supernet Training] lr: 0.00059 epoch: 542/600, step: 521/521, train_loss: 0.023(0.064), train_acc: 100.000(97.700)
01/19 04:40:31 AM [Supernet Training] epoch: 542, train_loss: 0.064, train_acc: 97.700
01/19 04:40:34 AM [Supernet Validation] epoch: 542, val_loss: 0.470, val_acc: 89.680, best_acc: 89.900
01/19 04:40:34 AM 

01/19 04:40:35 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 001/521, train_loss: 0.021(0.021), train_acc: 100.000(100.000)
01/19 04:40:47 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 101/521, train_loss: 0.020(0.063), train_acc: 98.958(97.700)
01/19 04:41:00 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 201/521, train_loss: 0.043(0.065), train_acc: 97.917(97.683)
01/19 04:41:13 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 301/521, train_loss: 0.023(0.064), train_acc: 100.000(97.733)
01/19 04:41:26 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 401/521, train_loss: 0.033(0.062), train_acc: 98.958(97.792)
01/19 04:41:38 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 501/521, train_loss: 0.069(0.063), train_acc: 96.875(97.707)
01/19 04:41:41 AM [Supernet Training] lr: 0.00057 epoch: 543/600, step: 521/521, train_loss: 0.069(0.064), train_acc: 96.250(97.670)
01/19 04:41:41 AM [Supernet Training] epoch: 543, train_loss: 0.064, train_acc: 97.670
01/19 04:41:45 AM [Supernet Validation] epoch: 543, val_loss: 0.469, val_acc: 89.660, best_acc: 89.900
01/19 04:41:45 AM 

01/19 04:41:45 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 001/521, train_loss: 0.097(0.097), train_acc: 96.875(96.875)
01/19 04:41:58 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 101/521, train_loss: 0.098(0.058), train_acc: 95.833(97.968)
01/19 04:42:11 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 201/521, train_loss: 0.079(0.061), train_acc: 96.875(97.777)
01/19 04:42:23 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 301/521, train_loss: 0.011(0.063), train_acc: 100.000(97.712)
01/19 04:42:36 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 401/521, train_loss: 0.082(0.064), train_acc: 96.875(97.657)
01/19 04:42:49 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 501/521, train_loss: 0.111(0.064), train_acc: 95.833(97.659)
01/19 04:42:51 AM [Supernet Training] lr: 0.00055 epoch: 544/600, step: 521/521, train_loss: 0.018(0.064), train_acc: 100.000(97.656)
01/19 04:42:52 AM [Supernet Training] epoch: 544, train_loss: 0.064, train_acc: 97.656
01/19 04:42:55 AM [Supernet Validation] epoch: 544, val_loss: 0.471, val_acc: 89.610, best_acc: 89.900
01/19 04:42:55 AM 

01/19 04:42:56 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 001/521, train_loss: 0.184(0.184), train_acc: 95.833(95.833)
01/19 04:43:08 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 101/521, train_loss: 0.059(0.055), train_acc: 97.917(98.030)
01/19 04:43:21 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 201/521, train_loss: 0.080(0.059), train_acc: 97.917(97.891)
01/19 04:43:34 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 301/521, train_loss: 0.031(0.060), train_acc: 97.917(97.851)
01/19 04:43:47 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 401/521, train_loss: 0.046(0.061), train_acc: 97.917(97.818)
01/19 04:43:59 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 501/521, train_loss: 0.051(0.062), train_acc: 97.917(97.794)
01/19 04:44:02 AM [Supernet Training] lr: 0.00053 epoch: 545/600, step: 521/521, train_loss: 0.043(0.062), train_acc: 96.250(97.804)
01/19 04:44:02 AM [Supernet Training] epoch: 545, train_loss: 0.062, train_acc: 97.804
01/19 04:44:06 AM [Supernet Validation] epoch: 545, val_loss: 0.473, val_acc: 89.580, best_acc: 89.900
01/19 04:44:06 AM 

01/19 04:44:06 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 001/521, train_loss: 0.126(0.126), train_acc: 96.875(96.875)
01/19 04:44:19 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 101/521, train_loss: 0.056(0.064), train_acc: 98.958(97.638)
01/19 04:44:32 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 201/521, train_loss: 0.075(0.065), train_acc: 96.875(97.678)
01/19 04:44:44 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 301/521, train_loss: 0.056(0.064), train_acc: 98.958(97.640)
01/19 04:44:57 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 401/521, train_loss: 0.106(0.063), train_acc: 95.833(97.688)
01/19 04:45:10 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 501/521, train_loss: 0.066(0.063), train_acc: 96.875(97.694)
01/19 04:45:13 AM [Supernet Training] lr: 0.00051 epoch: 546/600, step: 521/521, train_loss: 0.114(0.063), train_acc: 96.250(97.682)
01/19 04:45:13 AM [Supernet Training] epoch: 546, train_loss: 0.063, train_acc: 97.682
01/19 04:45:16 AM [Supernet Validation] epoch: 546, val_loss: 0.477, val_acc: 89.390, best_acc: 89.900
01/19 04:45:16 AM 

01/19 04:45:17 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 001/521, train_loss: 0.017(0.017), train_acc: 100.000(100.000)
01/19 04:45:29 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 101/521, train_loss: 0.068(0.068), train_acc: 96.875(97.514)
01/19 04:45:42 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 201/521, train_loss: 0.037(0.066), train_acc: 97.917(97.663)
01/19 04:45:55 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 301/521, train_loss: 0.038(0.065), train_acc: 97.917(97.671)
01/19 04:46:08 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 401/521, train_loss: 0.027(0.066), train_acc: 98.958(97.631)
01/19 04:46:20 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 501/521, train_loss: 0.011(0.066), train_acc: 100.000(97.651)
01/19 04:46:23 AM [Supernet Training] lr: 0.00050 epoch: 547/600, step: 521/521, train_loss: 0.046(0.065), train_acc: 98.750(97.662)
01/19 04:46:23 AM [Supernet Training] epoch: 547, train_loss: 0.065, train_acc: 97.662
01/19 04:46:27 AM [Supernet Validation] epoch: 547, val_loss: 0.480, val_acc: 89.470, best_acc: 89.900
01/19 04:46:27 AM 

01/19 04:46:27 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 001/521, train_loss: 0.031(0.031), train_acc: 100.000(100.000)
01/19 04:46:40 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 101/521, train_loss: 0.057(0.070), train_acc: 98.958(97.380)
01/19 04:46:53 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 201/521, train_loss: 0.082(0.068), train_acc: 96.875(97.476)
01/19 04:47:05 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 301/521, train_loss: 0.067(0.066), train_acc: 96.875(97.557)
01/19 04:47:18 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 401/521, train_loss: 0.030(0.064), train_acc: 98.958(97.644)
01/19 04:47:31 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 501/521, train_loss: 0.085(0.064), train_acc: 95.833(97.615)
01/19 04:47:33 AM [Supernet Training] lr: 0.00048 epoch: 548/600, step: 521/521, train_loss: 0.005(0.064), train_acc: 100.000(97.614)
01/19 04:47:33 AM [Supernet Training] epoch: 548, train_loss: 0.064, train_acc: 97.614
01/19 04:47:37 AM [Supernet Validation] epoch: 548, val_loss: 0.478, val_acc: 89.520, best_acc: 89.900
01/19 04:47:37 AM 

01/19 04:47:37 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 001/521, train_loss: 0.053(0.053), train_acc: 98.958(98.958)
01/19 04:47:50 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 101/521, train_loss: 0.023(0.060), train_acc: 98.958(97.948)
01/19 04:48:03 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 201/521, train_loss: 0.073(0.060), train_acc: 95.833(97.875)
01/19 04:48:16 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 301/521, train_loss: 0.034(0.062), train_acc: 98.958(97.823)
01/19 04:48:29 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 401/521, train_loss: 0.036(0.063), train_acc: 98.958(97.779)
01/19 04:48:41 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 501/521, train_loss: 0.053(0.062), train_acc: 97.917(97.802)
01/19 04:48:44 AM [Supernet Training] lr: 0.00046 epoch: 549/600, step: 521/521, train_loss: 0.111(0.063), train_acc: 93.750(97.788)
01/19 04:48:44 AM [Supernet Training] epoch: 549, train_loss: 0.063, train_acc: 97.788
01/19 04:48:47 AM [Supernet Validation] epoch: 549, val_loss: 0.469, val_acc: 89.750, best_acc: 89.900
01/19 04:48:47 AM 

01/19 04:48:48 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 001/521, train_loss: 0.043(0.043), train_acc: 100.000(100.000)
01/19 04:49:01 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 101/521, train_loss: 0.103(0.061), train_acc: 95.833(97.855)
01/19 04:49:13 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 201/521, train_loss: 0.033(0.061), train_acc: 98.958(97.854)
01/19 04:49:26 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 301/521, train_loss: 0.071(0.060), train_acc: 96.875(97.809)
01/19 04:49:39 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 401/521, train_loss: 0.074(0.061), train_acc: 97.917(97.763)
01/19 04:49:52 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 501/521, train_loss: 0.073(0.061), train_acc: 97.917(97.727)
01/19 04:49:54 AM [Supernet Training] lr: 0.00044 epoch: 550/600, step: 521/521, train_loss: 0.055(0.061), train_acc: 97.500(97.732)
01/19 04:49:54 AM [Supernet Training] epoch: 550, train_loss: 0.061, train_acc: 97.732
01/19 04:49:58 AM [Supernet Validation] epoch: 550, val_loss: 0.466, val_acc: 89.520, best_acc: 89.900
01/19 04:49:58 AM 

01/19 04:49:58 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 001/521, train_loss: 0.075(0.075), train_acc: 95.833(95.833)
01/19 04:50:11 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 101/521, train_loss: 0.066(0.063), train_acc: 96.875(97.556)
01/19 04:50:24 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 201/521, train_loss: 0.075(0.062), train_acc: 97.917(97.715)
01/19 04:50:37 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 301/521, train_loss: 0.084(0.061), train_acc: 95.833(97.754)
01/19 04:50:49 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 401/521, train_loss: 0.029(0.063), train_acc: 98.958(97.722)
01/19 04:51:02 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 501/521, train_loss: 0.077(0.063), train_acc: 95.833(97.723)
01/19 04:51:05 AM [Supernet Training] lr: 0.00043 epoch: 551/600, step: 521/521, train_loss: 0.027(0.063), train_acc: 98.750(97.710)
01/19 04:51:05 AM [Supernet Training] epoch: 551, train_loss: 0.063, train_acc: 97.710
01/19 04:51:08 AM [Supernet Validation] epoch: 551, val_loss: 0.476, val_acc: 89.780, best_acc: 89.900
01/19 04:51:08 AM 

01/19 04:51:09 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 001/521, train_loss: 0.017(0.017), train_acc: 100.000(100.000)
01/19 04:51:21 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 101/521, train_loss: 0.070(0.056), train_acc: 97.917(98.020)
01/19 04:51:34 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 201/521, train_loss: 0.112(0.059), train_acc: 94.792(97.886)
01/19 04:51:47 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 301/521, train_loss: 0.085(0.061), train_acc: 95.833(97.816)
01/19 04:52:00 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 401/521, train_loss: 0.115(0.060), train_acc: 95.833(97.821)
01/19 04:52:13 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 501/521, train_loss: 0.013(0.060), train_acc: 100.000(97.850)
01/19 04:52:15 AM [Supernet Training] lr: 0.00041 epoch: 552/600, step: 521/521, train_loss: 0.146(0.060), train_acc: 96.250(97.842)
01/19 04:52:15 AM [Supernet Training] epoch: 552, train_loss: 0.060, train_acc: 97.842
01/19 04:52:19 AM [Supernet Validation] epoch: 552, val_loss: 0.465, val_acc: 89.750, best_acc: 89.900
01/19 04:52:19 AM 

01/19 04:52:19 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 001/521, train_loss: 0.096(0.096), train_acc: 96.875(96.875)
01/19 04:52:32 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 101/521, train_loss: 0.029(0.062), train_acc: 98.958(97.762)
01/19 04:52:45 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 201/521, train_loss: 0.072(0.061), train_acc: 97.917(97.870)
01/19 04:52:57 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 301/521, train_loss: 0.058(0.062), train_acc: 96.875(97.792)
01/19 04:53:10 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 401/521, train_loss: 0.130(0.063), train_acc: 95.833(97.743)
01/19 04:53:23 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 501/521, train_loss: 0.048(0.064), train_acc: 98.958(97.705)
01/19 04:53:26 AM [Supernet Training] lr: 0.00039 epoch: 553/600, step: 521/521, train_loss: 0.081(0.063), train_acc: 95.000(97.730)
01/19 04:53:26 AM [Supernet Training] epoch: 553, train_loss: 0.063, train_acc: 97.730
01/19 04:53:29 AM [Supernet Validation] epoch: 553, val_loss: 0.475, val_acc: 89.360, best_acc: 89.900
01/19 04:53:29 AM 

01/19 04:53:30 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 100.000(100.000)
01/19 04:53:42 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 101/521, train_loss: 0.054(0.061), train_acc: 96.875(97.844)
01/19 04:53:55 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 201/521, train_loss: 0.057(0.062), train_acc: 97.917(97.772)
01/19 04:54:08 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 301/521, train_loss: 0.068(0.062), train_acc: 98.958(97.761)
01/19 04:54:21 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 401/521, train_loss: 0.119(0.063), train_acc: 96.875(97.787)
01/19 04:54:34 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 501/521, train_loss: 0.037(0.064), train_acc: 97.917(97.742)
01/19 04:54:36 AM [Supernet Training] lr: 0.00038 epoch: 554/600, step: 521/521, train_loss: 0.065(0.064), train_acc: 97.500(97.720)
01/19 04:54:36 AM [Supernet Training] epoch: 554, train_loss: 0.064, train_acc: 97.720
01/19 04:54:40 AM [Supernet Validation] epoch: 554, val_loss: 0.460, val_acc: 89.560, best_acc: 89.900
01/19 04:54:40 AM 

01/19 04:54:40 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 001/521, train_loss: 0.092(0.092), train_acc: 96.875(96.875)
01/19 04:54:53 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 101/521, train_loss: 0.030(0.064), train_acc: 98.958(97.649)
01/19 04:55:06 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 201/521, train_loss: 0.016(0.060), train_acc: 100.000(97.854)
01/19 04:55:19 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 301/521, train_loss: 0.028(0.060), train_acc: 98.958(97.886)
01/19 04:55:31 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 401/521, train_loss: 0.102(0.060), train_acc: 95.833(97.870)
01/19 04:55:44 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 501/521, train_loss: 0.081(0.062), train_acc: 97.917(97.836)
01/19 04:55:47 AM [Supernet Training] lr: 0.00036 epoch: 555/600, step: 521/521, train_loss: 0.137(0.062), train_acc: 93.750(97.834)
01/19 04:55:47 AM [Supernet Training] epoch: 555, train_loss: 0.062, train_acc: 97.834
01/19 04:55:50 AM [Supernet Validation] epoch: 555, val_loss: 0.460, val_acc: 89.530, best_acc: 89.900
01/19 04:55:50 AM 

01/19 04:55:51 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 001/521, train_loss: 0.080(0.080), train_acc: 96.875(96.875)
01/19 04:56:04 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 101/521, train_loss: 0.041(0.066), train_acc: 98.958(97.741)
01/19 04:56:16 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 201/521, train_loss: 0.054(0.065), train_acc: 96.875(97.720)
01/19 04:56:29 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 301/521, train_loss: 0.017(0.063), train_acc: 100.000(97.757)
01/19 04:56:42 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 401/521, train_loss: 0.055(0.064), train_acc: 98.958(97.696)
01/19 04:56:55 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 501/521, train_loss: 0.049(0.065), train_acc: 97.917(97.669)
01/19 04:56:57 AM [Supernet Training] lr: 0.00035 epoch: 556/600, step: 521/521, train_loss: 0.042(0.065), train_acc: 98.750(97.658)
01/19 04:56:57 AM [Supernet Training] epoch: 556, train_loss: 0.065, train_acc: 97.658
01/19 04:57:01 AM [Supernet Validation] epoch: 556, val_loss: 0.472, val_acc: 89.380, best_acc: 89.900
01/19 04:57:01 AM 

01/19 04:57:01 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 001/521, train_loss: 0.086(0.086), train_acc: 95.833(95.833)
01/19 04:57:14 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 101/521, train_loss: 0.024(0.061), train_acc: 100.000(97.865)
01/19 04:57:27 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 201/521, train_loss: 0.017(0.062), train_acc: 100.000(97.746)
01/19 04:57:39 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 301/521, train_loss: 0.073(0.063), train_acc: 97.917(97.699)
01/19 04:57:52 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 401/521, train_loss: 0.057(0.062), train_acc: 98.958(97.750)
01/19 04:58:05 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 501/521, train_loss: 0.086(0.062), train_acc: 96.875(97.777)
01/19 04:58:07 AM [Supernet Training] lr: 0.00033 epoch: 557/600, step: 521/521, train_loss: 0.037(0.062), train_acc: 97.500(97.774)
01/19 04:58:08 AM [Supernet Training] epoch: 557, train_loss: 0.062, train_acc: 97.774
01/19 04:58:11 AM [Supernet Validation] epoch: 557, val_loss: 0.467, val_acc: 89.280, best_acc: 89.900
01/19 04:58:11 AM 

01/19 04:58:11 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 001/521, train_loss: 0.046(0.046), train_acc: 97.917(97.917)
01/19 04:58:24 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 101/521, train_loss: 0.104(0.059), train_acc: 95.833(98.061)
01/19 04:58:37 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 201/521, train_loss: 0.053(0.058), train_acc: 97.917(97.948)
01/19 04:58:50 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 301/521, train_loss: 0.056(0.058), train_acc: 97.917(97.941)
01/19 04:59:03 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 401/521, train_loss: 0.043(0.059), train_acc: 98.958(97.909)
01/19 04:59:15 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 501/521, train_loss: 0.042(0.060), train_acc: 98.958(97.863)
01/19 04:59:18 AM [Supernet Training] lr: 0.00032 epoch: 558/600, step: 521/521, train_loss: 0.017(0.060), train_acc: 100.000(97.874)
01/19 04:59:18 AM [Supernet Training] epoch: 558, train_loss: 0.060, train_acc: 97.874
01/19 04:59:22 AM [Supernet Validation] epoch: 558, val_loss: 0.481, val_acc: 89.370, best_acc: 89.900
01/19 04:59:22 AM 

01/19 04:59:22 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 001/521, train_loss: 0.034(0.034), train_acc: 98.958(98.958)
01/19 04:59:35 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 101/521, train_loss: 0.076(0.064), train_acc: 96.875(97.690)
01/19 04:59:47 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 201/521, train_loss: 0.076(0.063), train_acc: 95.833(97.756)
01/19 05:00:00 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 301/521, train_loss: 0.127(0.063), train_acc: 95.833(97.737)
01/19 05:00:13 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 401/521, train_loss: 0.079(0.062), train_acc: 98.958(97.815)
01/19 05:00:26 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 501/521, train_loss: 0.062(0.062), train_acc: 96.875(97.784)
01/19 05:00:28 AM [Supernet Training] lr: 0.00030 epoch: 559/600, step: 521/521, train_loss: 0.032(0.061), train_acc: 98.750(97.798)
01/19 05:00:29 AM [Supernet Training] epoch: 559, train_loss: 0.061, train_acc: 97.798
01/19 05:00:32 AM [Supernet Validation] epoch: 559, val_loss: 0.473, val_acc: 89.720, best_acc: 89.900
01/19 05:00:32 AM 

01/19 05:00:33 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 001/521, train_loss: 0.085(0.085), train_acc: 97.917(97.917)
01/19 05:00:45 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 101/521, train_loss: 0.070(0.057), train_acc: 96.875(97.865)
01/19 05:00:58 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 201/521, train_loss: 0.058(0.059), train_acc: 96.875(97.844)
01/19 05:01:11 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 301/521, train_loss: 0.072(0.060), train_acc: 98.958(97.806)
01/19 05:01:24 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 401/521, train_loss: 0.101(0.062), train_acc: 96.875(97.750)
01/19 05:01:36 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 501/521, train_loss: 0.068(0.062), train_acc: 97.917(97.723)
01/19 05:01:39 AM [Supernet Training] lr: 0.00029 epoch: 560/600, step: 521/521, train_loss: 0.104(0.063), train_acc: 96.250(97.684)
01/19 05:01:39 AM [Supernet Training] epoch: 560, train_loss: 0.063, train_acc: 97.684
01/19 05:01:43 AM Save best checkpoints to ./checkpoints/spos_c10_train_supernet_best.pth
01/19 05:01:43 AM [Supernet Validation] epoch: 560, val_loss: 0.453, val_acc: 89.970, best_acc: 89.970
01/19 05:01:43 AM 

01/19 05:01:43 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 98.958(98.958)
01/19 05:01:56 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 101/521, train_loss: 0.024(0.058), train_acc: 100.000(97.783)
01/19 05:02:09 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 201/521, train_loss: 0.145(0.063), train_acc: 95.833(97.699)
01/19 05:02:21 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 301/521, train_loss: 0.056(0.060), train_acc: 96.875(97.847)
01/19 05:02:34 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 401/521, train_loss: 0.028(0.061), train_acc: 98.958(97.784)
01/19 05:02:47 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 501/521, train_loss: 0.096(0.062), train_acc: 94.792(97.786)
01/19 05:02:50 AM [Supernet Training] lr: 0.00027 epoch: 561/600, step: 521/521, train_loss: 0.033(0.062), train_acc: 100.000(97.782)
01/19 05:02:50 AM [Supernet Training] epoch: 561, train_loss: 0.062, train_acc: 97.782
01/19 05:02:53 AM [Supernet Validation] epoch: 561, val_loss: 0.465, val_acc: 89.780, best_acc: 89.970
01/19 05:02:53 AM 

01/19 05:02:54 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 98.958(98.958)
01/19 05:03:06 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 101/521, train_loss: 0.048(0.057), train_acc: 98.958(97.844)
01/19 05:03:19 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 201/521, train_loss: 0.067(0.057), train_acc: 95.833(97.901)
01/19 05:03:32 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 301/521, train_loss: 0.084(0.058), train_acc: 96.875(97.892)
01/19 05:03:44 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 401/521, train_loss: 0.122(0.060), train_acc: 94.792(97.831)
01/19 05:03:57 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 501/521, train_loss: 0.034(0.060), train_acc: 100.000(97.821)
01/19 05:04:00 AM [Supernet Training] lr: 0.00026 epoch: 562/600, step: 521/521, train_loss: 0.047(0.060), train_acc: 98.750(97.830)
01/19 05:04:00 AM [Supernet Training] epoch: 562, train_loss: 0.060, train_acc: 97.830
01/19 05:04:04 AM [Supernet Validation] epoch: 562, val_loss: 0.465, val_acc: 89.440, best_acc: 89.970
01/19 05:04:04 AM 

01/19 05:04:04 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 001/521, train_loss: 0.123(0.123), train_acc: 95.833(95.833)
01/19 05:04:17 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 101/521, train_loss: 0.100(0.067), train_acc: 95.833(97.597)
01/19 05:04:29 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 201/521, train_loss: 0.028(0.064), train_acc: 100.000(97.689)
01/19 05:04:42 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 301/521, train_loss: 0.025(0.065), train_acc: 98.958(97.640)
01/19 05:04:55 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 401/521, train_loss: 0.062(0.064), train_acc: 96.875(97.639)
01/19 05:05:08 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 501/521, train_loss: 0.091(0.064), train_acc: 95.833(97.703)
01/19 05:05:10 AM [Supernet Training] lr: 0.00025 epoch: 563/600, step: 521/521, train_loss: 0.042(0.063), train_acc: 98.750(97.706)
01/19 05:05:10 AM [Supernet Training] epoch: 563, train_loss: 0.063, train_acc: 97.706
01/19 05:05:14 AM [Supernet Validation] epoch: 563, val_loss: 0.471, val_acc: 89.510, best_acc: 89.970
01/19 05:05:14 AM 

01/19 05:05:14 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 001/521, train_loss: 0.072(0.072), train_acc: 96.875(96.875)
01/19 05:05:27 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 101/521, train_loss: 0.037(0.062), train_acc: 98.958(97.710)
01/19 05:05:40 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 201/521, train_loss: 0.104(0.061), train_acc: 95.833(97.756)
01/19 05:05:53 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 301/521, train_loss: 0.029(0.061), train_acc: 98.958(97.820)
01/19 05:06:05 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 401/521, train_loss: 0.092(0.062), train_acc: 95.833(97.758)
01/19 05:06:18 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 501/521, train_loss: 0.094(0.062), train_acc: 96.875(97.736)
01/19 05:06:21 AM [Supernet Training] lr: 0.00023 epoch: 564/600, step: 521/521, train_loss: 0.082(0.062), train_acc: 97.500(97.744)
01/19 05:06:21 AM [Supernet Training] epoch: 564, train_loss: 0.062, train_acc: 97.744
01/19 05:06:25 AM [Supernet Validation] epoch: 564, val_loss: 0.455, val_acc: 89.510, best_acc: 89.970
01/19 05:06:25 AM 

01/19 05:06:25 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 001/521, train_loss: 0.070(0.070), train_acc: 96.875(96.875)
01/19 05:06:38 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 101/521, train_loss: 0.057(0.061), train_acc: 97.917(97.710)
01/19 05:06:50 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 201/521, train_loss: 0.057(0.060), train_acc: 97.917(97.854)
01/19 05:07:03 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 301/521, train_loss: 0.029(0.061), train_acc: 98.958(97.841)
01/19 05:07:16 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 401/521, train_loss: 0.032(0.061), train_acc: 98.958(97.808)
01/19 05:07:29 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 501/521, train_loss: 0.046(0.062), train_acc: 98.958(97.746)
01/19 05:07:31 AM [Supernet Training] lr: 0.00022 epoch: 565/600, step: 521/521, train_loss: 0.023(0.062), train_acc: 100.000(97.750)
01/19 05:07:31 AM [Supernet Training] epoch: 565, train_loss: 0.062, train_acc: 97.750
01/19 05:07:35 AM [Supernet Validation] epoch: 565, val_loss: 0.481, val_acc: 89.600, best_acc: 89.970
01/19 05:07:35 AM 

01/19 05:07:35 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 001/521, train_loss: 0.120(0.120), train_acc: 93.750(93.750)
01/19 05:07:48 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 101/521, train_loss: 0.050(0.055), train_acc: 97.917(97.948)
01/19 05:08:01 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 201/521, train_loss: 0.014(0.060), train_acc: 100.000(97.808)
01/19 05:08:14 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 301/521, train_loss: 0.196(0.060), train_acc: 95.833(97.809)
01/19 05:08:27 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 401/521, train_loss: 0.088(0.060), train_acc: 96.875(97.789)
01/19 05:08:39 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 501/521, train_loss: 0.048(0.060), train_acc: 97.917(97.779)
01/19 05:08:42 AM [Supernet Training] lr: 0.00021 epoch: 566/600, step: 521/521, train_loss: 0.140(0.060), train_acc: 95.000(97.792)
01/19 05:08:42 AM [Supernet Training] epoch: 566, train_loss: 0.060, train_acc: 97.792
01/19 05:08:46 AM [Supernet Validation] epoch: 566, val_loss: 0.476, val_acc: 89.150, best_acc: 89.970
01/19 05:08:46 AM 

01/19 05:08:46 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 001/521, train_loss: 0.017(0.017), train_acc: 100.000(100.000)
01/19 05:08:59 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 101/521, train_loss: 0.051(0.058), train_acc: 98.958(97.814)
01/19 05:09:12 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 201/521, train_loss: 0.100(0.061), train_acc: 97.917(97.797)
01/19 05:09:24 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 301/521, train_loss: 0.101(0.063), train_acc: 96.875(97.706)
01/19 05:09:37 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 401/521, train_loss: 0.023(0.062), train_acc: 98.958(97.748)
01/19 05:09:50 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 501/521, train_loss: 0.013(0.063), train_acc: 100.000(97.746)
01/19 05:09:53 AM [Supernet Training] lr: 0.00020 epoch: 567/600, step: 521/521, train_loss: 0.137(0.063), train_acc: 96.250(97.746)
01/19 05:09:53 AM [Supernet Training] epoch: 567, train_loss: 0.063, train_acc: 97.746
01/19 05:09:56 AM [Supernet Validation] epoch: 567, val_loss: 0.479, val_acc: 89.370, best_acc: 89.970
01/19 05:09:56 AM 

01/19 05:09:57 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 97.917(97.917)
01/19 05:10:09 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 101/521, train_loss: 0.042(0.069), train_acc: 98.958(97.463)
01/19 05:10:22 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 201/521, train_loss: 0.076(0.065), train_acc: 97.917(97.642)
01/19 05:10:35 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 301/521, train_loss: 0.017(0.065), train_acc: 100.000(97.654)
01/19 05:10:48 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 401/521, train_loss: 0.058(0.064), train_acc: 97.917(97.657)
01/19 05:11:01 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 501/521, train_loss: 0.050(0.062), train_acc: 97.917(97.709)
01/19 05:11:03 AM [Supernet Training] lr: 0.00019 epoch: 568/600, step: 521/521, train_loss: 0.015(0.062), train_acc: 100.000(97.724)
01/19 05:11:03 AM [Supernet Training] epoch: 568, train_loss: 0.062, train_acc: 97.724
01/19 05:11:07 AM [Supernet Validation] epoch: 568, val_loss: 0.458, val_acc: 89.570, best_acc: 89.970
01/19 05:11:07 AM 

01/19 05:11:07 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 001/521, train_loss: 0.116(0.116), train_acc: 95.833(95.833)
01/19 05:11:20 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 101/521, train_loss: 0.063(0.057), train_acc: 97.917(97.979)
01/19 05:11:33 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 201/521, train_loss: 0.142(0.059), train_acc: 95.833(97.860)
01/19 05:11:46 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 301/521, train_loss: 0.025(0.060), train_acc: 100.000(97.789)
01/19 05:11:58 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 401/521, train_loss: 0.044(0.061), train_acc: 98.958(97.756)
01/19 05:12:11 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 501/521, train_loss: 0.095(0.060), train_acc: 95.833(97.784)
01/19 05:12:14 AM [Supernet Training] lr: 0.00018 epoch: 569/600, step: 521/521, train_loss: 0.033(0.060), train_acc: 98.750(97.786)
01/19 05:12:14 AM [Supernet Training] epoch: 569, train_loss: 0.060, train_acc: 97.786
01/19 05:12:17 AM [Supernet Validation] epoch: 569, val_loss: 0.459, val_acc: 89.760, best_acc: 89.970
01/19 05:12:17 AM 

01/19 05:12:18 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 001/521, train_loss: 0.045(0.045), train_acc: 98.958(98.958)
01/19 05:12:31 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 101/521, train_loss: 0.060(0.060), train_acc: 97.917(97.741)
01/19 05:12:43 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 201/521, train_loss: 0.040(0.062), train_acc: 98.958(97.683)
01/19 05:12:56 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 301/521, train_loss: 0.056(0.061), train_acc: 98.958(97.754)
01/19 05:13:09 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 401/521, train_loss: 0.058(0.062), train_acc: 97.917(97.745)
01/19 05:13:22 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 501/521, train_loss: 0.033(0.061), train_acc: 98.958(97.790)
01/19 05:13:24 AM [Supernet Training] lr: 0.00016 epoch: 570/600, step: 521/521, train_loss: 0.040(0.061), train_acc: 98.750(97.782)
01/19 05:13:24 AM [Supernet Training] epoch: 570, train_loss: 0.061, train_acc: 97.782
01/19 05:13:28 AM [Supernet Validation] epoch: 570, val_loss: 0.471, val_acc: 89.550, best_acc: 89.970
01/19 05:13:28 AM 

01/19 05:13:28 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 001/521, train_loss: 0.035(0.035), train_acc: 98.958(98.958)
01/19 05:13:41 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 101/521, train_loss: 0.029(0.059), train_acc: 98.958(97.906)
01/19 05:13:54 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 201/521, train_loss: 0.105(0.060), train_acc: 96.875(97.870)
01/19 05:14:07 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 301/521, train_loss: 0.100(0.058), train_acc: 95.833(97.899)
01/19 05:14:20 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 401/521, train_loss: 0.065(0.058), train_acc: 95.833(97.878)
01/19 05:14:32 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 501/521, train_loss: 0.063(0.059), train_acc: 97.917(97.873)
01/19 05:14:35 AM [Supernet Training] lr: 0.00015 epoch: 571/600, step: 521/521, train_loss: 0.112(0.059), train_acc: 93.750(97.870)
01/19 05:14:35 AM [Supernet Training] epoch: 571, train_loss: 0.059, train_acc: 97.870
01/19 05:14:39 AM [Supernet Validation] epoch: 571, val_loss: 0.469, val_acc: 89.410, best_acc: 89.970
01/19 05:14:39 AM 

01/19 05:14:39 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 001/521, train_loss: 0.022(0.022), train_acc: 98.958(98.958)
01/19 05:14:52 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 101/521, train_loss: 0.042(0.064), train_acc: 98.958(97.638)
01/19 05:15:04 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 201/521, train_loss: 0.027(0.064), train_acc: 98.958(97.730)
01/19 05:15:17 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 301/521, train_loss: 0.141(0.062), train_acc: 95.833(97.799)
01/19 05:15:30 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 401/521, train_loss: 0.027(0.062), train_acc: 98.958(97.776)
01/19 05:15:43 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 501/521, train_loss: 0.013(0.062), train_acc: 100.000(97.777)
01/19 05:15:45 AM [Supernet Training] lr: 0.00014 epoch: 572/600, step: 521/521, train_loss: 0.026(0.062), train_acc: 98.750(97.768)
01/19 05:15:45 AM [Supernet Training] epoch: 572, train_loss: 0.062, train_acc: 97.768
01/19 05:15:49 AM [Supernet Validation] epoch: 572, val_loss: 0.455, val_acc: 89.530, best_acc: 89.970
01/19 05:15:49 AM 

01/19 05:15:49 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 001/521, train_loss: 0.007(0.007), train_acc: 100.000(100.000)
01/19 05:16:02 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 101/521, train_loss: 0.040(0.065), train_acc: 98.958(97.783)
01/19 05:16:15 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 201/521, train_loss: 0.080(0.062), train_acc: 96.875(97.761)
01/19 05:16:28 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 301/521, train_loss: 0.133(0.061), train_acc: 92.708(97.796)
01/19 05:16:41 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 401/521, train_loss: 0.104(0.062), train_acc: 96.875(97.787)
01/19 05:16:53 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 501/521, train_loss: 0.061(0.063), train_acc: 96.875(97.732)
01/19 05:16:56 AM [Supernet Training] lr: 0.00013 epoch: 573/600, step: 521/521, train_loss: 0.070(0.063), train_acc: 96.250(97.734)
01/19 05:16:56 AM [Supernet Training] epoch: 573, train_loss: 0.063, train_acc: 97.734
01/19 05:17:00 AM [Supernet Validation] epoch: 573, val_loss: 0.468, val_acc: 89.780, best_acc: 89.970
01/19 05:17:00 AM 

01/19 05:17:00 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 100.000(100.000)
01/19 05:17:13 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 101/521, train_loss: 0.145(0.062), train_acc: 93.750(97.741)
01/19 05:17:26 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 201/521, train_loss: 0.039(0.062), train_acc: 97.917(97.699)
01/19 05:17:38 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 301/521, train_loss: 0.119(0.063), train_acc: 95.833(97.685)
01/19 05:17:51 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 401/521, train_loss: 0.131(0.062), train_acc: 95.833(97.685)
01/19 05:18:04 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 501/521, train_loss: 0.010(0.063), train_acc: 100.000(97.684)
01/19 05:18:07 AM [Supernet Training] lr: 0.00012 epoch: 574/600, step: 521/521, train_loss: 0.022(0.063), train_acc: 98.750(97.678)
01/19 05:18:07 AM [Supernet Training] epoch: 574, train_loss: 0.063, train_acc: 97.678
01/19 05:18:10 AM [Supernet Validation] epoch: 574, val_loss: 0.454, val_acc: 89.860, best_acc: 89.970
01/19 05:18:10 AM 

01/19 05:18:11 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 001/521, train_loss: 0.065(0.065), train_acc: 96.875(96.875)
01/19 05:18:23 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 101/521, train_loss: 0.163(0.062), train_acc: 94.792(97.793)
01/19 05:18:36 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 201/521, train_loss: 0.031(0.066), train_acc: 100.000(97.647)
01/19 05:18:49 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 301/521, train_loss: 0.157(0.065), train_acc: 93.750(97.640)
01/19 05:19:02 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 401/521, train_loss: 0.025(0.064), train_acc: 98.958(97.649)
01/19 05:19:15 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 501/521, train_loss: 0.076(0.064), train_acc: 97.917(97.659)
01/19 05:19:17 AM [Supernet Training] lr: 0.00012 epoch: 575/600, step: 521/521, train_loss: 0.048(0.064), train_acc: 97.500(97.652)
01/19 05:19:17 AM [Supernet Training] epoch: 575, train_loss: 0.064, train_acc: 97.652
01/19 05:19:21 AM [Supernet Validation] epoch: 575, val_loss: 0.490, val_acc: 89.030, best_acc: 89.970
01/19 05:19:21 AM 

01/19 05:19:21 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 001/521, train_loss: 0.035(0.035), train_acc: 98.958(98.958)
01/19 05:19:34 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 101/521, train_loss: 0.022(0.061), train_acc: 98.958(97.587)
01/19 05:19:47 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 201/521, train_loss: 0.063(0.060), train_acc: 96.875(97.704)
01/19 05:19:59 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 301/521, train_loss: 0.121(0.060), train_acc: 95.833(97.723)
01/19 05:20:12 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 401/521, train_loss: 0.060(0.061), train_acc: 97.917(97.714)
01/19 05:20:25 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 501/521, train_loss: 0.054(0.062), train_acc: 97.917(97.690)
01/19 05:20:28 AM [Supernet Training] lr: 0.00011 epoch: 576/600, step: 521/521, train_loss: 0.016(0.062), train_acc: 100.000(97.686)
01/19 05:20:28 AM [Supernet Training] epoch: 576, train_loss: 0.062, train_acc: 97.686
01/19 05:20:31 AM [Supernet Validation] epoch: 576, val_loss: 0.473, val_acc: 89.460, best_acc: 89.970
01/19 05:20:31 AM 

01/19 05:20:32 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 001/521, train_loss: 0.132(0.132), train_acc: 97.917(97.917)
01/19 05:20:44 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 101/521, train_loss: 0.045(0.056), train_acc: 97.917(98.020)
01/19 05:20:57 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 201/521, train_loss: 0.022(0.057), train_acc: 98.958(97.989)
01/19 05:21:10 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 301/521, train_loss: 0.064(0.059), train_acc: 96.875(97.903)
01/19 05:21:23 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 401/521, train_loss: 0.043(0.061), train_acc: 97.917(97.810)
01/19 05:21:36 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 501/521, train_loss: 0.022(0.061), train_acc: 100.000(97.815)
01/19 05:21:38 AM [Supernet Training] lr: 0.00010 epoch: 577/600, step: 521/521, train_loss: 0.156(0.061), train_acc: 95.000(97.806)
01/19 05:21:38 AM [Supernet Training] epoch: 577, train_loss: 0.061, train_acc: 97.806
01/19 05:21:42 AM [Supernet Validation] epoch: 577, val_loss: 0.455, val_acc: 89.620, best_acc: 89.970
01/19 05:21:42 AM 

01/19 05:21:42 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 001/521, train_loss: 0.070(0.070), train_acc: 96.875(96.875)
01/19 05:21:55 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 101/521, train_loss: 0.083(0.059), train_acc: 96.875(97.937)
01/19 05:22:08 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 201/521, train_loss: 0.055(0.063), train_acc: 97.917(97.730)
01/19 05:22:21 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 301/521, train_loss: 0.056(0.062), train_acc: 97.917(97.716)
01/19 05:22:33 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 401/521, train_loss: 0.052(0.061), train_acc: 97.917(97.776)
01/19 05:22:46 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 501/521, train_loss: 0.064(0.061), train_acc: 97.917(97.779)
01/19 05:22:49 AM [Supernet Training] lr: 0.00009 epoch: 578/600, step: 521/521, train_loss: 0.078(0.061), train_acc: 98.750(97.762)
01/19 05:22:49 AM [Supernet Training] epoch: 578, train_loss: 0.061, train_acc: 97.762
01/19 05:22:52 AM [Supernet Validation] epoch: 578, val_loss: 0.464, val_acc: 89.720, best_acc: 89.970
01/19 05:22:52 AM 

01/19 05:22:53 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 96.875(96.875)
01/19 05:23:05 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 101/521, train_loss: 0.032(0.059), train_acc: 98.958(98.051)
01/19 05:23:18 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 201/521, train_loss: 0.053(0.058), train_acc: 98.958(97.979)
01/19 05:23:31 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 301/521, train_loss: 0.083(0.060), train_acc: 95.833(97.886)
01/19 05:23:44 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 401/521, train_loss: 0.046(0.061), train_acc: 97.917(97.862)
01/19 05:23:57 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 501/521, train_loss: 0.027(0.061), train_acc: 98.958(97.829)
01/19 05:23:59 AM [Supernet Training] lr: 0.00008 epoch: 579/600, step: 521/521, train_loss: 0.118(0.061), train_acc: 92.500(97.832)
01/19 05:23:59 AM [Supernet Training] epoch: 579, train_loss: 0.061, train_acc: 97.832
01/19 05:24:03 AM [Supernet Validation] epoch: 579, val_loss: 0.461, val_acc: 89.600, best_acc: 89.970
01/19 05:24:03 AM 

01/19 05:24:03 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 001/521, train_loss: 0.073(0.073), train_acc: 97.917(97.917)
01/19 05:24:16 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 101/521, train_loss: 0.097(0.057), train_acc: 96.875(97.886)
01/19 05:24:29 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 201/521, train_loss: 0.074(0.060), train_acc: 96.875(97.772)
01/19 05:24:42 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 301/521, train_loss: 0.067(0.061), train_acc: 97.917(97.834)
01/19 05:24:54 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 401/521, train_loss: 0.075(0.062), train_acc: 97.917(97.771)
01/19 05:25:07 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 501/521, train_loss: 0.154(0.063), train_acc: 94.792(97.790)
01/19 05:25:10 AM [Supernet Training] lr: 0.00008 epoch: 580/600, step: 521/521, train_loss: 0.018(0.063), train_acc: 100.000(97.796)
01/19 05:25:10 AM [Supernet Training] epoch: 580, train_loss: 0.063, train_acc: 97.796
01/19 05:25:13 AM [Supernet Validation] epoch: 580, val_loss: 0.469, val_acc: 89.410, best_acc: 89.970
01/19 05:25:13 AM 

01/19 05:25:14 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 100.000(100.000)
01/19 05:25:26 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 101/521, train_loss: 0.052(0.063), train_acc: 98.958(97.772)
01/19 05:25:39 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 201/521, train_loss: 0.112(0.059), train_acc: 94.792(97.953)
01/19 05:25:52 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 301/521, train_loss: 0.061(0.060), train_acc: 96.875(97.896)
01/19 05:26:05 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 401/521, train_loss: 0.043(0.061), train_acc: 100.000(97.823)
01/19 05:26:18 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 501/521, train_loss: 0.126(0.062), train_acc: 93.750(97.784)
01/19 05:26:20 AM [Supernet Training] lr: 0.00007 epoch: 581/600, step: 521/521, train_loss: 0.018(0.061), train_acc: 100.000(97.800)
01/19 05:26:20 AM [Supernet Training] epoch: 581, train_loss: 0.061, train_acc: 97.800
01/19 05:26:24 AM [Supernet Validation] epoch: 581, val_loss: 0.467, val_acc: 89.790, best_acc: 89.970
01/19 05:26:24 AM 

01/19 05:26:24 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 001/521, train_loss: 0.055(0.055), train_acc: 98.958(98.958)
01/19 05:26:37 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 101/521, train_loss: 0.036(0.064), train_acc: 98.958(97.731)
01/19 05:26:50 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 201/521, train_loss: 0.049(0.060), train_acc: 97.917(97.808)
01/19 05:27:02 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 301/521, train_loss: 0.052(0.059), train_acc: 96.875(97.886)
01/19 05:27:15 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 401/521, train_loss: 0.066(0.058), train_acc: 96.875(97.893)
01/19 05:27:28 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 501/521, train_loss: 0.059(0.058), train_acc: 96.875(97.881)
01/19 05:27:31 AM [Supernet Training] lr: 0.00006 epoch: 582/600, step: 521/521, train_loss: 0.035(0.058), train_acc: 98.750(97.884)
01/19 05:27:31 AM [Supernet Training] epoch: 582, train_loss: 0.058, train_acc: 97.884
01/19 05:27:34 AM [Supernet Validation] epoch: 582, val_loss: 0.473, val_acc: 89.390, best_acc: 89.970
01/19 05:27:34 AM 

01/19 05:27:35 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 001/521, train_loss: 0.050(0.050), train_acc: 98.958(98.958)
01/19 05:27:48 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 101/521, train_loss: 0.045(0.065), train_acc: 98.958(97.514)
01/19 05:28:00 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 201/521, train_loss: 0.039(0.065), train_acc: 97.917(97.595)
01/19 05:28:13 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 301/521, train_loss: 0.046(0.064), train_acc: 97.917(97.609)
01/19 05:28:26 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 401/521, train_loss: 0.086(0.062), train_acc: 95.833(97.706)
01/19 05:28:39 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 501/521, train_loss: 0.030(0.061), train_acc: 97.917(97.769)
01/19 05:28:41 AM [Supernet Training] lr: 0.00006 epoch: 583/600, step: 521/521, train_loss: 0.073(0.061), train_acc: 96.250(97.772)
01/19 05:28:41 AM [Supernet Training] epoch: 583, train_loss: 0.061, train_acc: 97.772
01/19 05:28:45 AM [Supernet Validation] epoch: 583, val_loss: 0.470, val_acc: 89.470, best_acc: 89.970
01/19 05:28:45 AM 

01/19 05:28:45 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 001/521, train_loss: 0.056(0.056), train_acc: 98.958(98.958)
01/19 05:28:58 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 101/521, train_loss: 0.086(0.065), train_acc: 95.833(97.628)
01/19 05:29:11 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 201/521, train_loss: 0.042(0.064), train_acc: 97.917(97.621)
01/19 05:29:24 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 301/521, train_loss: 0.063(0.064), train_acc: 97.917(97.667)
01/19 05:29:37 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 401/521, train_loss: 0.076(0.062), train_acc: 97.917(97.737)
01/19 05:29:50 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 501/521, train_loss: 0.121(0.062), train_acc: 94.792(97.738)
01/19 05:29:52 AM [Supernet Training] lr: 0.00005 epoch: 584/600, step: 521/521, train_loss: 0.111(0.062), train_acc: 93.750(97.730)
01/19 05:29:52 AM [Supernet Training] epoch: 584, train_loss: 0.062, train_acc: 97.730
01/19 05:29:56 AM [Supernet Validation] epoch: 584, val_loss: 0.476, val_acc: 89.440, best_acc: 89.970
01/19 05:29:56 AM 

01/19 05:29:56 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 001/521, train_loss: 0.139(0.139), train_acc: 94.792(94.792)
01/19 05:30:09 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 101/521, train_loss: 0.031(0.058), train_acc: 98.958(98.051)
01/19 05:30:22 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 201/521, train_loss: 0.040(0.059), train_acc: 98.958(97.958)
01/19 05:30:35 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 301/521, train_loss: 0.028(0.060), train_acc: 98.958(97.958)
01/19 05:30:47 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 401/521, train_loss: 0.101(0.059), train_acc: 96.875(97.956)
01/19 05:31:00 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 501/521, train_loss: 0.037(0.060), train_acc: 98.958(97.925)
01/19 05:31:03 AM [Supernet Training] lr: 0.00004 epoch: 585/600, step: 521/521, train_loss: 0.043(0.060), train_acc: 98.750(97.912)
01/19 05:31:03 AM [Supernet Training] epoch: 585, train_loss: 0.060, train_acc: 97.912
01/19 05:31:06 AM [Supernet Validation] epoch: 585, val_loss: 0.454, val_acc: 89.620, best_acc: 89.970
01/19 05:31:06 AM 

01/19 05:31:07 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 001/521, train_loss: 0.075(0.075), train_acc: 97.917(97.917)
01/19 05:31:20 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 101/521, train_loss: 0.044(0.063), train_acc: 98.958(97.762)
01/19 05:31:32 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 201/521, train_loss: 0.025(0.060), train_acc: 100.000(97.787)
01/19 05:31:45 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 301/521, train_loss: 0.027(0.061), train_acc: 98.958(97.782)
01/19 05:31:58 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 401/521, train_loss: 0.042(0.061), train_acc: 98.958(97.771)
01/19 05:32:11 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 501/521, train_loss: 0.028(0.062), train_acc: 98.958(97.754)
01/19 05:32:13 AM [Supernet Training] lr: 0.00004 epoch: 586/600, step: 521/521, train_loss: 0.050(0.062), train_acc: 97.500(97.762)
01/19 05:32:13 AM [Supernet Training] epoch: 586, train_loss: 0.062, train_acc: 97.762
01/19 05:32:17 AM [Supernet Validation] epoch: 586, val_loss: 0.480, val_acc: 89.470, best_acc: 89.970
01/19 05:32:17 AM 

01/19 05:32:17 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 001/521, train_loss: 0.023(0.023), train_acc: 98.958(98.958)
01/19 05:32:30 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 101/521, train_loss: 0.034(0.062), train_acc: 98.958(97.783)
01/19 05:32:43 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 201/521, train_loss: 0.131(0.063), train_acc: 96.875(97.751)
01/19 05:32:56 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 301/521, train_loss: 0.039(0.062), train_acc: 97.917(97.771)
01/19 05:33:08 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 401/521, train_loss: 0.039(0.063), train_acc: 98.958(97.763)
01/19 05:33:21 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 501/521, train_loss: 0.107(0.063), train_acc: 93.750(97.754)
01/19 05:33:24 AM [Supernet Training] lr: 0.00003 epoch: 587/600, step: 521/521, train_loss: 0.071(0.063), train_acc: 95.000(97.756)
01/19 05:33:24 AM [Supernet Training] epoch: 587, train_loss: 0.063, train_acc: 97.756
01/19 05:33:27 AM [Supernet Validation] epoch: 587, val_loss: 0.456, val_acc: 89.820, best_acc: 89.970
01/19 05:33:27 AM 

01/19 05:33:28 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 98.958(98.958)
01/19 05:33:41 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 101/521, train_loss: 0.061(0.064), train_acc: 97.917(97.638)
01/19 05:33:53 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 201/521, train_loss: 0.031(0.062), train_acc: 98.958(97.694)
01/19 05:34:06 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 301/521, train_loss: 0.019(0.061), train_acc: 100.000(97.733)
01/19 05:34:19 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 401/521, train_loss: 0.127(0.062), train_acc: 94.792(97.698)
01/19 05:34:32 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 501/521, train_loss: 0.016(0.062), train_acc: 98.958(97.721)
01/19 05:34:34 AM [Supernet Training] lr: 0.00003 epoch: 588/600, step: 521/521, train_loss: 0.018(0.061), train_acc: 100.000(97.722)
01/19 05:34:34 AM [Supernet Training] epoch: 588, train_loss: 0.061, train_acc: 97.722
01/19 05:34:38 AM [Supernet Validation] epoch: 588, val_loss: 0.467, val_acc: 89.820, best_acc: 89.970
01/19 05:34:38 AM 

01/19 05:34:38 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 001/521, train_loss: 0.025(0.025), train_acc: 98.958(98.958)
01/19 05:34:51 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 101/521, train_loss: 0.098(0.058), train_acc: 95.833(97.917)
01/19 05:35:04 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 201/521, train_loss: 0.067(0.060), train_acc: 96.875(97.725)
01/19 05:35:17 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 301/521, train_loss: 0.116(0.061), train_acc: 95.833(97.726)
01/19 05:35:29 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 401/521, train_loss: 0.059(0.062), train_acc: 98.958(97.704)
01/19 05:35:42 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 501/521, train_loss: 0.012(0.063), train_acc: 100.000(97.659)
01/19 05:35:45 AM [Supernet Training] lr: 0.00002 epoch: 589/600, step: 521/521, train_loss: 0.178(0.064), train_acc: 95.000(97.644)
01/19 05:35:45 AM [Supernet Training] epoch: 589, train_loss: 0.064, train_acc: 97.644
01/19 05:35:48 AM [Supernet Validation] epoch: 589, val_loss: 0.474, val_acc: 89.600, best_acc: 89.970
01/19 05:35:48 AM 

01/19 05:35:49 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 001/521, train_loss: 0.036(0.036), train_acc: 98.958(98.958)
01/19 05:36:02 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 101/521, train_loss: 0.064(0.065), train_acc: 97.917(97.659)
01/19 05:36:14 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 201/521, train_loss: 0.055(0.063), train_acc: 96.875(97.673)
01/19 05:36:27 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 301/521, train_loss: 0.014(0.064), train_acc: 100.000(97.685)
01/19 05:36:40 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 401/521, train_loss: 0.070(0.063), train_acc: 97.917(97.769)
01/19 05:36:53 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 501/521, train_loss: 0.083(0.063), train_acc: 96.875(97.754)
01/19 05:36:55 AM [Supernet Training] lr: 0.00002 epoch: 590/600, step: 521/521, train_loss: 0.031(0.063), train_acc: 100.000(97.740)
01/19 05:36:55 AM [Supernet Training] epoch: 590, train_loss: 0.063, train_acc: 97.740
01/19 05:36:59 AM [Supernet Validation] epoch: 590, val_loss: 0.471, val_acc: 89.700, best_acc: 89.970
01/19 05:36:59 AM 

01/19 05:36:59 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 001/521, train_loss: 0.073(0.073), train_acc: 96.875(96.875)
01/19 05:37:12 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 101/521, train_loss: 0.031(0.060), train_acc: 98.958(97.948)
01/19 05:37:25 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 201/521, train_loss: 0.043(0.059), train_acc: 97.917(97.943)
01/19 05:37:38 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 301/521, train_loss: 0.011(0.059), train_acc: 100.000(97.910)
01/19 05:37:51 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 401/521, train_loss: 0.106(0.059), train_acc: 96.875(97.958)
01/19 05:38:03 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 501/521, train_loss: 0.061(0.059), train_acc: 97.917(97.929)
01/19 05:38:06 AM [Supernet Training] lr: 0.00002 epoch: 591/600, step: 521/521, train_loss: 0.059(0.059), train_acc: 97.500(97.940)
01/19 05:38:06 AM [Supernet Training] epoch: 591, train_loss: 0.059, train_acc: 97.940
01/19 05:38:10 AM [Supernet Validation] epoch: 591, val_loss: 0.470, val_acc: 89.400, best_acc: 89.970
01/19 05:38:10 AM 

01/19 05:38:10 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 001/521, train_loss: 0.105(0.105), train_acc: 97.917(97.917)
01/19 05:38:23 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 101/521, train_loss: 0.011(0.061), train_acc: 100.000(97.886)
01/19 05:38:36 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 201/521, train_loss: 0.083(0.062), train_acc: 97.917(97.834)
01/19 05:38:48 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 301/521, train_loss: 0.084(0.062), train_acc: 98.958(97.830)
01/19 05:39:01 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 401/521, train_loss: 0.031(0.061), train_acc: 98.958(97.849)
01/19 05:39:14 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 501/521, train_loss: 0.113(0.062), train_acc: 95.833(97.833)
01/19 05:39:17 AM [Supernet Training] lr: 0.00001 epoch: 592/600, step: 521/521, train_loss: 0.017(0.061), train_acc: 98.750(97.826)
01/19 05:39:17 AM [Supernet Training] epoch: 592, train_loss: 0.061, train_acc: 97.826
01/19 05:39:20 AM [Supernet Validation] epoch: 592, val_loss: 0.477, val_acc: 89.650, best_acc: 89.970
01/19 05:39:20 AM 

01/19 05:39:21 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 001/521, train_loss: 0.064(0.064), train_acc: 95.833(95.833)
01/19 05:39:33 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 101/521, train_loss: 0.069(0.062), train_acc: 97.917(97.793)
01/19 05:39:46 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 201/521, train_loss: 0.093(0.061), train_acc: 95.833(97.844)
01/19 05:39:59 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 301/521, train_loss: 0.111(0.060), train_acc: 95.833(97.830)
01/19 05:40:12 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 401/521, train_loss: 0.076(0.061), train_acc: 96.875(97.771)
01/19 05:40:25 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 501/521, train_loss: 0.033(0.062), train_acc: 98.958(97.732)
01/19 05:40:27 AM [Supernet Training] lr: 0.00001 epoch: 593/600, step: 521/521, train_loss: 0.035(0.062), train_acc: 98.750(97.726)
01/19 05:40:27 AM [Supernet Training] epoch: 593, train_loss: 0.062, train_acc: 97.726
01/19 05:40:31 AM [Supernet Validation] epoch: 593, val_loss: 0.478, val_acc: 89.470, best_acc: 89.970
01/19 05:40:31 AM 

01/19 05:40:31 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 001/521, train_loss: 0.085(0.085), train_acc: 96.875(96.875)
01/19 05:40:44 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 101/521, train_loss: 0.075(0.059), train_acc: 96.875(97.814)
01/19 05:40:57 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 201/521, train_loss: 0.031(0.058), train_acc: 98.958(97.823)
01/19 05:41:10 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 301/521, train_loss: 0.121(0.059), train_acc: 94.792(97.820)
01/19 05:41:23 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 401/521, train_loss: 0.117(0.062), train_acc: 94.792(97.675)
01/19 05:41:36 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 501/521, train_loss: 0.050(0.062), train_acc: 97.917(97.717)
01/19 05:41:38 AM [Supernet Training] lr: 0.00001 epoch: 594/600, step: 521/521, train_loss: 0.030(0.062), train_acc: 100.000(97.718)
01/19 05:41:38 AM [Supernet Training] epoch: 594, train_loss: 0.062, train_acc: 97.718
01/19 05:41:42 AM [Supernet Validation] epoch: 594, val_loss: 0.462, val_acc: 89.540, best_acc: 89.970
01/19 05:41:42 AM 

01/19 05:41:42 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 001/521, train_loss: 0.077(0.077), train_acc: 96.875(96.875)
01/19 05:41:55 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 101/521, train_loss: 0.059(0.060), train_acc: 97.917(97.948)
01/19 05:42:08 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 201/521, train_loss: 0.017(0.060), train_acc: 100.000(97.917)
01/19 05:42:20 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 301/521, train_loss: 0.074(0.062), train_acc: 96.875(97.854)
01/19 05:42:33 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 401/521, train_loss: 0.039(0.061), train_acc: 97.917(97.867)
01/19 05:42:46 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 501/521, train_loss: 0.088(0.061), train_acc: 94.792(97.846)
01/19 05:42:49 AM [Supernet Training] lr: 0.00001 epoch: 595/600, step: 521/521, train_loss: 0.035(0.061), train_acc: 98.750(97.850)
01/19 05:42:49 AM [Supernet Training] epoch: 595, train_loss: 0.061, train_acc: 97.850
01/19 05:42:52 AM [Supernet Validation] epoch: 595, val_loss: 0.466, val_acc: 89.590, best_acc: 89.970
01/19 05:42:52 AM 

01/19 05:42:53 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 001/521, train_loss: 0.065(0.065), train_acc: 97.917(97.917)
01/19 05:43:05 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 101/521, train_loss: 0.054(0.056), train_acc: 97.917(97.989)
01/19 05:43:18 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 201/521, train_loss: 0.038(0.061), train_acc: 98.958(97.839)
01/19 05:43:31 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 301/521, train_loss: 0.129(0.062), train_acc: 94.792(97.837)
01/19 05:43:44 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 401/521, train_loss: 0.033(0.063), train_acc: 98.958(97.805)
01/19 05:43:57 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 501/521, train_loss: 0.061(0.063), train_acc: 96.875(97.794)
01/19 05:43:59 AM [Supernet Training] lr: 0.00000 epoch: 596/600, step: 521/521, train_loss: 0.127(0.063), train_acc: 95.000(97.796)
01/19 05:43:59 AM [Supernet Training] epoch: 596, train_loss: 0.063, train_acc: 97.796
01/19 05:44:03 AM [Supernet Validation] epoch: 596, val_loss: 0.466, val_acc: 89.620, best_acc: 89.970
01/19 05:44:03 AM 

01/19 05:44:03 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 001/521, train_loss: 0.109(0.109), train_acc: 94.792(94.792)
01/19 05:44:16 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 101/521, train_loss: 0.034(0.064), train_acc: 98.958(97.710)
01/19 05:44:29 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 201/521, train_loss: 0.064(0.065), train_acc: 96.875(97.621)
01/19 05:44:42 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 301/521, train_loss: 0.108(0.065), train_acc: 94.792(97.636)
01/19 05:44:54 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 401/521, train_loss: 0.157(0.063), train_acc: 93.750(97.706)
01/19 05:45:07 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 501/521, train_loss: 0.040(0.063), train_acc: 98.958(97.736)
01/19 05:45:10 AM [Supernet Training] lr: 0.00000 epoch: 597/600, step: 521/521, train_loss: 0.013(0.063), train_acc: 100.000(97.750)
01/19 05:45:10 AM [Supernet Training] epoch: 597, train_loss: 0.063, train_acc: 97.750
01/19 05:45:13 AM [Supernet Validation] epoch: 597, val_loss: 0.483, val_acc: 89.630, best_acc: 89.970
01/19 05:45:13 AM 

01/19 05:45:14 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 001/521, train_loss: 0.041(0.041), train_acc: 97.917(97.917)
01/19 05:45:26 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 101/521, train_loss: 0.075(0.064), train_acc: 97.917(97.618)
01/19 05:45:39 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 201/521, train_loss: 0.058(0.065), train_acc: 97.917(97.585)
01/19 05:45:52 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 301/521, train_loss: 0.085(0.062), train_acc: 96.875(97.719)
01/19 05:46:05 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 401/521, train_loss: 0.022(0.062), train_acc: 100.000(97.709)
01/19 05:46:18 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 501/521, train_loss: 0.035(0.061), train_acc: 98.958(97.761)
01/19 05:46:20 AM [Supernet Training] lr: 0.00000 epoch: 598/600, step: 521/521, train_loss: 0.011(0.061), train_acc: 100.000(97.772)
01/19 05:46:20 AM [Supernet Training] epoch: 598, train_loss: 0.061, train_acc: 97.772
01/19 05:46:24 AM [Supernet Validation] epoch: 598, val_loss: 0.478, val_acc: 89.530, best_acc: 89.970
01/19 05:46:24 AM 

01/19 05:46:24 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 001/521, train_loss: 0.072(0.072), train_acc: 97.917(97.917)
01/19 05:46:37 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 101/521, train_loss: 0.026(0.058), train_acc: 100.000(97.948)
01/19 05:46:49 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 201/521, train_loss: 0.065(0.061), train_acc: 95.833(97.818)
01/19 05:47:02 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 301/521, train_loss: 0.167(0.062), train_acc: 95.833(97.778)
01/19 05:47:15 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 401/521, train_loss: 0.196(0.063), train_acc: 92.708(97.776)
01/19 05:47:28 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 501/521, train_loss: 0.094(0.061), train_acc: 95.833(97.846)
01/19 05:47:30 AM [Supernet Training] lr: 0.00000 epoch: 599/600, step: 521/521, train_loss: 0.084(0.061), train_acc: 97.500(97.852)
01/19 05:47:30 AM [Supernet Training] epoch: 599, train_loss: 0.061, train_acc: 97.852
01/19 05:47:34 AM [Supernet Validation] epoch: 599, val_loss: 0.502, val_acc: 89.110, best_acc: 89.970
01/19 05:47:34 AM 

01/19 05:47:34 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 001/521, train_loss: 0.035(0.035), train_acc: 98.958(98.958)
01/19 05:47:47 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 101/521, train_loss: 0.056(0.063), train_acc: 98.958(97.618)
01/19 05:48:00 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 201/521, train_loss: 0.041(0.061), train_acc: 98.958(97.766)
01/19 05:48:13 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 301/521, train_loss: 0.063(0.062), train_acc: 96.875(97.723)
01/19 05:48:26 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 401/521, train_loss: 0.037(0.061), train_acc: 97.917(97.795)
01/19 05:48:38 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 501/521, train_loss: 0.068(0.060), train_acc: 97.917(97.784)
01/19 05:48:41 AM [Supernet Training] lr: 0.00000 epoch: 600/600, step: 521/521, train_loss: 0.104(0.060), train_acc: 96.250(97.798)
01/19 05:48:41 AM [Supernet Training] epoch: 600, train_loss: 0.060, train_acc: 97.798
01/19 05:48:44 AM [Supernet Validation] epoch: 600, val_loss: 0.469, val_acc: 89.900, best_acc: 89.970
01/19 05:48:44 AM 

01/19 05:48:44 AM Elapsed time: 11h 45min 16s
